Problems:
1. Cropping section has problem. solve it later.

2. (SOLVED) these are my internal error system
Method suggest_optuna_n_trials not found in OptunaOptimizer.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
Method suggest_kernel_size_range not found in OptunaOptimizer.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.
suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.

3. GRADIENT EXPLOSION IN THE NETWORK USING SPECIFIC PARAMETERS. (Probably solved)
Learning Rate: A learning rate that is too high can cause gradients to explode. During backpropagation, 
large updates are made to the weights, leading to instability in the training process.

Weight Initialization: Poor weight initialization can also contribute to gradient explosion. If weights are 
initialized with too large values, gradients during backpropagation can grow exponentially.

Network Architecture: Certain network architectures, especially deep ones with many layers, are prone to gradient 
explosion. This is particularly true if the network lacks mechanisms like skip connections (as seen in ResNet) or normalization layers (like batch normalization).
(One reason can be large kernel size with smal filter like filter 8 and kernel size 7)
Activation Functions: Activation functions like ReLU can lead to gradient explosion if not handled properly. 
For instance, ReLU does not bound its output, which can result in large gradients.

Loss Function: The choice of the loss function and its behavior with respect to the data can also lead to exploding gradients.
 Loss functions that produce large gradients for certain input values can cause instability.

4. Overfitting

5. Result image is saved only for the last experiment

6. Some of the loops breaks before complete convergence.

7. Does NOT save the correct version for seventh iteration

8. Check why the early stop does not turn the model off if it does not anyyy progress from the first epoch
   to the last one.

9. Early stop performs poorly when we have an increase in the loss function in a cosine pattern. 

Points:
1. Large LRs can work perfectly for this proble, BUT the problem is that with OPTUNA random choices, 
that may make unnormal choices of the number of layers and kernel size, it can lead to gradient explosion! like 
lr=0.002
kernel_size_1=7
filter_1=8
So, for the first 20 totally random optuna choices, one should be quite careful about the errors.