2024-06-20 23:03:00,445 - INFO: Device: cuda.
2024-06-20 23:03:00,445 - INFO: Torch version: 2.0.1+cu117.
2024-06-20 23:03:00,445 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-20 23:03:00,445 - INFO: Model name: Dual_DCNN_LReLu
2024-06-20 23:03:00,445 - INFO: Seed: 4
2024-06-20 23:03:00,445 - INFO: 42 patients have been found in the data directory.
2024-06-20 23:03:00,485 - INFO: Train set contains 32 patients.
2024-06-20 23:03:00,485 - INFO: Val set contains 5 patients.
2024-06-20 23:03:00,485 - INFO: Test set contains 5 patients.
2024-06-20 23:03:00,485 - INFO: Fold: 0
2024-06-20 23:03:00,486 - INFO: Performing 2-fold Cross Validation.
2024-06-20 23:03:00,487 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-20 23:03:00,487 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-20 23:03:00,487 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-20 23:03:00,617 - INFO: To_device: False.
2024-06-20 23:03:00,619 - INFO: Transformers have been made successfully.
2024-06-20 23:03:00,619 - INFO: Dataset type: cache.
2024-06-20 23:03:00,619 - INFO: Dataloader type: standard.
2024-06-20 23:04:51,020 - INFO: Train dataloader arguments.
2024-06-20 23:04:51,020 - INFO: 	Batch_size: 32.
2024-06-20 23:04:51,020 - INFO: 	Shuffle: True.
2024-06-20 23:04:51,020 - INFO: 	Sampler: None.
2024-06-20 23:04:51,020 - INFO: 	Num_workers: 4.
2024-06-20 23:04:51,020 - INFO: 	Drop_last: False.
2024-06-20 23:04:51,070 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-20 23:04:51,915 - INFO: Weight init name: kaiming_uniform.
2024-06-20 23:04:54,331 - INFO: Number of training iterations per epoch: 29.
2024-06-20 23:04:54,332 - INFO: Epoch 1/200...
2024-06-20 23:04:54,332 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:04:54,332 - INFO: Batch size: 32.
2024-06-20 23:04:54,332 - INFO: Dataset:
2024-06-20 23:04:54,332 - INFO: Batch size:
2024-06-20 23:04:54,332 - INFO: Number of workers:
2024-06-20 23:04:57,249 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 70.996
2024-06-20 23:04:57,554 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 83.105
2024-06-20 23:04:57,944 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 19153.533
2024-06-20 23:04:58,250 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 5204.818
2024-06-20 23:04:58,675 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 79394.266
2024-06-20 23:04:58,981 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 110632.680
2024-06-20 23:04:59,369 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 76685.234
2024-06-20 23:04:59,687 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 128506.250
2024-06-20 23:05:00,108 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 115896.461
2024-06-20 23:05:00,407 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 14401.586
2024-06-20 23:05:00,795 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 3606.475
2024-06-20 23:05:01,113 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 3007.651
2024-06-20 23:05:01,557 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 1151.817
2024-06-20 23:05:01,861 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 433.926
2024-06-20 23:05:02,260 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 271.152
2024-06-20 23:05:02,576 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 202.061
2024-06-20 23:05:03,034 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 171.597
2024-06-20 23:05:03,337 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 151.734
2024-06-20 23:05:03,724 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 203.019
2024-06-20 23:05:04,024 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 185.843
2024-06-20 23:05:04,461 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 144.080
2024-06-20 23:05:04,763 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 116.611
2024-06-20 23:05:05,151 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 150.586
2024-06-20 23:05:05,453 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 98.869
2024-06-20 23:05:05,881 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 92.806
2024-06-20 23:05:06,182 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 161.764
2024-06-20 23:05:06,568 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 130.107
2024-06-20 23:05:06,868 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 103.861
2024-06-20 23:05:08,154 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 90.211
2024-06-20 23:05:19,117 - INFO: 1/200 final results:
2024-06-20 23:05:19,117 - INFO: Training loss: 19327.693.
2024-06-20 23:05:19,117 - INFO: Training MAE: 65.303.
2024-06-20 23:05:19,117 - INFO: Training MSE: 19708.217.
2024-06-20 23:05:39,209 - INFO: Epoch: 1/200, Loss_train: 19327.693098660173, Loss_val: 1530.5950927734375
2024-06-20 23:05:39,209 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-20 23:05:39,209 - INFO: Epoch 2/200...
2024-06-20 23:05:39,209 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:05:39,209 - INFO: Batch size: 32.
2024-06-20 23:05:39,212 - INFO: Dataset:
2024-06-20 23:05:39,213 - INFO: Batch size:
2024-06-20 23:05:39,213 - INFO: Number of workers:
2024-06-20 23:05:40,329 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 104.739
2024-06-20 23:05:40,651 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 72.476
2024-06-20 23:05:41,053 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 98.087
2024-06-20 23:05:41,377 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 87.648
2024-06-20 23:05:41,782 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 69.490
2024-06-20 23:05:42,102 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 80.562
2024-06-20 23:05:42,507 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 88.348
2024-06-20 23:05:42,835 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 86.107
2024-06-20 23:05:43,248 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 106.974
2024-06-20 23:05:43,571 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 92.037
2024-06-20 23:05:43,989 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 89.442
2024-06-20 23:05:44,316 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 85.692
2024-06-20 23:05:44,724 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 56.235
2024-06-20 23:05:45,044 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 109.072
2024-06-20 23:05:45,451 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 102.888
2024-06-20 23:05:45,770 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 91.379
2024-06-20 23:05:46,177 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 103.724
2024-06-20 23:05:46,495 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 99.727
2024-06-20 23:05:46,901 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 106.476
2024-06-20 23:05:47,217 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 86.424
2024-06-20 23:05:47,622 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 76.446
2024-06-20 23:05:47,940 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 76.435
2024-06-20 23:05:48,344 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 82.072
2024-06-20 23:05:48,661 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 68.569
2024-06-20 23:05:49,059 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 50.086
2024-06-20 23:05:49,373 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 54.824
2024-06-20 23:05:49,774 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 76.213
2024-06-20 23:05:50,087 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 60.363
2024-06-20 23:05:50,304 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 106.006
2024-06-20 23:06:01,277 - INFO: 2/200 final results:
2024-06-20 23:06:01,277 - INFO: Training loss: 85.122.
2024-06-20 23:06:01,277 - INFO: Training MAE: 7.416.
2024-06-20 23:06:01,277 - INFO: Training MSE: 84.709.
2024-06-20 23:06:21,612 - INFO: Epoch: 2/200, Loss_train: 85.12214489640861, Loss_val: 104.74891767830684
2024-06-20 23:06:21,659 - INFO: Saved new best metric model for epoch 2.
2024-06-20 23:06:21,659 - INFO: Best internal validation val_loss: 104.749 at epoch: 2.
2024-06-20 23:06:21,659 - INFO: Epoch 3/200...
2024-06-20 23:06:21,659 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:06:21,659 - INFO: Batch size: 32.
2024-06-20 23:06:21,663 - INFO: Dataset:
2024-06-20 23:06:21,663 - INFO: Batch size:
2024-06-20 23:06:21,663 - INFO: Number of workers:
2024-06-20 23:06:22,796 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 129.180
2024-06-20 23:06:23,121 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 181.691
2024-06-20 23:06:23,525 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 143.345
2024-06-20 23:06:23,847 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 70.204
2024-06-20 23:06:24,266 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 59.534
2024-06-20 23:06:24,584 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 102.840
2024-06-20 23:06:24,979 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 141.871
2024-06-20 23:06:25,296 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 79.664
2024-06-20 23:06:25,710 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 77.909
2024-06-20 23:06:26,024 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 55.044
2024-06-20 23:06:26,414 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 64.340
2024-06-20 23:06:26,732 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 61.795
2024-06-20 23:06:27,158 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 67.071
2024-06-20 23:06:27,479 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 71.108
2024-06-20 23:06:27,882 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 119.151
2024-06-20 23:06:28,200 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 64.102
2024-06-20 23:06:28,623 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 89.773
2024-06-20 23:06:28,938 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 89.995
2024-06-20 23:06:29,332 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 59.389
2024-06-20 23:06:29,644 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 58.210
2024-06-20 23:06:30,059 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 70.279
2024-06-20 23:06:30,375 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 76.979
2024-06-20 23:06:30,765 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 92.817
2024-06-20 23:06:31,081 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 97.521
2024-06-20 23:06:31,491 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 144.417
2024-06-20 23:06:31,805 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 171.656
2024-06-20 23:06:32,193 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 272.813
2024-06-20 23:06:32,506 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 294.505
2024-06-20 23:06:32,727 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 180.627
2024-06-20 23:06:43,632 - INFO: 3/200 final results:
2024-06-20 23:06:43,633 - INFO: Training loss: 109.925.
2024-06-20 23:06:43,633 - INFO: Training MAE: 8.396.
2024-06-20 23:06:43,633 - INFO: Training MSE: 108.527.
2024-06-20 23:07:03,720 - INFO: Epoch: 3/200, Loss_train: 109.92511959733635, Loss_val: 72.44183152297447
2024-06-20 23:07:03,777 - INFO: Saved new best metric model for epoch 3.
2024-06-20 23:07:03,777 - INFO: Best internal validation val_loss: 72.442 at epoch: 3.
2024-06-20 23:07:03,777 - INFO: Epoch 4/200...
2024-06-20 23:07:03,777 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:07:03,777 - INFO: Batch size: 32.
2024-06-20 23:07:03,781 - INFO: Dataset:
2024-06-20 23:07:03,781 - INFO: Batch size:
2024-06-20 23:07:03,781 - INFO: Number of workers:
2024-06-20 23:07:04,922 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 60.634
2024-06-20 23:07:05,233 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 54.296
2024-06-20 23:07:05,619 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 58.487
2024-06-20 23:07:05,940 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 68.218
2024-06-20 23:07:06,361 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 52.934
2024-06-20 23:07:06,667 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 49.609
2024-06-20 23:07:07,063 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 76.566
2024-06-20 23:07:07,384 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 47.620
2024-06-20 23:07:07,807 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 63.688
2024-06-20 23:07:08,108 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 61.874
2024-06-20 23:07:08,492 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 50.623
2024-06-20 23:07:08,814 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 39.984
2024-06-20 23:07:09,240 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 64.004
2024-06-20 23:07:09,548 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 92.042
2024-06-20 23:07:09,950 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 65.663
2024-06-20 23:07:10,268 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 53.482
2024-06-20 23:07:10,705 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 52.637
2024-06-20 23:07:11,011 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 126.367
2024-06-20 23:07:11,403 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 334.435
2024-06-20 23:07:11,718 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 551.892
2024-06-20 23:07:12,146 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 602.020
2024-06-20 23:07:12,450 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 413.993
2024-06-20 23:07:12,835 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 202.750
2024-06-20 23:07:13,152 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 84.773
2024-06-20 23:07:13,575 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 50.034
2024-06-20 23:07:13,879 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 33.161
2024-06-20 23:07:14,257 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 39.162
2024-06-20 23:07:14,573 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 70.937
2024-06-20 23:07:14,793 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 41.839
2024-06-20 23:07:25,885 - INFO: 4/200 final results:
2024-06-20 23:07:25,885 - INFO: Training loss: 122.887.
2024-06-20 23:07:25,885 - INFO: Training MAE: 8.355.
2024-06-20 23:07:25,885 - INFO: Training MSE: 124.490.
2024-06-20 23:07:46,188 - INFO: Epoch: 4/200, Loss_train: 122.8870832509008, Loss_val: 67.3720580791605
2024-06-20 23:07:46,244 - INFO: Saved new best metric model for epoch 4.
2024-06-20 23:07:46,244 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:07:46,244 - INFO: Epoch 5/200...
2024-06-20 23:07:46,244 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:07:46,244 - INFO: Batch size: 32.
2024-06-20 23:07:46,247 - INFO: Dataset:
2024-06-20 23:07:46,247 - INFO: Batch size:
2024-06-20 23:07:46,247 - INFO: Number of workers:
2024-06-20 23:07:47,359 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 61.211
2024-06-20 23:07:47,681 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 33.497
2024-06-20 23:07:48,090 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 31.429
2024-06-20 23:07:48,410 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 43.574
2024-06-20 23:07:48,828 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 52.831
2024-06-20 23:07:49,135 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 40.210
2024-06-20 23:07:49,544 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 49.144
2024-06-20 23:07:49,863 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 59.583
2024-06-20 23:07:50,285 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 51.965
2024-06-20 23:07:50,589 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 45.468
2024-06-20 23:07:50,983 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 73.701
2024-06-20 23:07:51,314 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 182.938
2024-06-20 23:07:51,738 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 289.596
2024-06-20 23:07:52,046 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 319.821
2024-06-20 23:07:52,449 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 255.893
2024-06-20 23:07:52,765 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 152.984
2024-06-20 23:07:53,177 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 98.767
2024-06-20 23:07:53,479 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 73.725
2024-06-20 23:07:53,882 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 65.327
2024-06-20 23:07:54,195 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 40.094
2024-06-20 23:07:54,612 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 63.092
2024-06-20 23:07:54,915 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 134.167
2024-06-20 23:07:55,307 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 167.988
2024-06-20 23:07:55,623 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 173.980
2024-06-20 23:07:56,026 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 118.959
2024-06-20 23:07:56,327 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 76.536
2024-06-20 23:07:56,719 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 66.305
2024-06-20 23:07:57,032 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 49.253
2024-06-20 23:07:57,243 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 87.671
2024-06-20 23:08:08,112 - INFO: 5/200 final results:
2024-06-20 23:08:08,112 - INFO: Training loss: 102.059.
2024-06-20 23:08:08,112 - INFO: Training MAE: 8.087.
2024-06-20 23:08:08,112 - INFO: Training MSE: 102.344.
2024-06-20 23:08:28,652 - INFO: Epoch: 5/200, Loss_train: 102.0589494376347, Loss_val: 109.55139186464507
2024-06-20 23:08:28,652 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:08:28,652 - INFO: Epoch 6/200...
2024-06-20 23:08:28,652 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:08:28,652 - INFO: Batch size: 32.
2024-06-20 23:08:28,655 - INFO: Dataset:
2024-06-20 23:08:28,655 - INFO: Batch size:
2024-06-20 23:08:28,655 - INFO: Number of workers:
2024-06-20 23:08:29,790 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 107.160
2024-06-20 23:08:30,098 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 125.583
2024-06-20 23:08:30,507 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 105.163
2024-06-20 23:08:30,827 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 88.020
2024-06-20 23:08:31,230 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 62.965
2024-06-20 23:08:31,534 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 45.076
2024-06-20 23:08:31,941 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 40.189
2024-06-20 23:08:32,263 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 37.097
2024-06-20 23:08:32,679 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 39.969
2024-06-20 23:08:32,979 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 46.029
2024-06-20 23:08:33,384 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 33.118
2024-06-20 23:08:33,702 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 28.073
2024-06-20 23:08:34,127 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 34.452
2024-06-20 23:08:34,433 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 45.620
2024-06-20 23:08:34,839 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 47.121
2024-06-20 23:08:35,155 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 50.627
2024-06-20 23:08:35,581 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 41.605
2024-06-20 23:08:35,887 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 49.508
2024-06-20 23:08:36,285 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 40.142
2024-06-20 23:08:36,600 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 56.520
2024-06-20 23:08:37,021 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 101.369
2024-06-20 23:08:37,328 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 106.110
2024-06-20 23:08:37,734 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 117.269
2024-06-20 23:08:38,053 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 177.081
2024-06-20 23:08:38,463 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 277.116
2024-06-20 23:08:38,765 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 421.661
2024-06-20 23:08:39,156 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 618.077
2024-06-20 23:08:39,470 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 848.410
2024-06-20 23:08:39,685 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 860.086
2024-06-20 23:08:50,364 - INFO: 6/200 final results:
2024-06-20 23:08:50,364 - INFO: Training loss: 160.387.
2024-06-20 23:08:50,364 - INFO: Training MAE: 8.947.
2024-06-20 23:08:50,364 - INFO: Training MSE: 146.547.
2024-06-20 23:09:10,691 - INFO: Epoch: 6/200, Loss_train: 160.38682339109224, Loss_val: 549.2428283691406
2024-06-20 23:09:10,691 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:09:10,691 - INFO: Epoch 7/200...
2024-06-20 23:09:10,691 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:09:10,691 - INFO: Batch size: 32.
2024-06-20 23:09:10,694 - INFO: Dataset:
2024-06-20 23:09:10,694 - INFO: Batch size:
2024-06-20 23:09:10,694 - INFO: Number of workers:
2024-06-20 23:09:11,803 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 617.163
2024-06-20 23:09:12,130 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 323.068
2024-06-20 23:09:12,556 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 130.033
2024-06-20 23:09:12,867 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 57.774
2024-06-20 23:09:13,285 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 25.053
2024-06-20 23:09:13,593 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 57.819
2024-06-20 23:09:14,011 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 63.080
2024-06-20 23:09:14,320 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 35.081
2024-06-20 23:09:14,735 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 37.618
2024-06-20 23:09:15,039 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 24.346
2024-06-20 23:09:15,456 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 30.194
2024-06-20 23:09:15,765 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 43.391
2024-06-20 23:09:16,186 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 42.568
2024-06-20 23:09:16,494 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 35.328
2024-06-20 23:09:16,928 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 27.573
2024-06-20 23:09:17,233 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 34.408
2024-06-20 23:09:17,643 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 37.775
2024-06-20 23:09:17,949 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 31.227
2024-06-20 23:09:18,374 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 35.474
2024-06-20 23:09:18,674 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 37.219
2024-06-20 23:09:19,078 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 37.015
2024-06-20 23:09:19,381 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 51.489
2024-06-20 23:09:19,793 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 69.332
2024-06-20 23:09:20,096 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 49.845
2024-06-20 23:09:20,488 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 41.205
2024-06-20 23:09:20,788 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 36.287
2024-06-20 23:09:21,188 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 35.544
2024-06-20 23:09:21,487 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 46.571
2024-06-20 23:09:21,690 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 51.781
2024-06-20 23:09:32,482 - INFO: 7/200 final results:
2024-06-20 23:09:32,482 - INFO: Training loss: 73.974.
2024-06-20 23:09:32,482 - INFO: Training MAE: 6.362.
2024-06-20 23:09:32,482 - INFO: Training MSE: 74.413.
2024-06-20 23:09:52,644 - INFO: Epoch: 7/200, Loss_train: 73.97448769931135, Loss_val: 41.468528484476025
2024-06-20 23:09:52,701 - INFO: Saved new best metric model for epoch 7.
2024-06-20 23:09:52,701 - INFO: Best internal validation val_loss: 41.469 at epoch: 7.
2024-06-20 23:09:52,701 - INFO: Epoch 8/200...
2024-06-20 23:09:52,701 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:09:52,701 - INFO: Batch size: 32.
2024-06-20 23:09:52,705 - INFO: Dataset:
2024-06-20 23:09:52,705 - INFO: Batch size:
2024-06-20 23:09:52,705 - INFO: Number of workers:
2024-06-20 23:09:53,823 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 38.618
2024-06-20 23:09:54,148 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 41.944
2024-06-20 23:09:54,565 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 48.771
2024-06-20 23:09:54,872 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 52.428
2024-06-20 23:09:55,277 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 45.116
2024-06-20 23:09:55,581 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 33.103
2024-06-20 23:09:55,986 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 31.079
2024-06-20 23:09:56,290 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 24.984
2024-06-20 23:09:56,695 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 20.688
2024-06-20 23:09:56,994 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 28.207
2024-06-20 23:09:57,411 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 36.365
2024-06-20 23:09:57,716 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 56.361
2024-06-20 23:09:58,124 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 68.938
2024-06-20 23:09:58,429 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 61.529
2024-06-20 23:09:58,864 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 59.960
2024-06-20 23:09:59,167 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 51.444
2024-06-20 23:09:59,590 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 49.460
2024-06-20 23:09:59,902 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 35.192
2024-06-20 23:10:00,324 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 33.559
2024-06-20 23:10:00,627 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 37.368
2024-06-20 23:10:01,030 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 54.750
2024-06-20 23:10:01,337 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 69.632
2024-06-20 23:10:01,761 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 57.687
2024-06-20 23:10:02,065 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 65.025
2024-06-20 23:10:02,455 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 71.458
2024-06-20 23:10:02,754 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 65.939
2024-06-20 23:10:03,164 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 60.644
2024-06-20 23:10:03,463 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 53.098
2024-06-20 23:10:03,671 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 62.079
2024-06-20 23:10:14,555 - INFO: 8/200 final results:
2024-06-20 23:10:14,555 - INFO: Training loss: 48.808.
2024-06-20 23:10:14,555 - INFO: Training MAE: 5.694.
2024-06-20 23:10:14,555 - INFO: Training MSE: 48.545.
2024-06-20 23:10:34,860 - INFO: Epoch: 8/200, Loss_train: 48.807859618088294, Loss_val: 79.64902312180092
2024-06-20 23:10:34,860 - INFO: Best internal validation val_loss: 41.469 at epoch: 7.
2024-06-20 23:10:34,860 - INFO: Epoch 9/200...
2024-06-20 23:10:34,860 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:10:34,860 - INFO: Batch size: 32.
2024-06-20 23:10:34,864 - INFO: Dataset:
2024-06-20 23:10:34,864 - INFO: Batch size:
2024-06-20 23:10:34,864 - INFO: Number of workers:
2024-06-20 23:10:35,995 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 73.719
2024-06-20 23:10:36,305 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 69.151
2024-06-20 23:10:36,729 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 56.008
2024-06-20 23:10:37,039 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 53.107
2024-06-20 23:10:37,458 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 52.475
2024-06-20 23:10:37,766 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 42.693
2024-06-20 23:10:38,183 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 36.157
2024-06-20 23:10:38,492 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 36.875
2024-06-20 23:10:38,911 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 54.286
2024-06-20 23:10:39,215 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 80.958
2024-06-20 23:10:39,633 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 98.612
2024-06-20 23:10:39,942 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 122.208
2024-06-20 23:10:40,380 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 160.685
2024-06-20 23:10:40,688 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 219.933
2024-06-20 23:10:41,108 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 351.693
2024-06-20 23:10:41,414 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 524.953
2024-06-20 23:10:41,836 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 552.777
2024-06-20 23:10:42,142 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 426.985
2024-06-20 23:10:42,558 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 221.434
2024-06-20 23:10:42,861 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 76.240
2024-06-20 23:10:43,277 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 24.555
2024-06-20 23:10:43,583 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 40.331
2024-06-20 23:10:43,992 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 77.682
2024-06-20 23:10:44,299 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 51.390
2024-06-20 23:10:44,703 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 28.633
2024-06-20 23:10:45,005 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 30.197
2024-06-20 23:10:45,398 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 30.749
2024-06-20 23:10:45,699 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 31.665
2024-06-20 23:10:45,911 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 32.776
2024-06-20 23:10:56,882 - INFO: 9/200 final results:
2024-06-20 23:10:56,882 - INFO: Training loss: 126.170.
2024-06-20 23:10:56,883 - INFO: Training MAE: 8.446.
2024-06-20 23:10:56,883 - INFO: Training MSE: 128.017.
2024-06-20 23:11:17,133 - INFO: Epoch: 9/200, Loss_train: 126.16986761421992, Loss_val: 24.343069339620655
2024-06-20 23:11:17,188 - INFO: Saved new best metric model for epoch 9.
2024-06-20 23:11:17,189 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:11:17,189 - INFO: Epoch 10/200...
2024-06-20 23:11:17,189 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:11:17,189 - INFO: Batch size: 32.
2024-06-20 23:11:17,192 - INFO: Dataset:
2024-06-20 23:11:17,192 - INFO: Batch size:
2024-06-20 23:11:17,192 - INFO: Number of workers:
2024-06-20 23:11:18,407 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 29.661
2024-06-20 23:11:18,719 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 31.081
2024-06-20 23:11:19,119 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 27.003
2024-06-20 23:11:19,442 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 35.249
2024-06-20 23:11:19,865 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 27.812
2024-06-20 23:11:20,173 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 32.614
2024-06-20 23:11:20,574 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 27.478
2024-06-20 23:11:20,894 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 36.958
2024-06-20 23:11:21,313 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 41.150
2024-06-20 23:11:21,616 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 35.155
2024-06-20 23:11:22,021 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 41.154
2024-06-20 23:11:22,343 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 29.056
2024-06-20 23:11:22,773 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 37.993
2024-06-20 23:11:23,082 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 56.992
2024-06-20 23:11:23,494 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 59.902
2024-06-20 23:11:23,813 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 45.533
2024-06-20 23:11:24,240 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 83.853
2024-06-20 23:11:24,546 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 131.683
2024-06-20 23:11:24,948 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 177.404
2024-06-20 23:11:25,264 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 225.948
2024-06-20 23:11:25,682 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 217.905
2024-06-20 23:11:25,987 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 187.028
2024-06-20 23:11:26,392 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 133.533
2024-06-20 23:11:26,709 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 72.163
2024-06-20 23:11:27,120 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 50.667
2024-06-20 23:11:27,421 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 53.457
2024-06-20 23:11:27,822 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 69.968
2024-06-20 23:11:28,136 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 95.731
2024-06-20 23:11:28,354 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 116.054
2024-06-20 23:11:39,456 - INFO: 10/200 final results:
2024-06-20 23:11:39,456 - INFO: Training loss: 76.213.
2024-06-20 23:11:39,456 - INFO: Training MAE: 6.767.
2024-06-20 23:11:39,456 - INFO: Training MSE: 75.425.
2024-06-20 23:11:59,855 - INFO: Epoch: 10/200, Loss_train: 76.21326637268066, Loss_val: 99.97080257021148
2024-06-20 23:11:59,855 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:11:59,855 - INFO: Epoch 11/200...
2024-06-20 23:11:59,855 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:11:59,855 - INFO: Batch size: 32.
2024-06-20 23:11:59,858 - INFO: Dataset:
2024-06-20 23:11:59,858 - INFO: Batch size:
2024-06-20 23:11:59,858 - INFO: Number of workers:
2024-06-20 23:12:00,972 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 100.535
2024-06-20 23:12:01,281 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 54.682
2024-06-20 23:12:01,682 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 25.830
2024-06-20 23:12:02,004 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 30.036
2024-06-20 23:12:02,416 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 27.969
2024-06-20 23:12:02,736 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 21.081
2024-06-20 23:12:03,127 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 25.342
2024-06-20 23:12:03,448 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 26.106
2024-06-20 23:12:03,873 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 26.148
2024-06-20 23:12:04,188 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 27.979
2024-06-20 23:12:04,572 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 26.880
2024-06-20 23:12:04,893 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 18.753
2024-06-20 23:12:05,334 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 24.605
2024-06-20 23:12:05,643 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 24.467
2024-06-20 23:12:06,042 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 24.148
2024-06-20 23:12:06,361 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 31.790
2024-06-20 23:12:06,796 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 50.622
2024-06-20 23:12:07,102 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 68.481
2024-06-20 23:12:07,503 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 83.143
2024-06-20 23:12:07,819 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 70.438
2024-06-20 23:12:08,268 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 55.579
2024-06-20 23:12:08,581 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 55.768
2024-06-20 23:12:08,975 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 60.572
2024-06-20 23:12:09,294 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 38.614
2024-06-20 23:12:09,723 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 19.814
2024-06-20 23:12:10,026 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 26.140
2024-06-20 23:12:10,418 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 23.016
2024-06-20 23:12:10,734 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 43.121
2024-06-20 23:12:10,955 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 79.542
2024-06-20 23:12:21,880 - INFO: 11/200 final results:
2024-06-20 23:12:21,881 - INFO: Training loss: 41.076.
2024-06-20 23:12:21,881 - INFO: Training MAE: 5.028.
2024-06-20 23:12:21,881 - INFO: Training MSE: 40.315.
2024-06-20 23:12:42,262 - INFO: Epoch: 11/200, Loss_train: 41.07588787736564, Loss_val: 146.44099478886045
2024-06-20 23:12:42,262 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:12:42,262 - INFO: Epoch 12/200...
2024-06-20 23:12:42,263 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:12:42,263 - INFO: Batch size: 32.
2024-06-20 23:12:42,266 - INFO: Dataset:
2024-06-20 23:12:42,266 - INFO: Batch size:
2024-06-20 23:12:42,266 - INFO: Number of workers:
2024-06-20 23:12:43,390 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 140.114
2024-06-20 23:12:43,732 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 198.890
2024-06-20 23:12:44,123 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 279.892
2024-06-20 23:12:44,447 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 350.675
2024-06-20 23:12:44,856 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 397.431
2024-06-20 23:12:45,200 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 350.895
2024-06-20 23:12:45,597 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 286.751
2024-06-20 23:12:45,905 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 175.741
2024-06-20 23:12:46,308 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 84.924
2024-06-20 23:12:46,659 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 50.916
2024-06-20 23:12:47,055 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 47.968
2024-06-20 23:12:47,364 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 33.014
2024-06-20 23:12:47,782 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 21.566
2024-06-20 23:12:48,125 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 24.048
2024-06-20 23:12:48,525 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 30.409
2024-06-20 23:12:48,829 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 46.675
2024-06-20 23:12:49,242 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 54.669
2024-06-20 23:12:49,584 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 46.861
2024-06-20 23:12:49,964 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 41.088
2024-06-20 23:12:50,267 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 44.490
2024-06-20 23:12:50,678 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 40.899
2024-06-20 23:12:51,018 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 36.319
2024-06-20 23:12:51,412 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 27.510
2024-06-20 23:12:51,717 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 28.273
2024-06-20 23:12:52,119 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 26.110
2024-06-20 23:12:52,456 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 22.174
2024-06-20 23:12:52,845 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 25.362
2024-06-20 23:12:53,147 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 55.661
2024-06-20 23:12:53,364 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 81.382
2024-06-20 23:13:04,274 - INFO: 12/200 final results:
2024-06-20 23:13:04,274 - INFO: Training loss: 105.197.
2024-06-20 23:13:04,274 - INFO: Training MAE: 7.592.
2024-06-20 23:13:04,274 - INFO: Training MSE: 105.668.
2024-06-20 23:13:24,520 - INFO: Epoch: 12/200, Loss_train: 105.19685475579624, Loss_val: 82.82678735667261
2024-06-20 23:13:24,520 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:13:24,520 - INFO: Epoch 13/200...
2024-06-20 23:13:24,520 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:13:24,520 - INFO: Batch size: 32.
2024-06-20 23:13:24,523 - INFO: Dataset:
2024-06-20 23:13:24,523 - INFO: Batch size:
2024-06-20 23:13:24,523 - INFO: Number of workers:
2024-06-20 23:13:25,634 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 69.781
2024-06-20 23:13:25,958 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 55.916
2024-06-20 23:13:26,364 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 36.694
2024-06-20 23:13:26,685 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 23.297
2024-06-20 23:13:27,096 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 19.395
2024-06-20 23:13:27,404 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 33.141
2024-06-20 23:13:27,804 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 60.019
2024-06-20 23:13:28,124 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 68.998
2024-06-20 23:13:28,544 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 52.007
2024-06-20 23:13:28,848 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 38.150
2024-06-20 23:13:29,255 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 37.897
2024-06-20 23:13:29,577 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 28.000
2024-06-20 23:13:30,000 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 25.609
2024-06-20 23:13:30,310 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 29.595
2024-06-20 23:13:30,718 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 29.657
2024-06-20 23:13:31,037 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 32.155
2024-06-20 23:13:31,453 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 45.428
2024-06-20 23:13:31,759 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 69.320
2024-06-20 23:13:32,163 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 89.773
2024-06-20 23:13:32,479 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 137.054
2024-06-20 23:13:32,892 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 197.708
2024-06-20 23:13:33,197 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 201.986
2024-06-20 23:13:33,604 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 156.316
2024-06-20 23:13:33,921 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 72.018
2024-06-20 23:13:34,326 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 30.718
2024-06-20 23:13:34,627 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 24.526
2024-06-20 23:13:35,028 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 23.853
2024-06-20 23:13:35,341 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 27.193
2024-06-20 23:13:35,564 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 35.755
2024-06-20 23:13:46,453 - INFO: 13/200 final results:
2024-06-20 23:13:46,453 - INFO: Training loss: 60.412.
2024-06-20 23:13:46,453 - INFO: Training MAE: 6.007.
2024-06-20 23:13:46,453 - INFO: Training MSE: 60.900.
2024-06-20 23:14:06,588 - INFO: Epoch: 13/200, Loss_train: 60.41231405323949, Loss_val: 36.43836777785729
2024-06-20 23:14:06,588 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:14:06,588 - INFO: Epoch 14/200...
2024-06-20 23:14:06,588 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:14:06,588 - INFO: Batch size: 32.
2024-06-20 23:14:06,591 - INFO: Dataset:
2024-06-20 23:14:06,591 - INFO: Batch size:
2024-06-20 23:14:06,591 - INFO: Number of workers:
2024-06-20 23:14:07,708 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 36.976
2024-06-20 23:14:08,031 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 40.106
2024-06-20 23:14:08,447 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 34.203
2024-06-20 23:14:08,768 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 29.611
2024-06-20 23:14:09,177 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 25.362
2024-06-20 23:14:09,494 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 25.496
2024-06-20 23:14:09,902 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 39.128
2024-06-20 23:14:10,219 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 65.478
2024-06-20 23:14:10,620 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 113.140
2024-06-20 23:14:10,933 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 215.216
2024-06-20 23:14:11,343 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 335.211
2024-06-20 23:14:11,663 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 405.582
2024-06-20 23:14:12,077 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 381.472
2024-06-20 23:14:12,396 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 167.520
2024-06-20 23:14:12,796 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 60.374
2024-06-20 23:14:13,114 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 54.178
2024-06-20 23:14:13,526 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 75.876
2024-06-20 23:14:13,843 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 52.438
2024-06-20 23:14:14,250 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 27.845
2024-06-20 23:14:14,564 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 21.986
2024-06-20 23:14:14,991 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 15.984
2024-06-20 23:14:15,318 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 18.753
2024-06-20 23:14:15,734 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 16.875
2024-06-20 23:14:16,060 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 17.532
2024-06-20 23:14:16,460 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 21.666
2024-06-20 23:14:16,773 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 20.904
2024-06-20 23:14:17,172 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 21.016
2024-06-20 23:14:17,484 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 17.637
2024-06-20 23:14:17,705 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 25.472
2024-06-20 23:14:28,594 - INFO: 14/200 final results:
2024-06-20 23:14:28,595 - INFO: Training loss: 82.174.
2024-06-20 23:14:28,595 - INFO: Training MAE: 6.489.
2024-06-20 23:14:28,595 - INFO: Training MSE: 83.295.
2024-06-20 23:14:48,916 - INFO: Epoch: 14/200, Loss_train: 82.17370549563704, Loss_val: 20.96248116986505
2024-06-20 23:14:48,974 - INFO: Saved new best metric model for epoch 14.
2024-06-20 23:14:48,974 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:14:48,974 - INFO: Epoch 15/200...
2024-06-20 23:14:48,974 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:14:48,974 - INFO: Batch size: 32.
2024-06-20 23:14:48,977 - INFO: Dataset:
2024-06-20 23:14:48,977 - INFO: Batch size:
2024-06-20 23:14:48,978 - INFO: Number of workers:
2024-06-20 23:14:50,107 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 21.317
2024-06-20 23:14:50,416 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 17.794
2024-06-20 23:14:50,828 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 20.325
2024-06-20 23:14:51,150 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 18.891
2024-06-20 23:14:51,571 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 17.639
2024-06-20 23:14:51,879 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 13.157
2024-06-20 23:14:52,273 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 20.492
2024-06-20 23:14:52,594 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 25.199
2024-06-20 23:14:53,016 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 27.349
2024-06-20 23:14:53,320 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 28.336
2024-06-20 23:14:53,717 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 21.830
2024-06-20 23:14:54,038 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 22.350
2024-06-20 23:14:54,467 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 25.092
2024-06-20 23:14:54,776 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 23.880
2024-06-20 23:14:55,179 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 16.543
2024-06-20 23:14:55,497 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 17.939
2024-06-20 23:14:55,922 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 18.385
2024-06-20 23:14:56,227 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 31.255
2024-06-20 23:14:56,621 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 30.276
2024-06-20 23:14:56,937 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 22.927
2024-06-20 23:14:57,354 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 19.892
2024-06-20 23:14:57,660 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 23.734
2024-06-20 23:14:58,068 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 20.377
2024-06-20 23:14:58,386 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 20.300
2024-06-20 23:14:58,799 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 21.711
2024-06-20 23:14:59,101 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 30.428
2024-06-20 23:14:59,504 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 54.348
2024-06-20 23:14:59,819 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 104.007
2024-06-20 23:15:00,042 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 171.523
2024-06-20 23:15:10,976 - INFO: 15/200 final results:
2024-06-20 23:15:10,976 - INFO: Training loss: 31.286.
2024-06-20 23:15:10,976 - INFO: Training MAE: 4.120.
2024-06-20 23:15:10,976 - INFO: Training MSE: 28.512.
2024-06-20 23:15:31,087 - INFO: Epoch: 15/200, Loss_train: 31.28610058488517, Loss_val: 216.55172992574757
2024-06-20 23:15:31,087 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:15:31,087 - INFO: Epoch 16/200...
2024-06-20 23:15:31,087 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:15:31,087 - INFO: Batch size: 32.
2024-06-20 23:15:31,091 - INFO: Dataset:
2024-06-20 23:15:31,091 - INFO: Batch size:
2024-06-20 23:15:31,091 - INFO: Number of workers:
2024-06-20 23:15:32,195 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 168.303
2024-06-20 23:15:32,530 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 142.291
2024-06-20 23:15:32,925 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 117.567
2024-06-20 23:15:33,245 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 76.968
2024-06-20 23:15:33,661 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 39.819
2024-06-20 23:15:33,978 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 18.423
2024-06-20 23:15:34,369 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 30.631
2024-06-20 23:15:34,688 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 28.765
2024-06-20 23:15:35,099 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 21.325
2024-06-20 23:15:35,412 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 32.139
2024-06-20 23:15:35,792 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 31.006
2024-06-20 23:15:36,110 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 23.916
2024-06-20 23:15:36,544 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 29.404
2024-06-20 23:15:36,849 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 21.508
2024-06-20 23:15:37,243 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 21.684
2024-06-20 23:15:37,557 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 16.547
2024-06-20 23:15:37,988 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 25.431
2024-06-20 23:15:38,289 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 30.083
2024-06-20 23:15:38,676 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 41.433
2024-06-20 23:15:38,988 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 50.583
2024-06-20 23:15:39,413 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 53.916
2024-06-20 23:15:39,716 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 44.429
2024-06-20 23:15:40,108 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 50.193
2024-06-20 23:15:40,426 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 60.891
2024-06-20 23:15:40,844 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 69.702
2024-06-20 23:15:41,144 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 81.628
2024-06-20 23:15:41,530 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 95.513
2024-06-20 23:15:41,843 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 101.558
2024-06-20 23:15:42,065 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 97.155
2024-06-20 23:15:52,953 - INFO: 16/200 final results:
2024-06-20 23:15:52,953 - INFO: Training loss: 55.959.
2024-06-20 23:15:52,954 - INFO: Training MAE: 5.906.
2024-06-20 23:15:52,954 - INFO: Training MSE: 55.144.
2024-06-20 23:16:12,983 - INFO: Epoch: 16/200, Loss_train: 55.958985493100926, Loss_val: 109.00802796462486
2024-06-20 23:16:12,983 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:16:12,983 - INFO: Epoch 17/200...
2024-06-20 23:16:12,983 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:16:12,983 - INFO: Batch size: 32.
2024-06-20 23:16:12,986 - INFO: Dataset:
2024-06-20 23:16:12,987 - INFO: Batch size:
2024-06-20 23:16:12,987 - INFO: Number of workers:
2024-06-20 23:16:14,098 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 108.522
2024-06-20 23:16:14,419 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 86.621
2024-06-20 23:16:14,821 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 43.997
2024-06-20 23:16:15,142 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 17.873
2024-06-20 23:16:15,550 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 17.631
2024-06-20 23:16:15,871 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 18.476
2024-06-20 23:16:16,266 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 19.872
2024-06-20 23:16:16,587 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 21.070
2024-06-20 23:16:16,987 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 22.999
2024-06-20 23:16:17,303 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 23.053
2024-06-20 23:16:17,711 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 25.639
2024-06-20 23:16:18,033 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 14.884
2024-06-20 23:16:18,448 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 15.186
2024-06-20 23:16:18,766 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 17.223
2024-06-20 23:16:19,175 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 29.362
2024-06-20 23:16:19,490 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 29.069
2024-06-20 23:16:19,896 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 31.605
2024-06-20 23:16:20,212 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 30.708
2024-06-20 23:16:20,611 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 35.983
2024-06-20 23:16:20,928 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 41.417
2024-06-20 23:16:21,333 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 20.269
2024-06-20 23:16:21,658 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 15.796
2024-06-20 23:16:22,085 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 18.402
2024-06-20 23:16:22,413 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 20.482
2024-06-20 23:16:22,836 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 17.448
2024-06-20 23:16:23,157 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 21.914
2024-06-20 23:16:23,560 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 15.331
2024-06-20 23:16:23,876 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 30.725
2024-06-20 23:16:24,100 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 32.365
2024-06-20 23:16:35,022 - INFO: 17/200 final results:
2024-06-20 23:16:35,023 - INFO: Training loss: 29.101.
2024-06-20 23:16:35,023 - INFO: Training MAE: 4.113.
2024-06-20 23:16:35,023 - INFO: Training MSE: 29.036.
2024-06-20 23:16:55,417 - INFO: Epoch: 17/200, Loss_train: 29.10069567581703, Loss_val: 33.189337565981106
2024-06-20 23:16:55,417 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:16:55,417 - INFO: Epoch 18/200...
2024-06-20 23:16:55,417 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:16:55,417 - INFO: Batch size: 32.
2024-06-20 23:16:55,420 - INFO: Dataset:
2024-06-20 23:16:55,420 - INFO: Batch size:
2024-06-20 23:16:55,421 - INFO: Number of workers:
2024-06-20 23:16:56,542 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 21.435
2024-06-20 23:16:56,878 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 18.157
2024-06-20 23:16:57,280 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 16.317
2024-06-20 23:16:57,601 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 30.448
2024-06-20 23:16:57,999 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 51.118
2024-06-20 23:16:58,328 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 108.370
2024-06-20 23:16:58,711 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 265.196
2024-06-20 23:16:59,032 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 506.576
2024-06-20 23:16:59,424 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 480.233
2024-06-20 23:16:59,758 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 140.405
2024-06-20 23:17:00,141 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 92.981
2024-06-20 23:17:00,462 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 83.574
2024-06-20 23:17:00,877 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 30.323
2024-06-20 23:17:01,212 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 25.492
2024-06-20 23:17:01,601 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 21.217
2024-06-20 23:17:01,919 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 20.905
2024-06-20 23:17:02,331 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 17.496
2024-06-20 23:17:02,661 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 19.345
2024-06-20 23:17:03,055 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 24.762
2024-06-20 23:17:03,370 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 16.868
2024-06-20 23:17:03,775 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 25.646
2024-06-20 23:17:04,104 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 28.140
2024-06-20 23:17:04,496 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 25.178
2024-06-20 23:17:04,812 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 17.282
2024-06-20 23:17:05,211 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 17.850
2024-06-20 23:17:05,535 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 18.171
2024-06-20 23:17:05,922 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 21.507
2024-06-20 23:17:06,234 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 17.678
2024-06-20 23:17:06,455 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 17.746
2024-06-20 23:17:17,334 - INFO: 18/200 final results:
2024-06-20 23:17:17,334 - INFO: Training loss: 75.187.
2024-06-20 23:17:17,334 - INFO: Training MAE: 6.038.
2024-06-20 23:17:17,334 - INFO: Training MSE: 76.323.
2024-06-20 23:17:37,466 - INFO: Epoch: 18/200, Loss_train: 75.18679921380405, Loss_val: 20.244176963279987
2024-06-20 23:17:37,523 - INFO: Saved new best metric model for epoch 18.
2024-06-20 23:17:37,523 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:17:37,523 - INFO: Epoch 19/200...
2024-06-20 23:17:37,523 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:17:37,524 - INFO: Batch size: 32.
2024-06-20 23:17:37,527 - INFO: Dataset:
2024-06-20 23:17:37,527 - INFO: Batch size:
2024-06-20 23:17:37,527 - INFO: Number of workers:
2024-06-20 23:17:38,681 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 27.962
2024-06-20 23:17:38,989 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 42.540
2024-06-20 23:17:39,391 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 55.703
2024-06-20 23:17:39,711 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 85.728
2024-06-20 23:17:40,139 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 119.125
2024-06-20 23:17:40,447 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 107.066
2024-06-20 23:17:40,829 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 69.058
2024-06-20 23:17:41,149 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 31.474
2024-06-20 23:17:41,591 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 19.071
2024-06-20 23:17:41,895 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 25.911
2024-06-20 23:17:42,278 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 25.188
2024-06-20 23:17:42,600 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 18.567
2024-06-20 23:17:43,040 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 22.757
2024-06-20 23:17:43,349 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 12.637
2024-06-20 23:17:43,739 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 16.145
2024-06-20 23:17:44,057 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 19.298
2024-06-20 23:17:44,490 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 20.607
2024-06-20 23:17:44,794 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 15.923
2024-06-20 23:17:45,171 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 16.393
2024-06-20 23:17:45,487 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 12.379
2024-06-20 23:17:45,911 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 15.170
2024-06-20 23:17:46,218 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 18.375
2024-06-20 23:17:46,600 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 15.227
2024-06-20 23:17:46,920 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 22.059
2024-06-20 23:17:47,334 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 22.063
2024-06-20 23:17:47,638 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 25.233
2024-06-20 23:17:48,019 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 22.310
2024-06-20 23:17:48,335 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 23.600
2024-06-20 23:17:48,551 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 24.279
2024-06-20 23:17:59,637 - INFO: 19/200 final results:
2024-06-20 23:17:59,637 - INFO: Training loss: 32.822.
2024-06-20 23:17:59,637 - INFO: Training MAE: 4.349.
2024-06-20 23:17:59,637 - INFO: Training MSE: 32.991.
2024-06-20 23:18:20,086 - INFO: Epoch: 19/200, Loss_train: 32.822263914963294, Loss_val: 37.03066766673121
2024-06-20 23:18:20,086 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:18:20,086 - INFO: Epoch 20/200...
2024-06-20 23:18:20,086 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:18:20,087 - INFO: Batch size: 32.
2024-06-20 23:18:20,089 - INFO: Dataset:
2024-06-20 23:18:20,090 - INFO: Batch size:
2024-06-20 23:18:20,090 - INFO: Number of workers:
2024-06-20 23:18:21,222 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 46.971
2024-06-20 23:18:21,531 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 48.652
2024-06-20 23:18:21,943 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 38.041
2024-06-20 23:18:22,264 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 30.604
2024-06-20 23:18:22,693 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 20.787
2024-06-20 23:18:22,997 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 19.377
2024-06-20 23:18:23,390 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 16.440
2024-06-20 23:18:23,707 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 14.600
2024-06-20 23:18:24,140 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 16.969
2024-06-20 23:18:24,442 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 22.870
2024-06-20 23:18:24,841 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 19.444
2024-06-20 23:18:25,162 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 20.528
2024-06-20 23:18:25,596 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 12.516
2024-06-20 23:18:25,901 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 12.739
2024-06-20 23:18:26,300 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 17.328
2024-06-20 23:18:26,614 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 22.071
2024-06-20 23:18:27,044 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 16.666
2024-06-20 23:18:27,346 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 22.956
2024-06-20 23:18:27,736 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 28.802
2024-06-20 23:18:28,048 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 48.320
2024-06-20 23:18:28,472 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 43.869
2024-06-20 23:18:28,776 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 26.774
2024-06-20 23:18:29,167 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 20.790
2024-06-20 23:18:29,483 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 16.512
2024-06-20 23:18:29,905 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 18.115
2024-06-20 23:18:30,206 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 24.399
2024-06-20 23:18:30,592 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 21.570
2024-06-20 23:18:30,905 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 25.708
2024-06-20 23:18:31,125 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 21.241
2024-06-20 23:18:41,942 - INFO: 20/200 final results:
2024-06-20 23:18:41,942 - INFO: Training loss: 24.678.
2024-06-20 23:18:41,942 - INFO: Training MAE: 3.900.
2024-06-20 23:18:41,942 - INFO: Training MSE: 24.746.
2024-06-20 23:19:02,500 - INFO: Epoch: 20/200, Loss_train: 24.677991275129646, Loss_val: 21.663095013848668
2024-06-20 23:19:02,500 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:19:02,500 - INFO: Epoch 21/200...
2024-06-20 23:19:02,500 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:19:02,500 - INFO: Batch size: 32.
2024-06-20 23:19:02,504 - INFO: Dataset:
2024-06-20 23:19:02,504 - INFO: Batch size:
2024-06-20 23:19:02,504 - INFO: Number of workers:
2024-06-20 23:19:03,688 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 26.377
2024-06-20 23:19:04,000 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 33.193
2024-06-20 23:19:04,403 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 27.915
2024-06-20 23:19:04,726 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 16.364
2024-06-20 23:19:05,169 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 11.198
2024-06-20 23:19:05,473 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 14.608
2024-06-20 23:19:05,865 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 23.743
2024-06-20 23:19:06,170 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 35.741
2024-06-20 23:19:06,626 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 65.766
2024-06-20 23:19:06,928 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 115.895
2024-06-20 23:19:07,323 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 174.212
2024-06-20 23:19:07,630 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 207.094
2024-06-20 23:19:08,082 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 86.682
2024-06-20 23:19:08,389 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 22.055
2024-06-20 23:19:08,792 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 28.185
2024-06-20 23:19:09,096 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 28.613
2024-06-20 23:19:09,543 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 19.728
2024-06-20 23:19:09,848 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 18.063
2024-06-20 23:19:10,242 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 17.549
2024-06-20 23:19:10,544 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 20.499
2024-06-20 23:19:10,988 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 16.115
2024-06-20 23:19:11,294 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 18.126
2024-06-20 23:19:11,683 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 19.525
2024-06-20 23:19:11,990 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 25.345
2024-06-20 23:19:12,429 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 16.914
2024-06-20 23:19:12,732 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 19.888
2024-06-20 23:19:13,115 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 29.994
2024-06-20 23:19:13,418 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 41.197
2024-06-20 23:19:13,639 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 34.170
2024-06-20 23:19:24,315 - INFO: 21/200 final results:
2024-06-20 23:19:24,316 - INFO: Training loss: 41.888.
2024-06-20 23:19:24,316 - INFO: Training MAE: 4.717.
2024-06-20 23:19:24,316 - INFO: Training MSE: 42.041.
2024-06-20 23:19:44,497 - INFO: Epoch: 21/200, Loss_train: 41.887994075643604, Loss_val: 49.95502498232085
2024-06-20 23:19:44,497 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:19:44,497 - INFO: Epoch 22/200...
2024-06-20 23:19:44,497 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:19:44,497 - INFO: Batch size: 32.
2024-06-20 23:19:44,501 - INFO: Dataset:
2024-06-20 23:19:44,501 - INFO: Batch size:
2024-06-20 23:19:44,501 - INFO: Number of workers:
2024-06-20 23:19:45,619 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 44.155
2024-06-20 23:19:45,940 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 31.496
2024-06-20 23:19:46,344 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 20.320
2024-06-20 23:19:46,668 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 22.162
2024-06-20 23:19:47,076 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 27.918
2024-06-20 23:19:47,396 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 29.282
2024-06-20 23:19:47,790 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 17.914
2024-06-20 23:19:48,112 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 19.135
2024-06-20 23:19:48,509 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 25.572
2024-06-20 23:19:48,825 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 30.720
2024-06-20 23:19:49,239 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 26.067
2024-06-20 23:19:49,560 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 27.036
2024-06-20 23:19:49,974 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 23.453
2024-06-20 23:19:50,295 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 25.393
2024-06-20 23:19:50,709 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 34.178
2024-06-20 23:19:51,027 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 39.743
2024-06-20 23:19:51,439 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 38.317
2024-06-20 23:19:51,757 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 29.803
2024-06-20 23:19:52,165 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 18.298
2024-06-20 23:19:52,481 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 15.671
2024-06-20 23:19:52,886 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 28.623
2024-06-20 23:19:53,206 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 24.246
2024-06-20 23:19:53,615 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 21.975
2024-06-20 23:19:53,935 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 21.071
2024-06-20 23:19:54,338 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 18.434
2024-06-20 23:19:54,654 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 14.741
2024-06-20 23:19:55,058 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 15.134
2024-06-20 23:19:55,374 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 14.028
2024-06-20 23:19:55,595 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 15.569
2024-06-20 23:20:06,569 - INFO: 22/200 final results:
2024-06-20 23:20:06,570 - INFO: Training loss: 24.843.
2024-06-20 23:20:06,570 - INFO: Training MAE: 3.954.
2024-06-20 23:20:06,570 - INFO: Training MSE: 25.027.
2024-06-20 23:20:27,049 - INFO: Epoch: 22/200, Loss_train: 24.84321666585988, Loss_val: 29.776788086726746
2024-06-20 23:20:27,049 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:20:27,049 - INFO: Epoch 23/200...
2024-06-20 23:20:27,049 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:20:27,050 - INFO: Batch size: 32.
2024-06-20 23:20:27,053 - INFO: Dataset:
2024-06-20 23:20:27,053 - INFO: Batch size:
2024-06-20 23:20:27,053 - INFO: Number of workers:
2024-06-20 23:20:28,202 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 22.373
2024-06-20 23:20:28,512 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 19.379
2024-06-20 23:20:28,910 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 18.034
2024-06-20 23:20:29,232 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 19.688
2024-06-20 23:20:29,656 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 25.486
2024-06-20 23:20:29,961 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 23.465
2024-06-20 23:20:30,353 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 29.117
2024-06-20 23:20:30,671 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 25.767
2024-06-20 23:20:31,106 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 21.331
2024-06-20 23:20:31,406 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 24.872
2024-06-20 23:20:31,802 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 40.529
2024-06-20 23:20:32,122 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 89.039
2024-06-20 23:20:32,562 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 135.424
2024-06-20 23:20:32,871 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 98.713
2024-06-20 23:20:33,275 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 36.960
2024-06-20 23:20:33,595 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 37.079
2024-06-20 23:20:34,030 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 22.031
2024-06-20 23:20:34,336 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 15.571
2024-06-20 23:20:34,731 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 18.158
2024-06-20 23:20:35,047 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 29.813
2024-06-20 23:20:35,478 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 30.625
2024-06-20 23:20:35,785 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 50.372
2024-06-20 23:20:36,170 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 60.958
2024-06-20 23:20:36,490 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 73.372
2024-06-20 23:20:36,907 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 55.125
2024-06-20 23:20:37,210 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 31.294
2024-06-20 23:20:37,586 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 25.276
2024-06-20 23:20:37,901 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 35.198
2024-06-20 23:20:38,115 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 21.158
2024-06-20 23:20:49,031 - INFO: 23/200 final results:
2024-06-20 23:20:49,031 - INFO: Training loss: 39.180.
2024-06-20 23:20:49,031 - INFO: Training MAE: 4.852.
2024-06-20 23:20:49,031 - INFO: Training MSE: 39.536.
2024-06-20 23:21:09,515 - INFO: Epoch: 23/200, Loss_train: 39.1795948291647, Loss_val: 21.044230526891248
2024-06-20 23:21:09,515 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:21:09,515 - INFO: Epoch 24/200...
2024-06-20 23:21:09,515 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:21:09,515 - INFO: Batch size: 32.
2024-06-20 23:21:09,518 - INFO: Dataset:
2024-06-20 23:21:09,518 - INFO: Batch size:
2024-06-20 23:21:09,518 - INFO: Number of workers:
2024-06-20 23:21:10,683 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 20.914
2024-06-20 23:21:11,005 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 16.381
2024-06-20 23:21:11,405 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 26.716
2024-06-20 23:21:11,724 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 22.335
2024-06-20 23:21:12,146 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 19.195
2024-06-20 23:21:12,464 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 26.155
2024-06-20 23:21:12,859 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 30.521
2024-06-20 23:21:13,178 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 33.452
2024-06-20 23:21:13,599 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 32.030
2024-06-20 23:21:13,913 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 24.773
2024-06-20 23:21:14,307 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 24.090
2024-06-20 23:21:14,627 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 31.177
2024-06-20 23:21:15,053 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 42.318
2024-06-20 23:21:15,373 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 41.445
2024-06-20 23:21:15,776 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 24.278
2024-06-20 23:21:16,092 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 17.933
2024-06-20 23:21:16,512 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 20.540
2024-06-20 23:21:16,829 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 13.308
2024-06-20 23:21:17,221 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 13.739
2024-06-20 23:21:17,534 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 17.174
2024-06-20 23:21:17,945 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 17.553
2024-06-20 23:21:18,263 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 16.362
2024-06-20 23:21:18,657 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 23.266
2024-06-20 23:21:18,976 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 25.364
2024-06-20 23:21:19,389 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 23.418
2024-06-20 23:21:19,704 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 19.996
2024-06-20 23:21:20,094 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 19.735
2024-06-20 23:21:20,408 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 20.431
2024-06-20 23:21:20,626 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 11.089
2024-06-20 23:21:31,303 - INFO: 24/200 final results:
2024-06-20 23:21:31,303 - INFO: Training loss: 23.300.
2024-06-20 23:21:31,303 - INFO: Training MAE: 3.828.
2024-06-20 23:21:31,303 - INFO: Training MSE: 23.541.
2024-06-20 23:21:51,577 - INFO: Epoch: 24/200, Loss_train: 23.299630099329455, Loss_val: 16.40978901961754
2024-06-20 23:21:51,633 - INFO: Saved new best metric model for epoch 24.
2024-06-20 23:21:51,634 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:21:51,634 - INFO: Epoch 25/200...
2024-06-20 23:21:51,634 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:21:51,634 - INFO: Batch size: 32.
2024-06-20 23:21:51,637 - INFO: Dataset:
2024-06-20 23:21:51,637 - INFO: Batch size:
2024-06-20 23:21:51,637 - INFO: Number of workers:
2024-06-20 23:21:52,760 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 15.071
2024-06-20 23:21:53,072 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 13.900
2024-06-20 23:21:53,484 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 13.795
2024-06-20 23:21:53,792 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 22.782
2024-06-20 23:21:54,212 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 45.408
2024-06-20 23:21:54,520 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 74.123
2024-06-20 23:21:54,926 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 78.874
2024-06-20 23:21:55,235 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 51.984
2024-06-20 23:21:55,653 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 47.123
2024-06-20 23:21:55,956 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 51.117
2024-06-20 23:21:56,366 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 24.833
2024-06-20 23:21:56,675 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 21.333
2024-06-20 23:21:57,104 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 25.199
2024-06-20 23:21:57,413 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 27.933
2024-06-20 23:21:57,838 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 35.735
2024-06-20 23:21:58,143 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 54.313
2024-06-20 23:21:58,555 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 82.367
2024-06-20 23:21:58,860 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 81.053
2024-06-20 23:21:59,278 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 50.653
2024-06-20 23:21:59,582 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 19.429
2024-06-20 23:21:59,988 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 14.300
2024-06-20 23:22:00,296 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 13.011
2024-06-20 23:22:00,729 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 21.276
2024-06-20 23:22:01,036 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 18.529
2024-06-20 23:22:01,440 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 17.800
2024-06-20 23:22:01,743 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 17.339
2024-06-20 23:22:02,165 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 16.340
2024-06-20 23:22:02,469 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 15.896
2024-06-20 23:22:02,685 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 21.799
2024-06-20 23:22:13,650 - INFO: 25/200 final results:
2024-06-20 23:22:13,650 - INFO: Training loss: 34.252.
2024-06-20 23:22:13,650 - INFO: Training MAE: 4.678.
2024-06-20 23:22:13,650 - INFO: Training MSE: 34.499.
2024-06-20 23:22:34,069 - INFO: Epoch: 25/200, Loss_train: 34.25227625616665, Loss_val: 27.314928153465534
2024-06-20 23:22:34,069 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:22:34,069 - INFO: Epoch 26/200...
2024-06-20 23:22:34,069 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:22:34,069 - INFO: Batch size: 32.
2024-06-20 23:22:34,072 - INFO: Dataset:
2024-06-20 23:22:34,072 - INFO: Batch size:
2024-06-20 23:22:34,072 - INFO: Number of workers:
2024-06-20 23:22:35,196 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 24.884
2024-06-20 23:22:35,508 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 22.150
2024-06-20 23:22:35,940 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 24.378
2024-06-20 23:22:36,251 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 29.086
2024-06-20 23:22:36,671 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 32.610
2024-06-20 23:22:36,979 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 24.724
2024-06-20 23:22:37,385 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 21.209
2024-06-20 23:22:37,693 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 20.110
2024-06-20 23:22:38,109 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 24.634
2024-06-20 23:22:38,413 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 23.364
2024-06-20 23:22:38,831 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 22.027
2024-06-20 23:22:39,140 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 24.318
2024-06-20 23:22:39,567 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 24.087
2024-06-20 23:22:39,875 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 29.656
2024-06-20 23:22:40,301 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 25.018
2024-06-20 23:22:40,606 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 28.761
2024-06-20 23:22:41,017 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 41.506
2024-06-20 23:22:41,323 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 28.720
2024-06-20 23:22:41,745 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 15.130
2024-06-20 23:22:42,048 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 15.370
2024-06-20 23:22:42,457 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 17.522
2024-06-20 23:22:42,765 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 22.776
2024-06-20 23:22:43,198 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 22.305
2024-06-20 23:22:43,506 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 27.409
2024-06-20 23:22:43,912 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 37.790
2024-06-20 23:22:44,216 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 57.680
2024-06-20 23:22:44,628 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 67.570
2024-06-20 23:22:44,928 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 44.536
2024-06-20 23:22:45,142 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 40.002
2024-06-20 23:22:56,159 - INFO: 26/200 final results:
2024-06-20 23:22:56,159 - INFO: Training loss: 28.942.
2024-06-20 23:22:56,159 - INFO: Training MAE: 4.213.
2024-06-20 23:22:56,159 - INFO: Training MSE: 28.724.
2024-06-20 23:23:16,155 - INFO: Epoch: 26/200, Loss_train: 28.942419019238702, Loss_val: 40.95774354605839
2024-06-20 23:23:16,155 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:23:16,155 - INFO: Epoch 27/200...
2024-06-20 23:23:16,155 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:23:16,155 - INFO: Batch size: 32.
2024-06-20 23:23:16,158 - INFO: Dataset:
2024-06-20 23:23:16,158 - INFO: Batch size:
2024-06-20 23:23:16,158 - INFO: Number of workers:
2024-06-20 23:23:17,295 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 41.263
2024-06-20 23:23:17,608 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 25.714
2024-06-20 23:23:18,042 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 18.066
2024-06-20 23:23:18,373 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 22.845
2024-06-20 23:23:18,770 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 30.607
2024-06-20 23:23:19,101 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 30.452
2024-06-20 23:23:19,492 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 20.105
2024-06-20 23:23:19,811 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 20.069
2024-06-20 23:23:20,215 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 25.456
2024-06-20 23:23:20,550 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 21.942
2024-06-20 23:23:20,946 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 15.856
2024-06-20 23:23:21,266 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 19.102
2024-06-20 23:23:21,668 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 26.134
2024-06-20 23:23:22,002 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 30.909
2024-06-20 23:23:22,399 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 31.043
2024-06-20 23:23:22,717 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 24.676
2024-06-20 23:23:23,128 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 25.266
2024-06-20 23:23:23,459 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 17.227
2024-06-20 23:23:23,850 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 18.744
2024-06-20 23:23:24,166 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 18.798
2024-06-20 23:23:24,561 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 22.143
2024-06-20 23:23:24,890 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 19.502
2024-06-20 23:23:25,269 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 25.273
2024-06-20 23:23:25,586 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 36.121
2024-06-20 23:23:25,977 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 34.454
2024-06-20 23:23:26,304 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 38.529
2024-06-20 23:23:26,682 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 31.344
2024-06-20 23:23:26,998 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 26.659
2024-06-20 23:23:27,210 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 29.274
2024-06-20 23:23:38,141 - INFO: 27/200 final results:
2024-06-20 23:23:38,141 - INFO: Training loss: 25.778.
2024-06-20 23:23:38,141 - INFO: Training MAE: 4.018.
2024-06-20 23:23:38,141 - INFO: Training MSE: 25.709.
2024-06-20 23:23:58,675 - INFO: Epoch: 27/200, Loss_train: 25.778320937321105, Loss_val: 43.82487343097555
2024-06-20 23:23:58,675 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:23:58,675 - INFO: Epoch 28/200...
2024-06-20 23:23:58,675 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:23:58,675 - INFO: Batch size: 32.
2024-06-20 23:23:58,678 - INFO: Dataset:
2024-06-20 23:23:58,678 - INFO: Batch size:
2024-06-20 23:23:58,678 - INFO: Number of workers:
2024-06-20 23:23:59,798 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 42.845
2024-06-20 23:24:00,120 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 37.340
2024-06-20 23:24:00,533 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 19.358
2024-06-20 23:24:00,854 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 27.386
2024-06-20 23:24:01,268 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 25.291
2024-06-20 23:24:01,571 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 28.256
2024-06-20 23:24:01,975 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 41.708
2024-06-20 23:24:02,293 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 40.385
2024-06-20 23:24:02,713 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 22.612
2024-06-20 23:24:03,016 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 25.229
2024-06-20 23:24:03,412 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 13.194
2024-06-20 23:24:03,731 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 17.074
2024-06-20 23:24:04,160 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 21.870
2024-06-20 23:24:04,469 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 35.980
2024-06-20 23:24:04,881 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 27.578
2024-06-20 23:24:05,197 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 20.168
2024-06-20 23:24:05,622 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 25.291
2024-06-20 23:24:05,927 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 17.097
2024-06-20 23:24:06,321 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 17.599
2024-06-20 23:24:06,635 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 18.398
2024-06-20 23:24:07,049 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 28.455
2024-06-20 23:24:07,353 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 39.760
2024-06-20 23:24:07,762 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 26.600
2024-06-20 23:24:08,081 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 34.979
2024-06-20 23:24:08,492 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 52.680
2024-06-20 23:24:08,793 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 66.935
2024-06-20 23:24:09,196 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 59.748
2024-06-20 23:24:09,512 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 17.106
2024-06-20 23:24:09,735 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 18.772
2024-06-20 23:24:20,740 - INFO: 28/200 final results:
2024-06-20 23:24:20,740 - INFO: Training loss: 29.989.
2024-06-20 23:24:20,740 - INFO: Training MAE: 4.267.
2024-06-20 23:24:20,740 - INFO: Training MSE: 30.211.
2024-06-20 23:24:40,922 - INFO: Epoch: 28/200, Loss_train: 29.98938537466115, Loss_val: 22.80927089164997
2024-06-20 23:24:40,922 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:24:40,922 - INFO: Epoch 29/200...
2024-06-20 23:24:40,922 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:24:40,922 - INFO: Batch size: 32.
2024-06-20 23:24:40,925 - INFO: Dataset:
2024-06-20 23:24:40,925 - INFO: Batch size:
2024-06-20 23:24:40,925 - INFO: Number of workers:
2024-06-20 23:24:42,038 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 27.932
2024-06-20 23:24:42,350 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 38.225
2024-06-20 23:24:42,755 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 15.895
2024-06-20 23:24:43,079 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 25.008
2024-06-20 23:24:43,503 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 23.934
2024-06-20 23:24:43,810 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 16.873
2024-06-20 23:24:44,206 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 21.549
2024-06-20 23:24:44,527 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 29.312
2024-06-20 23:24:44,951 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 25.239
2024-06-20 23:24:45,255 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 27.387
2024-06-20 23:24:45,651 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 29.276
2024-06-20 23:24:45,973 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 17.276
2024-06-20 23:24:46,398 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 21.048
2024-06-20 23:24:46,706 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 32.371
2024-06-20 23:24:47,107 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 24.440
2024-06-20 23:24:47,425 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 18.090
2024-06-20 23:24:47,852 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 18.371
2024-06-20 23:24:48,157 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 19.325
2024-06-20 23:24:48,549 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 19.582
2024-06-20 23:24:48,863 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 22.326
2024-06-20 23:24:49,288 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 23.392
2024-06-20 23:24:49,591 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 23.121
2024-06-20 23:24:49,974 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 11.779
2024-06-20 23:24:50,291 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 16.354
2024-06-20 23:24:50,706 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 31.025
2024-06-20 23:24:51,006 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 43.341
2024-06-20 23:24:51,377 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 32.644
2024-06-20 23:24:51,689 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 25.483
2024-06-20 23:24:51,899 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 37.614
2024-06-20 23:25:02,836 - INFO: 29/200 final results:
2024-06-20 23:25:02,836 - INFO: Training loss: 24.766.
2024-06-20 23:25:02,836 - INFO: Training MAE: 3.931.
2024-06-20 23:25:02,836 - INFO: Training MSE: 24.512.
2024-06-20 23:25:22,942 - INFO: Epoch: 29/200, Loss_train: 24.76589140398749, Loss_val: 54.630624836888806
2024-06-20 23:25:22,942 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:25:22,942 - INFO: Epoch 30/200...
2024-06-20 23:25:22,942 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:25:22,942 - INFO: Batch size: 32.
2024-06-20 23:25:22,945 - INFO: Dataset:
2024-06-20 23:25:22,945 - INFO: Batch size:
2024-06-20 23:25:22,945 - INFO: Number of workers:
2024-06-20 23:25:24,057 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 52.424
2024-06-20 23:25:24,382 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 27.879
2024-06-20 23:25:24,787 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 29.401
2024-06-20 23:25:25,110 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 29.730
2024-06-20 23:25:25,533 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 25.486
2024-06-20 23:25:25,853 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 21.298
2024-06-20 23:25:26,248 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 20.835
2024-06-20 23:25:26,567 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 14.327
2024-06-20 23:25:26,982 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 14.765
2024-06-20 23:25:27,296 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 20.655
2024-06-20 23:25:27,691 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 25.906
2024-06-20 23:25:28,011 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 42.809
2024-06-20 23:25:28,435 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 70.271
2024-06-20 23:25:28,754 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 57.760
2024-06-20 23:25:29,154 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 51.578
2024-06-20 23:25:29,469 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 65.690
2024-06-20 23:25:29,887 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 37.783
2024-06-20 23:25:30,202 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 38.702
2024-06-20 23:25:30,592 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 38.423
2024-06-20 23:25:30,905 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 19.633
2024-06-20 23:25:31,321 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 15.914
2024-06-20 23:25:31,638 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 19.349
2024-06-20 23:25:32,015 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 19.994
2024-06-20 23:25:32,331 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 16.714
2024-06-20 23:25:32,734 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 17.061
2024-06-20 23:25:33,048 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 19.570
2024-06-20 23:25:33,427 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 19.010
2024-06-20 23:25:33,739 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 15.743
2024-06-20 23:25:33,954 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 13.172
2024-06-20 23:25:44,804 - INFO: 30/200 final results:
2024-06-20 23:25:44,804 - INFO: Training loss: 29.720.
2024-06-20 23:25:44,804 - INFO: Training MAE: 4.325.
2024-06-20 23:25:44,804 - INFO: Training MSE: 30.047.
2024-06-20 23:26:04,985 - INFO: Epoch: 30/200, Loss_train: 29.72009020838244, Loss_val: 16.71727104844718
2024-06-20 23:26:04,985 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:26:04,985 - INFO: Epoch 31/200...
2024-06-20 23:26:04,985 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:26:04,985 - INFO: Batch size: 32.
2024-06-20 23:26:04,989 - INFO: Dataset:
2024-06-20 23:26:04,989 - INFO: Batch size:
2024-06-20 23:26:04,989 - INFO: Number of workers:
2024-06-20 23:26:06,119 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 19.302
2024-06-20 23:26:06,442 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 17.336
2024-06-20 23:26:06,843 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 18.670
2024-06-20 23:26:07,164 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 11.954
2024-06-20 23:26:07,580 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 16.805
2024-06-20 23:26:07,897 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 12.220
2024-06-20 23:26:08,288 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 12.904
2024-06-20 23:26:08,607 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 14.305
2024-06-20 23:26:09,022 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 13.508
2024-06-20 23:26:09,334 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 16.954
2024-06-20 23:26:09,725 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 18.817
2024-06-20 23:26:10,043 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 47.325
2024-06-20 23:26:10,458 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 85.996
2024-06-20 23:26:10,776 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 49.453
2024-06-20 23:26:11,182 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 46.925
2024-06-20 23:26:11,501 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 49.990
2024-06-20 23:26:11,923 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 71.212
2024-06-20 23:26:12,241 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 106.017
2024-06-20 23:26:12,636 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 64.576
2024-06-20 23:26:12,951 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 22.469
2024-06-20 23:26:13,368 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 23.976
2024-06-20 23:26:13,688 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 28.265
2024-06-20 23:26:14,070 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 22.862
2024-06-20 23:26:14,388 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 23.717
2024-06-20 23:26:14,788 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 18.029
2024-06-20 23:26:15,101 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 18.052
2024-06-20 23:26:15,473 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 19.973
2024-06-20 23:26:15,785 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 18.922
2024-06-20 23:26:16,001 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 25.278
2024-06-20 23:26:26,921 - INFO: 31/200 final results:
2024-06-20 23:26:26,921 - INFO: Training loss: 31.580.
2024-06-20 23:26:26,921 - INFO: Training MAE: 4.260.
2024-06-20 23:26:26,921 - INFO: Training MSE: 31.704.
2024-06-20 23:26:47,483 - INFO: Epoch: 31/200, Loss_train: 31.579706652411097, Loss_val: 19.333952574894347
2024-06-20 23:26:47,483 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:26:47,483 - INFO: Epoch 32/200...
2024-06-20 23:26:47,483 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:26:47,483 - INFO: Batch size: 32.
2024-06-20 23:26:47,486 - INFO: Dataset:
2024-06-20 23:26:47,486 - INFO: Batch size:
2024-06-20 23:26:47,487 - INFO: Number of workers:
2024-06-20 23:26:48,619 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 17.834
2024-06-20 23:26:48,929 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 18.070
2024-06-20 23:26:49,333 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 17.266
2024-06-20 23:26:49,654 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 15.104
2024-06-20 23:26:50,072 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 18.179
2024-06-20 23:26:50,377 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 16.107
2024-06-20 23:26:50,783 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 16.268
2024-06-20 23:26:51,101 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 13.778
2024-06-20 23:26:51,516 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 19.085
2024-06-20 23:26:51,817 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 15.989
2024-06-20 23:26:52,226 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 16.845
2024-06-20 23:26:52,544 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 14.901
2024-06-20 23:26:52,967 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 15.261
2024-06-20 23:26:53,276 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 13.076
2024-06-20 23:26:53,679 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 18.118
2024-06-20 23:26:53,998 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 12.102
2024-06-20 23:26:54,419 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 16.817
2024-06-20 23:26:54,725 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 13.169
2024-06-20 23:26:55,123 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 14.088
2024-06-20 23:26:55,439 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 12.826
2024-06-20 23:26:55,854 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 13.919
2024-06-20 23:26:56,158 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 11.939
2024-06-20 23:26:56,554 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 13.557
2024-06-20 23:26:56,870 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 13.274
2024-06-20 23:26:57,274 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 13.656
2024-06-20 23:26:57,575 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 19.989
2024-06-20 23:26:57,965 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 15.782
2024-06-20 23:26:58,278 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 12.474
2024-06-20 23:26:58,488 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 14.776
2024-06-20 23:27:09,377 - INFO: 32/200 final results:
2024-06-20 23:27:09,377 - INFO: Training loss: 15.319.
2024-06-20 23:27:09,377 - INFO: Training MAE: 3.103.
2024-06-20 23:27:09,377 - INFO: Training MSE: 15.330.
2024-06-20 23:27:29,542 - INFO: Epoch: 32/200, Loss_train: 15.318879028846478, Loss_val: 17.039303089010303
2024-06-20 23:27:29,542 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:27:29,542 - INFO: Epoch 33/200...
2024-06-20 23:27:29,542 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:27:29,542 - INFO: Batch size: 32.
2024-06-20 23:27:29,546 - INFO: Dataset:
2024-06-20 23:27:29,546 - INFO: Batch size:
2024-06-20 23:27:29,546 - INFO: Number of workers:
2024-06-20 23:27:30,668 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 16.893
2024-06-20 23:27:30,989 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 13.135
2024-06-20 23:27:31,403 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 13.962
2024-06-20 23:27:31,723 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 15.013
2024-06-20 23:27:32,137 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 13.781
2024-06-20 23:27:32,441 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 10.372
2024-06-20 23:27:32,851 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 14.590
2024-06-20 23:27:33,171 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 11.801
2024-06-20 23:27:33,593 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 13.389
2024-06-20 23:27:33,897 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 14.065
2024-06-20 23:27:34,293 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 11.509
2024-06-20 23:27:34,614 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 12.868
2024-06-20 23:27:35,041 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 15.158
2024-06-20 23:27:35,349 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 17.074
2024-06-20 23:27:35,763 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 13.275
2024-06-20 23:27:36,081 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 12.835
2024-06-20 23:27:36,499 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 19.462
2024-06-20 23:27:36,802 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 13.481
2024-06-20 23:27:37,193 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 11.628
2024-06-20 23:27:37,509 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 9.532
2024-06-20 23:27:37,928 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 16.868
2024-06-20 23:27:38,235 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 19.892
2024-06-20 23:27:38,640 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 23.709
2024-06-20 23:27:38,960 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 16.840
2024-06-20 23:27:39,363 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 10.967
2024-06-20 23:27:39,666 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 11.375
2024-06-20 23:27:40,052 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 11.740
2024-06-20 23:27:40,368 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 13.188
2024-06-20 23:27:40,589 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 11.603
2024-06-20 23:27:51,499 - INFO: 33/200 final results:
2024-06-20 23:27:51,499 - INFO: Training loss: 14.138.
2024-06-20 23:27:51,499 - INFO: Training MAE: 2.950.
2024-06-20 23:27:51,499 - INFO: Training MSE: 14.188.
2024-06-20 23:28:11,847 - INFO: Epoch: 33/200, Loss_train: 14.137971023033405, Loss_val: 13.350200685961493
2024-06-20 23:28:11,905 - INFO: Saved new best metric model for epoch 33.
2024-06-20 23:28:11,905 - INFO: Best internal validation val_loss: 13.350 at epoch: 33.
2024-06-20 23:28:11,905 - INFO: Epoch 34/200...
2024-06-20 23:28:11,905 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:28:11,905 - INFO: Batch size: 32.
2024-06-20 23:28:11,909 - INFO: Dataset:
2024-06-20 23:28:11,909 - INFO: Batch size:
2024-06-20 23:28:11,909 - INFO: Number of workers:
2024-06-20 23:28:13,044 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 17.177
2024-06-20 23:28:13,357 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 27.789
2024-06-20 23:28:13,773 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 21.303
2024-06-20 23:28:14,083 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 11.391
2024-06-20 23:28:14,505 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 15.314
2024-06-20 23:28:14,812 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 16.075
2024-06-20 23:28:15,222 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 9.614
2024-06-20 23:28:15,531 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 14.864
2024-06-20 23:28:15,957 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 15.637
2024-06-20 23:28:16,260 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 11.955
2024-06-20 23:28:16,673 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 12.560
2024-06-20 23:28:16,981 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 19.750
2024-06-20 23:28:17,408 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 20.724
2024-06-20 23:28:17,716 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 16.121
2024-06-20 23:28:18,132 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 19.406
2024-06-20 23:28:18,437 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 20.433
2024-06-20 23:28:18,857 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 14.604
2024-06-20 23:28:19,162 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 19.031
2024-06-20 23:28:19,578 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 18.113
2024-06-20 23:28:19,881 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 18.595
2024-06-20 23:28:20,299 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 15.581
2024-06-20 23:28:20,606 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 13.847
2024-06-20 23:28:21,026 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 10.161
2024-06-20 23:28:21,334 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 11.063
2024-06-20 23:28:21,748 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 15.314
2024-06-20 23:28:22,052 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 20.216
2024-06-20 23:28:22,455 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 15.214
2024-06-20 23:28:22,759 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 10.812
2024-06-20 23:28:22,977 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 8.585
2024-06-20 23:28:33,909 - INFO: 34/200 final results:
2024-06-20 23:28:33,909 - INFO: Training loss: 15.905.
2024-06-20 23:28:33,909 - INFO: Training MAE: 3.143.
2024-06-20 23:28:33,909 - INFO: Training MSE: 16.050.
2024-06-20 23:28:54,166 - INFO: Epoch: 34/200, Loss_train: 15.905151136990252, Loss_val: 11.42141763095198
2024-06-20 23:28:54,224 - INFO: Saved new best metric model for epoch 34.
2024-06-20 23:28:54,224 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:28:54,224 - INFO: Epoch 35/200...
2024-06-20 23:28:54,224 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:28:54,224 - INFO: Batch size: 32.
2024-06-20 23:28:54,227 - INFO: Dataset:
2024-06-20 23:28:54,227 - INFO: Batch size:
2024-06-20 23:28:54,228 - INFO: Number of workers:
2024-06-20 23:28:55,386 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 11.068
2024-06-20 23:28:55,695 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 11.089
2024-06-20 23:28:56,112 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 15.753
2024-06-20 23:28:56,724 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 15.993
2024-06-20 23:28:57,152 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 16.319
2024-06-20 23:28:57,457 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 19.446
2024-06-20 23:28:57,851 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 25.674
2024-06-20 23:28:58,168 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 27.378
2024-06-20 23:28:58,599 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 22.587
2024-06-20 23:28:58,900 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 14.521
2024-06-20 23:28:59,298 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 20.201
2024-06-20 23:28:59,614 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 20.820
2024-06-20 23:29:00,060 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 16.900
2024-06-20 23:29:00,367 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 19.115
2024-06-20 23:29:00,769 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 13.214
2024-06-20 23:29:01,072 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 12.157
2024-06-20 23:29:01,516 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 10.888
2024-06-20 23:29:01,819 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 15.713
2024-06-20 23:29:02,210 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 23.100
2024-06-20 23:29:02,510 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 26.532
2024-06-20 23:29:02,948 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 19.255
2024-06-20 23:29:03,251 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 10.492
2024-06-20 23:29:03,642 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 18.996
2024-06-20 23:29:03,946 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 24.582
2024-06-20 23:29:04,381 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 14.228
2024-06-20 23:29:04,682 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 16.135
2024-06-20 23:29:05,068 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 34.908
2024-06-20 23:29:05,369 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 53.395
2024-06-20 23:29:05,589 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 21.200
2024-06-20 23:29:16,195 - INFO: 35/200 final results:
2024-06-20 23:29:16,195 - INFO: Training loss: 19.712.
2024-06-20 23:29:16,195 - INFO: Training MAE: 3.445.
2024-06-20 23:29:16,195 - INFO: Training MSE: 19.683.
2024-06-20 23:29:36,244 - INFO: Epoch: 35/200, Loss_train: 19.712336046942347, Loss_val: 13.231776385471738
2024-06-20 23:29:36,244 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:29:36,244 - INFO: Epoch 36/200...
2024-06-20 23:29:36,244 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:29:36,244 - INFO: Batch size: 32.
2024-06-20 23:29:36,247 - INFO: Dataset:
2024-06-20 23:29:36,247 - INFO: Batch size:
2024-06-20 23:29:36,247 - INFO: Number of workers:
2024-06-20 23:29:37,409 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 11.899
2024-06-20 23:29:37,717 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 11.911
2024-06-20 23:29:38,117 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 10.638
2024-06-20 23:29:38,437 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 10.113
2024-06-20 23:29:38,884 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 14.660
2024-06-20 23:29:39,191 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 13.414
2024-06-20 23:29:39,587 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 11.361
2024-06-20 23:29:39,895 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 9.632
2024-06-20 23:29:40,349 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 8.882
2024-06-20 23:29:40,652 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 10.104
2024-06-20 23:29:41,052 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 15.983
2024-06-20 23:29:41,361 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 17.436
2024-06-20 23:29:41,808 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 25.699
2024-06-20 23:29:42,114 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 49.358
2024-06-20 23:29:42,513 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 62.331
2024-06-20 23:29:42,815 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 31.118
2024-06-20 23:29:43,264 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 13.490
2024-06-20 23:29:43,569 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 25.548
2024-06-20 23:29:43,963 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 14.273
2024-06-20 23:29:44,265 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 19.294
2024-06-20 23:29:44,710 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 17.196
2024-06-20 23:29:45,016 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 19.148
2024-06-20 23:29:45,411 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 19.402
2024-06-20 23:29:45,718 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 16.433
2024-06-20 23:29:46,156 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 10.392
2024-06-20 23:29:46,458 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 13.957
2024-06-20 23:29:46,847 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 10.978
2024-06-20 23:29:47,149 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 12.668
2024-06-20 23:29:47,371 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 16.250
2024-06-20 23:29:58,310 - INFO: 36/200 final results:
2024-06-20 23:29:58,310 - INFO: Training loss: 18.054.
2024-06-20 23:29:58,310 - INFO: Training MAE: 3.329.
2024-06-20 23:29:58,310 - INFO: Training MSE: 18.090.
2024-06-20 23:30:18,658 - INFO: Epoch: 36/200, Loss_train: 18.05408714557516, Loss_val: 30.23344901512409
2024-06-20 23:30:18,658 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:30:18,658 - INFO: Epoch 37/200...
2024-06-20 23:30:18,658 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:30:18,658 - INFO: Batch size: 32.
2024-06-20 23:30:18,661 - INFO: Dataset:
2024-06-20 23:30:18,662 - INFO: Batch size:
2024-06-20 23:30:18,662 - INFO: Number of workers:
2024-06-20 23:30:19,757 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 26.346
2024-06-20 23:30:20,078 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 36.694
2024-06-20 23:30:20,479 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 30.494
2024-06-20 23:30:20,797 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 14.500
2024-06-20 23:30:21,213 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 17.868
2024-06-20 23:30:21,518 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 19.288
2024-06-20 23:30:21,923 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 11.710
2024-06-20 23:30:22,242 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 14.892
2024-06-20 23:30:22,665 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 11.842
2024-06-20 23:30:22,967 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 11.413
2024-06-20 23:30:23,367 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 11.059
2024-06-20 23:30:23,686 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 15.257
2024-06-20 23:30:24,099 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 12.485
2024-06-20 23:30:24,407 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 12.395
2024-06-20 23:30:24,817 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 16.579
2024-06-20 23:30:25,133 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 16.672
2024-06-20 23:30:25,543 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 15.603
2024-06-20 23:30:25,848 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 12.644
2024-06-20 23:30:26,249 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 24.643
2024-06-20 23:30:26,564 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 38.113
2024-06-20 23:30:26,972 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 27.271
2024-06-20 23:30:27,276 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 24.505
2024-06-20 23:30:27,672 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 38.049
2024-06-20 23:30:27,989 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 28.015
2024-06-20 23:30:28,392 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 13.670
2024-06-20 23:30:28,695 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 11.691
2024-06-20 23:30:29,091 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 18.531
2024-06-20 23:30:29,407 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 19.782
2024-06-20 23:30:29,621 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 14.958
2024-06-20 23:30:40,617 - INFO: 37/200 final results:
2024-06-20 23:30:40,618 - INFO: Training loss: 19.551.
2024-06-20 23:30:40,618 - INFO: Training MAE: 3.467.
2024-06-20 23:30:40,618 - INFO: Training MSE: 19.641.
2024-06-20 23:31:00,868 - INFO: Epoch: 37/200, Loss_train: 19.550638593476393, Loss_val: 13.282214296275171
2024-06-20 23:31:00,868 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:31:00,868 - INFO: Epoch 38/200...
2024-06-20 23:31:00,868 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:31:00,868 - INFO: Batch size: 32.
2024-06-20 23:31:00,871 - INFO: Dataset:
2024-06-20 23:31:00,872 - INFO: Batch size:
2024-06-20 23:31:00,872 - INFO: Number of workers:
2024-06-20 23:31:02,004 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 9.771
2024-06-20 23:31:02,317 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 9.872
2024-06-20 23:31:02,724 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 9.771
2024-06-20 23:31:03,045 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 11.893
2024-06-20 23:31:03,461 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 14.384
2024-06-20 23:31:03,765 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 11.938
2024-06-20 23:31:04,169 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 11.915
2024-06-20 23:31:04,487 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 11.723
2024-06-20 23:31:04,906 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 15.053
2024-06-20 23:31:05,210 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 14.340
2024-06-20 23:31:05,616 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 10.506
2024-06-20 23:31:05,937 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 11.802
2024-06-20 23:31:06,364 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 13.902
2024-06-20 23:31:06,672 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 13.505
2024-06-20 23:31:07,085 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 12.993
2024-06-20 23:31:07,404 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 12.783
2024-06-20 23:31:07,826 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 13.129
2024-06-20 23:31:08,131 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 13.147
2024-06-20 23:31:08,536 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 14.854
2024-06-20 23:31:08,851 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 22.420
2024-06-20 23:31:09,268 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 18.080
2024-06-20 23:31:09,576 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 23.785
2024-06-20 23:31:09,981 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 35.887
2024-06-20 23:31:10,300 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 41.457
2024-06-20 23:31:10,715 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 41.529
2024-06-20 23:31:11,019 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 57.131
2024-06-20 23:31:11,424 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 48.610
2024-06-20 23:31:11,740 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 25.038
2024-06-20 23:31:11,964 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 19.379
2024-06-20 23:31:23,043 - INFO: 38/200 final results:
2024-06-20 23:31:23,043 - INFO: Training loss: 19.676.
2024-06-20 23:31:23,043 - INFO: Training MAE: 3.426.
2024-06-20 23:31:23,043 - INFO: Training MSE: 19.682.
2024-06-20 23:31:43,267 - INFO: Epoch: 38/200, Loss_train: 19.675778520518335, Loss_val: 14.400071374301252
2024-06-20 23:31:43,267 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:31:43,267 - INFO: Epoch 39/200...
2024-06-20 23:31:43,267 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:31:43,267 - INFO: Batch size: 32.
2024-06-20 23:31:43,270 - INFO: Dataset:
2024-06-20 23:31:43,271 - INFO: Batch size:
2024-06-20 23:31:43,271 - INFO: Number of workers:
2024-06-20 23:31:44,392 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 12.661
2024-06-20 23:31:44,715 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 15.247
2024-06-20 23:31:45,116 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 18.673
2024-06-20 23:31:45,437 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 11.153
2024-06-20 23:31:45,857 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 10.375
2024-06-20 23:31:46,176 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 12.198
2024-06-20 23:31:46,571 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 13.507
2024-06-20 23:31:46,891 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 14.012
2024-06-20 23:31:47,310 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 10.013
2024-06-20 23:31:47,624 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 11.724
2024-06-20 23:31:48,019 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 14.630
2024-06-20 23:31:48,339 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 14.129
2024-06-20 23:31:48,767 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 13.236
2024-06-20 23:31:49,086 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 14.015
2024-06-20 23:31:49,487 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 18.189
2024-06-20 23:31:49,803 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 22.975
2024-06-20 23:31:50,227 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 25.295
2024-06-20 23:31:50,543 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 15.491
2024-06-20 23:31:50,937 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 8.802
2024-06-20 23:31:51,251 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 8.823
2024-06-20 23:31:51,668 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 9.000
2024-06-20 23:31:51,985 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 9.625
2024-06-20 23:31:52,378 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 8.946
2024-06-20 23:31:52,695 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 8.448
2024-06-20 23:31:53,105 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 13.931
2024-06-20 23:31:53,419 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 12.067
2024-06-20 23:31:53,808 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 9.902
2024-06-20 23:31:54,121 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 11.093
2024-06-20 23:31:54,344 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 7.677
2024-06-20 23:32:05,211 - INFO: 39/200 final results:
2024-06-20 23:32:05,211 - INFO: Training loss: 12.960.
2024-06-20 23:32:05,211 - INFO: Training MAE: 2.838.
2024-06-20 23:32:05,211 - INFO: Training MSE: 13.064.
2024-06-20 23:32:25,533 - INFO: Epoch: 39/200, Loss_train: 12.959802068513016, Loss_val: 20.851204378851527
2024-06-20 23:32:25,533 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:32:25,533 - INFO: Epoch 40/200...
2024-06-20 23:32:25,533 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:32:25,533 - INFO: Batch size: 32.
2024-06-20 23:32:25,536 - INFO: Dataset:
2024-06-20 23:32:25,536 - INFO: Batch size:
2024-06-20 23:32:25,536 - INFO: Number of workers:
2024-06-20 23:32:26,663 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 19.806
2024-06-20 23:32:26,972 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 25.366
2024-06-20 23:32:27,387 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 40.137
2024-06-20 23:32:27,708 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 44.476
2024-06-20 23:32:28,125 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 27.014
2024-06-20 23:32:28,429 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 15.012
2024-06-20 23:32:28,836 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 31.430
2024-06-20 23:32:29,153 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 24.584
2024-06-20 23:32:29,571 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 28.770
2024-06-20 23:32:29,871 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 27.858
2024-06-20 23:32:30,282 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 12.613
2024-06-20 23:32:30,600 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 10.328
2024-06-20 23:32:31,023 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 13.093
2024-06-20 23:32:31,328 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 9.568
2024-06-20 23:32:31,741 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 10.938
2024-06-20 23:32:32,056 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 17.538
2024-06-20 23:32:32,476 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 8.886
2024-06-20 23:32:32,778 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 9.214
2024-06-20 23:32:33,182 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 16.059
2024-06-20 23:32:33,495 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 14.140
2024-06-20 23:32:33,909 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 11.468
2024-06-20 23:32:34,213 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 14.016
2024-06-20 23:32:34,619 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 11.725
2024-06-20 23:32:34,936 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 13.737
2024-06-20 23:32:35,347 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 11.237
2024-06-20 23:32:35,648 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 13.487
2024-06-20 23:32:36,048 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 13.516
2024-06-20 23:32:36,362 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 13.054
2024-06-20 23:32:36,583 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 7.909
2024-06-20 23:32:47,461 - INFO: 40/200 final results:
2024-06-20 23:32:47,461 - INFO: Training loss: 17.827.
2024-06-20 23:32:47,461 - INFO: Training MAE: 3.256.
2024-06-20 23:32:47,461 - INFO: Training MSE: 18.023.
2024-06-20 23:33:07,410 - INFO: Epoch: 40/200, Loss_train: 17.826813484060352, Loss_val: 9.685338661588471
2024-06-20 23:33:07,468 - INFO: Saved new best metric model for epoch 40.
2024-06-20 23:33:07,468 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:33:07,468 - INFO: Epoch 41/200...
2024-06-20 23:33:07,468 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:33:07,469 - INFO: Batch size: 32.
2024-06-20 23:33:07,472 - INFO: Dataset:
2024-06-20 23:33:07,472 - INFO: Batch size:
2024-06-20 23:33:07,472 - INFO: Number of workers:
2024-06-20 23:33:08,629 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 9.272
2024-06-20 23:33:08,942 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 18.348
2024-06-20 23:33:09,336 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 23.630
2024-06-20 23:33:09,660 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 25.312
2024-06-20 23:33:10,091 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 19.667
2024-06-20 23:33:10,398 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 10.231
2024-06-20 23:33:10,779 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 12.206
2024-06-20 23:33:11,099 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 13.375
2024-06-20 23:33:11,534 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 13.914
2024-06-20 23:33:11,838 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 12.865
2024-06-20 23:33:12,231 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 11.890
2024-06-20 23:33:12,552 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 9.121
2024-06-20 23:33:12,996 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 11.281
2024-06-20 23:33:13,305 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 11.638
2024-06-20 23:33:13,695 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 8.863
2024-06-20 23:33:14,013 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 10.237
2024-06-20 23:33:14,448 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 14.005
2024-06-20 23:33:14,754 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 17.252
2024-06-20 23:33:15,143 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 22.572
2024-06-20 23:33:15,460 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 18.556
2024-06-20 23:33:15,892 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 17.280
2024-06-20 23:33:16,199 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 22.279
2024-06-20 23:33:16,583 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 12.179
2024-06-20 23:33:16,903 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 11.972
2024-06-20 23:33:17,320 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 28.475
2024-06-20 23:33:17,623 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 64.945
2024-06-20 23:33:17,998 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 81.425
2024-06-20 23:33:18,314 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 22.628
2024-06-20 23:33:18,526 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 20.776
2024-06-20 23:33:29,473 - INFO: 41/200 final results:
2024-06-20 23:33:29,473 - INFO: Training loss: 19.869.
2024-06-20 23:33:29,473 - INFO: Training MAE: 3.452.
2024-06-20 23:33:29,473 - INFO: Training MSE: 19.851.
2024-06-20 23:33:49,953 - INFO: Epoch: 41/200, Loss_train: 19.86883965853987, Loss_val: 13.9575100931628
2024-06-20 23:33:49,953 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:33:49,953 - INFO: Epoch 42/200...
2024-06-20 23:33:49,953 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:33:49,953 - INFO: Batch size: 32.
2024-06-20 23:33:49,957 - INFO: Dataset:
2024-06-20 23:33:49,957 - INFO: Batch size:
2024-06-20 23:33:49,957 - INFO: Number of workers:
2024-06-20 23:33:51,107 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 13.470
2024-06-20 23:33:51,417 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 20.255
2024-06-20 23:33:51,802 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 13.420
2024-06-20 23:33:52,122 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 12.838
2024-06-20 23:33:52,551 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 24.433
2024-06-20 23:33:52,855 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 33.322
2024-06-20 23:33:53,237 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 31.719
2024-06-20 23:33:53,555 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 39.066
2024-06-20 23:33:53,979 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 25.768
2024-06-20 23:33:54,280 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 21.846
2024-06-20 23:33:54,657 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 11.210
2024-06-20 23:33:54,975 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 12.404
2024-06-20 23:33:55,410 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 15.209
2024-06-20 23:33:55,716 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 11.038
2024-06-20 23:33:56,100 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 14.008
2024-06-20 23:33:56,415 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 16.325
2024-06-20 23:33:56,844 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 7.401
2024-06-20 23:33:57,147 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 9.362
2024-06-20 23:33:57,522 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 11.771
2024-06-20 23:33:57,833 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 10.777
2024-06-20 23:33:58,259 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 12.919
2024-06-20 23:33:58,563 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 14.759
2024-06-20 23:33:58,946 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 16.304
2024-06-20 23:33:59,262 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 10.186
2024-06-20 23:33:59,672 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 9.129
2024-06-20 23:33:59,972 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 8.440
2024-06-20 23:34:00,342 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 6.772
2024-06-20 23:34:00,654 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 6.736
2024-06-20 23:34:00,866 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 6.882
2024-06-20 23:34:11,766 - INFO: 42/200 final results:
2024-06-20 23:34:11,766 - INFO: Training loss: 15.440.
2024-06-20 23:34:11,766 - INFO: Training MAE: 3.071.
2024-06-20 23:34:11,766 - INFO: Training MSE: 15.610.
2024-06-20 23:34:32,086 - INFO: Epoch: 42/200, Loss_train: 15.440301237435177, Loss_val: 11.163223003518992
2024-06-20 23:34:32,086 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:34:32,086 - INFO: Epoch 43/200...
2024-06-20 23:34:32,086 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:34:32,086 - INFO: Batch size: 32.
2024-06-20 23:34:32,089 - INFO: Dataset:
2024-06-20 23:34:32,089 - INFO: Batch size:
2024-06-20 23:34:32,089 - INFO: Number of workers:
2024-06-20 23:34:33,214 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 10.056
2024-06-20 23:34:33,540 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 14.272
2024-06-20 23:34:33,956 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 13.812
2024-06-20 23:34:34,280 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 16.078
2024-06-20 23:34:34,700 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 13.310
2024-06-20 23:34:35,007 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 14.101
2024-06-20 23:34:35,416 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 16.372
2024-06-20 23:34:35,738 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 19.748
2024-06-20 23:34:36,157 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 19.026
2024-06-20 23:34:36,461 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 26.680
2024-06-20 23:34:36,873 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 24.063
2024-06-20 23:34:37,194 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 7.374
2024-06-20 23:34:37,621 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 11.869
2024-06-20 23:34:37,929 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 13.609
2024-06-20 23:34:38,345 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 10.594
2024-06-20 23:34:38,663 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 10.621
2024-06-20 23:34:39,084 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 10.598
2024-06-20 23:34:39,390 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 9.360
2024-06-20 23:34:39,795 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 11.707
2024-06-20 23:34:40,110 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 10.684
2024-06-20 23:34:40,528 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 8.509
2024-06-20 23:34:40,835 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 7.194
2024-06-20 23:34:41,244 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 6.852
2024-06-20 23:34:41,563 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 7.546
2024-06-20 23:34:41,978 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 8.562
2024-06-20 23:34:42,281 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 15.886
2024-06-20 23:34:42,685 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 34.938
2024-06-20 23:34:43,001 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 61.400
2024-06-20 23:34:43,224 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 32.119
2024-06-20 23:34:54,201 - INFO: 43/200 final results:
2024-06-20 23:34:54,201 - INFO: Training loss: 16.101.
2024-06-20 23:34:54,201 - INFO: Training MAE: 3.034.
2024-06-20 23:34:54,201 - INFO: Training MSE: 15.785.
2024-06-20 23:35:14,690 - INFO: Epoch: 43/200, Loss_train: 16.101466935256433, Loss_val: 12.546308073504218
2024-06-20 23:35:14,690 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:35:14,690 - INFO: Epoch 44/200...
2024-06-20 23:35:14,690 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:35:14,690 - INFO: Batch size: 32.
2024-06-20 23:35:14,693 - INFO: Dataset:
2024-06-20 23:35:14,693 - INFO: Batch size:
2024-06-20 23:35:14,693 - INFO: Number of workers:
2024-06-20 23:35:15,846 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 14.342
2024-06-20 23:35:16,158 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 19.139
2024-06-20 23:35:16,560 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 19.595
2024-06-20 23:35:16,885 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 39.499
2024-06-20 23:35:17,320 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 59.712
2024-06-20 23:35:17,627 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 117.971
2024-06-20 23:35:18,016 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 80.747
2024-06-20 23:35:18,337 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 97.230
2024-06-20 23:35:18,764 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 109.314
2024-06-20 23:35:19,062 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 107.690
2024-06-20 23:35:19,454 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 243.727
2024-06-20 23:35:19,772 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 215.655
2024-06-20 23:35:20,209 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 128.997
2024-06-20 23:35:20,513 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 33.779
2024-06-20 23:35:20,906 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 35.323
2024-06-20 23:35:21,221 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 24.501
2024-06-20 23:35:21,653 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 30.406
2024-06-20 23:35:21,954 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 29.990
2024-06-20 23:35:22,340 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 22.180
2024-06-20 23:35:22,652 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 26.783
2024-06-20 23:35:23,077 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 19.524
2024-06-20 23:35:23,380 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 20.479
2024-06-20 23:35:23,771 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 14.778
2024-06-20 23:35:24,087 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 16.907
2024-06-20 23:35:24,509 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 13.248
2024-06-20 23:35:24,808 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 13.037
2024-06-20 23:35:25,195 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 13.876
2024-06-20 23:35:25,507 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 14.936
2024-06-20 23:35:25,725 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 12.695
2024-06-20 23:35:36,586 - INFO: 44/200 final results:
2024-06-20 23:35:36,586 - INFO: Training loss: 55.037.
2024-06-20 23:35:36,586 - INFO: Training MAE: 5.496.
2024-06-20 23:35:36,586 - INFO: Training MSE: 55.874.
2024-06-20 23:35:56,993 - INFO: Epoch: 44/200, Loss_train: 55.036530626231226, Loss_val: 18.276927652030157
2024-06-20 23:35:56,993 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:35:56,993 - INFO: Epoch 45/200...
2024-06-20 23:35:56,993 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:35:56,993 - INFO: Batch size: 32.
2024-06-20 23:35:56,997 - INFO: Dataset:
2024-06-20 23:35:56,997 - INFO: Batch size:
2024-06-20 23:35:56,997 - INFO: Number of workers:
2024-06-20 23:35:58,106 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 17.902
2024-06-20 23:35:58,429 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 20.778
2024-06-20 23:35:58,840 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 16.396
2024-06-20 23:35:59,162 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 12.697
2024-06-20 23:35:59,561 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 13.263
2024-06-20 23:35:59,878 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 11.088
2024-06-20 23:36:00,280 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 12.573
2024-06-20 23:36:00,600 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 8.845
2024-06-20 23:36:00,987 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 11.914
2024-06-20 23:36:01,304 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 11.100
2024-06-20 23:36:01,711 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 9.094
2024-06-20 23:36:02,031 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 12.060
2024-06-20 23:36:02,450 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 12.908
2024-06-20 23:36:02,770 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 11.349
2024-06-20 23:36:03,178 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 14.880
2024-06-20 23:36:03,495 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 12.034
2024-06-20 23:36:03,907 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 16.177
2024-06-20 23:36:04,224 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 16.963
2024-06-20 23:36:04,626 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 14.143
2024-06-20 23:36:04,940 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 13.489
2024-06-20 23:36:05,349 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 9.416
2024-06-20 23:36:05,667 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 6.739
2024-06-20 23:36:06,073 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 7.206
2024-06-20 23:36:06,392 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 6.655
2024-06-20 23:36:06,793 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 8.499
2024-06-20 23:36:07,108 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 10.226
2024-06-20 23:36:07,502 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 15.843
2024-06-20 23:36:07,817 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 15.470
2024-06-20 23:36:08,037 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 11.612
2024-06-20 23:36:18,690 - INFO: 45/200 final results:
2024-06-20 23:36:18,690 - INFO: Training loss: 12.459.
2024-06-20 23:36:18,690 - INFO: Training MAE: 2.782.
2024-06-20 23:36:18,690 - INFO: Training MSE: 12.476.
2024-06-20 23:36:38,982 - INFO: Epoch: 45/200, Loss_train: 12.459171196510052, Loss_val: 17.49535810536352
2024-06-20 23:36:38,982 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:36:38,982 - INFO: Epoch 46/200...
2024-06-20 23:36:38,982 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:36:38,982 - INFO: Batch size: 32.
2024-06-20 23:36:38,985 - INFO: Dataset:
2024-06-20 23:36:38,985 - INFO: Batch size:
2024-06-20 23:36:38,985 - INFO: Number of workers:
2024-06-20 23:36:40,124 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 16.893
2024-06-20 23:36:40,437 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 20.443
2024-06-20 23:36:40,841 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 6.152
2024-06-20 23:36:41,166 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 15.121
2024-06-20 23:36:41,598 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 18.410
2024-06-20 23:36:41,906 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 25.445
2024-06-20 23:36:42,298 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 38.905
2024-06-20 23:36:42,619 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 38.924
2024-06-20 23:36:43,050 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 22.041
2024-06-20 23:36:43,353 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 10.803
2024-06-20 23:36:43,744 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 14.907
2024-06-20 23:36:44,066 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 13.626
2024-06-20 23:36:44,502 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 11.913
2024-06-20 23:36:44,811 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 13.554
2024-06-20 23:36:45,212 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 9.584
2024-06-20 23:36:45,530 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 5.959
2024-06-20 23:36:45,960 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 9.309
2024-06-20 23:36:46,263 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 9.373
2024-06-20 23:36:46,654 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 14.098
2024-06-20 23:36:46,967 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 18.743
2024-06-20 23:36:47,389 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 15.669
2024-06-20 23:36:47,694 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 7.169
2024-06-20 23:36:48,074 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 8.598
2024-06-20 23:36:48,391 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 8.298
2024-06-20 23:36:48,810 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 7.202
2024-06-20 23:36:49,112 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 8.205
2024-06-20 23:36:49,488 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 8.948
2024-06-20 23:36:49,803 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 8.884
2024-06-20 23:36:50,022 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 4.218
2024-06-20 23:37:00,921 - INFO: 46/200 final results:
2024-06-20 23:37:00,921 - INFO: Training loss: 14.186.
2024-06-20 23:37:00,921 - INFO: Training MAE: 2.837.
2024-06-20 23:37:00,921 - INFO: Training MSE: 14.383.
2024-06-20 23:37:21,414 - INFO: Epoch: 46/200, Loss_train: 14.185986880598398, Loss_val: 8.871050925090396
2024-06-20 23:37:21,471 - INFO: Saved new best metric model for epoch 46.
2024-06-20 23:37:21,471 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:37:21,471 - INFO: Epoch 47/200...
2024-06-20 23:37:21,472 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:37:21,472 - INFO: Batch size: 32.
2024-06-20 23:37:21,475 - INFO: Dataset:
2024-06-20 23:37:21,475 - INFO: Batch size:
2024-06-20 23:37:21,475 - INFO: Number of workers:
2024-06-20 23:37:22,616 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 7.343
2024-06-20 23:37:22,927 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 10.923
2024-06-20 23:37:23,345 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 12.413
2024-06-20 23:37:23,667 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 9.687
2024-06-20 23:37:24,086 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 14.613
2024-06-20 23:37:24,393 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 28.439
2024-06-20 23:37:24,793 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 28.999
2024-06-20 23:37:25,115 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 15.074
2024-06-20 23:37:25,523 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 11.311
2024-06-20 23:37:25,826 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 9.499
2024-06-20 23:37:26,222 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 8.456
2024-06-20 23:37:26,544 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 9.611
2024-06-20 23:37:26,970 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 10.148
2024-06-20 23:37:27,279 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 7.922
2024-06-20 23:37:27,690 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 8.985
2024-06-20 23:37:28,008 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 10.926
2024-06-20 23:37:28,425 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 13.982
2024-06-20 23:37:28,731 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 14.224
2024-06-20 23:37:29,126 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 17.085
2024-06-20 23:37:29,442 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 17.038
2024-06-20 23:37:29,853 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 7.201
2024-06-20 23:37:30,160 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 7.931
2024-06-20 23:37:30,562 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 9.891
2024-06-20 23:37:30,881 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 7.703
2024-06-20 23:37:31,290 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 6.572
2024-06-20 23:37:31,590 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 9.810
2024-06-20 23:37:31,974 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 9.908
2024-06-20 23:37:32,287 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 17.974
2024-06-20 23:37:32,498 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 19.831
2024-06-20 23:37:43,370 - INFO: 47/200 final results:
2024-06-20 23:37:43,370 - INFO: Training loss: 12.534.
2024-06-20 23:37:43,370 - INFO: Training MAE: 2.728.
2024-06-20 23:37:43,370 - INFO: Training MSE: 12.390.
2024-06-20 23:38:03,855 - INFO: Epoch: 47/200, Loss_train: 12.53444607504483, Loss_val: 24.71023829229947
2024-06-20 23:38:03,856 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:38:03,856 - INFO: Epoch 48/200...
2024-06-20 23:38:03,856 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:38:03,856 - INFO: Batch size: 32.
2024-06-20 23:38:03,859 - INFO: Dataset:
2024-06-20 23:38:03,859 - INFO: Batch size:
2024-06-20 23:38:03,859 - INFO: Number of workers:
2024-06-20 23:38:04,990 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 22.045
2024-06-20 23:38:05,303 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 15.339
2024-06-20 23:38:05,726 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 12.963
2024-06-20 23:38:06,038 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 17.571
2024-06-20 23:38:06,459 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 14.765
2024-06-20 23:38:06,767 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 18.015
2024-06-20 23:38:07,182 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 13.374
2024-06-20 23:38:07,491 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 8.579
2024-06-20 23:38:07,909 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 9.108
2024-06-20 23:38:08,213 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 8.185
2024-06-20 23:38:08,650 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 10.164
2024-06-20 23:38:08,960 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 8.422
2024-06-20 23:38:09,389 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 7.520
2024-06-20 23:38:09,698 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 9.872
2024-06-20 23:38:10,132 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 13.495
2024-06-20 23:38:10,444 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 13.637
2024-06-20 23:38:10,848 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 8.167
2024-06-20 23:38:11,152 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 8.409
2024-06-20 23:38:11,576 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 8.619
2024-06-20 23:38:11,878 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 7.396
2024-06-20 23:38:12,286 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 5.431
2024-06-20 23:38:12,593 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 6.857
2024-06-20 23:38:13,026 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 8.689
2024-06-20 23:38:13,333 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 11.570
2024-06-20 23:38:13,736 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 11.998
2024-06-20 23:38:14,038 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 11.033
2024-06-20 23:38:14,459 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 14.114
2024-06-20 23:38:14,762 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 18.575
2024-06-20 23:38:14,971 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 21.241
2024-06-20 23:38:25,940 - INFO: 48/200 final results:
2024-06-20 23:38:25,940 - INFO: Training loss: 11.902.
2024-06-20 23:38:25,940 - INFO: Training MAE: 2.663.
2024-06-20 23:38:25,941 - INFO: Training MSE: 11.717.
2024-06-20 23:38:46,169 - INFO: Epoch: 48/200, Loss_train: 11.901887038658405, Loss_val: 24.771332115962586
2024-06-20 23:38:46,169 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:38:46,169 - INFO: Epoch 49/200...
2024-06-20 23:38:46,169 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:38:46,169 - INFO: Batch size: 32.
2024-06-20 23:38:46,172 - INFO: Dataset:
2024-06-20 23:38:46,172 - INFO: Batch size:
2024-06-20 23:38:46,173 - INFO: Number of workers:
2024-06-20 23:38:47,310 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 25.408
2024-06-20 23:38:47,623 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 23.600
2024-06-20 23:38:48,039 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 24.430
2024-06-20 23:38:48,364 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 26.506
2024-06-20 23:38:48,783 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 20.747
2024-06-20 23:38:49,089 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 11.505
2024-06-20 23:38:49,491 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 10.677
2024-06-20 23:38:49,812 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 12.553
2024-06-20 23:38:50,229 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 7.638
2024-06-20 23:38:50,532 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 7.996
2024-06-20 23:38:50,942 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 9.447
2024-06-20 23:38:51,262 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 6.653
2024-06-20 23:38:51,689 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 6.439
2024-06-20 23:38:51,997 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 10.069
2024-06-20 23:38:52,410 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 9.430
2024-06-20 23:38:52,727 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 11.243
2024-06-20 23:38:53,148 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 14.867
2024-06-20 23:38:53,452 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 20.827
2024-06-20 23:38:53,855 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 32.389
2024-06-20 23:38:54,169 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 26.995
2024-06-20 23:38:54,586 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 16.461
2024-06-20 23:38:54,893 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 14.123
2024-06-20 23:38:55,302 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 17.515
2024-06-20 23:38:55,622 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 11.489
2024-06-20 23:38:56,037 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 9.610
2024-06-20 23:38:56,340 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 10.725
2024-06-20 23:38:56,742 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 11.094
2024-06-20 23:38:57,058 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 7.227
2024-06-20 23:38:57,281 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 11.304
2024-06-20 23:39:08,254 - INFO: 49/200 final results:
2024-06-20 23:39:08,254 - INFO: Training loss: 14.792.
2024-06-20 23:39:08,254 - INFO: Training MAE: 2.995.
2024-06-20 23:39:08,254 - INFO: Training MSE: 14.861.
2024-06-20 23:39:28,484 - INFO: Epoch: 49/200, Loss_train: 14.792020140023068, Loss_val: 29.1626128163831
2024-06-20 23:39:28,484 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:39:28,484 - INFO: Epoch 50/200...
2024-06-20 23:39:28,484 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:39:28,484 - INFO: Batch size: 32.
2024-06-20 23:39:28,488 - INFO: Dataset:
2024-06-20 23:39:28,488 - INFO: Batch size:
2024-06-20 23:39:28,488 - INFO: Number of workers:
2024-06-20 23:39:29,616 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 18.736
2024-06-20 23:39:29,939 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 18.666
2024-06-20 23:39:30,336 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 15.359
2024-06-20 23:39:30,657 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 10.131
2024-06-20 23:39:31,085 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 8.311
2024-06-20 23:39:31,389 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 8.972
2024-06-20 23:39:31,766 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 7.607
2024-06-20 23:39:32,083 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 5.324
2024-06-20 23:39:32,515 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 8.614
2024-06-20 23:39:32,818 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 10.186
2024-06-20 23:39:33,202 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 10.111
2024-06-20 23:39:33,524 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 17.579
2024-06-20 23:39:33,962 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 26.903
2024-06-20 23:39:34,271 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 15.647
2024-06-20 23:39:34,665 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 8.032
2024-06-20 23:39:34,983 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 18.012
2024-06-20 23:39:35,412 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 29.325
2024-06-20 23:39:35,717 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 31.351
2024-06-20 23:39:36,097 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 14.757
2024-06-20 23:39:36,412 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 9.623
2024-06-20 23:39:36,839 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 10.622
2024-06-20 23:39:37,146 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 6.492
2024-06-20 23:39:37,528 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 10.239
2024-06-20 23:39:37,847 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 12.417
2024-06-20 23:39:38,262 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 15.102
2024-06-20 23:39:38,565 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 14.398
2024-06-20 23:39:38,941 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 12.634
2024-06-20 23:39:39,256 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 6.603
2024-06-20 23:39:39,468 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 7.212
2024-06-20 23:39:50,575 - INFO: 50/200 final results:
2024-06-20 23:39:50,575 - INFO: Training loss: 13.413.
2024-06-20 23:39:50,575 - INFO: Training MAE: 2.835.
2024-06-20 23:39:50,575 - INFO: Training MSE: 13.535.
2024-06-20 23:40:10,931 - INFO: Epoch: 50/200, Loss_train: 13.412553063754377, Loss_val: 11.72341325365264
2024-06-20 23:40:10,931 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:40:10,931 - INFO: Epoch 51/200...
2024-06-20 23:40:10,931 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:40:10,931 - INFO: Batch size: 32.
2024-06-20 23:40:10,934 - INFO: Dataset:
2024-06-20 23:40:10,934 - INFO: Batch size:
2024-06-20 23:40:10,934 - INFO: Number of workers:
2024-06-20 23:40:12,070 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 11.286
2024-06-20 23:40:12,383 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 7.198
2024-06-20 23:40:12,783 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 7.148
2024-06-20 23:40:13,108 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 11.229
2024-06-20 23:40:13,556 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 25.847
2024-06-20 23:40:13,861 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 26.802
2024-06-20 23:40:14,249 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 23.632
2024-06-20 23:40:14,556 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 23.397
2024-06-20 23:40:15,008 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 13.465
2024-06-20 23:40:15,311 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 10.172
2024-06-20 23:40:15,696 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 8.604
2024-06-20 23:40:16,004 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 7.337
2024-06-20 23:40:16,443 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 5.961
2024-06-20 23:40:16,752 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 5.771
2024-06-20 23:40:17,145 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 6.073
2024-06-20 23:40:17,450 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 8.352
2024-06-20 23:40:17,904 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 7.385
2024-06-20 23:40:18,210 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 7.633
2024-06-20 23:40:18,603 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 6.500
2024-06-20 23:40:18,906 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 7.005
2024-06-20 23:40:19,351 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 8.827
2024-06-20 23:40:19,657 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 13.995
2024-06-20 23:40:20,047 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 17.716
2024-06-20 23:40:20,353 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 21.161
2024-06-20 23:40:20,789 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 20.612
2024-06-20 23:40:21,090 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 15.272
2024-06-20 23:40:21,474 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 6.308
2024-06-20 23:40:21,776 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 5.904
2024-06-20 23:40:21,992 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 7.856
2024-06-20 23:40:32,876 - INFO: 51/200 final results:
2024-06-20 23:40:32,876 - INFO: Training loss: 12.015.
2024-06-20 23:40:32,876 - INFO: Training MAE: 2.632.
2024-06-20 23:40:32,876 - INFO: Training MSE: 12.098.
2024-06-20 23:40:53,204 - INFO: Epoch: 51/200, Loss_train: 12.015449112859265, Loss_val: 10.95815060056489
2024-06-20 23:40:53,204 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:40:53,204 - INFO: Epoch 52/200...
2024-06-20 23:40:53,204 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:40:53,204 - INFO: Batch size: 32.
2024-06-20 23:40:53,207 - INFO: Dataset:
2024-06-20 23:40:53,207 - INFO: Batch size:
2024-06-20 23:40:53,207 - INFO: Number of workers:
2024-06-20 23:40:54,323 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 10.859
2024-06-20 23:40:54,649 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 6.244
2024-06-20 23:40:55,065 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 10.514
2024-06-20 23:40:55,388 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 18.907
2024-06-20 23:40:55,793 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 23.361
2024-06-20 23:40:56,111 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 25.260
2024-06-20 23:40:56,517 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 40.621
2024-06-20 23:40:56,838 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 45.387
2024-06-20 23:40:57,236 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 20.065
2024-06-20 23:40:57,553 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 12.503
2024-06-20 23:40:57,956 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 11.457
2024-06-20 23:40:58,277 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 7.917
2024-06-20 23:40:58,690 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 12.940
2024-06-20 23:40:59,012 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 9.028
2024-06-20 23:40:59,421 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 10.002
2024-06-20 23:40:59,739 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 8.401
2024-06-20 23:41:00,146 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 7.621
2024-06-20 23:41:00,463 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 5.725
2024-06-20 23:41:00,866 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 8.093
2024-06-20 23:41:01,181 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 7.679
2024-06-20 23:41:01,584 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 8.320
2024-06-20 23:41:01,904 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 6.795
2024-06-20 23:41:02,300 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 6.053
2024-06-20 23:41:02,619 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 5.945
2024-06-20 23:41:03,009 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 6.206
2024-06-20 23:41:03,325 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 6.655
2024-06-20 23:41:03,714 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 7.507
2024-06-20 23:41:04,029 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 6.419
2024-06-20 23:41:04,242 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 5.439
2024-06-20 23:41:14,714 - INFO: 52/200 final results:
2024-06-20 23:41:14,714 - INFO: Training loss: 12.480.
2024-06-20 23:41:14,715 - INFO: Training MAE: 2.623.
2024-06-20 23:41:14,715 - INFO: Training MSE: 12.619.
2024-06-20 23:41:35,106 - INFO: Epoch: 52/200, Loss_train: 12.480043542796167, Loss_val: 8.29516052377635
2024-06-20 23:41:35,161 - INFO: Saved new best metric model for epoch 52.
2024-06-20 23:41:35,161 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:41:35,161 - INFO: Epoch 53/200...
2024-06-20 23:41:35,161 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:41:35,161 - INFO: Batch size: 32.
2024-06-20 23:41:35,164 - INFO: Dataset:
2024-06-20 23:41:35,165 - INFO: Batch size:
2024-06-20 23:41:35,165 - INFO: Number of workers:
2024-06-20 23:41:36,280 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 7.467
2024-06-20 23:41:36,604 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 6.768
2024-06-20 23:41:37,009 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 7.562
2024-06-20 23:41:37,332 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 7.378
2024-06-20 23:41:37,726 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 8.880
2024-06-20 23:41:38,043 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 13.018
2024-06-20 23:41:38,445 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 9.475
2024-06-20 23:41:38,763 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 10.970
2024-06-20 23:41:39,148 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 20.461
2024-06-20 23:41:39,461 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 43.769
2024-06-20 23:41:39,867 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 47.393
2024-06-20 23:41:40,188 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 22.832
2024-06-20 23:41:40,606 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 18.196
2024-06-20 23:41:40,928 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 41.999
2024-06-20 23:41:41,343 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 49.829
2024-06-20 23:41:41,662 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 32.890
2024-06-20 23:41:42,068 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 22.081
2024-06-20 23:41:42,386 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 19.241
2024-06-20 23:41:42,791 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 12.282
2024-06-20 23:41:43,108 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 11.508
2024-06-20 23:41:43,507 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 8.515
2024-06-20 23:41:43,828 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 9.460
2024-06-20 23:41:44,226 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 11.583
2024-06-20 23:41:44,546 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 6.062
2024-06-20 23:41:44,944 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 8.522
2024-06-20 23:41:45,260 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 6.850
2024-06-20 23:41:45,655 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 9.017
2024-06-20 23:41:45,971 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 10.241
2024-06-20 23:41:46,192 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 7.162
2024-06-20 23:41:57,149 - INFO: 53/200 final results:
2024-06-20 23:41:57,149 - INFO: Training loss: 16.945.
2024-06-20 23:41:57,149 - INFO: Training MAE: 3.086.
2024-06-20 23:41:57,149 - INFO: Training MSE: 17.139.
2024-06-20 23:42:17,280 - INFO: Epoch: 53/200, Loss_train: 16.94515790610478, Loss_val: 8.587418901509253
2024-06-20 23:42:17,281 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:42:17,281 - INFO: Epoch 54/200...
2024-06-20 23:42:17,281 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:42:17,281 - INFO: Batch size: 32.
2024-06-20 23:42:17,284 - INFO: Dataset:
2024-06-20 23:42:17,284 - INFO: Batch size:
2024-06-20 23:42:17,284 - INFO: Number of workers:
2024-06-20 23:42:18,401 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 8.919
2024-06-20 23:42:18,725 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 10.391
2024-06-20 23:42:19,147 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 6.388
2024-06-20 23:42:19,456 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 6.253
2024-06-20 23:42:19,859 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 9.557
2024-06-20 23:42:20,181 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 11.600
2024-06-20 23:42:20,602 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 12.274
2024-06-20 23:42:20,912 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 9.623
2024-06-20 23:42:21,316 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 8.432
2024-06-20 23:42:21,633 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 12.696
2024-06-20 23:42:22,059 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 9.000
2024-06-20 23:42:22,370 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 7.202
2024-06-20 23:42:22,785 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 7.864
2024-06-20 23:42:23,108 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 7.222
2024-06-20 23:42:23,547 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 8.310
2024-06-20 23:42:23,853 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 6.080
2024-06-20 23:42:24,251 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 6.077
2024-06-20 23:42:24,570 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 6.442
2024-06-20 23:42:25,000 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 6.724
2024-06-20 23:42:25,303 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 5.701
2024-06-20 23:42:25,698 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 6.921
2024-06-20 23:42:26,019 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 9.578
2024-06-20 23:42:26,438 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 9.546
2024-06-20 23:42:26,745 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 11.432
2024-06-20 23:42:27,125 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 15.755
2024-06-20 23:42:27,440 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 14.705
2024-06-20 23:42:27,845 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 10.078
2024-06-20 23:42:28,147 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 11.747
2024-06-20 23:42:28,352 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 21.429
2024-06-20 23:42:39,278 - INFO: 54/200 final results:
2024-06-20 23:42:39,278 - INFO: Training loss: 9.584.
2024-06-20 23:42:39,278 - INFO: Training MAE: 2.374.
2024-06-20 23:42:39,278 - INFO: Training MSE: 9.350.
2024-06-20 23:42:59,829 - INFO: Epoch: 54/200, Loss_train: 9.584306108540503, Loss_val: 25.11360010607489
2024-06-20 23:42:59,829 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:42:59,829 - INFO: Epoch 55/200...
2024-06-20 23:42:59,829 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:42:59,829 - INFO: Batch size: 32.
2024-06-20 23:42:59,832 - INFO: Dataset:
2024-06-20 23:42:59,833 - INFO: Batch size:
2024-06-20 23:42:59,833 - INFO: Number of workers:
2024-06-20 23:43:00,960 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 26.289
2024-06-20 23:43:01,270 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 24.019
2024-06-20 23:43:01,671 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 33.688
2024-06-20 23:43:01,992 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 43.233
2024-06-20 23:43:02,421 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 28.113
2024-06-20 23:43:02,725 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 19.139
2024-06-20 23:43:03,116 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 23.476
2024-06-20 23:43:03,433 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 10.362
2024-06-20 23:43:03,870 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 11.814
2024-06-20 23:43:04,170 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 12.012
2024-06-20 23:43:04,558 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 12.808
2024-06-20 23:43:04,877 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 12.928
2024-06-20 23:43:05,313 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 6.640
2024-06-20 23:43:05,619 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 8.791
2024-06-20 23:43:06,016 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 12.845
2024-06-20 23:43:06,330 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 9.889
2024-06-20 23:43:06,759 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 13.237
2024-06-20 23:43:07,061 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 12.867
2024-06-20 23:43:07,448 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 10.769
2024-06-20 23:43:07,764 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 7.503
2024-06-20 23:43:08,195 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 3.221
2024-06-20 23:43:08,503 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 5.244
2024-06-20 23:43:08,892 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 6.201
2024-06-20 23:43:09,213 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 7.150
2024-06-20 23:43:09,628 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 8.391
2024-06-20 23:43:09,931 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 9.914
2024-06-20 23:43:10,307 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 10.694
2024-06-20 23:43:10,623 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 11.342
2024-06-20 23:43:10,836 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 19.701
2024-06-20 23:43:21,769 - INFO: 55/200 final results:
2024-06-20 23:43:21,769 - INFO: Training loss: 14.561.
2024-06-20 23:43:21,769 - INFO: Training MAE: 2.859.
2024-06-20 23:43:21,769 - INFO: Training MSE: 14.460.
2024-06-20 23:43:41,692 - INFO: Epoch: 55/200, Loss_train: 14.56138693053147, Loss_val: 44.228185390603954
2024-06-20 23:43:41,692 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:43:41,692 - INFO: Epoch 56/200...
2024-06-20 23:43:41,692 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:43:41,693 - INFO: Batch size: 32.
2024-06-20 23:43:41,696 - INFO: Dataset:
2024-06-20 23:43:41,696 - INFO: Batch size:
2024-06-20 23:43:41,696 - INFO: Number of workers:
2024-06-20 23:43:42,816 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 36.906
2024-06-20 23:43:43,127 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 26.513
2024-06-20 23:43:43,528 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 18.114
2024-06-20 23:43:43,850 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 21.256
2024-06-20 23:43:44,286 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 14.992
2024-06-20 23:43:44,594 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 11.085
2024-06-20 23:43:44,987 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 7.676
2024-06-20 23:43:45,308 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 7.065
2024-06-20 23:43:45,745 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 6.557
2024-06-20 23:43:46,048 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 6.417
2024-06-20 23:43:46,451 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 6.814
2024-06-20 23:43:46,775 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 6.175
2024-06-20 23:43:47,204 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 10.292
2024-06-20 23:43:47,513 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 8.030
2024-06-20 23:43:47,916 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 9.464
2024-06-20 23:43:48,234 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 6.779
2024-06-20 23:43:48,671 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 7.072
2024-06-20 23:43:48,976 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 7.334
2024-06-20 23:43:49,372 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 8.877
2024-06-20 23:43:49,688 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 9.794
2024-06-20 23:43:50,122 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 9.555
2024-06-20 23:43:50,430 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 7.703
2024-06-20 23:43:50,829 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 7.429
2024-06-20 23:43:51,149 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 3.578
2024-06-20 23:43:51,578 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 5.933
2024-06-20 23:43:51,881 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 6.605
2024-06-20 23:43:52,270 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 6.756
2024-06-20 23:43:52,586 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 7.787
2024-06-20 23:43:52,809 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 12.246
2024-06-20 23:44:03,936 - INFO: 56/200 final results:
2024-06-20 23:44:03,936 - INFO: Training loss: 10.511.
2024-06-20 23:44:03,936 - INFO: Training MAE: 2.450.
2024-06-20 23:44:03,936 - INFO: Training MSE: 10.476.
2024-06-20 23:44:24,326 - INFO: Epoch: 56/200, Loss_train: 10.510510033574597, Loss_val: 20.745111564110065
2024-06-20 23:44:24,327 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:44:24,327 - INFO: Epoch 57/200...
2024-06-20 23:44:24,327 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:44:24,327 - INFO: Batch size: 32.
2024-06-20 23:44:24,330 - INFO: Dataset:
2024-06-20 23:44:24,330 - INFO: Batch size:
2024-06-20 23:44:24,330 - INFO: Number of workers:
2024-06-20 23:44:25,451 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 21.079
2024-06-20 23:44:25,762 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 24.297
2024-06-20 23:44:26,164 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 13.257
2024-06-20 23:44:26,487 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 7.747
2024-06-20 23:44:26,931 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 7.269
2024-06-20 23:44:27,236 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 8.793
2024-06-20 23:44:27,630 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 6.964
2024-06-20 23:44:27,937 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 5.995
2024-06-20 23:44:28,378 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 6.873
2024-06-20 23:44:28,680 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 9.058
2024-06-20 23:44:29,078 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 8.095
2024-06-20 23:44:29,386 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 19.210
2024-06-20 23:44:29,839 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 26.033
2024-06-20 23:44:30,146 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 27.749
2024-06-20 23:44:30,547 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 13.764
2024-06-20 23:44:30,850 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 9.843
2024-06-20 23:44:31,292 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 18.414
2024-06-20 23:44:31,596 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 21.220
2024-06-20 23:44:31,986 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 15.058
2024-06-20 23:44:32,287 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 7.587
2024-06-20 23:44:32,728 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 6.052
2024-06-20 23:44:33,034 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 7.548
2024-06-20 23:44:33,429 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 10.868
2024-06-20 23:44:33,734 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 16.108
2024-06-20 23:44:34,167 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 8.423
2024-06-20 23:44:34,468 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 7.411
2024-06-20 23:44:34,856 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 7.627
2024-06-20 23:44:35,156 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 10.883
2024-06-20 23:44:35,376 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 12.942
2024-06-20 23:44:46,237 - INFO: 57/200 final results:
2024-06-20 23:44:46,237 - INFO: Training loss: 12.626.
2024-06-20 23:44:46,237 - INFO: Training MAE: 2.743.
2024-06-20 23:44:46,237 - INFO: Training MSE: 12.620.
2024-06-20 23:45:06,622 - INFO: Epoch: 57/200, Loss_train: 12.62645252819719, Loss_val: 16.18989191384151
2024-06-20 23:45:06,623 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:45:06,623 - INFO: Epoch 58/200...
2024-06-20 23:45:06,623 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:45:06,623 - INFO: Batch size: 32.
2024-06-20 23:45:06,626 - INFO: Dataset:
2024-06-20 23:45:06,626 - INFO: Batch size:
2024-06-20 23:45:06,626 - INFO: Number of workers:
2024-06-20 23:45:07,747 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 13.424
2024-06-20 23:45:08,058 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 8.505
2024-06-20 23:45:08,459 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 5.771
2024-06-20 23:45:08,782 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 8.909
2024-06-20 23:45:09,222 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 10.421
2024-06-20 23:45:09,527 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 7.981
2024-06-20 23:45:09,917 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 7.819
2024-06-20 23:45:10,224 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 9.482
2024-06-20 23:45:10,676 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 15.089
2024-06-20 23:45:10,977 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 20.021
2024-06-20 23:45:11,369 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 19.140
2024-06-20 23:45:11,677 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 13.512
2024-06-20 23:45:12,127 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 8.050
2024-06-20 23:45:12,434 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 9.971
2024-06-20 23:45:12,831 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 9.030
2024-06-20 23:45:13,135 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 9.078
2024-06-20 23:45:13,576 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 4.532
2024-06-20 23:45:13,881 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 4.745
2024-06-20 23:45:14,271 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 6.973
2024-06-20 23:45:14,574 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 7.698
2024-06-20 23:45:15,014 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 7.064
2024-06-20 23:45:15,319 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 6.092
2024-06-20 23:45:15,708 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 8.906
2024-06-20 23:45:16,014 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 10.508
2024-06-20 23:45:16,443 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 7.356
2024-06-20 23:45:16,745 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 3.564
2024-06-20 23:45:17,117 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 3.291
2024-06-20 23:45:17,419 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 6.235
2024-06-20 23:45:17,630 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 11.115
2024-06-20 23:45:28,508 - INFO: 58/200 final results:
2024-06-20 23:45:28,509 - INFO: Training loss: 9.113.
2024-06-20 23:45:28,509 - INFO: Training MAE: 2.285.
2024-06-20 23:45:28,509 - INFO: Training MSE: 9.074.
2024-06-20 23:45:48,329 - INFO: Epoch: 58/200, Loss_train: 9.113219088521497, Loss_val: 11.091456742122256
2024-06-20 23:45:48,329 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:45:48,329 - INFO: Epoch 59/200...
2024-06-20 23:45:48,329 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:45:48,329 - INFO: Batch size: 32.
2024-06-20 23:45:48,332 - INFO: Dataset:
2024-06-20 23:45:48,332 - INFO: Batch size:
2024-06-20 23:45:48,332 - INFO: Number of workers:
2024-06-20 23:45:49,452 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 9.208
2024-06-20 23:45:49,777 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 6.606
2024-06-20 23:45:50,190 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 7.239
2024-06-20 23:45:50,517 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 7.702
2024-06-20 23:45:50,925 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 10.356
2024-06-20 23:45:51,247 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 19.639
2024-06-20 23:45:51,652 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 31.401
2024-06-20 23:45:51,973 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 31.164
2024-06-20 23:45:52,378 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 72.206
2024-06-20 23:45:52,695 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 88.140
2024-06-20 23:45:53,092 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 1203.665
2024-06-20 23:45:53,415 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 411.993
2024-06-20 23:45:53,829 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 1797.436
2024-06-20 23:45:54,151 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 865.872
2024-06-20 23:45:54,558 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 193.960
2024-06-20 23:45:54,877 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 88.193
2024-06-20 23:45:55,280 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 59.836
2024-06-20 23:45:55,598 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 43.473
2024-06-20 23:45:56,000 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 36.788
2024-06-20 23:45:56,316 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 28.796
2024-06-20 23:45:56,714 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 28.984
2024-06-20 23:45:57,031 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 32.067
2024-06-20 23:45:57,430 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 22.849
2024-06-20 23:45:57,747 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 22.502
2024-06-20 23:45:58,139 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 29.099
2024-06-20 23:45:58,451 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 23.946
2024-06-20 23:45:58,842 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 22.061
2024-06-20 23:45:59,155 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 18.436
2024-06-20 23:45:59,373 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 17.719
2024-06-20 23:46:10,245 - INFO: 59/200 final results:
2024-06-20 23:46:10,245 - INFO: Training loss: 180.391.
2024-06-20 23:46:10,245 - INFO: Training MAE: 8.012.
2024-06-20 23:46:10,245 - INFO: Training MSE: 183.609.
2024-06-20 23:46:30,728 - INFO: Epoch: 59/200, Loss_train: 180.39087471468696, Loss_val: 37.431474093733165
2024-06-20 23:46:30,728 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:46:30,728 - INFO: Epoch 60/200...
2024-06-20 23:46:30,728 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:46:30,728 - INFO: Batch size: 32.
2024-06-20 23:46:30,732 - INFO: Dataset:
2024-06-20 23:46:30,732 - INFO: Batch size:
2024-06-20 23:46:30,732 - INFO: Number of workers:
2024-06-20 23:46:31,873 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 35.483
2024-06-20 23:46:32,185 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 25.327
2024-06-20 23:46:32,602 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 22.366
2024-06-20 23:46:32,926 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 20.446
2024-06-20 23:46:33,338 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 19.723
2024-06-20 23:46:33,654 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 22.599
2024-06-20 23:46:34,065 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 17.539
2024-06-20 23:46:34,386 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 23.622
2024-06-20 23:46:34,785 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 20.348
2024-06-20 23:46:35,102 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 17.429
2024-06-20 23:46:35,517 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 12.638
2024-06-20 23:46:35,839 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 17.316
2024-06-20 23:46:36,253 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 18.131
2024-06-20 23:46:36,575 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 19.981
2024-06-20 23:46:36,993 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 21.316
2024-06-20 23:46:37,311 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 16.338
2024-06-20 23:46:37,715 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 16.911
2024-06-20 23:46:38,034 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 18.739
2024-06-20 23:46:38,442 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 19.817
2024-06-20 23:46:38,760 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 15.627
2024-06-20 23:46:39,167 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 19.315
2024-06-20 23:46:39,488 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 17.010
2024-06-20 23:46:39,893 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 27.650
2024-06-20 23:46:40,214 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 17.138
2024-06-20 23:46:40,612 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 16.329
2024-06-20 23:46:40,928 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 17.100
2024-06-20 23:46:41,323 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 10.840
2024-06-20 23:46:41,639 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 12.502
2024-06-20 23:46:41,855 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 18.175
2024-06-20 23:46:52,845 - INFO: 60/200 final results:
2024-06-20 23:46:52,846 - INFO: Training loss: 19.233.
2024-06-20 23:46:52,846 - INFO: Training MAE: 3.468.
2024-06-20 23:46:52,846 - INFO: Training MSE: 19.254.
2024-06-20 23:47:13,460 - INFO: Epoch: 60/200, Loss_train: 19.23291308304359, Loss_val: 18.14210184689226
2024-06-20 23:47:13,460 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:47:13,460 - INFO: Epoch 61/200...
2024-06-20 23:47:13,460 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:47:13,460 - INFO: Batch size: 32.
2024-06-20 23:47:13,463 - INFO: Dataset:
2024-06-20 23:47:13,463 - INFO: Batch size:
2024-06-20 23:47:13,463 - INFO: Number of workers:
2024-06-20 23:47:14,601 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 18.956
2024-06-20 23:47:14,911 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 18.340
2024-06-20 23:47:15,311 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 16.853
2024-06-20 23:47:15,633 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 12.339
2024-06-20 23:47:16,053 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 15.003
2024-06-20 23:47:16,369 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 16.085
2024-06-20 23:47:16,759 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 12.187
2024-06-20 23:47:17,077 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 10.850
2024-06-20 23:47:17,486 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 15.905
2024-06-20 23:47:17,798 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 16.388
2024-06-20 23:47:18,188 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 10.852
2024-06-20 23:47:18,506 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 12.447
2024-06-20 23:47:18,931 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 16.039
2024-06-20 23:47:19,249 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 13.407
2024-06-20 23:47:19,645 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 12.859
2024-06-20 23:47:19,959 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 12.546
2024-06-20 23:47:20,367 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 13.577
2024-06-20 23:47:20,681 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 11.642
2024-06-20 23:47:21,067 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 11.789
2024-06-20 23:47:21,377 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 11.134
2024-06-20 23:47:21,785 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 15.826
2024-06-20 23:47:22,102 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 13.220
2024-06-20 23:47:22,491 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 10.452
2024-06-20 23:47:22,807 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 10.316
2024-06-20 23:47:23,217 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 12.363
2024-06-20 23:47:23,529 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 10.991
2024-06-20 23:47:23,915 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 14.552
2024-06-20 23:47:24,227 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 13.460
2024-06-20 23:47:24,443 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 8.481
2024-06-20 23:47:35,404 - INFO: 61/200 final results:
2024-06-20 23:47:35,405 - INFO: Training loss: 13.409.
2024-06-20 23:47:35,405 - INFO: Training MAE: 2.896.
2024-06-20 23:47:35,405 - INFO: Training MSE: 13.506.
2024-06-20 23:47:55,341 - INFO: Epoch: 61/200, Loss_train: 13.408825874328613, Loss_val: 12.173755810178559
2024-06-20 23:47:55,341 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:47:55,341 - INFO: Epoch 62/200...
2024-06-20 23:47:55,341 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:47:55,341 - INFO: Batch size: 32.
2024-06-20 23:47:55,344 - INFO: Dataset:
2024-06-20 23:47:55,344 - INFO: Batch size:
2024-06-20 23:47:55,344 - INFO: Number of workers:
2024-06-20 23:47:56,457 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 11.396
2024-06-20 23:47:56,783 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 15.690
2024-06-20 23:47:57,204 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 10.155
2024-06-20 23:47:57,529 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 11.876
2024-06-20 23:47:57,942 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 12.008
2024-06-20 23:47:58,261 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 9.112
2024-06-20 23:47:58,670 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 9.670
2024-06-20 23:47:58,991 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 9.736
2024-06-20 23:47:59,396 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 10.821
2024-06-20 23:47:59,712 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 12.925
2024-06-20 23:48:00,120 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 10.143
2024-06-20 23:48:00,443 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 10.590
2024-06-20 23:48:00,861 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 7.351
2024-06-20 23:48:01,183 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 10.103
2024-06-20 23:48:01,598 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 8.012
2024-06-20 23:48:01,916 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 10.232
2024-06-20 23:48:02,331 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 8.042
2024-06-20 23:48:02,649 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 10.668
2024-06-20 23:48:03,054 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 10.565
2024-06-20 23:48:03,370 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 13.550
2024-06-20 23:48:03,779 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 10.933
2024-06-20 23:48:04,100 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 8.989
2024-06-20 23:48:04,505 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 7.964
2024-06-20 23:48:04,826 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 7.453
2024-06-20 23:48:05,226 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 9.615
2024-06-20 23:48:05,542 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 8.326
2024-06-20 23:48:05,938 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 9.174
2024-06-20 23:48:06,254 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 7.408
2024-06-20 23:48:06,472 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 8.730
2024-06-20 23:48:17,473 - INFO: 62/200 final results:
2024-06-20 23:48:17,473 - INFO: Training loss: 10.043.
2024-06-20 23:48:17,473 - INFO: Training MAE: 2.461.
2024-06-20 23:48:17,473 - INFO: Training MSE: 10.069.
2024-06-20 23:48:37,995 - INFO: Epoch: 62/200, Loss_train: 10.042655320003115, Loss_val: 11.290961725958462
2024-06-20 23:48:37,995 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:48:37,995 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
2024-06-20 23:49:21,291 - INFO: Experiment has been saved in 20240620_234921_341_Dual_DCNN_LReLu_0_52_tr_7.707_val_8.295_test_8.833 folder.
2024-06-20 23:49:21,291 - INFO: Fold: 1
2024-06-20 23:49:21,292 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-20 23:49:21,292 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-20 23:49:21,292 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-20 23:49:21,423 - INFO: To_device: False.
2024-06-20 23:49:21,425 - INFO: Transformers have been made successfully.
2024-06-20 23:49:21,425 - INFO: Dataset type: cache.
2024-06-20 23:49:21,425 - INFO: Dataloader type: standard.
2024-06-20 23:51:13,077 - INFO: Train dataloader arguments.
2024-06-20 23:51:13,077 - INFO: 	Batch_size: 32.
2024-06-20 23:51:13,077 - INFO: 	Shuffle: True.
2024-06-20 23:51:13,077 - INFO: 	Sampler: None.
2024-06-20 23:51:13,077 - INFO: 	Num_workers: 4.
2024-06-20 23:51:13,077 - INFO: 	Drop_last: False.
2024-06-20 23:51:13,359 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-20 23:51:13,365 - INFO: Weight init name: kaiming_uniform.
2024-06-20 23:51:14,097 - INFO: Number of training iterations per epoch: 29.
2024-06-20 23:51:14,097 - INFO: Epoch 1/200...
2024-06-20 23:51:14,097 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:51:14,097 - INFO: Batch size: 32.
2024-06-20 23:51:14,097 - INFO: Dataset:
2024-06-20 23:51:14,097 - INFO: Batch size:
2024-06-20 23:51:14,097 - INFO: Number of workers:
2024-06-20 23:51:15,396 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 67.050
2024-06-20 23:51:15,707 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 83.691
2024-06-20 23:51:16,105 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 6292.456
2024-06-20 23:51:16,415 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 14667.340
2024-06-20 23:51:16,842 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 214302.594
2024-06-20 23:51:17,146 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 65004.406
2024-06-20 23:51:17,536 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 1335399.375
2024-06-20 23:51:17,853 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 55747.531
2024-06-20 23:51:18,271 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 5209.903
2024-06-20 23:51:18,569 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 6437.458
2024-06-20 23:51:18,953 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 2262.283
2024-06-20 23:51:19,272 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 1658.260
2024-06-20 23:51:19,722 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 848.828
2024-06-20 23:51:20,028 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 447.490
2024-06-20 23:51:20,425 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 501.350
2024-06-20 23:51:20,741 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 433.440
2024-06-20 23:51:21,176 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 291.132
2024-06-20 23:51:21,482 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 425.515
2024-06-20 23:51:21,872 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 507.834
2024-06-20 23:51:22,186 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 260.094
2024-06-20 23:51:22,611 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 565.068
2024-06-20 23:51:22,917 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 734.638
2024-06-20 23:51:23,305 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 219.593
2024-06-20 23:51:23,624 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 336.619
2024-06-20 23:51:24,044 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 489.876
2024-06-20 23:51:24,345 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 310.695
2024-06-20 23:51:24,720 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 263.761
2024-06-20 23:51:25,034 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 214.571
2024-06-20 23:51:25,253 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 227.424
2024-06-20 23:51:36,240 - INFO: 1/200 final results:
2024-06-20 23:51:36,241 - INFO: Training loss: 59110.699.
2024-06-20 23:51:36,241 - INFO: Training MAE: 88.217.
2024-06-20 23:51:36,241 - INFO: Training MSE: 60275.430.
2024-06-20 23:51:56,610 - INFO: Epoch: 1/200, Loss_train: 59110.69918454927, Loss_val: 3954.8401426117994
2024-06-20 23:51:56,610 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-20 23:51:56,610 - INFO: Epoch 2/200...
2024-06-20 23:51:56,610 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:51:56,610 - INFO: Batch size: 32.
2024-06-20 23:51:56,614 - INFO: Dataset:
2024-06-20 23:51:56,614 - INFO: Batch size:
2024-06-20 23:51:56,614 - INFO: Number of workers:
2024-06-20 23:51:57,912 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 481.744
2024-06-20 23:51:58,223 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 196.455
2024-06-20 23:51:58,627 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 202.947
2024-06-20 23:51:58,949 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 224.208
2024-06-20 23:51:59,387 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 312.840
2024-06-20 23:51:59,691 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 251.052
2024-06-20 23:52:00,084 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 458.505
2024-06-20 23:52:00,390 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 278.487
2024-06-20 23:52:00,826 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 297.977
2024-06-20 23:52:01,123 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 230.355
2024-06-20 23:52:01,517 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 667.074
2024-06-20 23:52:01,823 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 683.758
2024-06-20 23:52:02,265 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 150.687
2024-06-20 23:52:02,572 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 161.096
2024-06-20 23:52:02,974 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 217.146
2024-06-20 23:52:03,277 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 196.218
2024-06-20 23:52:03,714 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 411.878
2024-06-20 23:52:04,017 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 460.950
2024-06-20 23:52:04,410 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 255.401
2024-06-20 23:52:04,709 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 238.854
2024-06-20 23:52:05,140 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 324.517
2024-06-20 23:52:05,445 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 431.691
2024-06-20 23:52:05,838 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 578.530
2024-06-20 23:52:06,143 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 233.190
2024-06-20 23:52:06,577 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 199.916
2024-06-20 23:52:06,880 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 188.380
2024-06-20 23:52:07,264 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 205.173
2024-06-20 23:52:07,567 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 119.561
2024-06-20 23:52:07,791 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 109.907
2024-06-20 23:52:18,790 - INFO: 2/200 final results:
2024-06-20 23:52:18,790 - INFO: Training loss: 302.362.
2024-06-20 23:52:18,790 - INFO: Training MAE: 13.483.
2024-06-20 23:52:18,790 - INFO: Training MSE: 306.169.
2024-06-20 23:52:39,367 - INFO: Epoch: 2/200, Loss_train: 302.3620216106546, Loss_val: 147.87262068123653
2024-06-20 23:52:39,415 - INFO: Saved new best metric model for epoch 2.
2024-06-20 23:52:39,416 - INFO: Best internal validation val_loss: 147.873 at epoch: 2.
2024-06-20 23:52:39,416 - INFO: Epoch 3/200...
2024-06-20 23:52:39,416 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:52:39,416 - INFO: Batch size: 32.
2024-06-20 23:52:39,420 - INFO: Dataset:
2024-06-20 23:52:39,420 - INFO: Batch size:
2024-06-20 23:52:39,420 - INFO: Number of workers:
2024-06-20 23:52:40,686 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 146.799
2024-06-20 23:52:40,999 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 145.960
2024-06-20 23:52:41,413 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 155.863
2024-06-20 23:52:41,738 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 188.510
2024-06-20 23:52:42,167 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 261.963
2024-06-20 23:52:42,473 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 262.615
2024-06-20 23:52:42,863 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 279.423
2024-06-20 23:52:43,182 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 1121.487
2024-06-20 23:52:43,605 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 1150.380
2024-06-20 23:52:43,905 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 203.081
2024-06-20 23:52:44,291 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 212.251
2024-06-20 23:52:44,614 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 459.233
2024-06-20 23:52:45,049 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 224.364
2024-06-20 23:52:45,358 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 178.324
2024-06-20 23:52:45,756 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 131.960
2024-06-20 23:52:46,075 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 133.022
2024-06-20 23:52:46,504 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 183.319
2024-06-20 23:52:46,810 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 287.014
2024-06-20 23:52:47,198 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 281.678
2024-06-20 23:52:47,511 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 185.908
2024-06-20 23:52:47,936 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 118.300
2024-06-20 23:52:48,243 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 157.115
2024-06-20 23:52:48,627 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 115.825
2024-06-20 23:52:48,946 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 96.339
2024-06-20 23:52:49,358 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 156.935
2024-06-20 23:52:49,658 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 99.069
2024-06-20 23:52:50,027 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 119.425
2024-06-20 23:52:50,338 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 134.493
2024-06-20 23:52:50,551 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 152.853
2024-06-20 23:53:01,276 - INFO: 3/200 final results:
2024-06-20 23:53:01,276 - INFO: Training loss: 253.224.
2024-06-20 23:53:01,276 - INFO: Training MAE: 12.001.
2024-06-20 23:53:01,276 - INFO: Training MSE: 255.210.
2024-06-20 23:53:21,654 - INFO: Epoch: 3/200, Loss_train: 253.22436996986127, Loss_val: 177.19961074302935
2024-06-20 23:53:21,654 - INFO: Best internal validation val_loss: 147.873 at epoch: 2.
2024-06-20 23:53:21,654 - INFO: Epoch 4/200...
2024-06-20 23:53:21,654 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:53:21,654 - INFO: Batch size: 32.
2024-06-20 23:53:21,658 - INFO: Dataset:
2024-06-20 23:53:21,658 - INFO: Batch size:
2024-06-20 23:53:21,658 - INFO: Number of workers:
2024-06-20 23:53:22,891 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 153.259
2024-06-20 23:53:23,216 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 208.633
2024-06-20 23:53:23,634 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 195.761
2024-06-20 23:53:23,957 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 85.283
2024-06-20 23:53:24,369 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 181.800
2024-06-20 23:53:24,673 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 130.743
2024-06-20 23:53:25,071 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 90.533
2024-06-20 23:53:25,388 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 99.979
2024-06-20 23:53:25,787 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 177.124
2024-06-20 23:53:26,089 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 210.627
2024-06-20 23:53:26,493 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 126.414
2024-06-20 23:53:26,815 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 73.043
2024-06-20 23:53:27,248 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 77.313
2024-06-20 23:53:27,558 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 81.537
2024-06-20 23:53:27,969 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 92.318
2024-06-20 23:53:28,287 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 211.155
2024-06-20 23:53:28,714 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 269.172
2024-06-20 23:53:29,019 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 123.886
2024-06-20 23:53:29,418 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 93.951
2024-06-20 23:53:29,733 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 212.703
2024-06-20 23:53:30,156 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 228.082
2024-06-20 23:53:30,464 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 106.733
2024-06-20 23:53:30,874 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 103.638
2024-06-20 23:53:31,195 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 102.567
2024-06-20 23:53:31,610 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 91.552
2024-06-20 23:53:31,913 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 79.568
2024-06-20 23:53:32,311 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 102.528
2024-06-20 23:53:32,627 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 393.806
2024-06-20 23:53:32,852 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 240.892
2024-06-20 23:53:43,874 - INFO: 4/200 final results:
2024-06-20 23:53:43,874 - INFO: Training loss: 149.814.
2024-06-20 23:53:43,874 - INFO: Training MAE: 9.345.
2024-06-20 23:53:43,874 - INFO: Training MSE: 148.012.
2024-06-20 23:54:04,280 - INFO: Epoch: 4/200, Loss_train: 149.8138709232725, Loss_val: 124.85160590862405
2024-06-20 23:54:04,328 - INFO: Saved new best metric model for epoch 4.
2024-06-20 23:54:04,328 - INFO: Best internal validation val_loss: 124.852 at epoch: 4.
2024-06-20 23:54:04,328 - INFO: Epoch 5/200...
2024-06-20 23:54:04,329 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:54:04,329 - INFO: Batch size: 32.
2024-06-20 23:54:04,333 - INFO: Dataset:
2024-06-20 23:54:04,333 - INFO: Batch size:
2024-06-20 23:54:04,333 - INFO: Number of workers:
2024-06-20 23:54:05,603 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 72.932
2024-06-20 23:54:05,914 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 111.040
2024-06-20 23:54:06,328 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 82.589
2024-06-20 23:54:06,654 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 89.608
2024-06-20 23:54:07,073 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 217.049
2024-06-20 23:54:07,380 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 149.830
2024-06-20 23:54:07,776 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 62.704
2024-06-20 23:54:08,096 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 316.348
2024-06-20 23:54:08,498 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 197.671
2024-06-20 23:54:08,800 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 177.380
2024-06-20 23:54:09,199 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 151.082
2024-06-20 23:54:09,521 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 135.120
2024-06-20 23:54:09,953 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 197.940
2024-06-20 23:54:10,263 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 259.456
2024-06-20 23:54:10,672 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 153.303
2024-06-20 23:54:10,988 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 99.785
2024-06-20 23:54:11,705 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 76.850
2024-06-20 23:54:12,008 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 70.405
2024-06-20 23:54:12,402 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 105.539
2024-06-20 23:54:12,713 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 147.561
2024-06-20 23:54:13,130 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 151.995
2024-06-20 23:54:13,436 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 125.564
2024-06-20 23:54:13,839 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 85.495
2024-06-20 23:54:14,157 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 84.657
2024-06-20 23:54:14,570 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 100.464
2024-06-20 23:54:14,873 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 92.594
2024-06-20 23:54:15,261 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 78.708
2024-06-20 23:54:15,577 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 244.707
2024-06-20 23:54:15,789 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 505.666
2024-06-20 23:54:26,554 - INFO: 5/200 final results:
2024-06-20 23:54:26,554 - INFO: Training loss: 149.795.
2024-06-20 23:54:26,554 - INFO: Training MAE: 9.145.
2024-06-20 23:54:26,554 - INFO: Training MSE: 142.755.
2024-06-20 23:54:47,154 - INFO: Epoch: 5/200, Loss_train: 149.79458762859477, Loss_val: 838.083582384833
2024-06-20 23:54:47,154 - INFO: Best internal validation val_loss: 124.852 at epoch: 4.
2024-06-20 23:54:47,154 - INFO: Epoch 6/200...
2024-06-20 23:54:47,154 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:54:47,154 - INFO: Batch size: 32.
2024-06-20 23:54:47,158 - INFO: Dataset:
2024-06-20 23:54:47,159 - INFO: Batch size:
2024-06-20 23:54:47,159 - INFO: Number of workers:
2024-06-20 23:54:48,431 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 490.355
2024-06-20 23:54:48,743 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 125.240
2024-06-20 23:54:49,133 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 201.145
2024-06-20 23:54:49,455 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 117.719
2024-06-20 23:54:49,878 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 112.533
2024-06-20 23:54:50,182 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 107.447
2024-06-20 23:54:50,556 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 55.211
2024-06-20 23:54:50,871 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 58.752
2024-06-20 23:54:51,286 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 61.649
2024-06-20 23:54:51,584 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 54.802
2024-06-20 23:54:51,983 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 63.422
2024-06-20 23:54:52,305 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 253.850
2024-06-20 23:54:52,746 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 458.232
2024-06-20 23:54:53,053 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 201.034
2024-06-20 23:54:53,453 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 87.362
2024-06-20 23:54:53,770 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 88.783
2024-06-20 23:54:54,204 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 115.828
2024-06-20 23:54:54,507 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 209.749
2024-06-20 23:54:54,896 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 399.347
2024-06-20 23:54:55,209 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 214.335
2024-06-20 23:54:55,639 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 68.663
2024-06-20 23:54:55,945 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 57.000
2024-06-20 23:54:56,337 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 80.267
2024-06-20 23:54:56,655 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 75.070
2024-06-20 23:54:57,082 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 54.126
2024-06-20 23:54:57,385 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 75.163
2024-06-20 23:54:57,767 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 63.376
2024-06-20 23:54:58,082 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 331.926
2024-06-20 23:54:58,303 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 483.705
2024-06-20 23:55:09,430 - INFO: 6/200 final results:
2024-06-20 23:55:09,430 - INFO: Training loss: 164.348.
2024-06-20 23:55:09,430 - INFO: Training MAE: 9.561.
2024-06-20 23:55:09,430 - INFO: Training MSE: 158.031.
2024-06-20 23:55:30,053 - INFO: Epoch: 6/200, Loss_train: 164.34799996737777, Loss_val: 716.3931558543238
2024-06-20 23:55:30,053 - INFO: Best internal validation val_loss: 124.852 at epoch: 4.
2024-06-20 23:55:30,053 - INFO: Epoch 7/200...
2024-06-20 23:55:30,053 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:55:30,053 - INFO: Batch size: 32.
2024-06-20 23:55:30,057 - INFO: Dataset:
2024-06-20 23:55:30,057 - INFO: Batch size:
2024-06-20 23:55:30,057 - INFO: Number of workers:
2024-06-20 23:55:31,298 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 740.955
2024-06-20 23:55:31,611 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 522.505
2024-06-20 23:55:32,026 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 114.764
2024-06-20 23:55:32,352 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 96.961
2024-06-20 23:55:32,760 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 209.847
2024-06-20 23:55:33,066 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 201.575
2024-06-20 23:55:33,467 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 103.438
2024-06-20 23:55:33,785 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 75.749
2024-06-20 23:55:34,188 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 95.029
2024-06-20 23:55:34,488 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 109.543
2024-06-20 23:55:34,884 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 99.864
2024-06-20 23:55:35,205 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 81.486
2024-06-20 23:55:35,631 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 73.497
2024-06-20 23:55:35,939 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 142.642
2024-06-20 23:55:36,352 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 162.352
2024-06-20 23:55:36,669 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 76.802
2024-06-20 23:55:37,086 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 53.756
2024-06-20 23:55:37,390 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 87.113
2024-06-20 23:55:37,785 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 85.155
2024-06-20 23:55:38,099 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 186.890
2024-06-20 23:55:38,508 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 110.061
2024-06-20 23:55:38,814 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 114.065
2024-06-20 23:55:39,219 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 334.628
2024-06-20 23:55:39,538 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 330.967
2024-06-20 23:55:39,953 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 95.244
2024-06-20 23:55:40,256 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 126.939
2024-06-20 23:55:40,658 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 129.412
2024-06-20 23:55:40,974 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 79.776
2024-06-20 23:55:41,193 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 308.938
2024-06-20 23:55:52,191 - INFO: 7/200 final results:
2024-06-20 23:55:52,192 - INFO: Training loss: 170.688.
2024-06-20 23:55:52,192 - INFO: Training MAE: 9.640.
2024-06-20 23:55:52,192 - INFO: Training MSE: 167.953.
2024-06-20 23:56:12,884 - INFO: Epoch: 7/200, Loss_train: 170.6880401085163, Loss_val: 772.9172300141433
2024-06-20 23:56:12,884 - INFO: Best internal validation val_loss: 124.852 at epoch: 4.
2024-06-20 23:56:12,884 - INFO: Epoch 8/200...
2024-06-20 23:56:12,884 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:56:12,884 - INFO: Batch size: 32.
2024-06-20 23:56:12,889 - INFO: Dataset:
2024-06-20 23:56:12,889 - INFO: Batch size:
2024-06-20 23:56:12,889 - INFO: Number of workers:
2024-06-20 23:56:14,211 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 928.651
2024-06-20 23:56:14,525 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 814.920
2024-06-20 23:56:14,926 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 569.530
2024-06-20 23:56:15,251 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 277.503
2024-06-20 23:56:15,685 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 82.161
2024-06-20 23:56:15,990 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 50.528
2024-06-20 23:56:16,384 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 71.629
2024-06-20 23:56:16,705 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 102.288
2024-06-20 23:56:17,136 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 66.789
2024-06-20 23:56:17,437 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 46.110
2024-06-20 23:56:17,832 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 61.325
2024-06-20 23:56:18,155 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 51.020
2024-06-20 23:56:18,599 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 43.717
2024-06-20 23:56:18,908 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 44.516
2024-06-20 23:56:19,314 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 66.098
2024-06-20 23:56:19,633 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 50.859
2024-06-20 23:56:20,071 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 136.514
2024-06-20 23:56:20,377 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 158.439
2024-06-20 23:56:20,770 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 105.513
2024-06-20 23:56:21,085 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 101.818
2024-06-20 23:56:21,521 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 57.107
2024-06-20 23:56:21,829 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 60.062
2024-06-20 23:56:22,226 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 56.863
2024-06-20 23:56:22,548 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 54.029
2024-06-20 23:56:22,972 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 92.187
2024-06-20 23:56:23,275 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 98.034
2024-06-20 23:56:23,659 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 80.006
2024-06-20 23:56:23,976 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 62.995
2024-06-20 23:56:24,192 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 118.046
2024-06-20 23:56:35,269 - INFO: 8/200 final results:
2024-06-20 23:56:35,269 - INFO: Training loss: 155.492.
2024-06-20 23:56:35,269 - INFO: Training MAE: 8.544.
2024-06-20 23:56:35,269 - INFO: Training MSE: 156.232.
2024-06-20 23:56:55,721 - INFO: Epoch: 8/200, Loss_train: 155.49156110040073, Loss_val: 110.16879325077451
2024-06-20 23:56:55,770 - INFO: Saved new best metric model for epoch 8.
2024-06-20 23:56:55,770 - INFO: Best internal validation val_loss: 110.169 at epoch: 8.
2024-06-20 23:56:55,770 - INFO: Epoch 9/200...
2024-06-20 23:56:55,770 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:56:55,770 - INFO: Batch size: 32.
2024-06-20 23:56:55,774 - INFO: Dataset:
2024-06-20 23:56:55,774 - INFO: Batch size:
2024-06-20 23:56:55,774 - INFO: Number of workers:
2024-06-20 23:56:57,060 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 105.402
2024-06-20 23:56:57,374 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 49.397
2024-06-20 23:56:57,776 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 88.081
2024-06-20 23:56:58,103 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 38.438
2024-06-20 23:56:58,537 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 60.445
2024-06-20 23:56:58,842 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 52.721
2024-06-20 23:56:59,240 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 78.466
2024-06-20 23:56:59,560 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 175.885
2024-06-20 23:57:00,000 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 351.887
2024-06-20 23:57:00,301 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 472.536
2024-06-20 23:57:00,696 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 420.678
2024-06-20 23:57:01,018 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 274.695
2024-06-20 23:57:01,464 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 148.799
2024-06-20 23:57:01,773 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 70.224
2024-06-20 23:57:02,175 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 34.849
2024-06-20 23:57:02,493 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 61.338
2024-06-20 23:57:02,927 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 70.479
2024-06-20 23:57:03,232 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 53.241
2024-06-20 23:57:03,626 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 36.707
2024-06-20 23:57:03,941 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 122.731
2024-06-20 23:57:04,370 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 416.150
2024-06-20 23:57:04,677 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 702.328
2024-06-20 23:57:05,069 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 439.166
2024-06-20 23:57:05,389 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 149.074
2024-06-20 23:57:05,818 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 44.139
2024-06-20 23:57:06,120 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 56.259
2024-06-20 23:57:06,509 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 54.673
2024-06-20 23:57:06,824 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 58.564
2024-06-20 23:57:07,036 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 59.420
2024-06-20 23:57:18,127 - INFO: 9/200 final results:
2024-06-20 23:57:18,128 - INFO: Training loss: 163.682.
2024-06-20 23:57:18,128 - INFO: Training MAE: 9.037.
2024-06-20 23:57:18,128 - INFO: Training MSE: 165.744.
2024-06-20 23:57:38,784 - INFO: Epoch: 9/200, Loss_train: 163.68176019602808, Loss_val: 343.18935052279767
2024-06-20 23:57:38,784 - INFO: Best internal validation val_loss: 110.169 at epoch: 8.
2024-06-20 23:57:38,784 - INFO: Epoch 10/200...
2024-06-20 23:57:38,784 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:57:38,784 - INFO: Batch size: 32.
2024-06-20 23:57:38,788 - INFO: Dataset:
2024-06-20 23:57:38,788 - INFO: Batch size:
2024-06-20 23:57:38,788 - INFO: Number of workers:
2024-06-20 23:57:40,038 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 342.118
2024-06-20 23:57:40,351 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 242.756
2024-06-20 23:57:40,772 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 111.195
2024-06-20 23:57:41,096 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 60.305
2024-06-20 23:57:41,504 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 32.589
2024-06-20 23:57:41,823 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 42.452
2024-06-20 23:57:42,215 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 59.075
2024-06-20 23:57:42,536 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 248.431
2024-06-20 23:57:42,934 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 304.904
2024-06-20 23:57:43,247 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 265.790
2024-06-20 23:57:43,644 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 193.770
2024-06-20 23:57:43,967 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 194.289
2024-06-20 23:57:44,387 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 93.909
2024-06-20 23:57:44,708 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 47.484
2024-06-20 23:57:45,122 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 80.321
2024-06-20 23:57:45,440 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 88.464
2024-06-20 23:57:45,851 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 86.547
2024-06-20 23:57:46,168 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 62.739
2024-06-20 23:57:46,571 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 46.742
2024-06-20 23:57:46,885 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 70.826
2024-06-20 23:57:47,292 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 40.904
2024-06-20 23:57:47,619 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 44.285
2024-06-20 23:57:48,032 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 54.327
2024-06-20 23:57:48,360 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 49.069
2024-06-20 23:57:48,779 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 46.498
2024-06-20 23:57:49,100 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 58.650
2024-06-20 23:57:49,487 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 43.332
2024-06-20 23:57:49,802 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 51.744
2024-06-20 23:57:50,017 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 33.684
2024-06-20 23:58:01,047 - INFO: 10/200 final results:
2024-06-20 23:58:01,047 - INFO: Training loss: 106.800.
2024-06-20 23:58:01,047 - INFO: Training MAE: 7.788.
2024-06-20 23:58:01,047 - INFO: Training MSE: 108.246.
2024-06-20 23:58:21,619 - INFO: Epoch: 10/200, Loss_train: 106.80003527937264, Loss_val: 39.69683890507139
2024-06-20 23:58:21,666 - INFO: Saved new best metric model for epoch 10.
2024-06-20 23:58:21,666 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-20 23:58:21,666 - INFO: Epoch 11/200...
2024-06-20 23:58:21,666 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:58:21,666 - INFO: Batch size: 32.
2024-06-20 23:58:21,671 - INFO: Dataset:
2024-06-20 23:58:21,671 - INFO: Batch size:
2024-06-20 23:58:21,671 - INFO: Number of workers:
2024-06-20 23:58:22,915 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 32.723
2024-06-20 23:58:23,253 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 42.275
2024-06-20 23:58:23,661 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 38.856
2024-06-20 23:58:23,984 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 33.503
2024-06-20 23:58:24,392 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 34.609
2024-06-20 23:58:24,722 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 45.662
2024-06-20 23:58:25,116 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 83.320
2024-06-20 23:58:25,434 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 83.444
2024-06-20 23:58:25,831 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 40.799
2024-06-20 23:58:26,158 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 86.853
2024-06-20 23:58:26,549 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 147.858
2024-06-20 23:58:26,868 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 132.915
2024-06-20 23:58:27,289 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 187.990
2024-06-20 23:58:27,620 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 204.748
2024-06-20 23:58:28,025 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 128.776
2024-06-20 23:58:28,341 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 144.057
2024-06-20 23:58:28,751 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 117.672
2024-06-20 23:58:29,079 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 79.852
2024-06-20 23:58:29,473 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 110.725
2024-06-20 23:58:29,785 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 143.984
2024-06-20 23:58:30,192 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 111.346
2024-06-20 23:58:30,522 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 44.778
2024-06-20 23:58:30,914 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 76.693
2024-06-20 23:58:31,231 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 252.912
2024-06-20 23:58:31,633 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 457.855
2024-06-20 23:58:31,958 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 546.866
2024-06-20 23:58:32,340 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 379.317
2024-06-20 23:58:32,654 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 198.133
2024-06-20 23:58:32,874 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 65.689
2024-06-20 23:58:43,804 - INFO: 11/200 final results:
2024-06-20 23:58:43,804 - INFO: Training loss: 139.800.
2024-06-20 23:58:43,804 - INFO: Training MAE: 8.398.
2024-06-20 23:58:43,804 - INFO: Training MSE: 141.266.
2024-06-20 23:59:04,462 - INFO: Epoch: 11/200, Loss_train: 139.80042148458546, Loss_val: 58.70273399353027
2024-06-20 23:59:04,462 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-20 23:59:04,462 - INFO: Epoch 12/200...
2024-06-20 23:59:04,462 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:59:04,462 - INFO: Batch size: 32.
2024-06-20 23:59:04,466 - INFO: Dataset:
2024-06-20 23:59:04,466 - INFO: Batch size:
2024-06-20 23:59:04,466 - INFO: Number of workers:
2024-06-20 23:59:05,732 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 47.763
2024-06-20 23:59:06,044 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 34.490
2024-06-20 23:59:06,448 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 69.927
2024-06-20 23:59:06,772 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 174.076
2024-06-20 23:59:07,218 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 163.161
2024-06-20 23:59:07,523 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 96.434
2024-06-20 23:59:07,916 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 51.701
2024-06-20 23:59:08,222 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 38.574
2024-06-20 23:59:08,669 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 36.959
2024-06-20 23:59:08,968 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 32.695
2024-06-20 23:59:09,361 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 33.165
2024-06-20 23:59:09,669 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 35.747
2024-06-20 23:59:10,127 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 36.955
2024-06-20 23:59:10,437 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 39.117
2024-06-20 23:59:10,843 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 59.892
2024-06-20 23:59:11,149 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 65.261
2024-06-20 23:59:11,599 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 63.912
2024-06-20 23:59:11,905 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 45.627
2024-06-20 23:59:12,299 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 47.463
2024-06-20 23:59:12,601 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 35.049
2024-06-20 23:59:13,046 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 37.880
2024-06-20 23:59:13,355 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 42.265
2024-06-20 23:59:13,750 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 54.941
2024-06-20 23:59:14,058 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 56.787
2024-06-20 23:59:14,494 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 77.892
2024-06-20 23:59:14,798 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 164.868
2024-06-20 23:59:15,175 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 87.612
2024-06-20 23:59:15,479 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 34.636
2024-06-20 23:59:15,700 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 67.557
2024-06-20 23:59:26,728 - INFO: 12/200 final results:
2024-06-20 23:59:26,728 - INFO: Training loss: 63.187.
2024-06-20 23:59:26,728 - INFO: Training MAE: 6.098.
2024-06-20 23:59:26,728 - INFO: Training MSE: 63.100.
2024-06-20 23:59:47,130 - INFO: Epoch: 12/200, Loss_train: 63.186517518142175, Loss_val: 177.20256252946524
2024-06-20 23:59:47,130 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-20 23:59:47,131 - INFO: Epoch 13/200...
2024-06-20 23:59:47,131 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:59:47,131 - INFO: Batch size: 32.
2024-06-20 23:59:47,135 - INFO: Dataset:
2024-06-20 23:59:47,135 - INFO: Batch size:
2024-06-20 23:59:47,135 - INFO: Number of workers:
2024-06-20 23:59:48,381 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 205.382
2024-06-20 23:59:48,708 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 227.926
2024-06-20 23:59:49,119 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 278.800
2024-06-20 23:59:49,444 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 307.430
2024-06-20 23:59:49,852 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 284.159
2024-06-20 23:59:50,158 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 197.735
2024-06-20 23:59:50,547 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 59.594
2024-06-20 23:59:50,866 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 32.968
2024-06-20 23:59:51,274 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 44.322
2024-06-20 23:59:51,575 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 44.456
2024-06-20 23:59:51,981 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 78.313
2024-06-20 23:59:52,302 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 195.695
2024-06-20 23:59:52,731 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 190.360
2024-06-20 23:59:53,040 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 105.590
2024-06-20 23:59:53,430 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 39.162
2024-06-20 23:59:53,747 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 43.700
2024-06-20 23:59:54,167 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 65.743
2024-06-20 23:59:54,472 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 95.588
2024-06-20 23:59:54,850 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 152.935
2024-06-20 23:59:55,165 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 142.055
2024-06-20 23:59:55,582 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 77.327
2024-06-20 23:59:55,889 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 45.777
2024-06-20 23:59:56,275 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 46.161
2024-06-20 23:59:56,595 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 32.938
2024-06-20 23:59:57,013 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 58.741
2024-06-20 23:59:57,316 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 66.602
2024-06-20 23:59:57,689 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 65.368
2024-06-20 23:59:58,004 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 70.001
2024-06-20 23:59:58,215 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 64.021
2024-06-21 00:00:09,225 - INFO: 13/200 final results:
2024-06-21 00:00:09,225 - INFO: Training loss: 114.443.
2024-06-21 00:00:09,225 - INFO: Training MAE: 7.749.
2024-06-21 00:00:09,225 - INFO: Training MSE: 115.440.
2024-06-21 00:00:29,675 - INFO: Epoch: 13/200, Loss_train: 114.44302631246633, Loss_val: 58.726266269026134
2024-06-21 00:00:29,676 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-21 00:00:29,676 - INFO: Epoch 14/200...
2024-06-21 00:00:29,676 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 00:00:29,676 - INFO: Batch size: 32.
2024-06-21 00:00:29,680 - INFO: Dataset:
2024-06-21 00:00:29,680 - INFO: Batch size:
2024-06-21 00:00:29,680 - INFO: Number of workers:
2024-06-21 00:00:30,940 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 76.631
2024-06-21 00:00:31,252 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 86.856
2024-06-21 00:00:31,645 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 102.460
2024-06-21 00:00:31,967 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 108.299
2024-06-21 00:00:32,417 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 86.732
2024-06-21 00:00:32,724 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 53.502
2024-06-21 00:00:33,120 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 42.077
2024-06-21 00:00:33,427 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 33.858
2024-06-21 00:00:33,875 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 36.910
2024-06-21 00:00:34,173 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 48.172
2024-06-21 00:00:34,567 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 43.644
2024-06-21 00:00:34,874 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 45.986
2024-06-21 00:00:35,333 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 44.295
2024-06-21 00:00:35,642 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 56.023
2024-06-21 00:00:36,050 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 116.895
2024-06-21 00:00:36,356 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 145.922
2024-06-21 00:00:36,802 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 140.543
2024-06-21 00:00:37,105 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 134.619
2024-06-21 00:00:37,496 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 162.428
2024-06-21 00:00:37,795 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 169.957
2024-06-21 00:00:38,236 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 154.272
2024-06-21 00:00:38,541 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 64.985
2024-06-21 00:00:38,937 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 40.077
2024-06-21 00:00:39,243 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 41.871
2024-06-21 00:00:39,680 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 31.980
2024-06-21 00:00:39,981 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 51.806
2024-06-21 00:00:40,361 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 98.702
2024-06-21 00:00:40,661 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 137.020
2024-06-21 00:00:40,884 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 150.530
2024-06-21 00:00:51,946 - INFO: 14/200 final results:
2024-06-21 00:00:51,946 - INFO: Training loss: 86.450.
2024-06-21 00:00:51,946 - INFO: Training MAE: 6.647.
2024-06-21 00:00:51,946 - INFO: Training MSE: 85.183.
2024-06-21 00:01:12,551 - INFO: Epoch: 14/200, Loss_train: 86.45003009664602, Loss_val: 121.03190007703057
2024-06-21 00:01:12,551 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-21 00:01:12,551 - INFO: Epoch 15/200...
2024-06-21 00:01:12,551 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 00:01:12,551 - INFO: Batch size: 32.
2024-06-21 00:01:12,555 - INFO: Dataset:
2024-06-21 00:01:12,555 - INFO: Batch size:
2024-06-21 00:01:12,555 - INFO: Number of workers:
2024-06-21 00:01:13,874 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 117.968
2024-06-21 00:01:14,185 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 61.078
2024-06-21 00:01:14,575 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 30.803
2024-06-21 00:01:14,897 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 38.480
2024-06-21 00:01:15,321 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 44.232
2024-06-21 00:01:15,628 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 39.783
2024-06-21 00:01:16,007 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 43.354
2024-06-21 00:01:16,327 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 46.970
2024-06-21 00:01:16,748 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 45.325
2024-06-21 00:01:17,046 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 35.797
2024-06-21 00:01:17,419 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 47.853
2024-06-21 00:01:17,743 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 96.094
2024-06-21 00:01:18,472 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 104.110
2024-06-21 00:01:18,782 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 148.345
2024-06-21 00:01:19,172 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 244.134
2024-06-21 00:01:19,490 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 390.564
2024-06-21 00:01:19,910 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 472.639
2024-06-21 00:01:20,215 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 341.227
2024-06-21 00:01:20,592 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 171.267
2024-06-21 00:01:20,907 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 88.121
2024-06-21 00:01:21,335 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 45.338
2024-06-21 00:01:21,642 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 26.071
2024-06-21 00:01:22,032 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 58.382
2024-06-21 00:01:22,351 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 119.918
2024-06-21 00:01:22,770 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 125.189
2024-06-21 00:01:23,072 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 65.446
2024-06-21 00:01:23,458 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 60.100
2024-06-21 00:01:23,774 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 83.123
2024-06-21 00:01:23,992 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 72.763
2024-06-21 00:01:34,739 - INFO: 15/200 final results:
2024-06-21 00:01:34,739 - INFO: Training loss: 112.568.
2024-06-21 00:01:34,740 - INFO: Training MAE: 7.197.
2024-06-21 00:01:34,740 - INFO: Training MSE: 113.355.
2024-06-21 00:01:55,501 - INFO: Epoch: 15/200, Loss_train: 112.56810859154011, Loss_val: 53.58927930634597
2024-06-21 00:01:55,501 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-21 00:01:55,501 - INFO: Epoch 16/200...
2024-06-21 00:01:55,501 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 00:01:55,501 - INFO: Batch size: 32.
2024-06-21 00:01:55,505 - INFO: Dataset:
2024-06-21 00:01:55,505 - INFO: Batch size:
2024-06-21 00:01:55,505 - INFO: Number of workers:
2024-06-21 00:01:56,776 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 55.513
2024-06-21 00:01:57,089 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 42.178
2024-06-21 00:01:57,505 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 106.730
2024-06-21 00:01:57,831 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 150.439
2024-06-21 00:01:58,254 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 134.553
2024-06-21 00:01:58,558 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 136.716
2024-06-21 00:01:58,961 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 174.715
2024-06-21 00:01:59,280 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 133.387
2024-06-21 00:01:59,694 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 52.123
2024-06-21 00:01:59,992 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 38.312
2024-06-21 00:02:00,397 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 53.599
2024-06-21 00:02:00,718 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 71.025
2024-06-21 00:02:01,150 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 105.368
2024-06-21 00:02:01,457 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 79.102
2024-06-21 00:02:01,873 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 30.442
2024-06-21 00:02:02,191 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 31.896
2024-06-21 00:02:02,601 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 34.877
2024-06-21 00:02:02,903 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 37.645
2024-06-21 00:02:03,303 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 33.025
2024-06-21 00:02:03,615 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 34.443
2024-06-21 00:02:04,027 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 70.169
2024-06-21 00:02:04,333 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 46.624
2024-06-21 00:02:04,741 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 43.740
2024-06-21 00:02:05,060 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 72.611
2024-06-21 00:02:05,474 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 45.236
2024-06-21 00:02:05,776 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 45.993
2024-06-21 00:02:06,170 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 65.358
2024-06-21 00:02:06,484 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 101.852
2024-06-21 00:02:06,707 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 147.219
2024-06-21 00:02:17,761 - INFO: 16/200 final results:
2024-06-21 00:02:17,761 - INFO: Training loss: 74.996.
2024-06-21 00:02:17,761 - INFO: Training MAE: 6.512.
2024-06-21 00:02:17,761 - INFO: Training MSE: 73.568.
2024-06-21 00:02:37,958 - INFO: Epoch: 16/200, Loss_train: 74.99625081029431, Loss_val: 184.58932495117188
2024-06-21 00:02:37,958 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-21 00:02:37,959 - INFO: Epoch 17/200...
2024-06-21 00:02:37,959 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 00:02:37,959 - INFO: Batch size: 32.
2024-06-21 00:02:37,962 - INFO: Dataset:
2024-06-21 00:02:37,963 - INFO: Batch size:
2024-06-21 00:02:37,963 - INFO: Number of workers:
2024-06-21 00:02:39,208 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 165.453
2024-06-21 00:02:39,521 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 181.748
2024-06-21 00:02:39,915 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 111.070
2024-06-21 00:02:40,238 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 53.382
2024-06-21 00:02:40,672 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 19.004
2024-06-21 00:02:40,976 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 25.284
2024-06-21 00:02:41,359 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 35.103
2024-06-21 00:02:41,663 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 58.037
2024-06-21 00:02:42,096 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 67.211
2024-06-21 00:02:42,395 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 75.333
2024-06-21 00:02:42,776 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 48.412
2024-06-21 00:02:43,082 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 69.870
2024-06-21 00:02:43,523 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 134.389
2024-06-21 00:02:43,831 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 165.203
2024-06-21 00:02:44,222 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 138.322
2024-06-21 00:02:44,526 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 123.025
2024-06-21 00:02:44,956 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 101.778
2024-06-21 00:02:45,260 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 42.832
2024-06-21 00:02:45,645 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 30.728
2024-06-21 00:02:45,946 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 49.514
2024-06-21 00:02:46,374 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 39.122
2024-06-21 00:02:46,681 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 37.750
2024-06-21 00:02:47,074 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 25.792
2024-06-21 00:02:47,382 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 32.702
2024-06-21 00:02:47,810 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 35.453
2024-06-21 00:02:48,111 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 36.108
2024-06-21 00:02:48,487 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 27.242
2024-06-21 00:02:48,788 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 44.531
2024-06-21 00:02:49,000 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 73.020
2024-06-21 00:02:59,980 - INFO: 17/200 final results:
2024-06-21 00:02:59,980 - INFO: Training loss: 70.601.
2024-06-21 00:02:59,980 - INFO: Training MAE: 6.081.
2024-06-21 00:02:59,980 - INFO: Training MSE: 70.553.
2024-06-21 00:03:20,367 - INFO: Epoch: 17/200, Loss_train: 70.60062763608735, Loss_val: 262.4457560572131
2024-06-21 00:03:20,367 - INFO: Best internal validation val_loss: 39.697 at epoch: 10.
2024-06-21 00:03:20,367 - INFO: Epoch 18/200...
2024-06-21 00:03:20,367 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:03:20,367 - INFO: Batch size: 32.
2024-06-21 00:03:20,371 - INFO: Dataset:
2024-06-21 00:03:20,371 - INFO: Batch size:
2024-06-21 00:03:20,371 - INFO: Number of workers:
2024-06-21 00:03:21,624 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 262.049
2024-06-21 00:03:21,934 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 26.333
2024-06-21 00:03:22,330 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 71.031
2024-06-21 00:03:22,651 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 22.451
2024-06-21 00:03:23,060 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 23.944
2024-06-21 00:03:23,363 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 22.132
2024-06-21 00:03:23,771 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 28.951
2024-06-21 00:03:24,091 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 24.766
2024-06-21 00:03:24,511 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 20.572
2024-06-21 00:03:24,812 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 30.625
2024-06-21 00:03:25,219 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 26.931
2024-06-21 00:03:25,542 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 25.478
2024-06-21 00:03:25,973 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 22.982
2024-06-21 00:03:26,283 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 29.696
2024-06-21 00:03:26,702 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 24.859
2024-06-21 00:03:27,021 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 29.005
2024-06-21 00:03:27,447 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 20.942
2024-06-21 00:03:27,753 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 35.648
2024-06-21 00:03:28,164 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 39.291
2024-06-21 00:03:28,479 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 24.064
2024-06-21 00:03:28,902 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 32.296
2024-06-21 00:03:29,210 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 29.988
2024-06-21 00:03:29,623 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 22.371
2024-06-21 00:03:29,945 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 22.787
2024-06-21 00:03:30,359 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 26.589
2024-06-21 00:03:30,659 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 29.370
2024-06-21 00:03:31,057 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 19.964
2024-06-21 00:03:31,370 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 21.583
2024-06-21 00:03:31,593 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 24.490
2024-06-21 00:03:42,582 - INFO: 18/200 final results:
2024-06-21 00:03:42,582 - INFO: Training loss: 35.903.
2024-06-21 00:03:42,582 - INFO: Training MAE: 4.405.
2024-06-21 00:03:42,582 - INFO: Training MSE: 36.129.
2024-06-21 00:04:02,983 - INFO: Epoch: 18/200, Loss_train: 35.90308038119612, Loss_val: 24.678269057438293
2024-06-21 00:04:03,030 - INFO: Saved new best metric model for epoch 18.
2024-06-21 00:04:03,030 - INFO: Best internal validation val_loss: 24.678 at epoch: 18.
2024-06-21 00:04:03,030 - INFO: Epoch 19/200...
2024-06-21 00:04:03,030 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:04:03,030 - INFO: Batch size: 32.
2024-06-21 00:04:03,034 - INFO: Dataset:
2024-06-21 00:04:03,034 - INFO: Batch size:
2024-06-21 00:04:03,034 - INFO: Number of workers:
2024-06-21 00:04:04,295 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 17.777
2024-06-21 00:04:04,610 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 26.220
2024-06-21 00:04:05,032 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 29.643
2024-06-21 00:04:05,358 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 39.142
2024-06-21 00:04:05,778 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 22.669
2024-06-21 00:04:06,084 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 21.705
2024-06-21 00:04:06,493 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 20.849
2024-06-21 00:04:06,813 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 21.723
2024-06-21 00:04:07,229 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 23.176
2024-06-21 00:04:07,530 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 18.983
2024-06-21 00:04:07,930 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 25.563
2024-06-21 00:04:08,254 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 15.533
2024-06-21 00:04:08,684 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 26.302
2024-06-21 00:04:08,990 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 24.777
2024-06-21 00:04:09,404 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 22.452
2024-06-21 00:04:09,719 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 20.349
2024-06-21 00:04:10,140 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 23.559
2024-06-21 00:04:10,441 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 24.080
2024-06-21 00:04:10,843 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 25.276
2024-06-21 00:04:11,154 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 20.684
2024-06-21 00:04:11,590 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 26.898
2024-06-21 00:04:11,911 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 17.891
2024-06-21 00:04:12,346 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 22.470
2024-06-21 00:04:12,675 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 19.517
2024-06-21 00:04:13,092 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 26.226
2024-06-21 00:04:13,392 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 37.112
2024-06-21 00:04:13,789 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 28.867
2024-06-21 00:04:14,101 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 14.673
2024-06-21 00:04:14,323 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 33.347
2024-06-21 00:04:25,334 - INFO: 19/200 final results:
2024-06-21 00:04:25,335 - INFO: Training loss: 24.051.
2024-06-21 00:04:25,335 - INFO: Training MAE: 3.749.
2024-06-21 00:04:25,335 - INFO: Training MSE: 23.867.
2024-06-21 00:04:45,743 - INFO: Epoch: 19/200, Loss_train: 24.05056568671917, Loss_val: 42.60770130157471
2024-06-21 00:04:45,743 - INFO: Best internal validation val_loss: 24.678 at epoch: 18.
2024-06-21 00:04:45,744 - INFO: Epoch 20/200...
2024-06-21 00:04:45,744 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:04:45,744 - INFO: Batch size: 32.
2024-06-21 00:04:45,748 - INFO: Dataset:
2024-06-21 00:04:45,748 - INFO: Batch size:
2024-06-21 00:04:45,748 - INFO: Number of workers:
2024-06-21 00:04:47,337 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 33.236
2024-06-21 00:04:47,649 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 31.426
2024-06-21 00:04:48,040 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 26.017
2024-06-21 00:04:48,363 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 19.406
2024-06-21 00:04:48,783 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 23.191
2024-06-21 00:04:49,087 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 31.120
2024-06-21 00:04:49,462 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 19.100
2024-06-21 00:04:49,778 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 20.604
2024-06-21 00:04:50,213 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 31.819
2024-06-21 00:04:50,510 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 37.666
2024-06-21 00:04:50,897 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 20.075
2024-06-21 00:04:51,219 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 24.458
2024-06-21 00:04:51,986 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 23.151
2024-06-21 00:04:52,294 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 21.834
2024-06-21 00:04:52,699 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 26.478
2024-06-21 00:04:53,016 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 21.556
2024-06-21 00:04:53,461 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 20.743
2024-06-21 00:04:53,763 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 20.982
2024-06-21 00:04:54,149 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 25.169
2024-06-21 00:04:54,448 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 18.827
2024-06-21 00:04:54,887 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 20.725
2024-06-21 00:04:55,193 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 24.356
2024-06-21 00:04:55,591 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 17.471
2024-06-21 00:04:55,898 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 24.923
2024-06-21 00:04:56,337 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 28.888
2024-06-21 00:04:56,639 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 30.879
2024-06-21 00:04:57,026 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 31.502
2024-06-21 00:04:57,327 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 27.552
2024-06-21 00:04:57,547 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 45.617
2024-06-21 00:05:08,303 - INFO: 20/200 final results:
2024-06-21 00:05:08,303 - INFO: Training loss: 25.820.
2024-06-21 00:05:08,303 - INFO: Training MAE: 3.870.
2024-06-21 00:05:08,303 - INFO: Training MSE: 25.428.
2024-06-21 00:05:29,140 - INFO: Epoch: 20/200, Loss_train: 25.819659594831794, Loss_val: 46.17281571749983
2024-06-21 00:05:29,140 - INFO: Best internal validation val_loss: 24.678 at epoch: 18.
2024-06-21 00:05:29,140 - INFO: Epoch 21/200...
2024-06-21 00:05:29,140 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:05:29,140 - INFO: Batch size: 32.
2024-06-21 00:05:29,144 - INFO: Dataset:
2024-06-21 00:05:29,144 - INFO: Batch size:
2024-06-21 00:05:29,145 - INFO: Number of workers:
2024-06-21 00:05:30,411 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 46.329
2024-06-21 00:05:30,726 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 37.371
2024-06-21 00:05:31,147 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 29.815
2024-06-21 00:05:31,472 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 26.901
2024-06-21 00:05:31,892 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 22.327
2024-06-21 00:05:32,199 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 25.151
2024-06-21 00:05:32,603 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 18.929
2024-06-21 00:05:32,923 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 24.148
2024-06-21 00:05:33,342 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 23.810
2024-06-21 00:05:33,644 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 24.686
2024-06-21 00:05:34,046 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 30.905
2024-06-21 00:05:34,367 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 15.349
2024-06-21 00:05:34,795 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 26.376
2024-06-21 00:05:35,104 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 24.262
2024-06-21 00:05:35,518 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 24.953
2024-06-21 00:05:35,834 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 28.376
2024-06-21 00:05:36,251 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 28.292
2024-06-21 00:05:36,556 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 14.927
2024-06-21 00:05:36,956 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 22.663
2024-06-21 00:05:37,269 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 18.155
2024-06-21 00:05:37,682 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 17.381
2024-06-21 00:05:37,987 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 23.325
2024-06-21 00:05:38,382 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 47.764
2024-06-21 00:05:38,699 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 33.784
2024-06-21 00:05:39,099 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 15.007
2024-06-21 00:05:39,399 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 23.590
2024-06-21 00:05:39,781 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 26.263
2024-06-21 00:05:40,091 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 35.330
2024-06-21 00:05:40,301 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 32.577
2024-06-21 00:05:51,235 - INFO: 21/200 final results:
2024-06-21 00:05:51,235 - INFO: Training loss: 26.508.
2024-06-21 00:05:51,235 - INFO: Training MAE: 3.916.
2024-06-21 00:05:51,235 - INFO: Training MSE: 26.388.
2024-06-21 00:06:11,864 - INFO: Epoch: 21/200, Loss_train: 26.50849776432432, Loss_val: 37.0568445468771
2024-06-21 00:06:11,865 - INFO: Best internal validation val_loss: 24.678 at epoch: 18.
2024-06-21 00:06:11,865 - INFO: Epoch 22/200...
2024-06-21 00:06:11,865 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:06:11,865 - INFO: Batch size: 32.
2024-06-21 00:06:11,868 - INFO: Dataset:
2024-06-21 00:06:11,869 - INFO: Batch size:
2024-06-21 00:06:11,869 - INFO: Number of workers:
2024-06-21 00:06:13,129 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 31.533
2024-06-21 00:06:13,442 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 28.944
2024-06-21 00:06:13,864 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 21.305
2024-06-21 00:06:14,189 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 22.106
2024-06-21 00:06:14,602 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 23.934
2024-06-21 00:06:14,907 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 23.939
2024-06-21 00:06:15,312 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 27.519
2024-06-21 00:06:15,631 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 20.182
2024-06-21 00:06:16,036 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 17.266
2024-06-21 00:06:16,335 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 25.097
2024-06-21 00:06:16,740 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 46.550
2024-06-21 00:06:17,060 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 41.455
2024-06-21 00:06:17,484 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 23.813
2024-06-21 00:06:17,792 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 25.968
2024-06-21 00:06:18,209 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 34.376
2024-06-21 00:06:18,525 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 41.606
2024-06-21 00:06:18,941 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 32.417
2024-06-21 00:06:19,245 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 29.356
2024-06-21 00:06:19,651 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 24.176
2024-06-21 00:06:19,964 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 16.194
2024-06-21 00:06:20,379 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 23.710
2024-06-21 00:06:20,687 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 25.526
2024-06-21 00:06:21,097 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 26.878
2024-06-21 00:06:21,417 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 22.559
2024-06-21 00:06:21,832 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 22.991
2024-06-21 00:06:22,135 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 22.463
2024-06-21 00:06:22,535 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 35.926
2024-06-21 00:06:22,849 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 32.293
2024-06-21 00:06:23,072 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 27.602
2024-06-21 00:06:33,813 - INFO: 22/200 final results:
2024-06-21 00:06:33,813 - INFO: Training loss: 27.506.
2024-06-21 00:06:33,813 - INFO: Training MAE: 4.119.
2024-06-21 00:06:33,813 - INFO: Training MSE: 27.505.
2024-06-21 00:06:54,361 - INFO: Epoch: 22/200, Loss_train: 27.506398102332806, Loss_val: 22.254608516035407
2024-06-21 00:06:54,409 - INFO: Saved new best metric model for epoch 22.
2024-06-21 00:06:54,409 - INFO: Best internal validation val_loss: 22.255 at epoch: 22.
2024-06-21 00:06:54,409 - INFO: Epoch 23/200...
2024-06-21 00:06:54,409 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:06:54,409 - INFO: Batch size: 32.
2024-06-21 00:06:54,413 - INFO: Dataset:
2024-06-21 00:06:54,414 - INFO: Batch size:
2024-06-21 00:06:54,414 - INFO: Number of workers:
2024-06-21 00:06:55,658 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 20.051
2024-06-21 00:06:55,971 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 17.007
2024-06-21 00:06:56,392 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 31.293
2024-06-21 00:06:56,716 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 67.363
2024-06-21 00:06:57,133 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 129.617
2024-06-21 00:06:57,437 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 84.190
2024-06-21 00:06:57,842 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 49.292
2024-06-21 00:06:58,160 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 30.517
2024-06-21 00:06:58,566 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 26.857
2024-06-21 00:06:58,868 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 26.830
2024-06-21 00:06:59,263 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 19.494
2024-06-21 00:06:59,585 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 24.003
2024-06-21 00:07:00,017 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 22.132
2024-06-21 00:07:00,327 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 24.528
2024-06-21 00:07:00,734 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 30.532
2024-06-21 00:07:01,051 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 24.767
2024-06-21 00:07:01,474 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 28.197
2024-06-21 00:07:01,779 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 27.386
2024-06-21 00:07:02,176 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 23.812
2024-06-21 00:07:02,490 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 43.895
2024-06-21 00:07:02,899 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 51.901
2024-06-21 00:07:03,208 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 47.908
2024-06-21 00:07:03,617 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 34.391
2024-06-21 00:07:03,938 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 20.599
2024-06-21 00:07:04,354 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 26.535
2024-06-21 00:07:04,658 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 20.679
2024-06-21 00:07:05,057 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 22.846
2024-06-21 00:07:05,373 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 33.163
2024-06-21 00:07:05,588 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 41.073
2024-06-21 00:07:16,586 - INFO: 23/200 final results:
2024-06-21 00:07:16,586 - INFO: Training loss: 36.236.
2024-06-21 00:07:16,586 - INFO: Training MAE: 4.621.
2024-06-21 00:07:16,586 - INFO: Training MSE: 36.141.
2024-06-21 00:07:37,201 - INFO: Epoch: 23/200, Loss_train: 36.23649314354206, Loss_val: 37.66578187613651
2024-06-21 00:07:37,201 - INFO: Best internal validation val_loss: 22.255 at epoch: 22.
2024-06-21 00:07:37,201 - INFO: Epoch 24/200...
2024-06-21 00:07:37,201 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:07:37,201 - INFO: Batch size: 32.
2024-06-21 00:07:37,205 - INFO: Dataset:
2024-06-21 00:07:37,205 - INFO: Batch size:
2024-06-21 00:07:37,205 - INFO: Number of workers:
2024-06-21 00:07:38,469 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 39.580
2024-06-21 00:07:38,781 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 48.101
2024-06-21 00:07:39,196 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 104.600
2024-06-21 00:07:39,522 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 192.551
2024-06-21 00:07:39,956 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 258.588
2024-06-21 00:07:40,263 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 103.751
2024-06-21 00:07:40,644 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 44.041
2024-06-21 00:07:40,964 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 35.551
2024-06-21 00:07:41,392 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 44.670
2024-06-21 00:07:41,693 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 23.068
2024-06-21 00:07:42,092 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 28.316
2024-06-21 00:07:42,414 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 33.172
2024-06-21 00:07:42,852 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 27.074
2024-06-21 00:07:43,162 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 19.445
2024-06-21 00:07:43,571 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 20.110
2024-06-21 00:07:43,888 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 27.361
2024-06-21 00:07:44,321 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 52.342
2024-06-21 00:07:44,627 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 52.072
2024-06-21 00:07:45,021 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 27.276
2024-06-21 00:07:45,335 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 24.915
2024-06-21 00:07:45,767 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 31.571
2024-06-21 00:07:46,073 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 22.082
2024-06-21 00:07:46,466 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 21.476
2024-06-21 00:07:46,785 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 27.552
2024-06-21 00:07:47,203 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 28.522
2024-06-21 00:07:47,504 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 41.383
2024-06-21 00:07:47,876 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 39.951
2024-06-21 00:07:48,189 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 41.346
2024-06-21 00:07:48,404 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 52.400
2024-06-21 00:07:59,336 - INFO: 24/200 final results:
2024-06-21 00:07:59,336 - INFO: Training loss: 52.168.
2024-06-21 00:07:59,336 - INFO: Training MAE: 5.071.
2024-06-21 00:07:59,336 - INFO: Training MSE: 52.163.
2024-06-21 00:08:19,839 - INFO: Epoch: 24/200, Loss_train: 52.16782293648556, Loss_val: 30.774926547346443
2024-06-21 00:08:19,839 - INFO: Best internal validation val_loss: 22.255 at epoch: 22.
2024-06-21 00:08:19,839 - INFO: Epoch 25/200...
2024-06-21 00:08:19,839 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:08:19,839 - INFO: Batch size: 32.
2024-06-21 00:08:19,843 - INFO: Dataset:
2024-06-21 00:08:19,843 - INFO: Batch size:
2024-06-21 00:08:19,843 - INFO: Number of workers:
2024-06-21 00:08:21,109 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 45.497
2024-06-21 00:08:21,423 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 37.167
2024-06-21 00:08:21,833 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 23.810
2024-06-21 00:08:22,159 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 18.163
2024-06-21 00:08:22,595 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 15.619
2024-06-21 00:08:22,902 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 25.647
2024-06-21 00:08:23,294 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 34.478
2024-06-21 00:08:23,601 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 38.440
2024-06-21 00:08:24,024 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 77.254
2024-06-21 00:08:24,322 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 133.215
2024-06-21 00:08:24,725 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 172.852
2024-06-21 00:08:25,032 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 105.032
2024-06-21 00:08:25,471 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 70.086
2024-06-21 00:08:25,780 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 56.100
2024-06-21 00:08:26,189 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 77.907
2024-06-21 00:08:26,494 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 157.647
2024-06-21 00:08:26,951 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 95.321
2024-06-21 00:08:27,257 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 35.262
2024-06-21 00:08:27,648 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 48.858
2024-06-21 00:08:27,962 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 46.218
2024-06-21 00:08:28,388 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 21.947
2024-06-21 00:08:28,693 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 29.404
2024-06-21 00:08:29,102 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 24.911
2024-06-21 00:08:29,408 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 26.780
2024-06-21 00:08:29,834 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 35.242
2024-06-21 00:08:30,133 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 36.720
2024-06-21 00:08:30,533 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 23.125
2024-06-21 00:08:30,833 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 20.820
2024-06-21 00:08:31,056 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 22.256
2024-06-21 00:08:42,009 - INFO: 25/200 final results:
2024-06-21 00:08:42,009 - INFO: Training loss: 53.648.
2024-06-21 00:08:42,009 - INFO: Training MAE: 5.451.
2024-06-21 00:08:42,009 - INFO: Training MSE: 54.268.
2024-06-21 00:09:02,447 - INFO: Epoch: 25/200, Loss_train: 53.647536442197605, Loss_val: 18.297565591746363
2024-06-21 00:09:02,496 - INFO: Saved new best metric model for epoch 25.
2024-06-21 00:09:02,496 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:09:02,496 - INFO: Epoch 26/200...
2024-06-21 00:09:02,496 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:09:02,496 - INFO: Batch size: 32.
2024-06-21 00:09:02,500 - INFO: Dataset:
2024-06-21 00:09:02,500 - INFO: Batch size:
2024-06-21 00:09:02,500 - INFO: Number of workers:
2024-06-21 00:09:03,749 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 13.914
2024-06-21 00:09:04,075 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 24.835
2024-06-21 00:09:04,496 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 25.127
2024-06-21 00:09:04,819 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 20.834
2024-06-21 00:09:05,240 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 18.808
2024-06-21 00:09:05,544 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 43.864
2024-06-21 00:09:05,949 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 35.351
2024-06-21 00:09:06,267 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 34.389
2024-06-21 00:09:06,679 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 28.267
2024-06-21 00:09:06,977 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 26.251
2024-06-21 00:09:07,385 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 20.689
2024-06-21 00:09:07,704 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 21.619
2024-06-21 00:09:08,136 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 22.080
2024-06-21 00:09:08,444 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 30.531
2024-06-21 00:09:08,856 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 17.525
2024-06-21 00:09:09,171 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 16.689
2024-06-21 00:09:09,592 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 20.111
2024-06-21 00:09:09,894 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 27.208
2024-06-21 00:09:10,279 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 43.944
2024-06-21 00:09:10,596 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 48.955
2024-06-21 00:09:11,020 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 38.114
2024-06-21 00:09:11,328 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 27.083
2024-06-21 00:09:11,740 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 29.727
2024-06-21 00:09:12,058 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 22.748
2024-06-21 00:09:12,473 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 12.842
2024-06-21 00:09:12,774 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 19.660
2024-06-21 00:09:13,156 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 17.122
2024-06-21 00:09:13,479 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 20.992
2024-06-21 00:09:13,694 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 25.242
2024-06-21 00:09:24,723 - INFO: 26/200 final results:
2024-06-21 00:09:24,724 - INFO: Training loss: 26.018.
2024-06-21 00:09:24,724 - INFO: Training MAE: 3.950.
2024-06-21 00:09:24,724 - INFO: Training MSE: 26.033.
2024-06-21 00:09:45,256 - INFO: Epoch: 26/200, Loss_train: 26.01792940600165, Loss_val: 33.529422595583156
2024-06-21 00:09:45,257 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:09:45,257 - INFO: Epoch 27/200...
2024-06-21 00:09:45,257 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:09:45,257 - INFO: Batch size: 32.
2024-06-21 00:09:45,261 - INFO: Dataset:
2024-06-21 00:09:45,261 - INFO: Batch size:
2024-06-21 00:09:45,261 - INFO: Number of workers:
2024-06-21 00:09:46,508 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 38.744
2024-06-21 00:09:46,846 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 64.236
2024-06-21 00:09:47,249 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 67.314
2024-06-21 00:09:47,572 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 54.727
2024-06-21 00:09:47,993 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 52.798
2024-06-21 00:09:48,309 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 41.877
2024-06-21 00:09:48,699 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 33.634
2024-06-21 00:09:49,016 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 49.633
2024-06-21 00:09:49,426 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 54.461
2024-06-21 00:09:49,737 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 48.537
2024-06-21 00:09:50,124 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 27.937
2024-06-21 00:09:50,444 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 18.914
2024-06-21 00:09:50,875 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 28.943
2024-06-21 00:09:51,194 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 25.569
2024-06-21 00:09:51,592 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 37.831
2024-06-21 00:09:51,907 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 50.501
2024-06-21 00:09:52,328 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 32.792
2024-06-21 00:09:52,642 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 23.442
2024-06-21 00:09:53,030 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 48.012
2024-06-21 00:09:53,341 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 61.673
2024-06-21 00:09:53,759 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 59.516
2024-06-21 00:09:54,076 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 77.155
2024-06-21 00:09:54,473 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 65.667
2024-06-21 00:09:54,790 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 36.079
2024-06-21 00:09:55,207 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 20.503
2024-06-21 00:09:55,521 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 17.399
2024-06-21 00:09:55,911 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 19.057
2024-06-21 00:09:56,225 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 21.395
2024-06-21 00:09:56,450 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 20.973
2024-06-21 00:10:07,489 - INFO: 27/200 final results:
2024-06-21 00:10:07,489 - INFO: Training loss: 41.356.
2024-06-21 00:10:07,489 - INFO: Training MAE: 4.916.
2024-06-21 00:10:07,489 - INFO: Training MSE: 41.759.
2024-06-21 00:10:27,941 - INFO: Epoch: 27/200, Loss_train: 41.35586376847892, Loss_val: 32.83236957418507
2024-06-21 00:10:27,942 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:10:27,942 - INFO: Epoch 28/200...
2024-06-21 00:10:27,942 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:10:27,942 - INFO: Batch size: 32.
2024-06-21 00:10:27,946 - INFO: Dataset:
2024-06-21 00:10:27,946 - INFO: Batch size:
2024-06-21 00:10:27,946 - INFO: Number of workers:
2024-06-21 00:10:29,207 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 24.419
2024-06-21 00:10:29,533 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 41.105
2024-06-21 00:10:29,941 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 30.375
2024-06-21 00:10:30,264 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 18.961
2024-06-21 00:10:30,690 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 23.009
2024-06-21 00:10:31,010 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 16.531
2024-06-21 00:10:31,405 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 16.060
2024-06-21 00:10:31,725 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 17.474
2024-06-21 00:10:32,171 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 18.258
2024-06-21 00:10:32,484 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 13.257
2024-06-21 00:10:32,879 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 24.045
2024-06-21 00:10:33,201 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 21.754
2024-06-21 00:10:33,621 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 14.780
2024-06-21 00:10:33,944 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 15.346
2024-06-21 00:10:34,353 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 15.148
2024-06-21 00:10:34,670 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 17.702
2024-06-21 00:10:35,090 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 19.034
2024-06-21 00:10:35,406 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 15.665
2024-06-21 00:10:35,799 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 13.680
2024-06-21 00:10:36,111 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 16.207
2024-06-21 00:10:36,518 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 25.473
2024-06-21 00:10:36,839 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 33.478
2024-06-21 00:10:37,239 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 45.355
2024-06-21 00:10:37,561 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 50.591
2024-06-21 00:10:37,979 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 53.253
2024-06-21 00:10:38,295 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 53.047
2024-06-21 00:10:38,687 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 68.237
2024-06-21 00:10:39,003 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 66.837
2024-06-21 00:10:39,226 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 63.164
2024-06-21 00:10:49,998 - INFO: 28/200 final results:
2024-06-21 00:10:49,998 - INFO: Training loss: 29.388.
2024-06-21 00:10:49,998 - INFO: Training MAE: 4.081.
2024-06-21 00:10:49,998 - INFO: Training MSE: 28.720.
2024-06-21 00:11:10,378 - INFO: Epoch: 28/200, Loss_train: 29.38771261017898, Loss_val: 96.97136411995723
2024-06-21 00:11:10,378 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:11:10,378 - INFO: Epoch 29/200...
2024-06-21 00:11:10,378 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:11:10,378 - INFO: Batch size: 32.
2024-06-21 00:11:10,382 - INFO: Dataset:
2024-06-21 00:11:10,382 - INFO: Batch size:
2024-06-21 00:11:10,382 - INFO: Number of workers:
2024-06-21 00:11:11,613 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 66.247
2024-06-21 00:11:11,939 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 55.544
2024-06-21 00:11:12,332 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 32.218
2024-06-21 00:11:12,655 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 21.947
2024-06-21 00:11:13,082 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 16.117
2024-06-21 00:11:13,388 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 25.592
2024-06-21 00:11:13,781 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 19.136
2024-06-21 00:11:14,101 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 16.385
2024-06-21 00:11:14,517 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 13.698
2024-06-21 00:11:14,817 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 20.450
2024-06-21 00:11:15,211 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 15.681
2024-06-21 00:11:15,534 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 23.898
2024-06-21 00:11:15,980 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 24.920
2024-06-21 00:11:16,287 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 32.812
2024-06-21 00:11:16,679 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 35.931
2024-06-21 00:11:16,997 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 31.445
2024-06-21 00:11:17,433 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 30.364
2024-06-21 00:11:17,738 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 29.425
2024-06-21 00:11:18,133 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 26.401
2024-06-21 00:11:18,446 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 47.062
2024-06-21 00:11:18,878 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 72.135
2024-06-21 00:11:19,184 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 77.987
2024-06-21 00:11:19,582 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 65.371
2024-06-21 00:11:19,902 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 73.939
2024-06-21 00:11:20,330 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 92.103
2024-06-21 00:11:20,631 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 63.952
2024-06-21 00:11:21,021 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 51.013
2024-06-21 00:11:21,335 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 63.334
2024-06-21 00:11:21,555 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 139.242
2024-06-21 00:11:32,546 - INFO: 29/200 final results:
2024-06-21 00:11:32,546 - INFO: Training loss: 44.288.
2024-06-21 00:11:32,546 - INFO: Training MAE: 4.981.
2024-06-21 00:11:32,546 - INFO: Training MSE: 42.410.
2024-06-21 00:11:52,843 - INFO: Epoch: 29/200, Loss_train: 44.28794009110023, Loss_val: 337.51985957704744
2024-06-21 00:11:52,843 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:11:52,843 - INFO: Epoch 30/200...
2024-06-21 00:11:52,843 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:11:52,843 - INFO: Batch size: 32.
2024-06-21 00:11:52,847 - INFO: Dataset:
2024-06-21 00:11:52,847 - INFO: Batch size:
2024-06-21 00:11:52,847 - INFO: Number of workers:
2024-06-21 00:11:54,119 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 353.901
2024-06-21 00:11:54,431 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 415.692
2024-06-21 00:11:54,848 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 172.040
2024-06-21 00:11:55,172 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 43.330
2024-06-21 00:11:55,580 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 22.084
2024-06-21 00:11:55,896 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 26.173
2024-06-21 00:11:56,304 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 22.937
2024-06-21 00:11:56,626 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 21.773
2024-06-21 00:11:57,025 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 24.452
2024-06-21 00:11:57,340 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 22.929
2024-06-21 00:11:57,740 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 34.352
2024-06-21 00:11:58,062 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 34.422
2024-06-21 00:11:58,482 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 33.668
2024-06-21 00:11:58,805 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 21.020
2024-06-21 00:11:59,219 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 23.996
2024-06-21 00:11:59,537 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 33.504
2024-06-21 00:11:59,947 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 21.839
2024-06-21 00:12:00,265 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 30.804
2024-06-21 00:12:00,670 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 22.826
2024-06-21 00:12:00,986 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 19.022
2024-06-21 00:12:01,392 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 21.978
2024-06-21 00:12:01,714 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 33.836
2024-06-21 00:12:02,114 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 38.074
2024-06-21 00:12:02,434 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 22.374
2024-06-21 00:12:02,830 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 23.291
2024-06-21 00:12:03,146 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 31.572
2024-06-21 00:12:03,539 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 27.243
2024-06-21 00:12:03,854 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 16.749
2024-06-21 00:12:04,067 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 31.249
2024-06-21 00:12:15,097 - INFO: 30/200 final results:
2024-06-21 00:12:15,098 - INFO: Training loss: 56.798.
2024-06-21 00:12:15,098 - INFO: Training MAE: 4.856.
2024-06-21 00:12:15,098 - INFO: Training MSE: 57.303.
2024-06-21 00:12:35,587 - INFO: Epoch: 30/200, Loss_train: 56.79765234322384, Loss_val: 76.55917187394768
2024-06-21 00:12:35,587 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:12:35,587 - INFO: Epoch 31/200...
2024-06-21 00:12:35,587 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:12:35,587 - INFO: Batch size: 32.
2024-06-21 00:12:35,591 - INFO: Dataset:
2024-06-21 00:12:35,591 - INFO: Batch size:
2024-06-21 00:12:35,591 - INFO: Number of workers:
2024-06-21 00:12:36,839 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 51.198
2024-06-21 00:12:37,163 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 29.961
2024-06-21 00:12:37,570 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 30.038
2024-06-21 00:12:37,892 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 26.357
2024-06-21 00:12:38,321 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 25.132
2024-06-21 00:12:38,624 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 25.316
2024-06-21 00:12:39,013 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 21.272
2024-06-21 00:12:39,329 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 23.298
2024-06-21 00:12:39,760 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 22.649
2024-06-21 00:12:40,057 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 24.867
2024-06-21 00:12:40,446 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 16.374
2024-06-21 00:12:40,765 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 24.215
2024-06-21 00:12:41,206 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 14.822
2024-06-21 00:12:41,512 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 26.452
2024-06-21 00:12:41,914 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 22.664
2024-06-21 00:12:42,229 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 22.136
2024-06-21 00:12:42,653 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 28.487
2024-06-21 00:12:42,955 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 22.513
2024-06-21 00:12:43,345 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 11.075
2024-06-21 00:12:43,654 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 25.433
2024-06-21 00:12:44,076 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 34.126
2024-06-21 00:12:44,381 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 20.599
2024-06-21 00:12:44,771 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 11.520
2024-06-21 00:12:45,088 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 23.698
2024-06-21 00:12:45,501 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 34.969
2024-06-21 00:12:45,800 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 37.173
2024-06-21 00:12:46,173 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 54.035
2024-06-21 00:12:46,485 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 34.971
2024-06-21 00:12:46,707 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 25.837
2024-06-21 00:12:57,630 - INFO: 31/200 final results:
2024-06-21 00:12:57,631 - INFO: Training loss: 26.593.
2024-06-21 00:12:57,631 - INFO: Training MAE: 3.968.
2024-06-21 00:12:57,631 - INFO: Training MSE: 26.608.
2024-06-21 00:13:18,283 - INFO: Epoch: 31/200, Loss_train: 26.592683989426185, Loss_val: 41.05403196400609
2024-06-21 00:13:18,283 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:13:18,283 - INFO: Epoch 32/200...
2024-06-21 00:13:18,283 - INFO: Learning rate: 0.0008815598160420683.
2024-06-21 00:13:18,283 - INFO: Batch size: 32.
2024-06-21 00:13:18,287 - INFO: Dataset:
2024-06-21 00:13:18,287 - INFO: Batch size:
2024-06-21 00:13:18,287 - INFO: Number of workers:
2024-06-21 00:13:19,525 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 39.942
2024-06-21 00:13:19,854 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 65.457
2024-06-21 00:13:20,266 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 49.332
2024-06-21 00:13:20,593 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 37.719
2024-06-21 00:13:21,018 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 26.001
2024-06-21 00:13:21,322 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 30.354
2024-06-21 00:13:21,716 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 54.541
2024-06-21 00:13:22,035 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 58.990
2024-06-21 00:13:22,457 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 34.879
2024-06-21 00:13:22,755 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 22.384
2024-06-21 00:13:23,162 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 18.604
2024-06-21 00:13:23,481 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 24.069
2024-06-21 00:13:23,911 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 32.121
2024-06-21 00:13:24,217 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 34.481
2024-06-21 00:13:24,627 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 18.547
2024-06-21 00:13:24,946 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 26.164
2024-06-21 00:13:25,367 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 35.392
2024-06-21 00:13:25,671 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 26.542
2024-06-21 00:13:26,072 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 20.772
2024-06-21 00:13:26,387 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 22.224
2024-06-21 00:13:26,806 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 19.613
2024-06-21 00:13:27,114 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 22.085
2024-06-21 00:13:27,519 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 13.566
2024-06-21 00:13:27,840 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 24.097
2024-06-21 00:13:28,257 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 44.635
2024-06-21 00:13:28,560 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 47.951
2024-06-21 00:13:28,963 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 34.335
2024-06-21 00:13:29,279 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 18.302
2024-06-21 00:13:29,504 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 23.874
2024-06-21 00:13:40,512 - INFO: 32/200 final results:
2024-06-21 00:13:40,512 - INFO: Training loss: 31.964.
2024-06-21 00:13:40,512 - INFO: Training MAE: 4.404.
2024-06-21 00:13:40,512 - INFO: Training MSE: 32.125.
2024-06-21 00:14:00,932 - INFO: Epoch: 32/200, Loss_train: 31.964493685755237, Loss_val: 23.360535884725635
2024-06-21 00:14:00,933 - INFO: Best internal validation val_loss: 18.298 at epoch: 25.
2024-06-21 00:14:00,933 - INFO: Epoch 33/200...
2024-06-21 00:14:00,933 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:14:00,933 - INFO: Batch size: 32.
2024-06-21 00:14:00,937 - INFO: Dataset:
2024-06-21 00:14:00,937 - INFO: Batch size:
2024-06-21 00:14:00,937 - INFO: Number of workers:
2024-06-21 00:14:02,183 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 23.161
2024-06-21 00:14:02,510 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 9.801
2024-06-21 00:14:02,928 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 16.442
2024-06-21 00:14:03,251 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 9.812
2024-06-21 00:14:03,670 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 11.399
2024-06-21 00:14:03,975 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 12.152
2024-06-21 00:14:04,376 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 9.339
2024-06-21 00:14:04,694 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 15.140
2024-06-21 00:14:05,100 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 13.255
2024-06-21 00:14:05,398 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 12.550
2024-06-21 00:14:05,800 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 14.340
2024-06-21 00:14:06,119 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 14.593
2024-06-21 00:14:06,546 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 10.113
2024-06-21 00:14:06,854 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 12.369
2024-06-21 00:14:07,266 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 13.305
2024-06-21 00:14:07,582 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 16.806
2024-06-21 00:14:08,001 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 14.808
2024-06-21 00:14:08,303 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 15.481
2024-06-21 00:14:08,704 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 11.958
2024-06-21 00:14:09,016 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 13.490
2024-06-21 00:14:09,429 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 15.745
2024-06-21 00:14:09,735 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 12.836
2024-06-21 00:14:10,134 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 11.927
2024-06-21 00:14:10,452 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 11.016
2024-06-21 00:14:10,853 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 12.774
2024-06-21 00:14:11,153 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 10.568
2024-06-21 00:14:11,536 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 14.047
2024-06-21 00:14:11,848 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 16.345
2024-06-21 00:14:12,057 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 14.471
2024-06-21 00:14:23,229 - INFO: 33/200 final results:
2024-06-21 00:14:23,229 - INFO: Training loss: 13.450.
2024-06-21 00:14:23,229 - INFO: Training MAE: 2.868.
2024-06-21 00:14:23,229 - INFO: Training MSE: 13.429.
2024-06-21 00:14:43,416 - INFO: Epoch: 33/200, Loss_train: 13.449687694681101, Loss_val: 13.741419561978045
2024-06-21 00:14:43,464 - INFO: Saved new best metric model for epoch 33.
2024-06-21 00:14:43,465 - INFO: Best internal validation val_loss: 13.741 at epoch: 33.
2024-06-21 00:14:43,465 - INFO: Epoch 34/200...
2024-06-21 00:14:43,465 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:14:43,465 - INFO: Batch size: 32.
2024-06-21 00:14:43,469 - INFO: Dataset:
2024-06-21 00:14:43,469 - INFO: Batch size:
2024-06-21 00:14:43,469 - INFO: Number of workers:
2024-06-21 00:14:44,737 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 17.499
2024-06-21 00:14:45,063 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 10.713
2024-06-21 00:14:45,460 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 11.171
2024-06-21 00:14:45,783 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 9.762
2024-06-21 00:14:46,214 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 13.430
2024-06-21 00:14:46,521 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 8.635
2024-06-21 00:14:46,917 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 10.537
2024-06-21 00:14:47,237 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 8.064
2024-06-21 00:14:47,656 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 10.132
2024-06-21 00:14:47,958 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 10.215
2024-06-21 00:14:48,345 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 13.135
2024-06-21 00:14:48,675 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 11.376
2024-06-21 00:14:49,104 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 7.297
2024-06-21 00:14:49,422 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 9.449
2024-06-21 00:14:49,839 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 14.601
2024-06-21 00:14:50,157 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 13.124
2024-06-21 00:14:50,591 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 11.210
2024-06-21 00:14:50,897 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 15.241
2024-06-21 00:14:51,284 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 11.658
2024-06-21 00:14:51,598 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 9.130
2024-06-21 00:14:52,018 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 12.023
2024-06-21 00:14:52,326 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 12.208
2024-06-21 00:14:52,718 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 14.553
2024-06-21 00:14:53,039 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 10.693
2024-06-21 00:14:53,463 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 10.140
2024-06-21 00:14:53,766 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 13.523
2024-06-21 00:14:54,150 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 12.342
2024-06-21 00:14:54,465 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 16.886
2024-06-21 00:14:54,682 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 14.753
2024-06-21 00:15:05,758 - INFO: 34/200 final results:
2024-06-21 00:15:05,758 - INFO: Training loss: 11.845.
2024-06-21 00:15:05,758 - INFO: Training MAE: 2.685.
2024-06-21 00:15:05,758 - INFO: Training MSE: 11.787.
2024-06-21 00:15:26,374 - INFO: Epoch: 34/200, Loss_train: 11.844841069188611, Loss_val: 12.156764967688199
2024-06-21 00:15:26,423 - INFO: Saved new best metric model for epoch 34.
2024-06-21 00:15:26,423 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:15:26,423 - INFO: Epoch 35/200...
2024-06-21 00:15:26,423 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:15:26,423 - INFO: Batch size: 32.
2024-06-21 00:15:26,427 - INFO: Dataset:
2024-06-21 00:15:26,427 - INFO: Batch size:
2024-06-21 00:15:26,427 - INFO: Number of workers:
2024-06-21 00:15:27,686 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 12.111
2024-06-21 00:15:28,011 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 8.947
2024-06-21 00:15:28,416 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 12.564
2024-06-21 00:15:28,743 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 11.781
2024-06-21 00:15:29,168 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 9.669
2024-06-21 00:15:29,488 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 11.078
2024-06-21 00:15:29,882 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 9.721
2024-06-21 00:15:30,204 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 7.260
2024-06-21 00:15:30,619 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 7.807
2024-06-21 00:15:30,933 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 12.030
2024-06-21 00:15:31,326 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 9.318
2024-06-21 00:15:31,648 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 22.822
2024-06-21 00:15:32,080 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 18.742
2024-06-21 00:15:32,403 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 8.934
2024-06-21 00:15:32,808 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 10.842
2024-06-21 00:15:33,125 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 11.531
2024-06-21 00:15:33,547 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 14.561
2024-06-21 00:15:33,865 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 10.771
2024-06-21 00:15:34,257 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 9.780
2024-06-21 00:15:34,571 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 9.744
2024-06-21 00:15:34,989 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 8.899
2024-06-21 00:15:35,310 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 12.089
2024-06-21 00:15:35,710 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 12.724
2024-06-21 00:15:36,030 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 9.632
2024-06-21 00:15:36,448 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 10.334
2024-06-21 00:15:36,765 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 10.856
2024-06-21 00:15:37,157 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 9.936
2024-06-21 00:15:37,473 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 7.793
2024-06-21 00:15:37,697 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 10.843
2024-06-21 00:15:48,737 - INFO: 35/200 final results:
2024-06-21 00:15:48,738 - INFO: Training loss: 11.142.
2024-06-21 00:15:48,738 - INFO: Training MAE: 2.608.
2024-06-21 00:15:48,738 - INFO: Training MSE: 11.148.
2024-06-21 00:16:09,016 - INFO: Epoch: 35/200, Loss_train: 11.141983295309133, Loss_val: 12.80490781520975
2024-06-21 00:16:09,016 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:16:09,016 - INFO: Epoch 36/200...
2024-06-21 00:16:09,016 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:16:09,016 - INFO: Batch size: 32.
2024-06-21 00:16:09,019 - INFO: Dataset:
2024-06-21 00:16:09,020 - INFO: Batch size:
2024-06-21 00:16:09,020 - INFO: Number of workers:
2024-06-21 00:16:10,277 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 11.570
2024-06-21 00:16:10,588 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 9.957
2024-06-21 00:16:11,006 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 14.587
2024-06-21 00:16:11,328 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 13.999
2024-06-21 00:16:11,747 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 12.290
2024-06-21 00:16:12,054 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 7.330
2024-06-21 00:16:12,462 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 7.393
2024-06-21 00:16:12,781 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 9.290
2024-06-21 00:16:13,197 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 8.577
2024-06-21 00:16:13,499 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 8.927
2024-06-21 00:16:13,903 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 9.975
2024-06-21 00:16:14,226 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 12.513
2024-06-21 00:16:14,669 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 14.021
2024-06-21 00:16:14,978 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 19.433
2024-06-21 00:16:15,374 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 21.710
2024-06-21 00:16:15,690 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 16.126
2024-06-21 00:16:16,419 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 14.764
2024-06-21 00:16:16,722 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 17.457
2024-06-21 00:16:17,104 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 15.541
2024-06-21 00:16:17,418 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 11.952
2024-06-21 00:16:17,850 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 14.468
2024-06-21 00:16:18,159 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 22.227
2024-06-21 00:16:18,549 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 32.386
2024-06-21 00:16:18,870 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 33.764
2024-06-21 00:16:19,291 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 24.858
2024-06-21 00:16:19,591 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 17.541
2024-06-21 00:16:19,970 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 15.094
2024-06-21 00:16:20,285 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 11.968
2024-06-21 00:16:20,495 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 14.908
2024-06-21 00:16:31,184 - INFO: 36/200 final results:
2024-06-21 00:16:31,184 - INFO: Training loss: 15.332.
2024-06-21 00:16:31,184 - INFO: Training MAE: 3.009.
2024-06-21 00:16:31,184 - INFO: Training MSE: 15.340.
2024-06-21 00:16:51,822 - INFO: Epoch: 36/200, Loss_train: 15.332048827204211, Loss_val: 29.13670326101369
2024-06-21 00:16:51,822 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:16:51,822 - INFO: Epoch 37/200...
2024-06-21 00:16:51,822 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:16:51,822 - INFO: Batch size: 32.
2024-06-21 00:16:51,826 - INFO: Dataset:
2024-06-21 00:16:51,826 - INFO: Batch size:
2024-06-21 00:16:51,826 - INFO: Number of workers:
2024-06-21 00:16:53,079 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 25.997
2024-06-21 00:16:53,394 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 26.765
2024-06-21 00:16:53,807 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 19.887
2024-06-21 00:16:54,133 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 21.612
2024-06-21 00:16:54,550 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 31.774
2024-06-21 00:16:54,856 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 30.620
2024-06-21 00:16:55,246 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 15.967
2024-06-21 00:16:55,563 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 7.709
2024-06-21 00:16:55,995 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 18.466
2024-06-21 00:16:56,303 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 10.777
2024-06-21 00:16:56,697 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 7.328
2024-06-21 00:16:57,019 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 11.315
2024-06-21 00:16:57,455 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 9.165
2024-06-21 00:16:57,764 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 14.404
2024-06-21 00:16:58,171 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 9.730
2024-06-21 00:16:58,489 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 11.012
2024-06-21 00:16:58,915 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 16.576
2024-06-21 00:16:59,220 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 17.748
2024-06-21 00:16:59,613 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 21.851
2024-06-21 00:16:59,928 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 28.022
2024-06-21 00:17:00,346 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 32.140
2024-06-21 00:17:00,651 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 20.146
2024-06-21 00:17:01,060 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 12.455
2024-06-21 00:17:01,378 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 12.053
2024-06-21 00:17:01,792 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 13.173
2024-06-21 00:17:02,092 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 10.769
2024-06-21 00:17:02,491 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 9.188
2024-06-21 00:17:02,803 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 15.370
2024-06-21 00:17:03,026 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 27.318
2024-06-21 00:17:13,952 - INFO: 37/200 final results:
2024-06-21 00:17:13,953 - INFO: Training loss: 17.563.
2024-06-21 00:17:13,953 - INFO: Training MAE: 3.198.
2024-06-21 00:17:13,953 - INFO: Training MSE: 17.370.
2024-06-21 00:17:34,656 - INFO: Epoch: 37/200, Loss_train: 17.56340931201803, Loss_val: 28.48372712628595
2024-06-21 00:17:34,656 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:17:34,656 - INFO: Epoch 38/200...
2024-06-21 00:17:34,656 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:17:34,656 - INFO: Batch size: 32.
2024-06-21 00:17:34,660 - INFO: Dataset:
2024-06-21 00:17:34,660 - INFO: Batch size:
2024-06-21 00:17:34,660 - INFO: Number of workers:
2024-06-21 00:17:35,906 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 37.277
2024-06-21 00:17:36,243 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 36.532
2024-06-21 00:17:36,648 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 35.917
2024-06-21 00:17:36,975 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 24.355
2024-06-21 00:17:37,378 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 20.289
2024-06-21 00:17:37,710 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 12.057
2024-06-21 00:17:38,095 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 15.801
2024-06-21 00:17:38,414 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 14.951
2024-06-21 00:17:38,815 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 14.269
2024-06-21 00:17:39,149 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 17.970
2024-06-21 00:17:39,541 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 18.600
2024-06-21 00:17:39,864 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 10.972
2024-06-21 00:17:40,271 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 10.446
2024-06-21 00:17:40,606 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 12.072
2024-06-21 00:17:41,006 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 17.778
2024-06-21 00:17:41,325 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 10.417
2024-06-21 00:17:41,728 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 12.385
2024-06-21 00:17:42,055 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 14.264
2024-06-21 00:17:42,442 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 7.865
2024-06-21 00:17:42,753 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 8.379
2024-06-21 00:17:43,152 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 8.867
2024-06-21 00:17:43,481 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 10.645
2024-06-21 00:17:43,867 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 22.392
2024-06-21 00:17:44,185 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 32.831
2024-06-21 00:17:44,578 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 13.874
2024-06-21 00:17:44,902 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 9.782
2024-06-21 00:17:45,284 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 18.790
2024-06-21 00:17:45,597 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 18.807
2024-06-21 00:17:45,811 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 8.614
2024-06-21 00:17:56,766 - INFO: 38/200 final results:
2024-06-21 00:17:56,766 - INFO: Training loss: 17.145.
2024-06-21 00:17:56,766 - INFO: Training MAE: 3.214.
2024-06-21 00:17:56,766 - INFO: Training MSE: 17.314.
2024-06-21 00:18:17,229 - INFO: Epoch: 38/200, Loss_train: 17.1448099695403, Loss_val: 13.391475118439773
2024-06-21 00:18:17,230 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:18:17,230 - INFO: Epoch 39/200...
2024-06-21 00:18:17,230 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:18:17,230 - INFO: Batch size: 32.
2024-06-21 00:18:17,234 - INFO: Dataset:
2024-06-21 00:18:17,234 - INFO: Batch size:
2024-06-21 00:18:17,234 - INFO: Number of workers:
2024-06-21 00:18:18,482 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 15.608
2024-06-21 00:18:19,393 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 13.736
2024-06-21 00:18:19,803 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 16.509
2024-06-21 00:18:20,127 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 19.654
2024-06-21 00:18:20,548 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 21.864
2024-06-21 00:18:20,852 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 22.101
2024-06-21 00:18:21,255 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 15.189
2024-06-21 00:18:21,573 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 15.210
2024-06-21 00:18:21,989 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 15.729
2024-06-21 00:18:22,285 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 14.687
2024-06-21 00:18:22,682 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 10.732
2024-06-21 00:18:23,004 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 9.185
2024-06-21 00:18:23,440 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 4.699
2024-06-21 00:18:23,750 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 6.762
2024-06-21 00:18:24,160 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 12.006
2024-06-21 00:18:24,479 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 14.791
2024-06-21 00:18:24,902 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 10.796
2024-06-21 00:18:25,205 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 12.609
2024-06-21 00:18:25,606 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 20.560
2024-06-21 00:18:25,916 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 20.180
2024-06-21 00:18:26,335 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 17.931
2024-06-21 00:18:26,642 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 24.843
2024-06-21 00:18:27,052 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 29.991
2024-06-21 00:18:27,371 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 25.566
2024-06-21 00:18:27,786 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 8.211
2024-06-21 00:18:28,087 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 9.903
2024-06-21 00:18:28,489 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 17.752
2024-06-21 00:18:28,805 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 14.844
2024-06-21 00:18:29,029 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 14.295
2024-06-21 00:18:39,815 - INFO: 39/200 final results:
2024-06-21 00:18:39,816 - INFO: Training loss: 15.722.
2024-06-21 00:18:39,816 - INFO: Training MAE: 3.087.
2024-06-21 00:18:39,816 - INFO: Training MSE: 15.750.
2024-06-21 00:19:00,205 - INFO: Epoch: 39/200, Loss_train: 15.72216903752294, Loss_val: 36.941024451420226
2024-06-21 00:19:00,205 - INFO: Best internal validation val_loss: 12.157 at epoch: 34.
2024-06-21 00:19:00,205 - INFO: Epoch 40/200...
2024-06-21 00:19:00,205 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:19:00,205 - INFO: Batch size: 32.
2024-06-21 00:19:00,209 - INFO: Dataset:
2024-06-21 00:19:00,209 - INFO: Batch size:
2024-06-21 00:19:00,210 - INFO: Number of workers:
2024-06-21 00:19:01,450 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 16.295
2024-06-21 00:19:01,774 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 10.219
2024-06-21 00:19:02,200 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 6.664
2024-06-21 00:19:02,528 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 7.630
2024-06-21 00:19:02,940 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 8.520
2024-06-21 00:19:03,260 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 10.654
2024-06-21 00:19:03,668 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 11.101
2024-06-21 00:19:03,989 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 8.176
2024-06-21 00:19:04,390 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 9.083
2024-06-21 00:19:04,706 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 7.972
2024-06-21 00:19:05,114 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 7.469
2024-06-21 00:19:05,437 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 10.232
2024-06-21 00:19:05,858 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 12.141
2024-06-21 00:19:06,182 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 7.360
2024-06-21 00:19:06,595 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 19.396
2024-06-21 00:19:06,912 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 28.119
2024-06-21 00:19:07,322 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 19.216
2024-06-21 00:19:07,641 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 17.714
2024-06-21 00:19:08,046 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 15.710
2024-06-21 00:19:08,362 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 14.059
2024-06-21 00:19:08,768 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 18.516
2024-06-21 00:19:09,090 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 22.957
2024-06-21 00:19:09,498 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 20.788
2024-06-21 00:19:09,819 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 19.225
2024-06-21 00:19:10,219 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 21.254
2024-06-21 00:19:10,535 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 11.465
2024-06-21 00:19:10,924 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 9.148
2024-06-21 00:19:11,240 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 8.574
2024-06-21 00:19:11,453 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 9.396
2024-06-21 00:19:22,486 - INFO: 40/200 final results:
2024-06-21 00:19:22,486 - INFO: Training loss: 13.416.
2024-06-21 00:19:22,486 - INFO: Training MAE: 2.812.
2024-06-21 00:19:22,486 - INFO: Training MSE: 13.495.
2024-06-21 00:19:43,058 - INFO: Epoch: 40/200, Loss_train: 13.415614210326096, Loss_val: 12.06425117624217
2024-06-21 00:19:43,104 - INFO: Saved new best metric model for epoch 40.
2024-06-21 00:19:43,104 - INFO: Best internal validation val_loss: 12.064 at epoch: 40.
2024-06-21 00:19:43,104 - INFO: Epoch 41/200...
2024-06-21 00:19:43,104 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:19:43,104 - INFO: Batch size: 32.
2024-06-21 00:19:43,108 - INFO: Dataset:
2024-06-21 00:19:43,108 - INFO: Batch size:
2024-06-21 00:19:43,108 - INFO: Number of workers:
2024-06-21 00:19:44,348 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 14.228
2024-06-21 00:19:44,685 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 21.067
2024-06-21 00:19:45,089 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 18.028
2024-06-21 00:19:45,412 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 13.348
2024-06-21 00:19:45,824 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 17.947
2024-06-21 00:19:46,140 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 17.670
2024-06-21 00:19:46,532 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 16.264
2024-06-21 00:19:46,849 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 16.348
2024-06-21 00:19:47,249 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 13.334
2024-06-21 00:19:47,560 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 6.819
2024-06-21 00:19:47,949 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 8.544
2024-06-21 00:19:48,268 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 9.178
2024-06-21 00:19:48,691 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 10.198
2024-06-21 00:19:49,011 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 6.359
2024-06-21 00:19:49,412 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 5.284
2024-06-21 00:19:49,726 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 7.229
2024-06-21 00:19:50,128 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 18.515
2024-06-21 00:19:50,442 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 41.213
2024-06-21 00:19:50,828 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 66.868
2024-06-21 00:19:51,139 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 52.805
2024-06-21 00:19:51,547 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 31.455
2024-06-21 00:19:51,865 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 17.511
2024-06-21 00:19:52,260 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 11.885
2024-06-21 00:19:52,580 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 8.775
2024-06-21 00:19:52,988 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 11.595
2024-06-21 00:19:53,302 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 12.979
2024-06-21 00:19:53,686 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 12.218
2024-06-21 00:19:54,000 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 5.863
2024-06-21 00:19:54,219 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 10.038
2024-06-21 00:20:05,237 - INFO: 41/200 final results:
2024-06-21 00:20:05,237 - INFO: Training loss: 17.364.
2024-06-21 00:20:05,237 - INFO: Training MAE: 3.089.
2024-06-21 00:20:05,237 - INFO: Training MSE: 17.509.
2024-06-21 00:20:25,624 - INFO: Epoch: 41/200, Loss_train: 17.364322695238837, Loss_val: 26.325227737426758
2024-06-21 00:20:25,624 - INFO: Best internal validation val_loss: 12.064 at epoch: 40.
2024-06-21 00:20:25,624 - INFO: Epoch 42/200...
2024-06-21 00:20:25,624 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:20:25,624 - INFO: Batch size: 32.
2024-06-21 00:20:25,628 - INFO: Dataset:
2024-06-21 00:20:25,628 - INFO: Batch size:
2024-06-21 00:20:25,628 - INFO: Number of workers:
2024-06-21 00:20:26,866 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 27.234
2024-06-21 00:20:27,194 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 27.610
2024-06-21 00:20:27,617 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 17.401
2024-06-21 00:20:27,942 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 18.718
2024-06-21 00:20:28,350 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 21.324
2024-06-21 00:20:28,669 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 16.242
2024-06-21 00:20:29,077 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 12.563
2024-06-21 00:20:29,396 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 11.122
2024-06-21 00:20:29,791 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 13.105
2024-06-21 00:20:30,104 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 10.293
2024-06-21 00:20:30,508 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 9.865
2024-06-21 00:20:30,829 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 6.715
2024-06-21 00:20:31,247 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 8.496
2024-06-21 00:20:31,568 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 11.176
2024-06-21 00:20:31,985 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 21.711
2024-06-21 00:20:32,302 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 15.765
2024-06-21 00:20:32,712 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 11.148
2024-06-21 00:20:33,028 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 18.365
2024-06-21 00:20:33,429 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 15.472
2024-06-21 00:20:33,742 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 12.599
2024-06-21 00:20:34,142 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 15.619
2024-06-21 00:20:34,462 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 10.176
2024-06-21 00:20:34,870 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 9.057
2024-06-21 00:20:35,190 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 16.358
2024-06-21 00:20:35,590 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 14.592
2024-06-21 00:20:35,905 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 9.161
2024-06-21 00:20:36,290 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 11.483
2024-06-21 00:20:36,603 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 13.144
2024-06-21 00:20:36,821 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 10.153
2024-06-21 00:20:47,826 - INFO: 42/200 final results:
2024-06-21 00:20:47,826 - INFO: Training loss: 14.368.
2024-06-21 00:20:47,826 - INFO: Training MAE: 2.962.
2024-06-21 00:20:47,826 - INFO: Training MSE: 14.451.
2024-06-21 00:21:08,108 - INFO: Epoch: 42/200, Loss_train: 14.367791619794122, Loss_val: 12.37946602393841
2024-06-21 00:21:08,108 - INFO: Best internal validation val_loss: 12.064 at epoch: 40.
2024-06-21 00:21:08,108 - INFO: Epoch 43/200...
2024-06-21 00:21:08,108 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:21:08,108 - INFO: Batch size: 32.
2024-06-21 00:21:08,112 - INFO: Dataset:
2024-06-21 00:21:08,112 - INFO: Batch size:
2024-06-21 00:21:08,112 - INFO: Number of workers:
2024-06-21 00:21:09,340 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 11.307
2024-06-21 00:21:09,668 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 12.556
2024-06-21 00:21:10,103 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 16.144
2024-06-21 00:21:10,417 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 19.554
2024-06-21 00:21:10,841 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 17.971
2024-06-21 00:21:11,146 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 14.178
2024-06-21 00:21:11,562 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 15.861
2024-06-21 00:21:11,869 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 11.669
2024-06-21 00:21:12,283 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 11.875
2024-06-21 00:21:12,582 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 11.028
2024-06-21 00:21:13,003 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 10.521
2024-06-21 00:21:13,311 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 11.194
2024-06-21 00:21:13,747 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 12.355
2024-06-21 00:21:14,057 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 9.260
2024-06-21 00:21:14,498 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 12.608
2024-06-21 00:21:14,803 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 17.832
2024-06-21 00:21:15,216 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 21.040
2024-06-21 00:21:15,522 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 21.379
2024-06-21 00:21:15,945 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 23.062
2024-06-21 00:21:16,246 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 16.359
2024-06-21 00:21:16,658 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 17.141
2024-06-21 00:21:16,966 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 11.539
2024-06-21 00:21:17,403 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 5.831
2024-06-21 00:21:17,712 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 10.134
2024-06-21 00:21:18,120 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 13.659
2024-06-21 00:21:18,424 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 21.954
2024-06-21 00:21:18,845 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 32.095
2024-06-21 00:21:19,149 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 19.264
2024-06-21 00:21:19,366 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 6.596
2024-06-21 00:21:30,387 - INFO: 43/200 final results:
2024-06-21 00:21:30,387 - INFO: Training loss: 15.033.
2024-06-21 00:21:30,387 - INFO: Training MAE: 3.018.
2024-06-21 00:21:30,387 - INFO: Training MSE: 15.200.
2024-06-21 00:21:50,830 - INFO: Epoch: 43/200, Loss_train: 15.033244428963497, Loss_val: 10.547081897998678
2024-06-21 00:21:50,875 - INFO: Saved new best metric model for epoch 43.
2024-06-21 00:21:50,876 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:21:50,876 - INFO: Epoch 44/200...
2024-06-21 00:21:50,876 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:21:50,876 - INFO: Batch size: 32.
2024-06-21 00:21:50,880 - INFO: Dataset:
2024-06-21 00:21:50,880 - INFO: Batch size:
2024-06-21 00:21:50,880 - INFO: Number of workers:
2024-06-21 00:21:52,132 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 13.802
2024-06-21 00:21:52,445 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 11.915
2024-06-21 00:21:52,850 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 11.989
2024-06-21 00:21:53,172 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 13.967
2024-06-21 00:21:53,597 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 12.496
2024-06-21 00:21:53,904 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 8.861
2024-06-21 00:21:54,313 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 9.700
2024-06-21 00:21:54,633 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 6.687
2024-06-21 00:21:55,045 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 7.533
2024-06-21 00:21:55,343 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 15.174
2024-06-21 00:21:55,753 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 17.998
2024-06-21 00:21:56,076 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 8.656
2024-06-21 00:21:56,511 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 6.653
2024-06-21 00:21:56,821 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 14.328
2024-06-21 00:21:57,236 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 23.278
2024-06-21 00:21:57,554 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 25.902
2024-06-21 00:21:57,975 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 17.833
2024-06-21 00:21:58,281 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 12.608
2024-06-21 00:21:58,687 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 9.291
2024-06-21 00:21:59,001 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 10.726
2024-06-21 00:21:59,419 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 12.804
2024-06-21 00:21:59,728 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 15.809
2024-06-21 00:22:00,132 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 13.267
2024-06-21 00:22:00,452 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 6.620
2024-06-21 00:22:00,864 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 14.165
2024-06-21 00:22:01,167 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 29.146
2024-06-21 00:22:01,564 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 34.270
2024-06-21 00:22:01,879 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 17.301
2024-06-21 00:22:02,101 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 11.719
2024-06-21 00:22:13,013 - INFO: 44/200 final results:
2024-06-21 00:22:13,013 - INFO: Training loss: 14.293.
2024-06-21 00:22:13,013 - INFO: Training MAE: 2.910.
2024-06-21 00:22:13,013 - INFO: Training MSE: 14.344.
2024-06-21 00:22:33,707 - INFO: Epoch: 44/200, Loss_train: 14.293072519631222, Loss_val: 14.18200055484114
2024-06-21 00:22:33,707 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:22:33,707 - INFO: Epoch 45/200...
2024-06-21 00:22:33,707 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:22:33,707 - INFO: Batch size: 32.
2024-06-21 00:22:33,711 - INFO: Dataset:
2024-06-21 00:22:33,711 - INFO: Batch size:
2024-06-21 00:22:33,711 - INFO: Number of workers:
2024-06-21 00:22:34,968 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 11.105
2024-06-21 00:22:35,284 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 11.937
2024-06-21 00:22:35,706 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 7.275
2024-06-21 00:22:36,033 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 7.077
2024-06-21 00:22:36,457 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 11.998
2024-06-21 00:22:36,763 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 12.829
2024-06-21 00:22:37,167 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 10.695
2024-06-21 00:22:37,486 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 9.698
2024-06-21 00:22:37,899 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 7.378
2024-06-21 00:22:38,200 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 9.543
2024-06-21 00:22:38,607 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 7.550
2024-06-21 00:22:38,928 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 7.498
2024-06-21 00:22:39,359 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 12.533
2024-06-21 00:22:39,668 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 10.667
2024-06-21 00:22:40,084 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 9.215
2024-06-21 00:22:40,401 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 5.856
2024-06-21 00:22:40,826 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 8.963
2024-06-21 00:22:41,131 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 13.320
2024-06-21 00:22:41,541 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 12.443
2024-06-21 00:22:41,854 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 22.821
2024-06-21 00:22:42,277 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 41.318
2024-06-21 00:22:42,586 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 64.112
2024-06-21 00:22:42,990 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 41.468
2024-06-21 00:22:43,311 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 15.264
2024-06-21 00:22:43,718 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 8.917
2024-06-21 00:22:44,019 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 12.449
2024-06-21 00:22:44,404 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 7.443
2024-06-21 00:22:44,718 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 5.564
2024-06-21 00:22:44,933 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 13.385
2024-06-21 00:22:55,951 - INFO: 45/200 final results:
2024-06-21 00:22:55,951 - INFO: Training loss: 14.494.
2024-06-21 00:22:55,951 - INFO: Training MAE: 2.843.
2024-06-21 00:22:55,951 - INFO: Training MSE: 14.516.
2024-06-21 00:23:16,337 - INFO: Epoch: 45/200, Loss_train: 14.493900052432355, Loss_val: 14.415111245780155
2024-06-21 00:23:16,337 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:23:16,337 - INFO: Epoch 46/200...
2024-06-21 00:23:16,337 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:23:16,337 - INFO: Batch size: 32.
2024-06-21 00:23:16,341 - INFO: Dataset:
2024-06-21 00:23:16,341 - INFO: Batch size:
2024-06-21 00:23:16,341 - INFO: Number of workers:
2024-06-21 00:23:17,603 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 9.847
2024-06-21 00:23:17,916 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 6.340
2024-06-21 00:23:18,340 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 7.510
2024-06-21 00:23:18,666 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 7.934
2024-06-21 00:23:19,081 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 10.437
2024-06-21 00:23:19,386 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 11.539
2024-06-21 00:23:19,791 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 9.598
2024-06-21 00:23:20,111 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 8.275
2024-06-21 00:23:20,524 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 7.614
2024-06-21 00:23:20,824 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 8.827
2024-06-21 00:23:21,236 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 10.218
2024-06-21 00:23:21,558 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 8.645
2024-06-21 00:23:21,983 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 11.007
2024-06-21 00:23:22,291 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 11.339
2024-06-21 00:23:22,710 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 8.259
2024-06-21 00:23:23,027 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 8.368
2024-06-21 00:23:23,448 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 5.315
2024-06-21 00:23:23,752 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 7.692
2024-06-21 00:23:24,147 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 12.089
2024-06-21 00:23:24,462 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 7.812
2024-06-21 00:23:24,879 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 6.863
2024-06-21 00:23:25,188 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 9.221
2024-06-21 00:23:25,603 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 9.042
2024-06-21 00:23:25,924 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 14.746
2024-06-21 00:23:26,342 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 16.024
2024-06-21 00:23:26,645 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 15.896
2024-06-21 00:23:27,048 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 14.640
2024-06-21 00:23:27,364 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 12.588
2024-06-21 00:23:27,588 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 15.402
2024-06-21 00:23:38,639 - INFO: 46/200 final results:
2024-06-21 00:23:38,639 - INFO: Training loss: 10.106.
2024-06-21 00:23:38,639 - INFO: Training MAE: 2.503.
2024-06-21 00:23:38,639 - INFO: Training MSE: 10.002.
2024-06-21 00:23:58,984 - INFO: Epoch: 46/200, Loss_train: 10.106432339240765, Loss_val: 24.426679940059266
2024-06-21 00:23:58,984 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:23:58,984 - INFO: Epoch 47/200...
2024-06-21 00:23:58,984 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:23:58,984 - INFO: Batch size: 32.
2024-06-21 00:23:58,988 - INFO: Dataset:
2024-06-21 00:23:58,988 - INFO: Batch size:
2024-06-21 00:23:58,989 - INFO: Number of workers:
2024-06-21 00:24:00,227 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 25.387
2024-06-21 00:24:00,555 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 22.886
2024-06-21 00:24:00,972 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 22.875
2024-06-21 00:24:01,298 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 16.850
2024-06-21 00:24:01,713 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 12.162
2024-06-21 00:24:02,034 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 15.750
2024-06-21 00:24:02,442 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 12.008
2024-06-21 00:24:02,763 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 5.605
2024-06-21 00:24:03,163 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 7.609
2024-06-21 00:24:03,478 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 7.793
2024-06-21 00:24:03,892 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 5.819
2024-06-21 00:24:04,215 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 7.999
2024-06-21 00:24:04,639 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 17.361
2024-06-21 00:24:04,962 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 15.385
2024-06-21 00:24:05,380 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 8.839
2024-06-21 00:24:05,698 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 6.299
2024-06-21 00:24:06,111 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 10.131
2024-06-21 00:24:06,429 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 11.522
2024-06-21 00:24:06,837 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 7.469
2024-06-21 00:24:07,153 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 7.164
2024-06-21 00:24:07,560 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 7.373
2024-06-21 00:24:07,881 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 13.114
2024-06-21 00:24:08,288 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 16.592
2024-06-21 00:24:08,609 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 23.301
2024-06-21 00:24:09,004 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 22.113
2024-06-21 00:24:09,320 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 22.884
2024-06-21 00:24:09,716 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 27.309
2024-06-21 00:24:10,032 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 22.980
2024-06-21 00:24:10,250 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 10.616
2024-06-21 00:24:21,302 - INFO: 47/200 final results:
2024-06-21 00:24:21,302 - INFO: Training loss: 14.248.
2024-06-21 00:24:21,302 - INFO: Training MAE: 2.963.
2024-06-21 00:24:21,302 - INFO: Training MSE: 14.320.
2024-06-21 00:24:41,784 - INFO: Epoch: 47/200, Loss_train: 14.248102549848886, Loss_val: 20.271933062323207
2024-06-21 00:24:41,784 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:24:41,784 - INFO: Epoch 48/200...
2024-06-21 00:24:41,784 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:24:41,784 - INFO: Batch size: 32.
2024-06-21 00:24:41,788 - INFO: Dataset:
2024-06-21 00:24:41,788 - INFO: Batch size:
2024-06-21 00:24:41,788 - INFO: Number of workers:
2024-06-21 00:24:43,070 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 14.458
2024-06-21 00:24:43,380 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 10.058
2024-06-21 00:24:43,787 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 7.022
2024-06-21 00:24:44,110 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 12.707
2024-06-21 00:24:44,541 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 10.652
2024-06-21 00:24:44,844 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 10.409
2024-06-21 00:24:45,233 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 8.814
2024-06-21 00:24:45,550 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 8.455
2024-06-21 00:24:45,983 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 12.546
2024-06-21 00:24:46,280 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 11.205
2024-06-21 00:24:46,674 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 7.892
2024-06-21 00:24:46,993 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 10.066
2024-06-21 00:24:47,432 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 23.096
2024-06-21 00:24:47,739 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 32.170
2024-06-21 00:24:48,142 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 16.406
2024-06-21 00:24:48,456 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 13.781
2024-06-21 00:24:48,886 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 14.155
2024-06-21 00:24:49,188 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 10.285
2024-06-21 00:24:49,578 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 8.760
2024-06-21 00:24:49,889 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 5.485
2024-06-21 00:24:50,315 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 14.434
2024-06-21 00:24:50,620 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 15.101
2024-06-21 00:24:51,017 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 10.792
2024-06-21 00:24:51,335 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 9.358
2024-06-21 00:24:51,758 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 9.809
2024-06-21 00:24:52,058 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 7.027
2024-06-21 00:24:52,443 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 6.223
2024-06-21 00:24:52,756 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 8.339
2024-06-21 00:24:52,974 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 16.661
2024-06-21 00:25:03,901 - INFO: 48/200 final results:
2024-06-21 00:25:03,902 - INFO: Training loss: 11.937.
2024-06-21 00:25:03,902 - INFO: Training MAE: 2.652.
2024-06-21 00:25:03,902 - INFO: Training MSE: 11.843.
2024-06-21 00:25:24,547 - INFO: Epoch: 48/200, Loss_train: 11.936727606017014, Loss_val: 19.35302921821331
2024-06-21 00:25:24,547 - INFO: Best internal validation val_loss: 10.547 at epoch: 43.
2024-06-21 00:25:24,547 - INFO: Epoch 49/200...
2024-06-21 00:25:24,547 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:25:24,547 - INFO: Batch size: 32.
2024-06-21 00:25:24,551 - INFO: Dataset:
2024-06-21 00:25:24,552 - INFO: Batch size:
2024-06-21 00:25:24,552 - INFO: Number of workers:
2024-06-21 00:25:25,794 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 23.188
2024-06-21 00:25:26,119 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 24.526
2024-06-21 00:25:26,525 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 7.709
2024-06-21 00:25:26,858 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 10.762
2024-06-21 00:25:27,297 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 13.880
2024-06-21 00:25:27,611 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 7.948
2024-06-21 00:25:28,031 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 5.858
2024-06-21 00:25:28,359 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 5.787
2024-06-21 00:25:28,780 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 7.142
2024-06-21 00:25:29,081 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 5.362
2024-06-21 00:25:29,483 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 6.025
2024-06-21 00:25:29,806 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 7.045
2024-06-21 00:25:30,237 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 15.998
2024-06-21 00:25:30,545 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 19.476
2024-06-21 00:25:30,955 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 23.210
2024-06-21 00:25:31,270 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 31.591
2024-06-21 00:25:31,690 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 30.564
2024-06-21 00:25:31,993 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 18.239
2024-06-21 00:25:32,394 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 6.036
2024-06-21 00:25:32,704 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 4.850
2024-06-21 00:25:33,123 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 5.907
2024-06-21 00:25:33,429 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 6.491
2024-06-21 00:25:33,837 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 6.085
2024-06-21 00:25:34,156 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 7.784
2024-06-21 00:25:34,568 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 8.127
2024-06-21 00:25:34,869 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 9.988
2024-06-21 00:25:35,255 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 9.583
2024-06-21 00:25:35,568 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 12.792
2024-06-21 00:25:35,780 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 6.302
2024-06-21 00:25:46,709 - INFO: 49/200 final results:
2024-06-21 00:25:46,709 - INFO: Training loss: 12.009.
2024-06-21 00:25:46,709 - INFO: Training MAE: 2.548.
2024-06-21 00:25:46,709 - INFO: Training MSE: 12.122.
2024-06-21 00:26:07,187 - INFO: Epoch: 49/200, Loss_train: 12.008712982309275, Loss_val: 10.255503243413465
2024-06-21 00:26:07,235 - INFO: Saved new best metric model for epoch 49.
2024-06-21 00:26:07,235 - INFO: Best internal validation val_loss: 10.256 at epoch: 49.
2024-06-21 00:26:07,235 - INFO: Epoch 50/200...
2024-06-21 00:26:07,235 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:26:07,235 - INFO: Batch size: 32.
2024-06-21 00:26:07,239 - INFO: Dataset:
2024-06-21 00:26:07,240 - INFO: Batch size:
2024-06-21 00:26:07,240 - INFO: Number of workers:
2024-06-21 00:26:08,482 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 8.849
2024-06-21 00:26:08,807 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 11.758
2024-06-21 00:26:09,238 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 10.353
2024-06-21 00:26:09,550 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 9.304
2024-06-21 00:26:09,974 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 9.122
2024-06-21 00:26:10,278 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 6.145
2024-06-21 00:26:10,696 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 5.035
2024-06-21 00:26:11,002 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 5.190
2024-06-21 00:26:11,414 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 6.456
2024-06-21 00:26:11,713 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 4.643
2024-06-21 00:26:12,131 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 6.103
2024-06-21 00:26:12,440 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 7.331
2024-06-21 00:26:12,863 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 5.763
2024-06-21 00:26:13,172 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 5.875
2024-06-21 00:26:13,598 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 4.228
2024-06-21 00:26:13,900 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 5.944
2024-06-21 00:26:14,299 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 8.311
2024-06-21 00:26:14,601 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 11.552
2024-06-21 00:26:15,012 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 15.287
2024-06-21 00:26:15,309 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 13.787
2024-06-21 00:26:15,703 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 9.689
2024-06-21 00:26:16,008 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 7.027
2024-06-21 00:26:16,429 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 9.340
2024-06-21 00:26:16,734 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 12.483
2024-06-21 00:26:17,127 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 11.166
2024-06-21 00:26:17,428 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 10.659
2024-06-21 00:26:17,827 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 16.444
2024-06-21 00:26:18,127 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 17.330
2024-06-21 00:26:18,330 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 12.382
2024-06-21 00:26:29,272 - INFO: 50/200 final results:
2024-06-21 00:26:29,272 - INFO: Training loss: 9.226.
2024-06-21 00:26:29,272 - INFO: Training MAE: 2.368.
2024-06-21 00:26:29,272 - INFO: Training MSE: 9.164.
2024-06-21 00:26:49,710 - INFO: Epoch: 50/200, Loss_train: 9.226105196722623, Loss_val: 14.881459663654196
2024-06-21 00:26:49,710 - INFO: Best internal validation val_loss: 10.256 at epoch: 49.
2024-06-21 00:26:49,710 - INFO: Epoch 51/200...
2024-06-21 00:26:49,710 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:26:49,710 - INFO: Batch size: 32.
2024-06-21 00:26:49,714 - INFO: Dataset:
2024-06-21 00:26:49,714 - INFO: Batch size:
2024-06-21 00:26:49,714 - INFO: Number of workers:
2024-06-21 00:26:50,956 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 16.240
2024-06-21 00:26:51,283 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 20.983
2024-06-21 00:26:51,704 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 9.519
2024-06-21 00:26:52,028 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 5.551
2024-06-21 00:26:52,450 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 10.404
2024-06-21 00:26:52,755 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 7.143
2024-06-21 00:26:53,150 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 7.541
2024-06-21 00:26:53,470 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 18.674
2024-06-21 00:26:53,876 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 27.213
2024-06-21 00:26:54,179 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 21.810
2024-06-21 00:26:54,572 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 11.897
2024-06-21 00:26:54,894 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 9.695
2024-06-21 00:26:55,327 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 5.495
2024-06-21 00:26:55,637 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 6.950
2024-06-21 00:26:56,053 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 10.281
2024-06-21 00:26:56,371 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 7.747
2024-06-21 00:26:56,796 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 5.202
2024-06-21 00:26:57,100 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 6.831
2024-06-21 00:26:57,503 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 11.455
2024-06-21 00:26:57,817 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 8.277
2024-06-21 00:26:58,239 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 10.141
2024-06-21 00:26:58,548 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 9.552
2024-06-21 00:26:58,950 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 5.148
2024-06-21 00:26:59,270 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 7.089
2024-06-21 00:26:59,676 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 5.684
2024-06-21 00:26:59,976 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 8.709
2024-06-21 00:27:00,368 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 5.796
2024-06-21 00:27:00,683 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 6.612
2024-06-21 00:27:00,895 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 6.226
2024-06-21 00:27:11,867 - INFO: 51/200 final results:
2024-06-21 00:27:11,867 - INFO: Training loss: 10.133.
2024-06-21 00:27:11,867 - INFO: Training MAE: 2.452.
2024-06-21 00:27:11,867 - INFO: Training MSE: 10.211.
2024-06-21 00:27:32,519 - INFO: Epoch: 51/200, Loss_train: 10.133303247649094, Loss_val: 21.54393534824766
2024-06-21 00:27:32,519 - INFO: Best internal validation val_loss: 10.256 at epoch: 49.
2024-06-21 00:27:32,519 - INFO: Epoch 52/200...
2024-06-21 00:27:32,519 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:27:32,519 - INFO: Batch size: 32.
2024-06-21 00:27:32,523 - INFO: Dataset:
2024-06-21 00:27:32,524 - INFO: Batch size:
2024-06-21 00:27:32,524 - INFO: Number of workers:
2024-06-21 00:27:33,785 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 11.874
2024-06-21 00:27:34,110 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 14.765
2024-06-21 00:27:34,539 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 10.511
2024-06-21 00:27:34,850 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 7.079
2024-06-21 00:27:35,248 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 7.739
2024-06-21 00:27:35,564 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 7.261
2024-06-21 00:27:35,980 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 19.730
2024-06-21 00:27:36,285 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 17.835
2024-06-21 00:27:36,667 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 10.028
2024-06-21 00:27:36,979 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 12.597
2024-06-21 00:27:37,400 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 12.960
2024-06-21 00:27:37,707 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 12.778
2024-06-21 00:27:38,108 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 12.184
2024-06-21 00:27:38,426 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 13.798
2024-06-21 00:27:38,865 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 26.582
2024-06-21 00:27:39,168 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 32.330
2024-06-21 00:27:39,566 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 15.619
2024-06-21 00:27:39,882 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 10.297
2024-06-21 00:27:40,309 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 6.117
2024-06-21 00:27:40,609 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 6.951
2024-06-21 00:27:41,006 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 8.070
2024-06-21 00:27:41,325 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 14.592
2024-06-21 00:27:41,760 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 11.938
2024-06-21 00:27:42,066 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 5.311
2024-06-21 00:27:42,461 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 6.667
2024-06-21 00:27:42,776 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 6.301
2024-06-21 00:27:43,194 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 5.680
2024-06-21 00:27:43,495 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 9.222
2024-06-21 00:27:43,712 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 8.029
2024-06-21 00:27:54,712 - INFO: 52/200 final results:
2024-06-21 00:27:54,712 - INFO: Training loss: 11.891.
2024-06-21 00:27:54,712 - INFO: Training MAE: 2.656.
2024-06-21 00:27:54,712 - INFO: Training MSE: 11.968.
2024-06-21 00:28:15,048 - INFO: Epoch: 52/200, Loss_train: 11.891223331977582, Loss_val: 6.3624163249443315
2024-06-21 00:28:15,096 - INFO: Saved new best metric model for epoch 52.
2024-06-21 00:28:15,096 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:28:15,096 - INFO: Epoch 53/200...
2024-06-21 00:28:15,096 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:28:15,096 - INFO: Batch size: 32.
2024-06-21 00:28:15,100 - INFO: Dataset:
2024-06-21 00:28:15,101 - INFO: Batch size:
2024-06-21 00:28:15,101 - INFO: Number of workers:
2024-06-21 00:28:16,336 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 6.115
2024-06-21 00:28:16,661 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 3.729
2024-06-21 00:28:17,079 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 5.738
2024-06-21 00:28:17,403 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 10.866
2024-06-21 00:28:17,829 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 10.884
2024-06-21 00:28:18,137 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 6.847
2024-06-21 00:28:18,544 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 6.755
2024-06-21 00:28:18,865 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 8.108
2024-06-21 00:28:19,276 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 8.290
2024-06-21 00:28:19,578 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 6.657
2024-06-21 00:28:19,984 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 5.773
2024-06-21 00:28:20,307 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 6.506
2024-06-21 00:28:20,739 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 7.461
2024-06-21 00:28:21,050 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 9.116
2024-06-21 00:28:21,467 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 14.407
2024-06-21 00:28:21,786 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 11.957
2024-06-21 00:28:22,206 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 9.555
2024-06-21 00:28:22,512 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 6.715
2024-06-21 00:28:22,917 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 6.406
2024-06-21 00:28:23,231 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 7.403
2024-06-21 00:28:23,648 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 8.303
2024-06-21 00:28:23,955 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 7.014
2024-06-21 00:28:24,354 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 5.912
2024-06-21 00:28:24,674 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 5.832
2024-06-21 00:28:25,080 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 7.682
2024-06-21 00:28:25,383 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 9.801
2024-06-21 00:28:25,770 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 4.003
2024-06-21 00:28:26,086 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 3.942
2024-06-21 00:28:26,298 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 10.568
2024-06-21 00:28:37,370 - INFO: 53/200 final results:
2024-06-21 00:28:37,370 - INFO: Training loss: 7.667.
2024-06-21 00:28:37,370 - INFO: Training MAE: 2.175.
2024-06-21 00:28:37,370 - INFO: Training MSE: 7.610.
2024-06-21 00:28:57,640 - INFO: Epoch: 53/200, Loss_train: 7.667054636725064, Loss_val: 23.124720573425293
2024-06-21 00:28:57,640 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:28:57,640 - INFO: Epoch 54/200...
2024-06-21 00:28:57,640 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:28:57,640 - INFO: Batch size: 32.
2024-06-21 00:28:57,645 - INFO: Dataset:
2024-06-21 00:28:57,645 - INFO: Batch size:
2024-06-21 00:28:57,645 - INFO: Number of workers:
2024-06-21 00:28:58,871 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 16.219
2024-06-21 00:28:59,211 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 21.292
2024-06-21 00:28:59,618 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 23.966
2024-06-21 00:28:59,946 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 18.415
2024-06-21 00:29:00,357 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 11.683
2024-06-21 00:29:00,677 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 5.111
2024-06-21 00:29:01,071 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 7.064
2024-06-21 00:29:01,392 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 9.579
2024-06-21 00:29:01,801 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 9.032
2024-06-21 00:29:02,115 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 6.994
2024-06-21 00:29:02,507 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 5.639
2024-06-21 00:29:02,830 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 4.901
2024-06-21 00:29:03,249 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 4.354
2024-06-21 00:29:03,570 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 6.242
2024-06-21 00:29:03,974 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 8.498
2024-06-21 00:29:04,292 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 9.203
2024-06-21 00:29:04,707 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 6.495
2024-06-21 00:29:05,025 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 4.447
2024-06-21 00:29:05,416 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 10.512
2024-06-21 00:29:05,729 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 23.052
2024-06-21 00:29:06,134 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 17.327
2024-06-21 00:29:06,455 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 4.566
2024-06-21 00:29:06,843 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 13.610
2024-06-21 00:29:07,163 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 12.403
2024-06-21 00:29:07,574 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 8.206
2024-06-21 00:29:07,890 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 11.738
2024-06-21 00:29:08,267 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 15.599
2024-06-21 00:29:08,582 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 10.929
2024-06-21 00:29:08,795 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 10.883
2024-06-21 00:29:19,825 - INFO: 54/200 final results:
2024-06-21 00:29:19,825 - INFO: Training loss: 10.964.
2024-06-21 00:29:19,825 - INFO: Training MAE: 2.537.
2024-06-21 00:29:19,825 - INFO: Training MSE: 10.966.
2024-06-21 00:29:40,589 - INFO: Epoch: 54/200, Loss_train: 10.964151678414181, Loss_val: 6.917788875514064
2024-06-21 00:29:40,589 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:29:40,589 - INFO: Epoch 55/200...
2024-06-21 00:29:40,589 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:29:40,589 - INFO: Batch size: 32.
2024-06-21 00:29:40,593 - INFO: Dataset:
2024-06-21 00:29:40,594 - INFO: Batch size:
2024-06-21 00:29:40,594 - INFO: Number of workers:
2024-06-21 00:29:41,838 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 10.131
2024-06-21 00:29:42,180 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 10.361
2024-06-21 00:29:42,588 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 12.000
2024-06-21 00:29:42,914 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 19.615
2024-06-21 00:29:43,336 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 20.619
2024-06-21 00:29:43,655 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 15.629
2024-06-21 00:29:44,051 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 5.828
2024-06-21 00:29:44,372 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 6.484
2024-06-21 00:29:44,788 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 6.376
2024-06-21 00:29:45,100 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 8.060
2024-06-21 00:29:45,490 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 17.721
2024-06-21 00:29:45,811 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 18.109
2024-06-21 00:29:46,239 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 11.607
2024-06-21 00:29:46,559 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 6.545
2024-06-21 00:29:46,961 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 4.147
2024-06-21 00:29:47,277 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 4.659
2024-06-21 00:29:47,693 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 4.986
2024-06-21 00:29:48,009 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 5.183
2024-06-21 00:29:48,398 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 5.706
2024-06-21 00:29:48,709 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 6.006
2024-06-21 00:29:49,125 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 10.298
2024-06-21 00:29:49,443 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 14.251
2024-06-21 00:29:49,827 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 14.187
2024-06-21 00:29:50,144 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 11.304
2024-06-21 00:29:50,547 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 4.599
2024-06-21 00:29:50,860 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 8.328
2024-06-21 00:29:51,231 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 10.097
2024-06-21 00:29:51,543 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 7.566
2024-06-21 00:29:51,760 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 4.799
2024-06-21 00:30:02,764 - INFO: 55/200 final results:
2024-06-21 00:30:02,764 - INFO: Training loss: 9.835.
2024-06-21 00:30:02,764 - INFO: Training MAE: 2.404.
2024-06-21 00:30:02,764 - INFO: Training MSE: 9.934.
2024-06-21 00:30:23,627 - INFO: Epoch: 55/200, Loss_train: 9.834567135777966, Loss_val: 6.494702388500345
2024-06-21 00:30:23,627 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:30:23,627 - INFO: Epoch 56/200...
2024-06-21 00:30:23,627 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:30:23,627 - INFO: Batch size: 32.
2024-06-21 00:30:23,631 - INFO: Dataset:
2024-06-21 00:30:23,631 - INFO: Batch size:
2024-06-21 00:30:23,631 - INFO: Number of workers:
2024-06-21 00:30:24,882 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 9.617
2024-06-21 00:30:25,209 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 17.934
2024-06-21 00:30:25,617 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 12.073
2024-06-21 00:30:25,945 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 11.883
2024-06-21 00:30:26,379 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 13.347
2024-06-21 00:30:26,682 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 6.472
2024-06-21 00:30:27,073 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 6.482
2024-06-21 00:30:27,390 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 8.023
2024-06-21 00:30:27,824 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 4.996
2024-06-21 00:30:28,120 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 6.902
2024-06-21 00:30:28,506 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 5.190
2024-06-21 00:30:28,825 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 6.029
2024-06-21 00:30:29,268 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 4.451
2024-06-21 00:30:29,574 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 5.403
2024-06-21 00:30:29,977 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 7.223
2024-06-21 00:30:30,292 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 5.970
2024-06-21 00:30:30,723 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 5.172
2024-06-21 00:30:31,024 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 5.468
2024-06-21 00:30:31,409 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 7.303
2024-06-21 00:30:31,720 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 11.079
2024-06-21 00:30:32,147 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 9.989
2024-06-21 00:30:32,451 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 8.164
2024-06-21 00:30:32,847 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 4.922
2024-06-21 00:30:33,164 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 4.871
2024-06-21 00:30:33,589 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 7.149
2024-06-21 00:30:33,889 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 10.782
2024-06-21 00:30:34,275 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 18.641
2024-06-21 00:30:34,588 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 27.618
2024-06-21 00:30:34,808 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 21.291
2024-06-21 00:30:45,726 - INFO: 56/200 final results:
2024-06-21 00:30:45,726 - INFO: Training loss: 9.464.
2024-06-21 00:30:45,726 - INFO: Training MAE: 2.381.
2024-06-21 00:30:45,726 - INFO: Training MSE: 9.230.
2024-06-21 00:31:05,861 - INFO: Epoch: 56/200, Loss_train: 9.463671881577064, Loss_val: 16.646764755249023
2024-06-21 00:31:05,861 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:31:05,861 - INFO: Epoch 57/200...
2024-06-21 00:31:05,861 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:31:05,861 - INFO: Batch size: 32.
2024-06-21 00:31:05,865 - INFO: Dataset:
2024-06-21 00:31:05,865 - INFO: Batch size:
2024-06-21 00:31:05,865 - INFO: Number of workers:
2024-06-21 00:31:07,117 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 13.381
2024-06-21 00:31:07,430 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 12.725
2024-06-21 00:31:07,851 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 10.468
2024-06-21 00:31:08,175 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 6.708
2024-06-21 00:31:08,599 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 9.473
2024-06-21 00:31:08,906 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 15.301
2024-06-21 00:31:09,309 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 20.814
2024-06-21 00:31:09,630 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 15.538
2024-06-21 00:31:10,037 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 8.784
2024-06-21 00:31:10,339 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 6.037
2024-06-21 00:31:10,746 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 9.263
2024-06-21 00:31:11,069 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 6.200
2024-06-21 00:31:11,503 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 8.137
2024-06-21 00:31:11,813 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 6.650
2024-06-21 00:31:12,230 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 4.613
2024-06-21 00:31:12,548 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 5.792
2024-06-21 00:31:12,973 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 4.491
2024-06-21 00:31:13,279 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 2.957
2024-06-21 00:31:13,685 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 5.133
2024-06-21 00:31:14,000 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 6.688
2024-06-21 00:31:14,418 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 8.103
2024-06-21 00:31:14,726 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 8.364
2024-06-21 00:31:15,127 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 10.309
2024-06-21 00:31:15,448 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 14.335
2024-06-21 00:31:15,859 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 11.254
2024-06-21 00:31:16,163 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 5.255
2024-06-21 00:31:16,553 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 4.613
2024-06-21 00:31:16,870 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 4.498
2024-06-21 00:31:17,083 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 3.232
2024-06-21 00:31:28,122 - INFO: 57/200 final results:
2024-06-21 00:31:28,122 - INFO: Training loss: 8.590.
2024-06-21 00:31:28,122 - INFO: Training MAE: 2.262.
2024-06-21 00:31:28,122 - INFO: Training MSE: 8.696.
2024-06-21 00:31:48,452 - INFO: Epoch: 57/200, Loss_train: 8.590225515694454, Loss_val: 9.551066809687121
2024-06-21 00:31:48,452 - INFO: Best internal validation val_loss: 6.362 at epoch: 52.
2024-06-21 00:31:48,452 - INFO: Epoch 58/200...
2024-06-21 00:31:48,452 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:31:48,452 - INFO: Batch size: 32.
2024-06-21 00:31:48,456 - INFO: Dataset:
2024-06-21 00:31:48,456 - INFO: Batch size:
2024-06-21 00:31:48,456 - INFO: Number of workers:
2024-06-21 00:31:49,700 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 12.092
2024-06-21 00:31:50,011 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 18.444
2024-06-21 00:31:50,420 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 9.645
2024-06-21 00:31:50,746 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 3.363
2024-06-21 00:31:51,170 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 4.709
2024-06-21 00:31:51,478 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 6.966
2024-06-21 00:31:51,869 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 9.509
2024-06-21 00:31:52,188 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 7.672
2024-06-21 00:31:52,604 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 5.274
2024-06-21 00:31:52,906 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 3.996
2024-06-21 00:31:53,299 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 4.729
2024-06-21 00:31:53,621 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 10.720
2024-06-21 00:31:54,051 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 14.141
2024-06-21 00:31:54,360 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 16.321
2024-06-21 00:31:54,763 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 17.606
2024-06-21 00:31:55,081 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 11.496
2024-06-21 00:31:55,502 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 10.897
2024-06-21 00:31:55,807 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 6.079
2024-06-21 00:31:56,198 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 5.701
2024-06-21 00:31:56,513 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 4.883
2024-06-21 00:31:56,935 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 5.595
2024-06-21 00:31:57,244 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 8.585
2024-06-21 00:31:57,644 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 10.050
2024-06-21 00:31:57,964 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 9.809
2024-06-21 00:31:58,370 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 9.915
2024-06-21 00:31:58,674 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 4.431
2024-06-21 00:31:59,058 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 5.710
2024-06-21 00:31:59,373 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 5.674
2024-06-21 00:31:59,590 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 5.459
2024-06-21 00:32:10,634 - INFO: 58/200 final results:
2024-06-21 00:32:10,634 - INFO: Training loss: 8.602.
2024-06-21 00:32:10,634 - INFO: Training MAE: 2.246.
2024-06-21 00:32:10,634 - INFO: Training MSE: 8.665.
2024-06-21 00:32:31,033 - INFO: Epoch: 58/200, Loss_train: 8.602481241883902, Loss_val: 5.1991903946317475
2024-06-21 00:32:31,082 - INFO: Saved new best metric model for epoch 58.
2024-06-21 00:32:31,082 - INFO: Best internal validation val_loss: 5.199 at epoch: 58.
2024-06-21 00:32:31,082 - INFO: Epoch 59/200...
2024-06-21 00:32:31,082 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:32:31,082 - INFO: Batch size: 32.
2024-06-21 00:32:31,086 - INFO: Dataset:
2024-06-21 00:32:31,086 - INFO: Batch size:
2024-06-21 00:32:31,086 - INFO: Number of workers:
2024-06-21 00:32:32,343 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 5.395
2024-06-21 00:32:32,657 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 7.882
2024-06-21 00:32:33,079 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 6.734
2024-06-21 00:32:33,405 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 5.650
2024-06-21 00:32:33,828 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 3.317
2024-06-21 00:32:34,136 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 4.136
2024-06-21 00:32:34,530 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 4.179
2024-06-21 00:32:34,851 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 5.383
2024-06-21 00:32:35,282 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 4.848
2024-06-21 00:32:35,584 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 3.918
2024-06-21 00:32:35,974 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 6.355
2024-06-21 00:32:36,297 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 6.744
2024-06-21 00:32:36,733 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 6.174
2024-06-21 00:32:37,043 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 5.205
2024-06-21 00:32:37,447 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 5.137
2024-06-21 00:32:37,766 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 5.828
2024-06-21 00:32:38,189 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 9.254
2024-06-21 00:32:38,494 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 12.211
2024-06-21 00:32:38,882 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 8.597
2024-06-21 00:32:39,196 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 3.973
2024-06-21 00:32:39,619 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 4.948
2024-06-21 00:32:39,928 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 4.412
2024-06-21 00:32:40,315 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 4.022
2024-06-21 00:32:40,637 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 5.729
2024-06-21 00:32:41,057 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 7.702
2024-06-21 00:32:41,360 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 11.768
2024-06-21 00:32:41,744 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 15.378
2024-06-21 00:32:42,059 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 15.460
2024-06-21 00:32:42,273 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 15.747
2024-06-21 00:32:53,473 - INFO: 59/200 final results:
2024-06-21 00:32:53,473 - INFO: Training loss: 7.106.
2024-06-21 00:32:53,473 - INFO: Training MAE: 2.016.
2024-06-21 00:32:53,473 - INFO: Training MSE: 6.935.
2024-06-21 00:33:13,989 - INFO: Epoch: 59/200, Loss_train: 7.106402561582368, Loss_val: 17.860870394213446
2024-06-21 00:33:13,990 - INFO: Best internal validation val_loss: 5.199 at epoch: 58.
2024-06-21 00:33:13,990 - INFO: Epoch 60/200...
2024-06-21 00:33:13,990 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:33:13,990 - INFO: Batch size: 32.
2024-06-21 00:33:13,993 - INFO: Dataset:
2024-06-21 00:33:13,994 - INFO: Batch size:
2024-06-21 00:33:13,994 - INFO: Number of workers:
2024-06-21 00:33:15,247 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 21.831
2024-06-21 00:33:15,561 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 20.128
2024-06-21 00:33:15,975 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 15.713
2024-06-21 00:33:16,302 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 10.610
2024-06-21 00:33:16,725 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 10.029
2024-06-21 00:33:17,032 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 8.137
2024-06-21 00:33:17,441 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 5.793
2024-06-21 00:33:17,762 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 6.171
2024-06-21 00:33:18,177 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 5.338
2024-06-21 00:33:18,479 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 5.680
2024-06-21 00:33:18,874 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 5.312
2024-06-21 00:33:19,196 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 3.280
2024-06-21 00:33:19,620 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 5.398
2024-06-21 00:33:19,929 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 5.587
2024-06-21 00:33:20,337 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 6.802
2024-06-21 00:33:20,655 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 8.567
2024-06-21 00:33:21,079 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 14.299
2024-06-21 00:33:21,384 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 9.032
2024-06-21 00:33:21,777 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 3.439
2024-06-21 00:33:22,093 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 8.264
2024-06-21 00:33:22,507 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 12.547
2024-06-21 00:33:22,815 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 12.682
2024-06-21 00:33:23,229 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 10.296
2024-06-21 00:33:23,551 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 12.344
2024-06-21 00:33:23,963 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 12.183
2024-06-21 00:33:24,263 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 8.096
2024-06-21 00:33:24,663 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 6.153
2024-06-21 00:33:24,976 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 3.896
2024-06-21 00:33:25,199 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 4.001
2024-06-21 00:33:36,279 - INFO: 60/200 final results:
2024-06-21 00:33:36,279 - INFO: Training loss: 9.021.
2024-06-21 00:33:36,279 - INFO: Training MAE: 2.343.
2024-06-21 00:33:36,279 - INFO: Training MSE: 9.120.
2024-06-21 00:33:56,742 - INFO: Epoch: 60/200, Loss_train: 9.021035334159588, Loss_val: 5.156924617701564
2024-06-21 00:33:56,791 - INFO: Saved new best metric model for epoch 60.
2024-06-21 00:33:56,791 - INFO: Best internal validation val_loss: 5.157 at epoch: 60.
2024-06-21 00:33:56,791 - INFO: Epoch 61/200...
2024-06-21 00:33:56,791 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:33:56,791 - INFO: Batch size: 32.
2024-06-21 00:33:56,795 - INFO: Dataset:
2024-06-21 00:33:56,795 - INFO: Batch size:
2024-06-21 00:33:56,795 - INFO: Number of workers:
2024-06-21 00:33:58,078 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 5.225
2024-06-21 00:33:58,393 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 4.739
2024-06-21 00:33:58,801 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 4.311
2024-06-21 00:33:59,127 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 3.106
2024-06-21 00:33:59,546 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 3.950
2024-06-21 00:33:59,850 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 3.369
2024-06-21 00:34:00,240 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 3.512
2024-06-21 00:34:00,558 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 9.251
2024-06-21 00:34:00,975 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 17.872
2024-06-21 00:34:01,274 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 19.970
2024-06-21 00:34:01,661 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 13.356
2024-06-21 00:34:01,981 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 6.997
2024-06-21 00:34:02,409 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 10.133
2024-06-21 00:34:02,716 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 8.293
2024-06-21 00:34:03,117 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 9.346
2024-06-21 00:34:03,431 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 11.766
2024-06-21 00:34:03,845 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 15.897
2024-06-21 00:34:04,148 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 15.847
2024-06-21 00:34:04,534 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 9.465
2024-06-21 00:34:04,845 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 7.156
2024-06-21 00:34:05,261 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 4.870
2024-06-21 00:34:05,566 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 4.017
2024-06-21 00:34:05,956 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 4.796
2024-06-21 00:34:06,276 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 5.823
2024-06-21 00:34:06,690 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 9.587
2024-06-21 00:34:06,990 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 13.283
2024-06-21 00:34:07,372 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 11.645
2024-06-21 00:34:07,685 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 10.512
2024-06-21 00:34:07,900 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 5.762
2024-06-21 00:34:18,904 - INFO: 61/200 final results:
2024-06-21 00:34:18,904 - INFO: Training loss: 8.754.
2024-06-21 00:34:18,904 - INFO: Training MAE: 2.309.
2024-06-21 00:34:18,904 - INFO: Training MSE: 8.813.
2024-06-21 00:34:39,130 - INFO: Epoch: 61/200, Loss_train: 8.753621068494073, Loss_val: 5.6022818417384705
2024-06-21 00:34:39,130 - INFO: Best internal validation val_loss: 5.157 at epoch: 60.
2024-06-21 00:34:39,130 - INFO: Epoch 62/200...
2024-06-21 00:34:39,130 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:34:39,130 - INFO: Batch size: 32.
2024-06-21 00:34:39,134 - INFO: Dataset:
2024-06-21 00:34:39,134 - INFO: Batch size:
2024-06-21 00:34:39,134 - INFO: Number of workers:
2024-06-21 00:34:40,367 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 4.575
2024-06-21 00:34:40,694 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 2.942
2024-06-21 00:34:41,115 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 4.340
2024-06-21 00:34:41,440 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 5.899
2024-06-21 00:34:41,862 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 7.570
2024-06-21 00:34:42,168 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 16.366
2024-06-21 00:34:42,573 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 21.673
2024-06-21 00:34:42,893 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 12.446
2024-06-21 00:34:43,301 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 8.259
2024-06-21 00:34:43,600 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 9.250
2024-06-21 00:34:44,001 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 6.131
2024-06-21 00:34:44,321 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 4.346
2024-06-21 00:34:44,748 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 4.830
2024-06-21 00:34:45,056 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 5.901
2024-06-21 00:34:45,469 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 5.721
2024-06-21 00:34:45,785 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 6.053
2024-06-21 00:34:46,200 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 3.973
2024-06-21 00:34:46,503 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 5.572
2024-06-21 00:34:46,904 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 9.340
2024-06-21 00:34:47,215 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 10.967
2024-06-21 00:34:47,632 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 9.716
2024-06-21 00:34:47,938 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 8.716
2024-06-21 00:34:48,334 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 7.763
2024-06-21 00:34:48,652 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 5.097
2024-06-21 00:34:49,054 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 6.091
2024-06-21 00:34:49,355 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 4.845
2024-06-21 00:34:49,738 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 3.910
2024-06-21 00:34:50,050 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 4.492
2024-06-21 00:34:50,264 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 6.311
2024-06-21 00:35:01,222 - INFO: 62/200 final results:
2024-06-21 00:35:01,222 - INFO: Training loss: 7.348.
2024-06-21 00:35:01,222 - INFO: Training MAE: 2.060.
2024-06-21 00:35:01,222 - INFO: Training MSE: 7.369.
2024-06-21 00:35:21,866 - INFO: Epoch: 62/200, Loss_train: 7.348087343676337, Loss_val: 9.328647432656124
2024-06-21 00:35:21,866 - INFO: Best internal validation val_loss: 5.157 at epoch: 60.
2024-06-21 00:35:21,866 - INFO: Epoch 63/200...
2024-06-21 00:35:21,866 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:35:21,866 - INFO: Batch size: 32.
2024-06-21 00:35:21,871 - INFO: Dataset:
2024-06-21 00:35:21,871 - INFO: Batch size:
2024-06-21 00:35:21,871 - INFO: Number of workers:
2024-06-21 00:35:23,132 - INFO: Epoch: 63/200, Batch: 1/29, Batch_Loss_Train: 7.642
2024-06-21 00:35:23,447 - INFO: Epoch: 63/200, Batch: 2/29, Batch_Loss_Train: 9.230
2024-06-21 00:35:23,874 - INFO: Epoch: 63/200, Batch: 3/29, Batch_Loss_Train: 7.460
2024-06-21 00:35:24,187 - INFO: Epoch: 63/200, Batch: 4/29, Batch_Loss_Train: 5.703
2024-06-21 00:35:24,606 - INFO: Epoch: 63/200, Batch: 5/29, Batch_Loss_Train: 4.581
2024-06-21 00:35:24,912 - INFO: Epoch: 63/200, Batch: 6/29, Batch_Loss_Train: 2.881
2024-06-21 00:35:25,321 - INFO: Epoch: 63/200, Batch: 7/29, Batch_Loss_Train: 4.789
2024-06-21 00:35:25,628 - INFO: Epoch: 63/200, Batch: 8/29, Batch_Loss_Train: 10.699
2024-06-21 00:35:26,044 - INFO: Epoch: 63/200, Batch: 9/29, Batch_Loss_Train: 12.364
2024-06-21 00:35:26,345 - INFO: Epoch: 63/200, Batch: 10/29, Batch_Loss_Train: 10.283
2024-06-21 00:35:26,762 - INFO: Epoch: 63/200, Batch: 11/29, Batch_Loss_Train: 4.751
2024-06-21 00:35:27,071 - INFO: Epoch: 63/200, Batch: 12/29, Batch_Loss_Train: 5.303
2024-06-21 00:35:27,507 - INFO: Epoch: 63/200, Batch: 13/29, Batch_Loss_Train: 6.481
2024-06-21 00:35:27,814 - INFO: Epoch: 63/200, Batch: 14/29, Batch_Loss_Train: 5.549
2024-06-21 00:35:28,238 - INFO: Epoch: 63/200, Batch: 15/29, Batch_Loss_Train: 5.495
2024-06-21 00:35:28,544 - INFO: Epoch: 63/200, Batch: 16/29, Batch_Loss_Train: 4.309
2024-06-21 00:35:28,965 - INFO: Epoch: 63/200, Batch: 17/29, Batch_Loss_Train: 2.847
2024-06-21 00:35:29,270 - INFO: Epoch: 63/200, Batch: 18/29, Batch_Loss_Train: 4.068
2024-06-21 00:35:29,683 - INFO: Epoch: 63/200, Batch: 19/29, Batch_Loss_Train: 7.594
2024-06-21 00:35:29,984 - INFO: Epoch: 63/200, Batch: 20/29, Batch_Loss_Train: 10.284
2024-06-21 00:35:30,400 - INFO: Epoch: 63/200, Batch: 21/29, Batch_Loss_Train: 25.669
2024-06-21 00:35:30,709 - INFO: Epoch: 63/200, Batch: 22/29, Batch_Loss_Train: 45.831
2024-06-21 00:35:31,133 - INFO: Epoch: 63/200, Batch: 23/29, Batch_Loss_Train: 19.225
2024-06-21 00:35:31,442 - INFO: Epoch: 63/200, Batch: 24/29, Batch_Loss_Train: 9.312
2024-06-21 00:35:31,861 - INFO: Epoch: 63/200, Batch: 25/29, Batch_Loss_Train: 15.980
2024-06-21 00:35:32,165 - INFO: Epoch: 63/200, Batch: 26/29, Batch_Loss_Train: 13.601
2024-06-21 00:35:32,568 - INFO: Epoch: 63/200, Batch: 27/29, Batch_Loss_Train: 6.778
2024-06-21 00:35:32,872 - INFO: Epoch: 63/200, Batch: 28/29, Batch_Loss_Train: 6.261
2024-06-21 00:35:33,097 - INFO: Epoch: 63/200, Batch: 29/29, Batch_Loss_Train: 4.706
2024-06-21 00:35:44,166 - INFO: 63/200 final results:
2024-06-21 00:35:44,167 - INFO: Training loss: 9.644.
2024-06-21 00:35:44,167 - INFO: Training MAE: 2.307.
2024-06-21 00:35:44,167 - INFO: Training MSE: 9.742.
2024-06-21 00:36:04,365 - INFO: Epoch: 63/200, Loss_train: 9.643996764873636, Loss_val: 7.671828187745193
2024-06-21 00:36:04,365 - INFO: Best internal validation val_loss: 5.157 at epoch: 60.
2024-06-21 00:36:04,365 - INFO: Epoch 64/200...
2024-06-21 00:36:04,365 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:36:04,365 - INFO: Batch size: 32.
2024-06-21 00:36:04,369 - INFO: Dataset:
2024-06-21 00:36:04,369 - INFO: Batch size:
2024-06-21 00:36:04,369 - INFO: Number of workers:
2024-06-21 00:36:05,664 - INFO: Epoch: 64/200, Batch: 1/29, Batch_Loss_Train: 6.871
2024-06-21 00:36:05,979 - INFO: Epoch: 64/200, Batch: 2/29, Batch_Loss_Train: 6.029
2024-06-21 00:36:06,392 - INFO: Epoch: 64/200, Batch: 3/29, Batch_Loss_Train: 6.096
2024-06-21 00:36:06,720 - INFO: Epoch: 64/200, Batch: 4/29, Batch_Loss_Train: 7.596
2024-06-21 00:36:07,168 - INFO: Epoch: 64/200, Batch: 5/29, Batch_Loss_Train: 6.896
2024-06-21 00:36:07,473 - INFO: Epoch: 64/200, Batch: 6/29, Batch_Loss_Train: 4.629
2024-06-21 00:36:07,868 - INFO: Epoch: 64/200, Batch: 7/29, Batch_Loss_Train: 4.484
2024-06-21 00:36:08,173 - INFO: Epoch: 64/200, Batch: 8/29, Batch_Loss_Train: 5.577
2024-06-21 00:36:08,626 - INFO: Epoch: 64/200, Batch: 9/29, Batch_Loss_Train: 3.758
2024-06-21 00:36:08,925 - INFO: Epoch: 64/200, Batch: 10/29, Batch_Loss_Train: 5.854
2024-06-21 00:36:09,318 - INFO: Epoch: 64/200, Batch: 11/29, Batch_Loss_Train: 5.079
2024-06-21 00:36:09,627 - INFO: Epoch: 64/200, Batch: 12/29, Batch_Loss_Train: 4.498
2024-06-21 00:36:10,083 - INFO: Epoch: 64/200, Batch: 13/29, Batch_Loss_Train: 3.846
2024-06-21 00:36:10,391 - INFO: Epoch: 64/200, Batch: 14/29, Batch_Loss_Train: 7.101
2024-06-21 00:36:10,796 - INFO: Epoch: 64/200, Batch: 15/29, Batch_Loss_Train: 8.428
2024-06-21 00:36:11,099 - INFO: Epoch: 64/200, Batch: 16/29, Batch_Loss_Train: 9.597
2024-06-21 00:36:11,547 - INFO: Epoch: 64/200, Batch: 17/29, Batch_Loss_Train: 11.344
2024-06-21 00:36:11,850 - INFO: Epoch: 64/200, Batch: 18/29, Batch_Loss_Train: 4.617
2024-06-21 00:36:12,244 - INFO: Epoch: 64/200, Batch: 19/29, Batch_Loss_Train: 3.811
2024-06-21 00:36:12,542 - INFO: Epoch: 64/200, Batch: 20/29, Batch_Loss_Train: 6.224
2024-06-21 00:36:12,984 - INFO: Epoch: 64/200, Batch: 21/29, Batch_Loss_Train: 5.284
2024-06-21 00:36:13,290 - INFO: Epoch: 64/200, Batch: 22/29, Batch_Loss_Train: 7.824
2024-06-21 00:36:13,687 - INFO: Epoch: 64/200, Batch: 23/29, Batch_Loss_Train: 7.920
2024-06-21 00:36:13,993 - INFO: Epoch: 64/200, Batch: 24/29, Batch_Loss_Train: 7.749
2024-06-21 00:36:14,427 - INFO: Epoch: 64/200, Batch: 25/29, Batch_Loss_Train: 7.875
2024-06-21 00:36:14,729 - INFO: Epoch: 64/200, Batch: 26/29, Batch_Loss_Train: 7.492
2024-06-21 00:36:15,117 - INFO: Epoch: 64/200, Batch: 27/29, Batch_Loss_Train: 5.184
2024-06-21 00:36:15,419 - INFO: Epoch: 64/200, Batch: 28/29, Batch_Loss_Train: 6.184
2024-06-21 00:36:15,639 - INFO: Epoch: 64/200, Batch: 29/29, Batch_Loss_Train: 9.772
2024-06-21 00:36:26,638 - INFO: 64/200 final results:
2024-06-21 00:36:26,638 - INFO: Training loss: 6.470.
2024-06-21 00:36:26,638 - INFO: Training MAE: 1.949.
2024-06-21 00:36:26,638 - INFO: Training MSE: 6.404.
2024-06-21 00:36:47,345 - INFO: Epoch: 64/200, Loss_train: 6.469629156178441, Loss_val: 9.02544855249339
2024-06-21 00:36:47,345 - INFO: Best internal validation val_loss: 5.157 at epoch: 60.
2024-06-21 00:36:47,345 - INFO: Epoch 65/200...
2024-06-21 00:36:47,345 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:36:47,345 - INFO: Batch size: 32.
2024-06-21 00:36:47,349 - INFO: Dataset:
2024-06-21 00:36:47,349 - INFO: Batch size:
2024-06-21 00:36:47,349 - INFO: Number of workers:
2024-06-21 00:36:48,584 - INFO: Epoch: 65/200, Batch: 1/29, Batch_Loss_Train: 12.003
2024-06-21 00:36:48,927 - INFO: Epoch: 65/200, Batch: 2/29, Batch_Loss_Train: 13.918
2024-06-21 00:36:49,336 - INFO: Epoch: 65/200, Batch: 3/29, Batch_Loss_Train: 9.714
2024-06-21 00:36:49,662 - INFO: Epoch: 65/200, Batch: 4/29, Batch_Loss_Train: 7.559
2024-06-21 00:36:50,071 - INFO: Epoch: 65/200, Batch: 5/29, Batch_Loss_Train: 9.462
2024-06-21 00:36:50,403 - INFO: Epoch: 65/200, Batch: 6/29, Batch_Loss_Train: 5.779
2024-06-21 00:36:50,782 - INFO: Epoch: 65/200, Batch: 7/29, Batch_Loss_Train: 5.392
2024-06-21 00:36:51,102 - INFO: Epoch: 65/200, Batch: 8/29, Batch_Loss_Train: 5.874
2024-06-21 00:36:51,498 - INFO: Epoch: 65/200, Batch: 9/29, Batch_Loss_Train: 3.289
2024-06-21 00:36:51,828 - INFO: Epoch: 65/200, Batch: 10/29, Batch_Loss_Train: 4.947
2024-06-21 00:36:52,208 - INFO: Epoch: 65/200, Batch: 11/29, Batch_Loss_Train: 5.831
2024-06-21 00:36:52,530 - INFO: Epoch: 65/200, Batch: 12/29, Batch_Loss_Train: 4.573
2024-06-21 00:36:52,947 - INFO: Epoch: 65/200, Batch: 13/29, Batch_Loss_Train: 4.308
2024-06-21 00:36:53,281 - INFO: Epoch: 65/200, Batch: 14/29, Batch_Loss_Train: 4.598
2024-06-21 00:36:53,674 - INFO: Epoch: 65/200, Batch: 15/29, Batch_Loss_Train: 4.541
2024-06-21 00:36:53,992 - INFO: Epoch: 65/200, Batch: 16/29, Batch_Loss_Train: 4.876
2024-06-21 00:36:54,399 - INFO: Epoch: 65/200, Batch: 17/29, Batch_Loss_Train: 5.354
2024-06-21 00:36:54,729 - INFO: Epoch: 65/200, Batch: 18/29, Batch_Loss_Train: 5.681
2024-06-21 00:36:55,106 - INFO: Epoch: 65/200, Batch: 19/29, Batch_Loss_Train: 5.991
2024-06-21 00:36:55,420 - INFO: Epoch: 65/200, Batch: 20/29, Batch_Loss_Train: 10.783
2024-06-21 00:36:55,823 - INFO: Epoch: 65/200, Batch: 21/29, Batch_Loss_Train: 16.319
2024-06-21 00:36:56,155 - INFO: Epoch: 65/200, Batch: 22/29, Batch_Loss_Train: 25.297
2024-06-21 00:36:56,542 - INFO: Epoch: 65/200, Batch: 23/29, Batch_Loss_Train: 16.916
2024-06-21 00:36:56,862 - INFO: Epoch: 65/200, Batch: 24/29, Batch_Loss_Train: 7.075
2024-06-21 00:36:57,256 - INFO: Epoch: 65/200, Batch: 25/29, Batch_Loss_Train: 7.249
2024-06-21 00:36:57,583 - INFO: Epoch: 65/200, Batch: 26/29, Batch_Loss_Train: 6.015
2024-06-21 00:36:57,957 - INFO: Epoch: 65/200, Batch: 27/29, Batch_Loss_Train: 6.105
2024-06-21 00:36:58,272 - INFO: Epoch: 65/200, Batch: 28/29, Batch_Loss_Train: 4.999
2024-06-21 00:36:58,484 - INFO: Epoch: 65/200, Batch: 29/29, Batch_Loss_Train: 4.156
2024-06-21 00:37:09,613 - INFO: 65/200 final results:
2024-06-21 00:37:09,613 - INFO: Training loss: 7.883.
2024-06-21 00:37:09,613 - INFO: Training MAE: 2.191.
2024-06-21 00:37:09,613 - INFO: Training MSE: 7.957.
2024-06-21 00:37:30,033 - INFO: Epoch: 65/200, Loss_train: 7.8829089526472425, Loss_val: 4.782475356397958
2024-06-21 00:37:30,080 - INFO: Saved new best metric model for epoch 65.
2024-06-21 00:37:30,080 - INFO: Best internal validation val_loss: 4.782 at epoch: 65.
2024-06-21 00:37:30,080 - INFO: Epoch 66/200...
2024-06-21 00:37:30,080 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:37:30,080 - INFO: Batch size: 32.
2024-06-21 00:37:30,084 - INFO: Dataset:
2024-06-21 00:37:30,084 - INFO: Batch size:
2024-06-21 00:37:30,084 - INFO: Number of workers:
2024-06-21 00:37:31,354 - INFO: Epoch: 66/200, Batch: 1/29, Batch_Loss_Train: 4.361
2024-06-21 00:37:31,668 - INFO: Epoch: 66/200, Batch: 2/29, Batch_Loss_Train: 5.183
2024-06-21 00:37:32,089 - INFO: Epoch: 66/200, Batch: 3/29, Batch_Loss_Train: 4.377
2024-06-21 00:37:32,415 - INFO: Epoch: 66/200, Batch: 4/29, Batch_Loss_Train: 6.510
2024-06-21 00:37:32,826 - INFO: Epoch: 66/200, Batch: 5/29, Batch_Loss_Train: 5.188
2024-06-21 00:37:33,133 - INFO: Epoch: 66/200, Batch: 6/29, Batch_Loss_Train: 5.780
2024-06-21 00:37:33,537 - INFO: Epoch: 66/200, Batch: 7/29, Batch_Loss_Train: 8.337
2024-06-21 00:37:33,857 - INFO: Epoch: 66/200, Batch: 8/29, Batch_Loss_Train: 7.176
2024-06-21 00:37:34,258 - INFO: Epoch: 66/200, Batch: 9/29, Batch_Loss_Train: 7.202
2024-06-21 00:37:34,558 - INFO: Epoch: 66/200, Batch: 10/29, Batch_Loss_Train: 6.404
2024-06-21 00:37:34,959 - INFO: Epoch: 66/200, Batch: 11/29, Batch_Loss_Train: 4.454
2024-06-21 00:37:35,281 - INFO: Epoch: 66/200, Batch: 12/29, Batch_Loss_Train: 4.951
2024-06-21 00:37:35,704 - INFO: Epoch: 66/200, Batch: 13/29, Batch_Loss_Train: 4.578
2024-06-21 00:37:36,013 - INFO: Epoch: 66/200, Batch: 14/29, Batch_Loss_Train: 4.149
2024-06-21 00:37:36,426 - INFO: Epoch: 66/200, Batch: 15/29, Batch_Loss_Train: 4.164
2024-06-21 00:37:36,743 - INFO: Epoch: 66/200, Batch: 16/29, Batch_Loss_Train: 6.153
2024-06-21 00:37:37,158 - INFO: Epoch: 66/200, Batch: 17/29, Batch_Loss_Train: 7.001
2024-06-21 00:37:37,462 - INFO: Epoch: 66/200, Batch: 18/29, Batch_Loss_Train: 7.964
2024-06-21 00:37:37,867 - INFO: Epoch: 66/200, Batch: 19/29, Batch_Loss_Train: 16.146
2024-06-21 00:37:38,182 - INFO: Epoch: 66/200, Batch: 20/29, Batch_Loss_Train: 40.121
2024-06-21 00:37:38,588 - INFO: Epoch: 66/200, Batch: 21/29, Batch_Loss_Train: 38.480
2024-06-21 00:37:38,896 - INFO: Epoch: 66/200, Batch: 22/29, Batch_Loss_Train: 14.861
2024-06-21 00:37:39,296 - INFO: Epoch: 66/200, Batch: 23/29, Batch_Loss_Train: 8.804
2024-06-21 00:37:39,616 - INFO: Epoch: 66/200, Batch: 24/29, Batch_Loss_Train: 8.265
2024-06-21 00:37:40,020 - INFO: Epoch: 66/200, Batch: 25/29, Batch_Loss_Train: 4.846
2024-06-21 00:37:40,322 - INFO: Epoch: 66/200, Batch: 26/29, Batch_Loss_Train: 4.913
2024-06-21 00:37:40,708 - INFO: Epoch: 66/200, Batch: 27/29, Batch_Loss_Train: 9.161
2024-06-21 00:37:41,024 - INFO: Epoch: 66/200, Batch: 28/29, Batch_Loss_Train: 5.848
2024-06-21 00:37:41,236 - INFO: Epoch: 66/200, Batch: 29/29, Batch_Loss_Train: 4.134
2024-06-21 00:37:52,247 - INFO: 66/200 final results:
2024-06-21 00:37:52,247 - INFO: Training loss: 8.949.
2024-06-21 00:37:52,247 - INFO: Training MAE: 2.244.
2024-06-21 00:37:52,247 - INFO: Training MSE: 9.044.
2024-06-21 00:38:12,638 - INFO: Epoch: 66/200, Loss_train: 8.948673544258908, Loss_val: 7.197538803363669
2024-06-21 00:38:12,639 - INFO: Best internal validation val_loss: 4.782 at epoch: 65.
2024-06-21 00:38:12,639 - INFO: Epoch 67/200...
2024-06-21 00:38:12,639 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:38:12,639 - INFO: Batch size: 32.
2024-06-21 00:38:12,642 - INFO: Dataset:
2024-06-21 00:38:12,643 - INFO: Batch size:
2024-06-21 00:38:12,643 - INFO: Number of workers:
2024-06-21 00:38:13,919 - INFO: Epoch: 67/200, Batch: 1/29, Batch_Loss_Train: 5.989
2024-06-21 00:38:14,232 - INFO: Epoch: 67/200, Batch: 2/29, Batch_Loss_Train: 3.727
2024-06-21 00:38:14,635 - INFO: Epoch: 67/200, Batch: 3/29, Batch_Loss_Train: 4.163
2024-06-21 00:38:14,947 - INFO: Epoch: 67/200, Batch: 4/29, Batch_Loss_Train: 3.342
2024-06-21 00:38:15,390 - INFO: Epoch: 67/200, Batch: 5/29, Batch_Loss_Train: 3.834
2024-06-21 00:38:15,696 - INFO: Epoch: 67/200, Batch: 6/29, Batch_Loss_Train: 4.143
2024-06-21 00:38:16,084 - INFO: Epoch: 67/200, Batch: 7/29, Batch_Loss_Train: 4.189
2024-06-21 00:38:16,391 - INFO: Epoch: 67/200, Batch: 8/29, Batch_Loss_Train: 4.946
2024-06-21 00:38:16,841 - INFO: Epoch: 67/200, Batch: 9/29, Batch_Loss_Train: 5.485
2024-06-21 00:38:17,143 - INFO: Epoch: 67/200, Batch: 10/29, Batch_Loss_Train: 3.862
2024-06-21 00:38:17,535 - INFO: Epoch: 67/200, Batch: 11/29, Batch_Loss_Train: 4.533
2024-06-21 00:38:17,845 - INFO: Epoch: 67/200, Batch: 12/29, Batch_Loss_Train: 2.956
2024-06-21 00:38:18,297 - INFO: Epoch: 67/200, Batch: 13/29, Batch_Loss_Train: 3.462
2024-06-21 00:38:18,607 - INFO: Epoch: 67/200, Batch: 14/29, Batch_Loss_Train: 2.575
2024-06-21 00:38:19,009 - INFO: Epoch: 67/200, Batch: 15/29, Batch_Loss_Train: 3.669
2024-06-21 00:38:19,314 - INFO: Epoch: 67/200, Batch: 16/29, Batch_Loss_Train: 6.536
2024-06-21 00:38:19,754 - INFO: Epoch: 67/200, Batch: 17/29, Batch_Loss_Train: 11.005
2024-06-21 00:38:20,057 - INFO: Epoch: 67/200, Batch: 18/29, Batch_Loss_Train: 10.779
2024-06-21 00:38:20,443 - INFO: Epoch: 67/200, Batch: 19/29, Batch_Loss_Train: 9.260
2024-06-21 00:38:20,743 - INFO: Epoch: 67/200, Batch: 20/29, Batch_Loss_Train: 7.266
2024-06-21 00:38:21,182 - INFO: Epoch: 67/200, Batch: 21/29, Batch_Loss_Train: 5.360
2024-06-21 00:38:21,491 - INFO: Epoch: 67/200, Batch: 22/29, Batch_Loss_Train: 4.240
2024-06-21 00:38:21,879 - INFO: Epoch: 67/200, Batch: 23/29, Batch_Loss_Train: 5.103
2024-06-21 00:38:22,187 - INFO: Epoch: 67/200, Batch: 24/29, Batch_Loss_Train: 4.538
2024-06-21 00:38:22,618 - INFO: Epoch: 67/200, Batch: 25/29, Batch_Loss_Train: 3.982
2024-06-21 00:38:22,921 - INFO: Epoch: 67/200, Batch: 26/29, Batch_Loss_Train: 3.323
2024-06-21 00:38:23,295 - INFO: Epoch: 67/200, Batch: 27/29, Batch_Loss_Train: 3.707
2024-06-21 00:38:23,597 - INFO: Epoch: 67/200, Batch: 28/29, Batch_Loss_Train: 4.394
2024-06-21 00:38:23,810 - INFO: Epoch: 67/200, Batch: 29/29, Batch_Loss_Train: 7.492
2024-06-21 00:38:34,824 - INFO: 67/200 final results:
2024-06-21 00:38:34,825 - INFO: Training loss: 5.099.
2024-06-21 00:38:34,825 - INFO: Training MAE: 1.742.
2024-06-21 00:38:34,825 - INFO: Training MSE: 5.051.
2024-06-21 00:38:55,191 - INFO: Epoch: 67/200, Loss_train: 5.098657295621675, Loss_val: 9.550424362051077
2024-06-21 00:38:55,191 - INFO: Best internal validation val_loss: 4.782 at epoch: 65.
2024-06-21 00:38:55,191 - INFO: Epoch 68/200...
2024-06-21 00:38:55,191 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:38:55,191 - INFO: Batch size: 32.
2024-06-21 00:38:55,195 - INFO: Dataset:
2024-06-21 00:38:55,195 - INFO: Batch size:
2024-06-21 00:38:55,195 - INFO: Number of workers:
2024-06-21 00:38:56,432 - INFO: Epoch: 68/200, Batch: 1/29, Batch_Loss_Train: 7.672
2024-06-21 00:38:56,757 - INFO: Epoch: 68/200, Batch: 2/29, Batch_Loss_Train: 6.956
2024-06-21 00:38:57,169 - INFO: Epoch: 68/200, Batch: 3/29, Batch_Loss_Train: 4.019
2024-06-21 00:38:57,492 - INFO: Epoch: 68/200, Batch: 4/29, Batch_Loss_Train: 10.025
2024-06-21 00:38:57,906 - INFO: Epoch: 68/200, Batch: 5/29, Batch_Loss_Train: 8.985
2024-06-21 00:38:58,209 - INFO: Epoch: 68/200, Batch: 6/29, Batch_Loss_Train: 8.714
2024-06-21 00:38:58,616 - INFO: Epoch: 68/200, Batch: 7/29, Batch_Loss_Train: 11.438
2024-06-21 00:38:58,936 - INFO: Epoch: 68/200, Batch: 8/29, Batch_Loss_Train: 7.823
2024-06-21 00:38:59,357 - INFO: Epoch: 68/200, Batch: 9/29, Batch_Loss_Train: 4.270
2024-06-21 00:38:59,658 - INFO: Epoch: 68/200, Batch: 10/29, Batch_Loss_Train: 6.539
2024-06-21 00:39:00,064 - INFO: Epoch: 68/200, Batch: 11/29, Batch_Loss_Train: 13.230
2024-06-21 00:39:00,387 - INFO: Epoch: 68/200, Batch: 12/29, Batch_Loss_Train: 10.533
2024-06-21 00:39:00,816 - INFO: Epoch: 68/200, Batch: 13/29, Batch_Loss_Train: 6.605
2024-06-21 00:39:01,125 - INFO: Epoch: 68/200, Batch: 14/29, Batch_Loss_Train: 6.163
2024-06-21 00:39:01,543 - INFO: Epoch: 68/200, Batch: 15/29, Batch_Loss_Train: 3.937
2024-06-21 00:39:01,862 - INFO: Epoch: 68/200, Batch: 16/29, Batch_Loss_Train: 4.420
2024-06-21 00:39:02,285 - INFO: Epoch: 68/200, Batch: 17/29, Batch_Loss_Train: 4.124
2024-06-21 00:39:02,591 - INFO: Epoch: 68/200, Batch: 18/29, Batch_Loss_Train: 5.214
2024-06-21 00:39:02,994 - INFO: Epoch: 68/200, Batch: 19/29, Batch_Loss_Train: 7.095
2024-06-21 00:39:03,309 - INFO: Epoch: 68/200, Batch: 20/29, Batch_Loss_Train: 7.680
2024-06-21 00:39:03,725 - INFO: Epoch: 68/200, Batch: 21/29, Batch_Loss_Train: 5.237
2024-06-21 00:39:04,033 - INFO: Epoch: 68/200, Batch: 22/29, Batch_Loss_Train: 4.334
2024-06-21 00:39:04,434 - INFO: Epoch: 68/200, Batch: 23/29, Batch_Loss_Train: 2.700
2024-06-21 00:39:04,754 - INFO: Epoch: 68/200, Batch: 24/29, Batch_Loss_Train: 3.681
2024-06-21 00:39:05,160 - INFO: Epoch: 68/200, Batch: 25/29, Batch_Loss_Train: 4.149
2024-06-21 00:39:05,462 - INFO: Epoch: 68/200, Batch: 26/29, Batch_Loss_Train: 5.551
2024-06-21 00:39:05,850 - INFO: Epoch: 68/200, Batch: 27/29, Batch_Loss_Train: 3.804
2024-06-21 00:39:06,164 - INFO: Epoch: 68/200, Batch: 28/29, Batch_Loss_Train: 3.928
2024-06-21 00:39:06,377 - INFO: Epoch: 68/200, Batch: 29/29, Batch_Loss_Train: 3.901
2024-06-21 00:39:17,393 - INFO: 68/200 final results:
2024-06-21 00:39:17,394 - INFO: Training loss: 6.301.
2024-06-21 00:39:17,394 - INFO: Training MAE: 1.927.
2024-06-21 00:39:17,394 - INFO: Training MSE: 6.348.
2024-06-21 00:39:37,980 - INFO: Epoch: 68/200, Loss_train: 6.300847102855814, Loss_val: 3.6290235437195877
2024-06-21 00:39:38,027 - INFO: Saved new best metric model for epoch 68.
2024-06-21 00:39:38,027 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:39:38,027 - INFO: Epoch 69/200...
2024-06-21 00:39:38,027 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:39:38,027 - INFO: Batch size: 32.
2024-06-21 00:39:38,031 - INFO: Dataset:
2024-06-21 00:39:38,031 - INFO: Batch size:
2024-06-21 00:39:38,031 - INFO: Number of workers:
2024-06-21 00:39:39,299 - INFO: Epoch: 69/200, Batch: 1/29, Batch_Loss_Train: 4.531
2024-06-21 00:39:39,613 - INFO: Epoch: 69/200, Batch: 2/29, Batch_Loss_Train: 3.875
2024-06-21 00:39:40,017 - INFO: Epoch: 69/200, Batch: 3/29, Batch_Loss_Train: 4.639
2024-06-21 00:39:40,343 - INFO: Epoch: 69/200, Batch: 4/29, Batch_Loss_Train: 9.606
2024-06-21 00:39:40,773 - INFO: Epoch: 69/200, Batch: 5/29, Batch_Loss_Train: 14.089
2024-06-21 00:39:41,078 - INFO: Epoch: 69/200, Batch: 6/29, Batch_Loss_Train: 12.941
2024-06-21 00:39:41,469 - INFO: Epoch: 69/200, Batch: 7/29, Batch_Loss_Train: 12.842
2024-06-21 00:39:41,789 - INFO: Epoch: 69/200, Batch: 8/29, Batch_Loss_Train: 10.555
2024-06-21 00:39:42,221 - INFO: Epoch: 69/200, Batch: 9/29, Batch_Loss_Train: 10.920
2024-06-21 00:39:42,523 - INFO: Epoch: 69/200, Batch: 10/29, Batch_Loss_Train: 11.029
2024-06-21 00:39:42,917 - INFO: Epoch: 69/200, Batch: 11/29, Batch_Loss_Train: 11.615
2024-06-21 00:39:43,242 - INFO: Epoch: 69/200, Batch: 12/29, Batch_Loss_Train: 7.773
2024-06-21 00:39:43,686 - INFO: Epoch: 69/200, Batch: 13/29, Batch_Loss_Train: 4.241
2024-06-21 00:39:43,996 - INFO: Epoch: 69/200, Batch: 14/29, Batch_Loss_Train: 3.851
2024-06-21 00:39:44,399 - INFO: Epoch: 69/200, Batch: 15/29, Batch_Loss_Train: 5.910
2024-06-21 00:39:44,719 - INFO: Epoch: 69/200, Batch: 16/29, Batch_Loss_Train: 5.188
2024-06-21 00:39:45,154 - INFO: Epoch: 69/200, Batch: 17/29, Batch_Loss_Train: 2.772
2024-06-21 00:39:45,459 - INFO: Epoch: 69/200, Batch: 18/29, Batch_Loss_Train: 6.280
2024-06-21 00:39:45,849 - INFO: Epoch: 69/200, Batch: 19/29, Batch_Loss_Train: 9.754
2024-06-21 00:39:46,163 - INFO: Epoch: 69/200, Batch: 20/29, Batch_Loss_Train: 8.762
2024-06-21 00:39:46,592 - INFO: Epoch: 69/200, Batch: 21/29, Batch_Loss_Train: 5.694
2024-06-21 00:39:46,901 - INFO: Epoch: 69/200, Batch: 22/29, Batch_Loss_Train: 3.575
2024-06-21 00:39:47,301 - INFO: Epoch: 69/200, Batch: 23/29, Batch_Loss_Train: 3.972
2024-06-21 00:39:47,623 - INFO: Epoch: 69/200, Batch: 24/29, Batch_Loss_Train: 2.798
2024-06-21 00:39:48,051 - INFO: Epoch: 69/200, Batch: 25/29, Batch_Loss_Train: 6.126
2024-06-21 00:39:48,353 - INFO: Epoch: 69/200, Batch: 26/29, Batch_Loss_Train: 10.247
2024-06-21 00:39:48,743 - INFO: Epoch: 69/200, Batch: 27/29, Batch_Loss_Train: 10.590
2024-06-21 00:39:49,059 - INFO: Epoch: 69/200, Batch: 28/29, Batch_Loss_Train: 11.514
2024-06-21 00:39:49,281 - INFO: Epoch: 69/200, Batch: 29/29, Batch_Loss_Train: 10.950
2024-06-21 00:40:00,287 - INFO: 69/200 final results:
2024-06-21 00:40:00,287 - INFO: Training loss: 7.815.
2024-06-21 00:40:00,288 - INFO: Training MAE: 2.152.
2024-06-21 00:40:00,288 - INFO: Training MSE: 7.753.
2024-06-21 00:40:20,560 - INFO: Epoch: 69/200, Loss_train: 7.815112845651035, Loss_val: 12.819187526045175
2024-06-21 00:40:20,560 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:40:20,560 - INFO: Epoch 70/200...
2024-06-21 00:40:20,560 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:40:20,561 - INFO: Batch size: 32.
2024-06-21 00:40:20,565 - INFO: Dataset:
2024-06-21 00:40:20,565 - INFO: Batch size:
2024-06-21 00:40:20,565 - INFO: Number of workers:
2024-06-21 00:40:21,878 - INFO: Epoch: 70/200, Batch: 1/29, Batch_Loss_Train: 11.202
2024-06-21 00:40:22,189 - INFO: Epoch: 70/200, Batch: 2/29, Batch_Loss_Train: 6.889
2024-06-21 00:40:22,579 - INFO: Epoch: 70/200, Batch: 3/29, Batch_Loss_Train: 6.747
2024-06-21 00:40:22,900 - INFO: Epoch: 70/200, Batch: 4/29, Batch_Loss_Train: 6.297
2024-06-21 00:40:23,317 - INFO: Epoch: 70/200, Batch: 5/29, Batch_Loss_Train: 4.600
2024-06-21 00:40:23,619 - INFO: Epoch: 70/200, Batch: 6/29, Batch_Loss_Train: 3.507
2024-06-21 00:40:24,004 - INFO: Epoch: 70/200, Batch: 7/29, Batch_Loss_Train: 3.510
2024-06-21 00:40:24,320 - INFO: Epoch: 70/200, Batch: 8/29, Batch_Loss_Train: 6.118
2024-06-21 00:40:24,732 - INFO: Epoch: 70/200, Batch: 9/29, Batch_Loss_Train: 7.883
2024-06-21 00:40:25,029 - INFO: Epoch: 70/200, Batch: 10/29, Batch_Loss_Train: 9.445
2024-06-21 00:40:25,430 - INFO: Epoch: 70/200, Batch: 11/29, Batch_Loss_Train: 6.624
2024-06-21 00:40:25,752 - INFO: Epoch: 70/200, Batch: 12/29, Batch_Loss_Train: 3.104
2024-06-21 00:40:26,202 - INFO: Epoch: 70/200, Batch: 13/29, Batch_Loss_Train: 7.033
2024-06-21 00:40:26,512 - INFO: Epoch: 70/200, Batch: 14/29, Batch_Loss_Train: 3.452
2024-06-21 00:40:26,919 - INFO: Epoch: 70/200, Batch: 15/29, Batch_Loss_Train: 4.104
2024-06-21 00:40:27,237 - INFO: Epoch: 70/200, Batch: 16/29, Batch_Loss_Train: 6.148
2024-06-21 00:40:27,673 - INFO: Epoch: 70/200, Batch: 17/29, Batch_Loss_Train: 4.064
2024-06-21 00:40:27,979 - INFO: Epoch: 70/200, Batch: 18/29, Batch_Loss_Train: 5.540
2024-06-21 00:40:28,376 - INFO: Epoch: 70/200, Batch: 19/29, Batch_Loss_Train: 5.771
2024-06-21 00:40:28,692 - INFO: Epoch: 70/200, Batch: 20/29, Batch_Loss_Train: 4.321
2024-06-21 00:40:29,128 - INFO: Epoch: 70/200, Batch: 21/29, Batch_Loss_Train: 4.267
2024-06-21 00:40:29,437 - INFO: Epoch: 70/200, Batch: 22/29, Batch_Loss_Train: 4.680
2024-06-21 00:40:29,837 - INFO: Epoch: 70/200, Batch: 23/29, Batch_Loss_Train: 3.976
2024-06-21 00:40:30,158 - INFO: Epoch: 70/200, Batch: 24/29, Batch_Loss_Train: 4.606
2024-06-21 00:40:30,587 - INFO: Epoch: 70/200, Batch: 25/29, Batch_Loss_Train: 4.776
2024-06-21 00:40:30,890 - INFO: Epoch: 70/200, Batch: 26/29, Batch_Loss_Train: 5.963
2024-06-21 00:40:31,281 - INFO: Epoch: 70/200, Batch: 27/29, Batch_Loss_Train: 7.692
2024-06-21 00:40:31,598 - INFO: Epoch: 70/200, Batch: 28/29, Batch_Loss_Train: 8.151
2024-06-21 00:40:31,821 - INFO: Epoch: 70/200, Batch: 29/29, Batch_Loss_Train: 7.959
2024-06-21 00:40:42,839 - INFO: 70/200 final results:
2024-06-21 00:40:42,839 - INFO: Training loss: 5.808.
2024-06-21 00:40:42,839 - INFO: Training MAE: 1.837.
2024-06-21 00:40:42,840 - INFO: Training MSE: 5.765.
2024-06-21 00:41:03,704 - INFO: Epoch: 70/200, Loss_train: 5.807844219536617, Loss_val: 4.958157638023639
2024-06-21 00:41:03,704 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:41:03,704 - INFO: Epoch 71/200...
2024-06-21 00:41:03,704 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:41:03,704 - INFO: Batch size: 32.
2024-06-21 00:41:03,708 - INFO: Dataset:
2024-06-21 00:41:03,708 - INFO: Batch size:
2024-06-21 00:41:03,709 - INFO: Number of workers:
2024-06-21 00:41:04,949 - INFO: Epoch: 71/200, Batch: 1/29, Batch_Loss_Train: 6.747
2024-06-21 00:41:05,274 - INFO: Epoch: 71/200, Batch: 2/29, Batch_Loss_Train: 4.654
2024-06-21 00:41:05,675 - INFO: Epoch: 71/200, Batch: 3/29, Batch_Loss_Train: 4.981
2024-06-21 00:41:05,997 - INFO: Epoch: 71/200, Batch: 4/29, Batch_Loss_Train: 4.043
2024-06-21 00:41:06,403 - INFO: Epoch: 71/200, Batch: 5/29, Batch_Loss_Train: 1.967
2024-06-21 00:41:06,718 - INFO: Epoch: 71/200, Batch: 6/29, Batch_Loss_Train: 5.027
2024-06-21 00:41:07,104 - INFO: Epoch: 71/200, Batch: 7/29, Batch_Loss_Train: 8.765
2024-06-21 00:41:07,419 - INFO: Epoch: 71/200, Batch: 8/29, Batch_Loss_Train: 11.994
2024-06-21 00:41:07,823 - INFO: Epoch: 71/200, Batch: 9/29, Batch_Loss_Train: 4.885
2024-06-21 00:41:08,136 - INFO: Epoch: 71/200, Batch: 10/29, Batch_Loss_Train: 6.390
2024-06-21 00:41:08,523 - INFO: Epoch: 71/200, Batch: 11/29, Batch_Loss_Train: 9.252
2024-06-21 00:41:08,844 - INFO: Epoch: 71/200, Batch: 12/29, Batch_Loss_Train: 11.216
2024-06-21 00:41:09,279 - INFO: Epoch: 71/200, Batch: 13/29, Batch_Loss_Train: 12.298
2024-06-21 00:41:09,602 - INFO: Epoch: 71/200, Batch: 14/29, Batch_Loss_Train: 9.966
2024-06-21 00:41:10,003 - INFO: Epoch: 71/200, Batch: 15/29, Batch_Loss_Train: 9.161
2024-06-21 00:41:10,321 - INFO: Epoch: 71/200, Batch: 16/29, Batch_Loss_Train: 6.779
2024-06-21 00:41:10,748 - INFO: Epoch: 71/200, Batch: 17/29, Batch_Loss_Train: 5.123
2024-06-21 00:41:11,066 - INFO: Epoch: 71/200, Batch: 18/29, Batch_Loss_Train: 3.849
2024-06-21 00:41:11,456 - INFO: Epoch: 71/200, Batch: 19/29, Batch_Loss_Train: 7.893
2024-06-21 00:41:11,771 - INFO: Epoch: 71/200, Batch: 20/29, Batch_Loss_Train: 7.159
2024-06-21 00:41:12,195 - INFO: Epoch: 71/200, Batch: 21/29, Batch_Loss_Train: 6.413
2024-06-21 00:41:12,516 - INFO: Epoch: 71/200, Batch: 22/29, Batch_Loss_Train: 4.856
2024-06-21 00:41:12,916 - INFO: Epoch: 71/200, Batch: 23/29, Batch_Loss_Train: 4.615
2024-06-21 00:41:13,238 - INFO: Epoch: 71/200, Batch: 24/29, Batch_Loss_Train: 4.342
2024-06-21 00:41:13,656 - INFO: Epoch: 71/200, Batch: 25/29, Batch_Loss_Train: 3.780
2024-06-21 00:41:13,972 - INFO: Epoch: 71/200, Batch: 26/29, Batch_Loss_Train: 4.863
2024-06-21 00:41:14,365 - INFO: Epoch: 71/200, Batch: 27/29, Batch_Loss_Train: 3.937
2024-06-21 00:41:14,681 - INFO: Epoch: 71/200, Batch: 28/29, Batch_Loss_Train: 4.266
2024-06-21 00:41:14,906 - INFO: Epoch: 71/200, Batch: 29/29, Batch_Loss_Train: 6.348
2024-06-21 00:41:26,099 - INFO: 71/200 final results:
2024-06-21 00:41:26,099 - INFO: Training loss: 6.399.
2024-06-21 00:41:26,099 - INFO: Training MAE: 1.983.
2024-06-21 00:41:26,099 - INFO: Training MSE: 6.400.
2024-06-21 00:41:46,353 - INFO: Epoch: 71/200, Loss_train: 6.398941977270718, Loss_val: 9.513689304220266
2024-06-21 00:41:46,353 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:41:46,353 - INFO: Epoch 72/200...
2024-06-21 00:41:46,353 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:41:46,353 - INFO: Batch size: 32.
2024-06-21 00:41:46,357 - INFO: Dataset:
2024-06-21 00:41:46,357 - INFO: Batch size:
2024-06-21 00:41:46,357 - INFO: Number of workers:
2024-06-21 00:41:47,615 - INFO: Epoch: 72/200, Batch: 1/29, Batch_Loss_Train: 10.024
2024-06-21 00:41:47,927 - INFO: Epoch: 72/200, Batch: 2/29, Batch_Loss_Train: 18.448
2024-06-21 00:41:48,334 - INFO: Epoch: 72/200, Batch: 3/29, Batch_Loss_Train: 15.346
2024-06-21 00:41:48,657 - INFO: Epoch: 72/200, Batch: 4/29, Batch_Loss_Train: 7.541
2024-06-21 00:41:49,062 - INFO: Epoch: 72/200, Batch: 5/29, Batch_Loss_Train: 8.449
2024-06-21 00:41:49,371 - INFO: Epoch: 72/200, Batch: 6/29, Batch_Loss_Train: 9.692
2024-06-21 00:41:49,780 - INFO: Epoch: 72/200, Batch: 7/29, Batch_Loss_Train: 8.109
2024-06-21 00:41:50,101 - INFO: Epoch: 72/200, Batch: 8/29, Batch_Loss_Train: 6.948
2024-06-21 00:41:50,517 - INFO: Epoch: 72/200, Batch: 9/29, Batch_Loss_Train: 6.521
2024-06-21 00:41:50,819 - INFO: Epoch: 72/200, Batch: 10/29, Batch_Loss_Train: 4.916
2024-06-21 00:41:51,232 - INFO: Epoch: 72/200, Batch: 11/29, Batch_Loss_Train: 3.829
2024-06-21 00:41:51,555 - INFO: Epoch: 72/200, Batch: 12/29, Batch_Loss_Train: 3.442
2024-06-21 00:41:51,993 - INFO: Epoch: 72/200, Batch: 13/29, Batch_Loss_Train: 3.914
2024-06-21 00:41:52,303 - INFO: Epoch: 72/200, Batch: 14/29, Batch_Loss_Train: 3.839
2024-06-21 00:41:52,722 - INFO: Epoch: 72/200, Batch: 15/29, Batch_Loss_Train: 5.913
2024-06-21 00:41:53,038 - INFO: Epoch: 72/200, Batch: 16/29, Batch_Loss_Train: 6.462
2024-06-21 00:41:53,464 - INFO: Epoch: 72/200, Batch: 17/29, Batch_Loss_Train: 7.539
2024-06-21 00:41:53,771 - INFO: Epoch: 72/200, Batch: 18/29, Batch_Loss_Train: 6.981
2024-06-21 00:41:54,181 - INFO: Epoch: 72/200, Batch: 19/29, Batch_Loss_Train: 8.299
2024-06-21 00:41:54,495 - INFO: Epoch: 72/200, Batch: 20/29, Batch_Loss_Train: 8.722
2024-06-21 00:41:54,917 - INFO: Epoch: 72/200, Batch: 21/29, Batch_Loss_Train: 5.186
2024-06-21 00:41:55,226 - INFO: Epoch: 72/200, Batch: 22/29, Batch_Loss_Train: 4.317
2024-06-21 00:41:55,632 - INFO: Epoch: 72/200, Batch: 23/29, Batch_Loss_Train: 4.614
2024-06-21 00:41:55,953 - INFO: Epoch: 72/200, Batch: 24/29, Batch_Loss_Train: 3.622
2024-06-21 00:41:56,369 - INFO: Epoch: 72/200, Batch: 25/29, Batch_Loss_Train: 5.156
2024-06-21 00:41:56,672 - INFO: Epoch: 72/200, Batch: 26/29, Batch_Loss_Train: 4.499
2024-06-21 00:41:57,076 - INFO: Epoch: 72/200, Batch: 27/29, Batch_Loss_Train: 5.257
2024-06-21 00:41:57,392 - INFO: Epoch: 72/200, Batch: 28/29, Batch_Loss_Train: 4.017
2024-06-21 00:41:57,615 - INFO: Epoch: 72/200, Batch: 29/29, Batch_Loss_Train: 6.045
2024-06-21 00:42:08,640 - INFO: 72/200 final results:
2024-06-21 00:42:08,640 - INFO: Training loss: 6.815.
2024-06-21 00:42:08,640 - INFO: Training MAE: 2.030.
2024-06-21 00:42:08,640 - INFO: Training MSE: 6.831.
2024-06-21 00:42:29,333 - INFO: Epoch: 72/200, Loss_train: 6.8154338639358, Loss_val: 10.475052290949328
2024-06-21 00:42:29,333 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:42:29,333 - INFO: Epoch 73/200...
2024-06-21 00:42:29,333 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:42:29,333 - INFO: Batch size: 32.
2024-06-21 00:42:29,337 - INFO: Dataset:
2024-06-21 00:42:29,337 - INFO: Batch size:
2024-06-21 00:42:29,337 - INFO: Number of workers:
2024-06-21 00:42:30,581 - INFO: Epoch: 73/200, Batch: 1/29, Batch_Loss_Train: 9.751
2024-06-21 00:42:30,895 - INFO: Epoch: 73/200, Batch: 2/29, Batch_Loss_Train: 9.712
2024-06-21 00:42:31,318 - INFO: Epoch: 73/200, Batch: 3/29, Batch_Loss_Train: 9.704
2024-06-21 00:42:31,643 - INFO: Epoch: 73/200, Batch: 4/29, Batch_Loss_Train: 5.845
2024-06-21 00:42:32,064 - INFO: Epoch: 73/200, Batch: 5/29, Batch_Loss_Train: 4.017
2024-06-21 00:42:32,371 - INFO: Epoch: 73/200, Batch: 6/29, Batch_Loss_Train: 4.609
2024-06-21 00:42:32,763 - INFO: Epoch: 73/200, Batch: 7/29, Batch_Loss_Train: 3.440
2024-06-21 00:42:33,083 - INFO: Epoch: 73/200, Batch: 8/29, Batch_Loss_Train: 5.364
2024-06-21 00:42:33,500 - INFO: Epoch: 73/200, Batch: 9/29, Batch_Loss_Train: 5.981
2024-06-21 00:42:33,802 - INFO: Epoch: 73/200, Batch: 10/29, Batch_Loss_Train: 6.208
2024-06-21 00:42:34,195 - INFO: Epoch: 73/200, Batch: 11/29, Batch_Loss_Train: 3.849
2024-06-21 00:42:34,518 - INFO: Epoch: 73/200, Batch: 12/29, Batch_Loss_Train: 4.907
2024-06-21 00:42:34,961 - INFO: Epoch: 73/200, Batch: 13/29, Batch_Loss_Train: 5.227
2024-06-21 00:42:35,271 - INFO: Epoch: 73/200, Batch: 14/29, Batch_Loss_Train: 3.247
2024-06-21 00:42:35,663 - INFO: Epoch: 73/200, Batch: 15/29, Batch_Loss_Train: 3.043
2024-06-21 00:42:35,982 - INFO: Epoch: 73/200, Batch: 16/29, Batch_Loss_Train: 4.850
2024-06-21 00:42:36,409 - INFO: Epoch: 73/200, Batch: 17/29, Batch_Loss_Train: 5.321
2024-06-21 00:42:36,713 - INFO: Epoch: 73/200, Batch: 18/29, Batch_Loss_Train: 6.138
2024-06-21 00:42:37,088 - INFO: Epoch: 73/200, Batch: 19/29, Batch_Loss_Train: 6.734
2024-06-21 00:42:37,404 - INFO: Epoch: 73/200, Batch: 20/29, Batch_Loss_Train: 3.440
2024-06-21 00:42:37,831 - INFO: Epoch: 73/200, Batch: 21/29, Batch_Loss_Train: 2.747
2024-06-21 00:42:38,139 - INFO: Epoch: 73/200, Batch: 22/29, Batch_Loss_Train: 4.084
2024-06-21 00:42:38,526 - INFO: Epoch: 73/200, Batch: 23/29, Batch_Loss_Train: 6.726
2024-06-21 00:42:38,847 - INFO: Epoch: 73/200, Batch: 24/29, Batch_Loss_Train: 11.015
2024-06-21 00:42:39,265 - INFO: Epoch: 73/200, Batch: 25/29, Batch_Loss_Train: 9.292
2024-06-21 00:42:39,568 - INFO: Epoch: 73/200, Batch: 26/29, Batch_Loss_Train: 6.923
2024-06-21 00:42:39,941 - INFO: Epoch: 73/200, Batch: 27/29, Batch_Loss_Train: 2.954
2024-06-21 00:42:40,256 - INFO: Epoch: 73/200, Batch: 28/29, Batch_Loss_Train: 3.689
2024-06-21 00:42:40,473 - INFO: Epoch: 73/200, Batch: 29/29, Batch_Loss_Train: 3.222
2024-06-21 00:42:51,516 - INFO: 73/200 final results:
2024-06-21 00:42:51,516 - INFO: Training loss: 5.587.
2024-06-21 00:42:51,516 - INFO: Training MAE: 1.821.
2024-06-21 00:42:51,516 - INFO: Training MSE: 5.634.
2024-06-21 00:43:11,677 - INFO: Epoch: 73/200, Loss_train: 5.5874509153694945, Loss_val: 4.531140685081482
2024-06-21 00:43:11,677 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:43:11,677 - INFO: Epoch 74/200...
2024-06-21 00:43:11,677 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:43:11,677 - INFO: Batch size: 32.
2024-06-21 00:43:11,681 - INFO: Dataset:
2024-06-21 00:43:11,681 - INFO: Batch size:
2024-06-21 00:43:11,681 - INFO: Number of workers:
2024-06-21 00:43:12,923 - INFO: Epoch: 74/200, Batch: 1/29, Batch_Loss_Train: 4.418
2024-06-21 00:43:13,250 - INFO: Epoch: 74/200, Batch: 2/29, Batch_Loss_Train: 4.222
2024-06-21 00:43:13,667 - INFO: Epoch: 74/200, Batch: 3/29, Batch_Loss_Train: 5.758
2024-06-21 00:43:13,994 - INFO: Epoch: 74/200, Batch: 4/29, Batch_Loss_Train: 5.366
2024-06-21 00:43:14,408 - INFO: Epoch: 74/200, Batch: 5/29, Batch_Loss_Train: 6.474
2024-06-21 00:43:14,728 - INFO: Epoch: 74/200, Batch: 6/29, Batch_Loss_Train: 4.876
2024-06-21 00:43:15,127 - INFO: Epoch: 74/200, Batch: 7/29, Batch_Loss_Train: 2.991
2024-06-21 00:43:15,447 - INFO: Epoch: 74/200, Batch: 8/29, Batch_Loss_Train: 4.479
2024-06-21 00:43:15,844 - INFO: Epoch: 74/200, Batch: 9/29, Batch_Loss_Train: 2.028
2024-06-21 00:43:16,158 - INFO: Epoch: 74/200, Batch: 10/29, Batch_Loss_Train: 4.620
2024-06-21 00:43:16,565 - INFO: Epoch: 74/200, Batch: 11/29, Batch_Loss_Train: 11.054
2024-06-21 00:43:16,887 - INFO: Epoch: 74/200, Batch: 12/29, Batch_Loss_Train: 12.003
2024-06-21 00:43:17,311 - INFO: Epoch: 74/200, Batch: 13/29, Batch_Loss_Train: 9.413
2024-06-21 00:43:17,635 - INFO: Epoch: 74/200, Batch: 14/29, Batch_Loss_Train: 6.719
2024-06-21 00:43:18,047 - INFO: Epoch: 74/200, Batch: 15/29, Batch_Loss_Train: 6.832
2024-06-21 00:43:18,365 - INFO: Epoch: 74/200, Batch: 16/29, Batch_Loss_Train: 7.004
2024-06-21 00:43:18,775 - INFO: Epoch: 74/200, Batch: 17/29, Batch_Loss_Train: 4.699
2024-06-21 00:43:19,092 - INFO: Epoch: 74/200, Batch: 18/29, Batch_Loss_Train: 5.752
2024-06-21 00:43:19,493 - INFO: Epoch: 74/200, Batch: 19/29, Batch_Loss_Train: 6.296
2024-06-21 00:43:19,806 - INFO: Epoch: 74/200, Batch: 20/29, Batch_Loss_Train: 5.947
2024-06-21 00:43:20,218 - INFO: Epoch: 74/200, Batch: 21/29, Batch_Loss_Train: 8.655
2024-06-21 00:43:20,540 - INFO: Epoch: 74/200, Batch: 22/29, Batch_Loss_Train: 14.781
2024-06-21 00:43:20,953 - INFO: Epoch: 74/200, Batch: 23/29, Batch_Loss_Train: 14.393
2024-06-21 00:43:21,275 - INFO: Epoch: 74/200, Batch: 24/29, Batch_Loss_Train: 7.548
2024-06-21 00:43:21,678 - INFO: Epoch: 74/200, Batch: 25/29, Batch_Loss_Train: 5.255
2024-06-21 00:43:21,994 - INFO: Epoch: 74/200, Batch: 26/29, Batch_Loss_Train: 6.957
2024-06-21 00:43:22,382 - INFO: Epoch: 74/200, Batch: 27/29, Batch_Loss_Train: 8.254
2024-06-21 00:43:22,697 - INFO: Epoch: 74/200, Batch: 28/29, Batch_Loss_Train: 8.927
2024-06-21 00:43:22,910 - INFO: Epoch: 74/200, Batch: 29/29, Batch_Loss_Train: 7.825
2024-06-21 00:43:34,160 - INFO: 74/200 final results:
2024-06-21 00:43:34,160 - INFO: Training loss: 7.019.
2024-06-21 00:43:34,160 - INFO: Training MAE: 2.072.
2024-06-21 00:43:34,160 - INFO: Training MSE: 7.003.
2024-06-21 00:43:54,812 - INFO: Epoch: 74/200, Loss_train: 7.018807312537884, Loss_val: 5.854396762519047
2024-06-21 00:43:54,812 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:43:54,812 - INFO: Epoch 75/200...
2024-06-21 00:43:54,812 - INFO: Learning rate: 0.00044077990802103415.
2024-06-21 00:43:54,812 - INFO: Batch size: 32.
2024-06-21 00:43:54,816 - INFO: Dataset:
2024-06-21 00:43:54,816 - INFO: Batch size:
2024-06-21 00:43:54,816 - INFO: Number of workers:
2024-06-21 00:43:56,097 - INFO: Epoch: 75/200, Batch: 1/29, Batch_Loss_Train: 2.981
2024-06-21 00:43:56,410 - INFO: Epoch: 75/200, Batch: 2/29, Batch_Loss_Train: 4.026
2024-06-21 00:43:56,817 - INFO: Epoch: 75/200, Batch: 3/29, Batch_Loss_Train: 2.691
2024-06-21 00:43:57,141 - INFO: Epoch: 75/200, Batch: 4/29, Batch_Loss_Train: 4.037
2024-06-21 00:43:57,575 - INFO: Epoch: 75/200, Batch: 5/29, Batch_Loss_Train: 3.241
2024-06-21 00:43:57,880 - INFO: Epoch: 75/200, Batch: 6/29, Batch_Loss_Train: 2.148
2024-06-21 00:43:58,273 - INFO: Epoch: 75/200, Batch: 7/29, Batch_Loss_Train: 3.466
2024-06-21 00:43:58,591 - INFO: Epoch: 75/200, Batch: 8/29, Batch_Loss_Train: 3.234
2024-06-21 00:43:59,030 - INFO: Epoch: 75/200, Batch: 9/29, Batch_Loss_Train: 5.267
2024-06-21 00:43:59,329 - INFO: Epoch: 75/200, Batch: 10/29, Batch_Loss_Train: 7.663
2024-06-21 00:43:59,726 - INFO: Epoch: 75/200, Batch: 11/29, Batch_Loss_Train: 6.894
2024-06-21 00:44:00,047 - INFO: Epoch: 75/200, Batch: 12/29, Batch_Loss_Train: 6.639
2024-06-21 00:44:00,502 - INFO: Epoch: 75/200, Batch: 13/29, Batch_Loss_Train: 4.472
2024-06-21 00:44:00,810 - INFO: Epoch: 75/200, Batch: 14/29, Batch_Loss_Train: 6.848
2024-06-21 00:44:01,212 - INFO: Epoch: 75/200, Batch: 15/29, Batch_Loss_Train: 9.080
2024-06-21 00:44:01,515 - INFO: Epoch: 75/200, Batch: 16/29, Batch_Loss_Train: 8.518
2024-06-21 00:44:01,957 - INFO: Epoch: 75/200, Batch: 17/29, Batch_Loss_Train: 8.771
2024-06-21 00:44:02,260 - INFO: Epoch: 75/200, Batch: 18/29, Batch_Loss_Train: 9.967
2024-06-21 00:44:02,653 - INFO: Epoch: 75/200, Batch: 19/29, Batch_Loss_Train: 14.347
2024-06-21 00:44:02,953 - INFO: Epoch: 75/200, Batch: 20/29, Batch_Loss_Train: 19.102
2024-06-21 00:44:03,396 - INFO: Epoch: 75/200, Batch: 21/29, Batch_Loss_Train: 11.101
2024-06-21 00:44:03,702 - INFO: Epoch: 75/200, Batch: 22/29, Batch_Loss_Train: 15.107
2024-06-21 00:44:04,094 - INFO: Epoch: 75/200, Batch: 23/29, Batch_Loss_Train: 23.552
2024-06-21 00:44:04,400 - INFO: Epoch: 75/200, Batch: 24/29, Batch_Loss_Train: 12.180
2024-06-21 00:44:04,842 - INFO: Epoch: 75/200, Batch: 25/29, Batch_Loss_Train: 5.928
2024-06-21 00:44:05,146 - INFO: Epoch: 75/200, Batch: 26/29, Batch_Loss_Train: 5.013
2024-06-21 00:44:05,537 - INFO: Epoch: 75/200, Batch: 27/29, Batch_Loss_Train: 11.026
2024-06-21 00:44:05,842 - INFO: Epoch: 75/200, Batch: 28/29, Batch_Loss_Train: 13.986
2024-06-21 00:44:06,066 - INFO: Epoch: 75/200, Batch: 29/29, Batch_Loss_Train: 10.593
2024-06-21 00:44:17,089 - INFO: 75/200 final results:
2024-06-21 00:44:17,089 - INFO: Training loss: 8.341.
2024-06-21 00:44:17,089 - INFO: Training MAE: 2.186.
2024-06-21 00:44:17,089 - INFO: Training MSE: 8.296.
2024-06-21 00:44:37,518 - INFO: Epoch: 75/200, Loss_train: 8.340699500051038, Loss_val: 5.753865669513571
2024-06-21 00:44:37,518 - INFO: Best internal validation val_loss: 3.629 at epoch: 68.
2024-06-21 00:44:37,518 - INFO: Epoch 76/200...
2024-06-21 00:44:37,518 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:44:37,518 - INFO: Batch size: 32.
2024-06-21 00:44:37,522 - INFO: Dataset:
2024-06-21 00:44:37,522 - INFO: Batch size:
2024-06-21 00:44:37,522 - INFO: Number of workers:
2024-06-21 00:44:38,768 - INFO: Epoch: 76/200, Batch: 1/29, Batch_Loss_Train: 6.105
2024-06-21 00:44:39,094 - INFO: Epoch: 76/200, Batch: 2/29, Batch_Loss_Train: 5.188
2024-06-21 00:44:39,513 - INFO: Epoch: 76/200, Batch: 3/29, Batch_Loss_Train: 4.967
2024-06-21 00:44:39,838 - INFO: Epoch: 76/200, Batch: 4/29, Batch_Loss_Train: 3.022
2024-06-21 00:44:40,257 - INFO: Epoch: 76/200, Batch: 5/29, Batch_Loss_Train: 4.753
2024-06-21 00:44:40,562 - INFO: Epoch: 76/200, Batch: 6/29, Batch_Loss_Train: 3.384
2024-06-21 00:44:40,964 - INFO: Epoch: 76/200, Batch: 7/29, Batch_Loss_Train: 3.881
2024-06-21 00:44:41,283 - INFO: Epoch: 76/200, Batch: 8/29, Batch_Loss_Train: 4.911
2024-06-21 00:44:41,702 - INFO: Epoch: 76/200, Batch: 9/29, Batch_Loss_Train: 3.608
2024-06-21 00:44:42,002 - INFO: Epoch: 76/200, Batch: 10/29, Batch_Loss_Train: 3.850
2024-06-21 00:44:42,405 - INFO: Epoch: 76/200, Batch: 11/29, Batch_Loss_Train: 3.579
2024-06-21 00:44:42,727 - INFO: Epoch: 76/200, Batch: 12/29, Batch_Loss_Train: 3.207
2024-06-21 00:44:43,155 - INFO: Epoch: 76/200, Batch: 13/29, Batch_Loss_Train: 2.916
2024-06-21 00:44:43,463 - INFO: Epoch: 76/200, Batch: 14/29, Batch_Loss_Train: 3.584
2024-06-21 00:44:43,875 - INFO: Epoch: 76/200, Batch: 15/29, Batch_Loss_Train: 3.469
2024-06-21 00:44:44,191 - INFO: Epoch: 76/200, Batch: 16/29, Batch_Loss_Train: 2.910
2024-06-21 00:44:44,607 - INFO: Epoch: 76/200, Batch: 17/29, Batch_Loss_Train: 3.467
2024-06-21 00:44:44,911 - INFO: Epoch: 76/200, Batch: 18/29, Batch_Loss_Train: 2.290
2024-06-21 00:44:45,312 - INFO: Epoch: 76/200, Batch: 19/29, Batch_Loss_Train: 2.136
2024-06-21 00:44:45,624 - INFO: Epoch: 76/200, Batch: 20/29, Batch_Loss_Train: 2.661
2024-06-21 00:44:46,042 - INFO: Epoch: 76/200, Batch: 21/29, Batch_Loss_Train: 3.104
2024-06-21 00:44:46,348 - INFO: Epoch: 76/200, Batch: 22/29, Batch_Loss_Train: 2.952
2024-06-21 00:44:46,755 - INFO: Epoch: 76/200, Batch: 23/29, Batch_Loss_Train: 2.356
2024-06-21 00:44:47,074 - INFO: Epoch: 76/200, Batch: 24/29, Batch_Loss_Train: 2.825
2024-06-21 00:44:47,482 - INFO: Epoch: 76/200, Batch: 25/29, Batch_Loss_Train: 3.174
2024-06-21 00:44:47,783 - INFO: Epoch: 76/200, Batch: 26/29, Batch_Loss_Train: 2.635
2024-06-21 00:44:48,180 - INFO: Epoch: 76/200, Batch: 27/29, Batch_Loss_Train: 3.407
2024-06-21 00:44:48,494 - INFO: Epoch: 76/200, Batch: 28/29, Batch_Loss_Train: 2.797
2024-06-21 00:44:48,708 - INFO: Epoch: 76/200, Batch: 29/29, Batch_Loss_Train: 2.582
2024-06-21 00:44:59,770 - INFO: 76/200 final results:
2024-06-21 00:44:59,770 - INFO: Training loss: 3.439.
2024-06-21 00:44:59,770 - INFO: Training MAE: 1.423.
2024-06-21 00:44:59,770 - INFO: Training MSE: 3.456.
2024-06-21 00:45:20,326 - INFO: Epoch: 76/200, Loss_train: 3.438643118430828, Loss_val: 2.951982185758393
2024-06-21 00:45:20,375 - INFO: Saved new best metric model for epoch 76.
2024-06-21 00:45:20,375 - INFO: Best internal validation val_loss: 2.952 at epoch: 76.
2024-06-21 00:45:20,375 - INFO: Epoch 77/200...
2024-06-21 00:45:20,375 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:45:20,375 - INFO: Batch size: 32.
2024-06-21 00:45:20,379 - INFO: Dataset:
2024-06-21 00:45:20,379 - INFO: Batch size:
2024-06-21 00:45:20,379 - INFO: Number of workers:
2024-06-21 00:45:21,645 - INFO: Epoch: 77/200, Batch: 1/29, Batch_Loss_Train: 3.001
2024-06-21 00:45:21,974 - INFO: Epoch: 77/200, Batch: 2/29, Batch_Loss_Train: 3.296
2024-06-21 00:45:22,376 - INFO: Epoch: 77/200, Batch: 3/29, Batch_Loss_Train: 2.253
2024-06-21 00:45:22,703 - INFO: Epoch: 77/200, Batch: 4/29, Batch_Loss_Train: 3.410
2024-06-21 00:45:23,140 - INFO: Epoch: 77/200, Batch: 5/29, Batch_Loss_Train: 2.460
2024-06-21 00:45:23,448 - INFO: Epoch: 77/200, Batch: 6/29, Batch_Loss_Train: 2.646
2024-06-21 00:45:23,832 - INFO: Epoch: 77/200, Batch: 7/29, Batch_Loss_Train: 3.168
2024-06-21 00:45:24,153 - INFO: Epoch: 77/200, Batch: 8/29, Batch_Loss_Train: 2.494
2024-06-21 00:45:24,580 - INFO: Epoch: 77/200, Batch: 9/29, Batch_Loss_Train: 2.725
2024-06-21 00:45:24,881 - INFO: Epoch: 77/200, Batch: 10/29, Batch_Loss_Train: 2.373
2024-06-21 00:45:25,261 - INFO: Epoch: 77/200, Batch: 11/29, Batch_Loss_Train: 2.219
2024-06-21 00:45:25,584 - INFO: Epoch: 77/200, Batch: 12/29, Batch_Loss_Train: 2.327
2024-06-21 00:45:26,026 - INFO: Epoch: 77/200, Batch: 13/29, Batch_Loss_Train: 2.493
2024-06-21 00:45:26,334 - INFO: Epoch: 77/200, Batch: 14/29, Batch_Loss_Train: 2.943
2024-06-21 00:45:26,724 - INFO: Epoch: 77/200, Batch: 15/29, Batch_Loss_Train: 3.557
2024-06-21 00:45:27,041 - INFO: Epoch: 77/200, Batch: 16/29, Batch_Loss_Train: 2.349
2024-06-21 00:45:27,474 - INFO: Epoch: 77/200, Batch: 17/29, Batch_Loss_Train: 2.542
2024-06-21 00:45:27,780 - INFO: Epoch: 77/200, Batch: 18/29, Batch_Loss_Train: 2.546
2024-06-21 00:45:28,159 - INFO: Epoch: 77/200, Batch: 19/29, Batch_Loss_Train: 2.676
2024-06-21 00:45:28,473 - INFO: Epoch: 77/200, Batch: 20/29, Batch_Loss_Train: 2.128
2024-06-21 00:45:28,895 - INFO: Epoch: 77/200, Batch: 21/29, Batch_Loss_Train: 1.866
2024-06-21 00:45:29,199 - INFO: Epoch: 77/200, Batch: 22/29, Batch_Loss_Train: 3.147
2024-06-21 00:45:29,590 - INFO: Epoch: 77/200, Batch: 23/29, Batch_Loss_Train: 2.578
2024-06-21 00:45:29,908 - INFO: Epoch: 77/200, Batch: 24/29, Batch_Loss_Train: 2.047
2024-06-21 00:45:30,334 - INFO: Epoch: 77/200, Batch: 25/29, Batch_Loss_Train: 2.481
2024-06-21 00:45:30,637 - INFO: Epoch: 77/200, Batch: 26/29, Batch_Loss_Train: 1.804
2024-06-21 00:45:31,028 - INFO: Epoch: 77/200, Batch: 27/29, Batch_Loss_Train: 2.067
2024-06-21 00:45:31,344 - INFO: Epoch: 77/200, Batch: 28/29, Batch_Loss_Train: 2.994
2024-06-21 00:45:31,563 - INFO: Epoch: 77/200, Batch: 29/29, Batch_Loss_Train: 1.892
2024-06-21 00:45:42,707 - INFO: 77/200 final results:
2024-06-21 00:45:42,707 - INFO: Training loss: 2.568.
2024-06-21 00:45:42,707 - INFO: Training MAE: 1.254.
2024-06-21 00:45:42,707 - INFO: Training MSE: 2.582.
2024-06-21 00:46:03,184 - INFO: Epoch: 77/200, Loss_train: 2.5682860037376143, Loss_val: 3.0535652267521827
2024-06-21 00:46:03,185 - INFO: Best internal validation val_loss: 2.952 at epoch: 76.
2024-06-21 00:46:03,185 - INFO: Epoch 78/200...
2024-06-21 00:46:03,185 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:46:03,185 - INFO: Batch size: 32.
2024-06-21 00:46:03,189 - INFO: Dataset:
2024-06-21 00:46:03,189 - INFO: Batch size:
2024-06-21 00:46:03,189 - INFO: Number of workers:
2024-06-21 00:46:04,445 - INFO: Epoch: 78/200, Batch: 1/29, Batch_Loss_Train: 1.729
2024-06-21 00:46:04,760 - INFO: Epoch: 78/200, Batch: 2/29, Batch_Loss_Train: 2.337
2024-06-21 00:46:05,193 - INFO: Epoch: 78/200, Batch: 3/29, Batch_Loss_Train: 2.175
2024-06-21 00:46:05,507 - INFO: Epoch: 78/200, Batch: 4/29, Batch_Loss_Train: 2.623
2024-06-21 00:46:05,931 - INFO: Epoch: 78/200, Batch: 5/29, Batch_Loss_Train: 1.257
2024-06-21 00:46:06,238 - INFO: Epoch: 78/200, Batch: 6/29, Batch_Loss_Train: 2.120
2024-06-21 00:46:06,657 - INFO: Epoch: 78/200, Batch: 7/29, Batch_Loss_Train: 2.687
2024-06-21 00:46:06,964 - INFO: Epoch: 78/200, Batch: 8/29, Batch_Loss_Train: 3.163
2024-06-21 00:46:07,378 - INFO: Epoch: 78/200, Batch: 9/29, Batch_Loss_Train: 2.403
2024-06-21 00:46:07,680 - INFO: Epoch: 78/200, Batch: 10/29, Batch_Loss_Train: 4.467
2024-06-21 00:46:08,100 - INFO: Epoch: 78/200, Batch: 11/29, Batch_Loss_Train: 3.430
2024-06-21 00:46:08,410 - INFO: Epoch: 78/200, Batch: 12/29, Batch_Loss_Train: 2.000
2024-06-21 00:46:08,842 - INFO: Epoch: 78/200, Batch: 13/29, Batch_Loss_Train: 3.252
2024-06-21 00:46:09,149 - INFO: Epoch: 78/200, Batch: 14/29, Batch_Loss_Train: 2.526
2024-06-21 00:46:09,589 - INFO: Epoch: 78/200, Batch: 15/29, Batch_Loss_Train: 2.736
2024-06-21 00:46:09,894 - INFO: Epoch: 78/200, Batch: 16/29, Batch_Loss_Train: 2.246
2024-06-21 00:46:10,303 - INFO: Epoch: 78/200, Batch: 17/29, Batch_Loss_Train: 3.413
2024-06-21 00:46:10,608 - INFO: Epoch: 78/200, Batch: 18/29, Batch_Loss_Train: 3.065
2024-06-21 00:46:11,037 - INFO: Epoch: 78/200, Batch: 19/29, Batch_Loss_Train: 2.041
2024-06-21 00:46:11,338 - INFO: Epoch: 78/200, Batch: 20/29, Batch_Loss_Train: 3.518
2024-06-21 00:46:11,743 - INFO: Epoch: 78/200, Batch: 21/29, Batch_Loss_Train: 3.457
2024-06-21 00:46:12,049 - INFO: Epoch: 78/200, Batch: 22/29, Batch_Loss_Train: 1.934
2024-06-21 00:46:12,473 - INFO: Epoch: 78/200, Batch: 23/29, Batch_Loss_Train: 1.818
2024-06-21 00:46:12,781 - INFO: Epoch: 78/200, Batch: 24/29, Batch_Loss_Train: 3.006
2024-06-21 00:46:13,176 - INFO: Epoch: 78/200, Batch: 25/29, Batch_Loss_Train: 3.312
2024-06-21 00:46:13,480 - INFO: Epoch: 78/200, Batch: 26/29, Batch_Loss_Train: 1.740
2024-06-21 00:46:13,883 - INFO: Epoch: 78/200, Batch: 27/29, Batch_Loss_Train: 3.200
2024-06-21 00:46:14,186 - INFO: Epoch: 78/200, Batch: 28/29, Batch_Loss_Train: 2.429
2024-06-21 00:46:14,393 - INFO: Epoch: 78/200, Batch: 29/29, Batch_Loss_Train: 2.583
2024-06-21 00:46:25,408 - INFO: 78/200 final results:
2024-06-21 00:46:25,408 - INFO: Training loss: 2.644.
2024-06-21 00:46:25,408 - INFO: Training MAE: 1.268.
2024-06-21 00:46:25,408 - INFO: Training MSE: 2.645.
2024-06-21 00:46:45,959 - INFO: Epoch: 78/200, Loss_train: 2.6435962956527184, Loss_val: 2.646598988565905
2024-06-21 00:46:46,006 - INFO: Saved new best metric model for epoch 78.
2024-06-21 00:46:46,006 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:46:46,006 - INFO: Epoch 79/200...
2024-06-21 00:46:46,006 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:46:46,006 - INFO: Batch size: 32.
2024-06-21 00:46:46,010 - INFO: Dataset:
2024-06-21 00:46:46,010 - INFO: Batch size:
2024-06-21 00:46:46,010 - INFO: Number of workers:
2024-06-21 00:46:47,246 - INFO: Epoch: 79/200, Batch: 1/29, Batch_Loss_Train: 2.962
2024-06-21 00:46:47,571 - INFO: Epoch: 79/200, Batch: 2/29, Batch_Loss_Train: 2.956
2024-06-21 00:46:47,988 - INFO: Epoch: 79/200, Batch: 3/29, Batch_Loss_Train: 4.455
2024-06-21 00:46:48,311 - INFO: Epoch: 79/200, Batch: 4/29, Batch_Loss_Train: 5.203
2024-06-21 00:46:48,711 - INFO: Epoch: 79/200, Batch: 5/29, Batch_Loss_Train: 3.635
2024-06-21 00:46:49,027 - INFO: Epoch: 79/200, Batch: 6/29, Batch_Loss_Train: 3.856
2024-06-21 00:46:49,425 - INFO: Epoch: 79/200, Batch: 7/29, Batch_Loss_Train: 3.336
2024-06-21 00:46:49,743 - INFO: Epoch: 79/200, Batch: 8/29, Batch_Loss_Train: 1.798
2024-06-21 00:46:50,132 - INFO: Epoch: 79/200, Batch: 9/29, Batch_Loss_Train: 3.039
2024-06-21 00:46:50,443 - INFO: Epoch: 79/200, Batch: 10/29, Batch_Loss_Train: 2.953
2024-06-21 00:46:50,849 - INFO: Epoch: 79/200, Batch: 11/29, Batch_Loss_Train: 2.683
2024-06-21 00:46:51,171 - INFO: Epoch: 79/200, Batch: 12/29, Batch_Loss_Train: 2.123
2024-06-21 00:46:51,587 - INFO: Epoch: 79/200, Batch: 13/29, Batch_Loss_Train: 2.338
2024-06-21 00:46:51,909 - INFO: Epoch: 79/200, Batch: 14/29, Batch_Loss_Train: 3.418
2024-06-21 00:46:52,314 - INFO: Epoch: 79/200, Batch: 15/29, Batch_Loss_Train: 2.310
2024-06-21 00:46:52,632 - INFO: Epoch: 79/200, Batch: 16/29, Batch_Loss_Train: 3.590
2024-06-21 00:46:53,043 - INFO: Epoch: 79/200, Batch: 17/29, Batch_Loss_Train: 3.660
2024-06-21 00:46:53,360 - INFO: Epoch: 79/200, Batch: 18/29, Batch_Loss_Train: 2.438
2024-06-21 00:46:53,757 - INFO: Epoch: 79/200, Batch: 19/29, Batch_Loss_Train: 2.540
2024-06-21 00:46:54,071 - INFO: Epoch: 79/200, Batch: 20/29, Batch_Loss_Train: 4.088
2024-06-21 00:46:54,466 - INFO: Epoch: 79/200, Batch: 21/29, Batch_Loss_Train: 2.640
2024-06-21 00:46:54,787 - INFO: Epoch: 79/200, Batch: 22/29, Batch_Loss_Train: 4.091
2024-06-21 00:46:55,194 - INFO: Epoch: 79/200, Batch: 23/29, Batch_Loss_Train: 4.080
2024-06-21 00:46:55,514 - INFO: Epoch: 79/200, Batch: 24/29, Batch_Loss_Train: 3.155
2024-06-21 00:46:55,920 - INFO: Epoch: 79/200, Batch: 25/29, Batch_Loss_Train: 4.025
2024-06-21 00:46:56,234 - INFO: Epoch: 79/200, Batch: 26/29, Batch_Loss_Train: 3.110
2024-06-21 00:46:56,636 - INFO: Epoch: 79/200, Batch: 27/29, Batch_Loss_Train: 2.345
2024-06-21 00:46:56,951 - INFO: Epoch: 79/200, Batch: 28/29, Batch_Loss_Train: 2.899
2024-06-21 00:46:57,170 - INFO: Epoch: 79/200, Batch: 29/29, Batch_Loss_Train: 3.669
2024-06-21 00:47:07,879 - INFO: 79/200 final results:
2024-06-21 00:47:07,879 - INFO: Training loss: 3.221.
2024-06-21 00:47:07,879 - INFO: Training MAE: 1.385.
2024-06-21 00:47:07,879 - INFO: Training MSE: 3.212.
2024-06-21 00:47:28,613 - INFO: Epoch: 79/200, Loss_train: 3.2205131958270896, Loss_val: 4.7920165966297015
2024-06-21 00:47:28,613 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:47:28,613 - INFO: Epoch 80/200...
2024-06-21 00:47:28,613 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:47:28,613 - INFO: Batch size: 32.
2024-06-21 00:47:28,617 - INFO: Dataset:
2024-06-21 00:47:28,617 - INFO: Batch size:
2024-06-21 00:47:28,617 - INFO: Number of workers:
2024-06-21 00:47:29,884 - INFO: Epoch: 80/200, Batch: 1/29, Batch_Loss_Train: 4.204
2024-06-21 00:47:30,196 - INFO: Epoch: 80/200, Batch: 2/29, Batch_Loss_Train: 2.922
2024-06-21 00:47:30,614 - INFO: Epoch: 80/200, Batch: 3/29, Batch_Loss_Train: 2.502
2024-06-21 00:47:30,937 - INFO: Epoch: 80/200, Batch: 4/29, Batch_Loss_Train: 2.419
2024-06-21 00:47:31,356 - INFO: Epoch: 80/200, Batch: 5/29, Batch_Loss_Train: 2.281
2024-06-21 00:47:31,660 - INFO: Epoch: 80/200, Batch: 6/29, Batch_Loss_Train: 1.930
2024-06-21 00:47:32,062 - INFO: Epoch: 80/200, Batch: 7/29, Batch_Loss_Train: 2.109
2024-06-21 00:47:32,380 - INFO: Epoch: 80/200, Batch: 8/29, Batch_Loss_Train: 2.217
2024-06-21 00:47:32,796 - INFO: Epoch: 80/200, Batch: 9/29, Batch_Loss_Train: 2.023
2024-06-21 00:47:33,094 - INFO: Epoch: 80/200, Batch: 10/29, Batch_Loss_Train: 2.615
2024-06-21 00:47:33,502 - INFO: Epoch: 80/200, Batch: 11/29, Batch_Loss_Train: 3.407
2024-06-21 00:47:33,821 - INFO: Epoch: 80/200, Batch: 12/29, Batch_Loss_Train: 2.780
2024-06-21 00:47:34,250 - INFO: Epoch: 80/200, Batch: 13/29, Batch_Loss_Train: 3.036
2024-06-21 00:47:34,557 - INFO: Epoch: 80/200, Batch: 14/29, Batch_Loss_Train: 4.101
2024-06-21 00:47:34,974 - INFO: Epoch: 80/200, Batch: 15/29, Batch_Loss_Train: 4.140
2024-06-21 00:47:35,289 - INFO: Epoch: 80/200, Batch: 16/29, Batch_Loss_Train: 3.127
2024-06-21 00:47:35,710 - INFO: Epoch: 80/200, Batch: 17/29, Batch_Loss_Train: 3.231
2024-06-21 00:47:36,011 - INFO: Epoch: 80/200, Batch: 18/29, Batch_Loss_Train: 3.862
2024-06-21 00:47:36,412 - INFO: Epoch: 80/200, Batch: 19/29, Batch_Loss_Train: 3.046
2024-06-21 00:47:36,724 - INFO: Epoch: 80/200, Batch: 20/29, Batch_Loss_Train: 4.306
2024-06-21 00:47:37,139 - INFO: Epoch: 80/200, Batch: 21/29, Batch_Loss_Train: 2.946
2024-06-21 00:47:37,445 - INFO: Epoch: 80/200, Batch: 22/29, Batch_Loss_Train: 3.175
2024-06-21 00:47:37,855 - INFO: Epoch: 80/200, Batch: 23/29, Batch_Loss_Train: 1.860
2024-06-21 00:47:38,173 - INFO: Epoch: 80/200, Batch: 24/29, Batch_Loss_Train: 2.446
2024-06-21 00:47:38,587 - INFO: Epoch: 80/200, Batch: 25/29, Batch_Loss_Train: 3.512
2024-06-21 00:47:38,887 - INFO: Epoch: 80/200, Batch: 26/29, Batch_Loss_Train: 1.862
2024-06-21 00:47:39,287 - INFO: Epoch: 80/200, Batch: 27/29, Batch_Loss_Train: 2.525
2024-06-21 00:47:39,601 - INFO: Epoch: 80/200, Batch: 28/29, Batch_Loss_Train: 2.378
2024-06-21 00:47:39,824 - INFO: Epoch: 80/200, Batch: 29/29, Batch_Loss_Train: 2.037
2024-06-21 00:47:50,765 - INFO: 80/200 final results:
2024-06-21 00:47:50,765 - INFO: Training loss: 2.862.
2024-06-21 00:47:50,765 - INFO: Training MAE: 1.315.
2024-06-21 00:47:50,765 - INFO: Training MSE: 2.878.
2024-06-21 00:48:11,105 - INFO: Epoch: 80/200, Loss_train: 2.8620763359398675, Loss_val: 4.175209333156717
2024-06-21 00:48:11,105 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:48:11,105 - INFO: Epoch 81/200...
2024-06-21 00:48:11,105 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:48:11,105 - INFO: Batch size: 32.
2024-06-21 00:48:11,109 - INFO: Dataset:
2024-06-21 00:48:11,109 - INFO: Batch size:
2024-06-21 00:48:11,109 - INFO: Number of workers:
2024-06-21 00:48:12,359 - INFO: Epoch: 81/200, Batch: 1/29, Batch_Loss_Train: 2.111
2024-06-21 00:48:12,672 - INFO: Epoch: 81/200, Batch: 2/29, Batch_Loss_Train: 3.793
2024-06-21 00:48:13,102 - INFO: Epoch: 81/200, Batch: 3/29, Batch_Loss_Train: 5.080
2024-06-21 00:48:13,414 - INFO: Epoch: 81/200, Batch: 4/29, Batch_Loss_Train: 3.752
2024-06-21 00:48:13,835 - INFO: Epoch: 81/200, Batch: 5/29, Batch_Loss_Train: 3.831
2024-06-21 00:48:14,140 - INFO: Epoch: 81/200, Batch: 6/29, Batch_Loss_Train: 4.136
2024-06-21 00:48:14,556 - INFO: Epoch: 81/200, Batch: 7/29, Batch_Loss_Train: 2.203
2024-06-21 00:48:14,861 - INFO: Epoch: 81/200, Batch: 8/29, Batch_Loss_Train: 2.308
2024-06-21 00:48:15,282 - INFO: Epoch: 81/200, Batch: 9/29, Batch_Loss_Train: 2.192
2024-06-21 00:48:15,581 - INFO: Epoch: 81/200, Batch: 10/29, Batch_Loss_Train: 2.840
2024-06-21 00:48:16,003 - INFO: Epoch: 81/200, Batch: 11/29, Batch_Loss_Train: 3.366
2024-06-21 00:48:16,312 - INFO: Epoch: 81/200, Batch: 12/29, Batch_Loss_Train: 3.345
2024-06-21 00:48:16,753 - INFO: Epoch: 81/200, Batch: 13/29, Batch_Loss_Train: 3.721
2024-06-21 00:48:17,061 - INFO: Epoch: 81/200, Batch: 14/29, Batch_Loss_Train: 3.101
2024-06-21 00:48:17,486 - INFO: Epoch: 81/200, Batch: 15/29, Batch_Loss_Train: 2.839
2024-06-21 00:48:17,790 - INFO: Epoch: 81/200, Batch: 16/29, Batch_Loss_Train: 2.435
2024-06-21 00:48:18,212 - INFO: Epoch: 81/200, Batch: 17/29, Batch_Loss_Train: 2.488
2024-06-21 00:48:18,516 - INFO: Epoch: 81/200, Batch: 18/29, Batch_Loss_Train: 3.021
2024-06-21 00:48:18,929 - INFO: Epoch: 81/200, Batch: 19/29, Batch_Loss_Train: 1.717
2024-06-21 00:48:19,229 - INFO: Epoch: 81/200, Batch: 20/29, Batch_Loss_Train: 2.768
2024-06-21 00:48:19,647 - INFO: Epoch: 81/200, Batch: 21/29, Batch_Loss_Train: 3.629
2024-06-21 00:48:19,954 - INFO: Epoch: 81/200, Batch: 22/29, Batch_Loss_Train: 2.970
2024-06-21 00:48:20,378 - INFO: Epoch: 81/200, Batch: 23/29, Batch_Loss_Train: 3.235
2024-06-21 00:48:20,685 - INFO: Epoch: 81/200, Batch: 24/29, Batch_Loss_Train: 5.827
2024-06-21 00:48:21,090 - INFO: Epoch: 81/200, Batch: 25/29, Batch_Loss_Train: 4.442
2024-06-21 00:48:21,391 - INFO: Epoch: 81/200, Batch: 26/29, Batch_Loss_Train: 3.334
2024-06-21 00:48:21,790 - INFO: Epoch: 81/200, Batch: 27/29, Batch_Loss_Train: 3.946
2024-06-21 00:48:22,092 - INFO: Epoch: 81/200, Batch: 28/29, Batch_Loss_Train: 2.835
2024-06-21 00:48:22,309 - INFO: Epoch: 81/200, Batch: 29/29, Batch_Loss_Train: 4.297
2024-06-21 00:48:33,315 - INFO: 81/200 final results:
2024-06-21 00:48:33,315 - INFO: Training loss: 3.295.
2024-06-21 00:48:33,315 - INFO: Training MAE: 1.400.
2024-06-21 00:48:33,315 - INFO: Training MSE: 3.275.
2024-06-21 00:48:54,199 - INFO: Epoch: 81/200, Loss_train: 3.295253063070363, Loss_val: 5.721422072114615
2024-06-21 00:48:54,199 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:48:54,199 - INFO: Epoch 82/200...
2024-06-21 00:48:54,199 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:48:54,199 - INFO: Batch size: 32.
2024-06-21 00:48:54,203 - INFO: Dataset:
2024-06-21 00:48:54,203 - INFO: Batch size:
2024-06-21 00:48:54,203 - INFO: Number of workers:
2024-06-21 00:48:55,468 - INFO: Epoch: 82/200, Batch: 1/29, Batch_Loss_Train: 4.648
2024-06-21 00:48:55,812 - INFO: Epoch: 82/200, Batch: 2/29, Batch_Loss_Train: 3.085
2024-06-21 00:48:56,220 - INFO: Epoch: 82/200, Batch: 3/29, Batch_Loss_Train: 2.536
2024-06-21 00:48:56,544 - INFO: Epoch: 82/200, Batch: 4/29, Batch_Loss_Train: 2.755
2024-06-21 00:48:56,963 - INFO: Epoch: 82/200, Batch: 5/29, Batch_Loss_Train: 2.915
2024-06-21 00:48:57,295 - INFO: Epoch: 82/200, Batch: 6/29, Batch_Loss_Train: 3.289
2024-06-21 00:48:57,691 - INFO: Epoch: 82/200, Batch: 7/29, Batch_Loss_Train: 2.586
2024-06-21 00:48:57,999 - INFO: Epoch: 82/200, Batch: 8/29, Batch_Loss_Train: 2.811
2024-06-21 00:48:58,419 - INFO: Epoch: 82/200, Batch: 9/29, Batch_Loss_Train: 4.068
2024-06-21 00:48:58,754 - INFO: Epoch: 82/200, Batch: 10/29, Batch_Loss_Train: 3.516
2024-06-21 00:48:59,146 - INFO: Epoch: 82/200, Batch: 11/29, Batch_Loss_Train: 2.456
2024-06-21 00:48:59,454 - INFO: Epoch: 82/200, Batch: 12/29, Batch_Loss_Train: 3.615
2024-06-21 00:48:59,895 - INFO: Epoch: 82/200, Batch: 13/29, Batch_Loss_Train: 3.208
2024-06-21 00:49:00,217 - INFO: Epoch: 82/200, Batch: 14/29, Batch_Loss_Train: 3.152
2024-06-21 00:49:00,626 - INFO: Epoch: 82/200, Batch: 15/29, Batch_Loss_Train: 2.457
2024-06-21 00:49:00,932 - INFO: Epoch: 82/200, Batch: 16/29, Batch_Loss_Train: 1.862
2024-06-21 00:49:01,369 - INFO: Epoch: 82/200, Batch: 17/29, Batch_Loss_Train: 2.990
2024-06-21 00:49:01,689 - INFO: Epoch: 82/200, Batch: 18/29, Batch_Loss_Train: 1.947
2024-06-21 00:49:02,083 - INFO: Epoch: 82/200, Batch: 19/29, Batch_Loss_Train: 2.813
2024-06-21 00:49:02,384 - INFO: Epoch: 82/200, Batch: 20/29, Batch_Loss_Train: 2.987
2024-06-21 00:49:02,813 - INFO: Epoch: 82/200, Batch: 21/29, Batch_Loss_Train: 1.922
2024-06-21 00:49:03,134 - INFO: Epoch: 82/200, Batch: 22/29, Batch_Loss_Train: 2.321
2024-06-21 00:49:03,536 - INFO: Epoch: 82/200, Batch: 23/29, Batch_Loss_Train: 2.847
2024-06-21 00:49:03,845 - INFO: Epoch: 82/200, Batch: 24/29, Batch_Loss_Train: 2.760
2024-06-21 00:49:04,276 - INFO: Epoch: 82/200, Batch: 25/29, Batch_Loss_Train: 2.510
2024-06-21 00:49:04,593 - INFO: Epoch: 82/200, Batch: 26/29, Batch_Loss_Train: 3.464
2024-06-21 00:49:04,984 - INFO: Epoch: 82/200, Batch: 27/29, Batch_Loss_Train: 4.682
2024-06-21 00:49:05,288 - INFO: Epoch: 82/200, Batch: 28/29, Batch_Loss_Train: 4.649
2024-06-21 00:49:05,512 - INFO: Epoch: 82/200, Batch: 29/29, Batch_Loss_Train: 5.295
2024-06-21 00:49:16,573 - INFO: 82/200 final results:
2024-06-21 00:49:16,573 - INFO: Training loss: 3.109.
2024-06-21 00:49:16,573 - INFO: Training MAE: 1.365.
2024-06-21 00:49:16,573 - INFO: Training MSE: 3.065.
2024-06-21 00:49:37,483 - INFO: Epoch: 82/200, Loss_train: 3.1085426231910445, Loss_val: 5.579925717978642
2024-06-21 00:49:37,483 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:49:37,483 - INFO: Epoch 83/200...
2024-06-21 00:49:37,483 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:49:37,483 - INFO: Batch size: 32.
2024-06-21 00:49:37,487 - INFO: Dataset:
2024-06-21 00:49:37,487 - INFO: Batch size:
2024-06-21 00:49:37,487 - INFO: Number of workers:
2024-06-21 00:49:38,752 - INFO: Epoch: 83/200, Batch: 1/29, Batch_Loss_Train: 5.833
2024-06-21 00:49:39,079 - INFO: Epoch: 83/200, Batch: 2/29, Batch_Loss_Train: 5.524
2024-06-21 00:49:39,487 - INFO: Epoch: 83/200, Batch: 3/29, Batch_Loss_Train: 2.066
2024-06-21 00:49:39,810 - INFO: Epoch: 83/200, Batch: 4/29, Batch_Loss_Train: 3.060
2024-06-21 00:49:40,229 - INFO: Epoch: 83/200, Batch: 5/29, Batch_Loss_Train: 2.979
2024-06-21 00:49:40,546 - INFO: Epoch: 83/200, Batch: 6/29, Batch_Loss_Train: 3.017
2024-06-21 00:49:40,939 - INFO: Epoch: 83/200, Batch: 7/29, Batch_Loss_Train: 3.238
2024-06-21 00:49:41,256 - INFO: Epoch: 83/200, Batch: 8/29, Batch_Loss_Train: 3.042
2024-06-21 00:49:41,669 - INFO: Epoch: 83/200, Batch: 9/29, Batch_Loss_Train: 4.416
2024-06-21 00:49:41,980 - INFO: Epoch: 83/200, Batch: 10/29, Batch_Loss_Train: 6.179
2024-06-21 00:49:42,357 - INFO: Epoch: 83/200, Batch: 11/29, Batch_Loss_Train: 4.898
2024-06-21 00:49:42,677 - INFO: Epoch: 83/200, Batch: 12/29, Batch_Loss_Train: 2.519
2024-06-21 00:49:43,106 - INFO: Epoch: 83/200, Batch: 13/29, Batch_Loss_Train: 2.177
2024-06-21 00:49:43,425 - INFO: Epoch: 83/200, Batch: 14/29, Batch_Loss_Train: 1.900
2024-06-21 00:49:43,815 - INFO: Epoch: 83/200, Batch: 15/29, Batch_Loss_Train: 1.781
2024-06-21 00:49:44,131 - INFO: Epoch: 83/200, Batch: 16/29, Batch_Loss_Train: 2.962
2024-06-21 00:49:44,548 - INFO: Epoch: 83/200, Batch: 17/29, Batch_Loss_Train: 4.030
2024-06-21 00:49:44,862 - INFO: Epoch: 83/200, Batch: 18/29, Batch_Loss_Train: 4.155
2024-06-21 00:49:45,255 - INFO: Epoch: 83/200, Batch: 19/29, Batch_Loss_Train: 2.747
2024-06-21 00:49:45,569 - INFO: Epoch: 83/200, Batch: 20/29, Batch_Loss_Train: 3.132
2024-06-21 00:49:45,986 - INFO: Epoch: 83/200, Batch: 21/29, Batch_Loss_Train: 3.933
2024-06-21 00:49:46,308 - INFO: Epoch: 83/200, Batch: 22/29, Batch_Loss_Train: 5.908
2024-06-21 00:49:46,707 - INFO: Epoch: 83/200, Batch: 23/29, Batch_Loss_Train: 5.432
2024-06-21 00:49:47,028 - INFO: Epoch: 83/200, Batch: 24/29, Batch_Loss_Train: 2.676
2024-06-21 00:49:47,448 - INFO: Epoch: 83/200, Batch: 25/29, Batch_Loss_Train: 2.386
2024-06-21 00:49:47,764 - INFO: Epoch: 83/200, Batch: 26/29, Batch_Loss_Train: 3.829
2024-06-21 00:49:48,156 - INFO: Epoch: 83/200, Batch: 27/29, Batch_Loss_Train: 3.414
2024-06-21 00:49:48,471 - INFO: Epoch: 83/200, Batch: 28/29, Batch_Loss_Train: 3.132
2024-06-21 00:49:48,696 - INFO: Epoch: 83/200, Batch: 29/29, Batch_Loss_Train: 3.557
2024-06-21 00:49:59,649 - INFO: 83/200 final results:
2024-06-21 00:49:59,649 - INFO: Training loss: 3.583.
2024-06-21 00:49:59,649 - INFO: Training MAE: 1.449.
2024-06-21 00:49:59,649 - INFO: Training MSE: 3.584.
2024-06-21 00:50:20,096 - INFO: Epoch: 83/200, Loss_train: 3.5834580043266557, Loss_val: 5.364997863769531
2024-06-21 00:50:20,096 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:50:20,096 - INFO: Epoch 84/200...
2024-06-21 00:50:20,096 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:50:20,096 - INFO: Batch size: 32.
2024-06-21 00:50:20,100 - INFO: Dataset:
2024-06-21 00:50:20,100 - INFO: Batch size:
2024-06-21 00:50:20,100 - INFO: Number of workers:
2024-06-21 00:50:21,358 - INFO: Epoch: 84/200, Batch: 1/29, Batch_Loss_Train: 6.688
2024-06-21 00:50:21,672 - INFO: Epoch: 84/200, Batch: 2/29, Batch_Loss_Train: 7.043
2024-06-21 00:50:22,096 - INFO: Epoch: 84/200, Batch: 3/29, Batch_Loss_Train: 4.041
2024-06-21 00:50:22,422 - INFO: Epoch: 84/200, Batch: 4/29, Batch_Loss_Train: 2.858
2024-06-21 00:50:22,846 - INFO: Epoch: 84/200, Batch: 5/29, Batch_Loss_Train: 1.777
2024-06-21 00:50:23,151 - INFO: Epoch: 84/200, Batch: 6/29, Batch_Loss_Train: 3.261
2024-06-21 00:50:23,555 - INFO: Epoch: 84/200, Batch: 7/29, Batch_Loss_Train: 3.636
2024-06-21 00:50:23,873 - INFO: Epoch: 84/200, Batch: 8/29, Batch_Loss_Train: 4.393
2024-06-21 00:50:24,293 - INFO: Epoch: 84/200, Batch: 9/29, Batch_Loss_Train: 4.358
2024-06-21 00:50:24,592 - INFO: Epoch: 84/200, Batch: 10/29, Batch_Loss_Train: 4.521
2024-06-21 00:50:24,996 - INFO: Epoch: 84/200, Batch: 11/29, Batch_Loss_Train: 3.907
2024-06-21 00:50:25,316 - INFO: Epoch: 84/200, Batch: 12/29, Batch_Loss_Train: 2.642
2024-06-21 00:50:25,743 - INFO: Epoch: 84/200, Batch: 13/29, Batch_Loss_Train: 2.740
2024-06-21 00:50:26,050 - INFO: Epoch: 84/200, Batch: 14/29, Batch_Loss_Train: 4.298
2024-06-21 00:50:26,465 - INFO: Epoch: 84/200, Batch: 15/29, Batch_Loss_Train: 3.325
2024-06-21 00:50:26,779 - INFO: Epoch: 84/200, Batch: 16/29, Batch_Loss_Train: 4.923
2024-06-21 00:50:27,200 - INFO: Epoch: 84/200, Batch: 17/29, Batch_Loss_Train: 4.066
2024-06-21 00:50:27,502 - INFO: Epoch: 84/200, Batch: 18/29, Batch_Loss_Train: 4.073
2024-06-21 00:50:27,905 - INFO: Epoch: 84/200, Batch: 19/29, Batch_Loss_Train: 2.344
2024-06-21 00:50:28,216 - INFO: Epoch: 84/200, Batch: 20/29, Batch_Loss_Train: 2.879
2024-06-21 00:50:28,633 - INFO: Epoch: 84/200, Batch: 21/29, Batch_Loss_Train: 1.968
2024-06-21 00:50:28,938 - INFO: Epoch: 84/200, Batch: 22/29, Batch_Loss_Train: 2.610
2024-06-21 00:50:29,347 - INFO: Epoch: 84/200, Batch: 23/29, Batch_Loss_Train: 3.568
2024-06-21 00:50:29,664 - INFO: Epoch: 84/200, Batch: 24/29, Batch_Loss_Train: 2.937
2024-06-21 00:50:30,078 - INFO: Epoch: 84/200, Batch: 25/29, Batch_Loss_Train: 2.666
2024-06-21 00:50:30,378 - INFO: Epoch: 84/200, Batch: 26/29, Batch_Loss_Train: 3.565
2024-06-21 00:50:30,776 - INFO: Epoch: 84/200, Batch: 27/29, Batch_Loss_Train: 3.616
2024-06-21 00:50:31,088 - INFO: Epoch: 84/200, Batch: 28/29, Batch_Loss_Train: 2.940
2024-06-21 00:50:31,309 - INFO: Epoch: 84/200, Batch: 29/29, Batch_Loss_Train: 5.226
2024-06-21 00:50:42,277 - INFO: 84/200 final results:
2024-06-21 00:50:42,277 - INFO: Training loss: 3.685.
2024-06-21 00:50:42,277 - INFO: Training MAE: 1.497.
2024-06-21 00:50:42,277 - INFO: Training MSE: 3.655.
2024-06-21 00:51:03,193 - INFO: Epoch: 84/200, Loss_train: 3.685095663728385, Loss_val: 5.284411545457511
2024-06-21 00:51:03,194 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:51:03,194 - INFO: Epoch 85/200...
2024-06-21 00:51:03,194 - INFO: Learning rate: 0.00022038995401051708.
2024-06-21 00:51:03,194 - INFO: Batch size: 32.
2024-06-21 00:51:03,197 - INFO: Dataset:
2024-06-21 00:51:03,198 - INFO: Batch size:
2024-06-21 00:51:03,198 - INFO: Number of workers:
2024-06-21 00:51:04,445 - INFO: Epoch: 85/200, Batch: 1/29, Batch_Loss_Train: 4.795
2024-06-21 00:51:04,772 - INFO: Epoch: 85/200, Batch: 2/29, Batch_Loss_Train: 4.155
2024-06-21 00:51:05,197 - INFO: Epoch: 85/200, Batch: 3/29, Batch_Loss_Train: 2.399
2024-06-21 00:51:05,524 - INFO: Epoch: 85/200, Batch: 4/29, Batch_Loss_Train: 2.323
2024-06-21 00:51:05,946 - INFO: Epoch: 85/200, Batch: 5/29, Batch_Loss_Train: 1.922
2024-06-21 00:51:06,254 - INFO: Epoch: 85/200, Batch: 6/29, Batch_Loss_Train: 2.585
2024-06-21 00:51:06,658 - INFO: Epoch: 85/200, Batch: 7/29, Batch_Loss_Train: 3.744
2024-06-21 00:51:06,979 - INFO: Epoch: 85/200, Batch: 8/29, Batch_Loss_Train: 3.337
2024-06-21 00:51:07,394 - INFO: Epoch: 85/200, Batch: 9/29, Batch_Loss_Train: 3.440
2024-06-21 00:51:07,695 - INFO: Epoch: 85/200, Batch: 10/29, Batch_Loss_Train: 3.491
2024-06-21 00:51:08,103 - INFO: Epoch: 85/200, Batch: 11/29, Batch_Loss_Train: 2.065
2024-06-21 00:51:08,426 - INFO: Epoch: 85/200, Batch: 12/29, Batch_Loss_Train: 3.051
2024-06-21 00:51:08,861 - INFO: Epoch: 85/200, Batch: 13/29, Batch_Loss_Train: 4.264
2024-06-21 00:51:09,171 - INFO: Epoch: 85/200, Batch: 14/29, Batch_Loss_Train: 2.801
2024-06-21 00:51:09,589 - INFO: Epoch: 85/200, Batch: 15/29, Batch_Loss_Train: 3.158
2024-06-21 00:51:09,907 - INFO: Epoch: 85/200, Batch: 16/29, Batch_Loss_Train: 3.940
2024-06-21 00:51:10,331 - INFO: Epoch: 85/200, Batch: 17/29, Batch_Loss_Train: 4.909
2024-06-21 00:51:10,636 - INFO: Epoch: 85/200, Batch: 18/29, Batch_Loss_Train: 2.449
2024-06-21 00:51:11,040 - INFO: Epoch: 85/200, Batch: 19/29, Batch_Loss_Train: 3.245
2024-06-21 00:51:11,356 - INFO: Epoch: 85/200, Batch: 20/29, Batch_Loss_Train: 2.569
2024-06-21 00:51:11,776 - INFO: Epoch: 85/200, Batch: 21/29, Batch_Loss_Train: 3.324
2024-06-21 00:51:12,085 - INFO: Epoch: 85/200, Batch: 22/29, Batch_Loss_Train: 4.699
2024-06-21 00:51:12,490 - INFO: Epoch: 85/200, Batch: 23/29, Batch_Loss_Train: 5.074
2024-06-21 00:51:12,811 - INFO: Epoch: 85/200, Batch: 24/29, Batch_Loss_Train: 2.678
2024-06-21 00:51:13,219 - INFO: Epoch: 85/200, Batch: 25/29, Batch_Loss_Train: 2.530
2024-06-21 00:51:13,522 - INFO: Epoch: 85/200, Batch: 26/29, Batch_Loss_Train: 1.877
2024-06-21 00:51:13,909 - INFO: Epoch: 85/200, Batch: 27/29, Batch_Loss_Train: 2.821
2024-06-21 00:51:14,226 - INFO: Epoch: 85/200, Batch: 28/29, Batch_Loss_Train: 2.745
2024-06-21 00:51:14,441 - INFO: Epoch: 85/200, Batch: 29/29, Batch_Loss_Train: 2.857
2024-06-21 00:51:25,445 - INFO: 85/200 final results:
2024-06-21 00:51:25,445 - INFO: Training loss: 3.215.
2024-06-21 00:51:25,445 - INFO: Training MAE: 1.389.
2024-06-21 00:51:25,445 - INFO: Training MSE: 3.223.
2024-06-21 00:51:46,263 - INFO: Epoch: 85/200, Loss_train: 3.21549182924731, Loss_val: 6.640769152805723
2024-06-21 00:51:46,263 - INFO: Best internal validation val_loss: 2.647 at epoch: 78.
2024-06-21 00:51:46,263 - INFO: Epoch 86/200...
2024-06-21 00:51:46,263 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:51:46,263 - INFO: Batch size: 32.
2024-06-21 00:51:46,267 - INFO: Dataset:
2024-06-21 00:51:46,267 - INFO: Batch size:
2024-06-21 00:51:46,267 - INFO: Number of workers:
2024-06-21 00:51:47,560 - INFO: Epoch: 86/200, Batch: 1/29, Batch_Loss_Train: 5.750
2024-06-21 00:51:47,872 - INFO: Epoch: 86/200, Batch: 2/29, Batch_Loss_Train: 2.063
2024-06-21 00:51:48,278 - INFO: Epoch: 86/200, Batch: 3/29, Batch_Loss_Train: 2.807
2024-06-21 00:51:48,602 - INFO: Epoch: 86/200, Batch: 4/29, Batch_Loss_Train: 2.985
2024-06-21 00:51:49,044 - INFO: Epoch: 86/200, Batch: 5/29, Batch_Loss_Train: 2.510
2024-06-21 00:51:49,348 - INFO: Epoch: 86/200, Batch: 6/29, Batch_Loss_Train: 1.657
2024-06-21 00:51:49,740 - INFO: Epoch: 86/200, Batch: 7/29, Batch_Loss_Train: 1.799
2024-06-21 00:51:50,043 - INFO: Epoch: 86/200, Batch: 8/29, Batch_Loss_Train: 2.298
2024-06-21 00:51:50,493 - INFO: Epoch: 86/200, Batch: 9/29, Batch_Loss_Train: 1.679
2024-06-21 00:51:50,790 - INFO: Epoch: 86/200, Batch: 10/29, Batch_Loss_Train: 1.967
2024-06-21 00:51:51,173 - INFO: Epoch: 86/200, Batch: 11/29, Batch_Loss_Train: 2.122
2024-06-21 00:51:51,483 - INFO: Epoch: 86/200, Batch: 12/29, Batch_Loss_Train: 2.473
2024-06-21 00:51:51,941 - INFO: Epoch: 86/200, Batch: 13/29, Batch_Loss_Train: 2.119
2024-06-21 00:51:52,252 - INFO: Epoch: 86/200, Batch: 14/29, Batch_Loss_Train: 2.494
2024-06-21 00:51:52,649 - INFO: Epoch: 86/200, Batch: 15/29, Batch_Loss_Train: 2.172
2024-06-21 00:51:52,955 - INFO: Epoch: 86/200, Batch: 16/29, Batch_Loss_Train: 2.902
2024-06-21 00:51:53,401 - INFO: Epoch: 86/200, Batch: 17/29, Batch_Loss_Train: 1.979
2024-06-21 00:51:53,707 - INFO: Epoch: 86/200, Batch: 18/29, Batch_Loss_Train: 2.450
2024-06-21 00:51:54,092 - INFO: Epoch: 86/200, Batch: 19/29, Batch_Loss_Train: 1.855
2024-06-21 00:51:54,394 - INFO: Epoch: 86/200, Batch: 20/29, Batch_Loss_Train: 2.334
2024-06-21 00:51:54,839 - INFO: Epoch: 86/200, Batch: 21/29, Batch_Loss_Train: 1.918
2024-06-21 00:51:55,148 - INFO: Epoch: 86/200, Batch: 22/29, Batch_Loss_Train: 2.509
2024-06-21 00:51:55,536 - INFO: Epoch: 86/200, Batch: 23/29, Batch_Loss_Train: 2.683
2024-06-21 00:51:55,845 - INFO: Epoch: 86/200, Batch: 24/29, Batch_Loss_Train: 2.457
2024-06-21 00:51:56,279 - INFO: Epoch: 86/200, Batch: 25/29, Batch_Loss_Train: 1.953
2024-06-21 00:51:56,583 - INFO: Epoch: 86/200, Batch: 26/29, Batch_Loss_Train: 2.751
2024-06-21 00:51:56,960 - INFO: Epoch: 86/200, Batch: 27/29, Batch_Loss_Train: 1.707
2024-06-21 00:51:57,264 - INFO: Epoch: 86/200, Batch: 28/29, Batch_Loss_Train: 1.650
2024-06-21 00:51:57,477 - INFO: Epoch: 86/200, Batch: 29/29, Batch_Loss_Train: 2.623
2024-06-21 00:52:08,529 - INFO: 86/200 final results:
2024-06-21 00:52:08,529 - INFO: Training loss: 2.368.
2024-06-21 00:52:08,529 - INFO: Training MAE: 1.181.
2024-06-21 00:52:08,529 - INFO: Training MSE: 2.363.
2024-06-21 00:52:28,832 - INFO: Epoch: 86/200, Loss_train: 2.3677226099474677, Loss_val: 2.364085349543341
2024-06-21 00:52:28,881 - INFO: Saved new best metric model for epoch 86.
2024-06-21 00:52:28,881 - INFO: Best internal validation val_loss: 2.364 at epoch: 86.
2024-06-21 00:52:28,881 - INFO: Epoch 87/200...
2024-06-21 00:52:28,881 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:52:28,881 - INFO: Batch size: 32.
2024-06-21 00:52:28,885 - INFO: Dataset:
2024-06-21 00:52:28,885 - INFO: Batch size:
2024-06-21 00:52:28,885 - INFO: Number of workers:
2024-06-21 00:52:30,166 - INFO: Epoch: 87/200, Batch: 1/29, Batch_Loss_Train: 2.137
2024-06-21 00:52:30,479 - INFO: Epoch: 87/200, Batch: 2/29, Batch_Loss_Train: 2.275
2024-06-21 00:52:30,901 - INFO: Epoch: 87/200, Batch: 3/29, Batch_Loss_Train: 1.773
2024-06-21 00:52:31,227 - INFO: Epoch: 87/200, Batch: 4/29, Batch_Loss_Train: 2.212
2024-06-21 00:52:31,639 - INFO: Epoch: 87/200, Batch: 5/29, Batch_Loss_Train: 2.378
2024-06-21 00:52:31,946 - INFO: Epoch: 87/200, Batch: 6/29, Batch_Loss_Train: 1.927
2024-06-21 00:52:32,354 - INFO: Epoch: 87/200, Batch: 7/29, Batch_Loss_Train: 2.797
2024-06-21 00:52:32,673 - INFO: Epoch: 87/200, Batch: 8/29, Batch_Loss_Train: 1.867
2024-06-21 00:52:33,087 - INFO: Epoch: 87/200, Batch: 9/29, Batch_Loss_Train: 2.079
2024-06-21 00:52:33,388 - INFO: Epoch: 87/200, Batch: 10/29, Batch_Loss_Train: 2.613
2024-06-21 00:52:33,795 - INFO: Epoch: 87/200, Batch: 11/29, Batch_Loss_Train: 2.515
2024-06-21 00:52:34,117 - INFO: Epoch: 87/200, Batch: 12/29, Batch_Loss_Train: 2.660
2024-06-21 00:52:34,552 - INFO: Epoch: 87/200, Batch: 13/29, Batch_Loss_Train: 1.794
2024-06-21 00:52:34,861 - INFO: Epoch: 87/200, Batch: 14/29, Batch_Loss_Train: 2.003
2024-06-21 00:52:35,273 - INFO: Epoch: 87/200, Batch: 15/29, Batch_Loss_Train: 1.995
2024-06-21 00:52:35,591 - INFO: Epoch: 87/200, Batch: 16/29, Batch_Loss_Train: 1.901
2024-06-21 00:52:36,027 - INFO: Epoch: 87/200, Batch: 17/29, Batch_Loss_Train: 2.320
2024-06-21 00:52:36,332 - INFO: Epoch: 87/200, Batch: 18/29, Batch_Loss_Train: 1.409
2024-06-21 00:52:36,733 - INFO: Epoch: 87/200, Batch: 19/29, Batch_Loss_Train: 1.845
2024-06-21 00:52:37,047 - INFO: Epoch: 87/200, Batch: 20/29, Batch_Loss_Train: 1.615
2024-06-21 00:52:37,468 - INFO: Epoch: 87/200, Batch: 21/29, Batch_Loss_Train: 2.572
2024-06-21 00:52:37,776 - INFO: Epoch: 87/200, Batch: 22/29, Batch_Loss_Train: 1.541
2024-06-21 00:52:38,182 - INFO: Epoch: 87/200, Batch: 23/29, Batch_Loss_Train: 1.893
2024-06-21 00:52:38,503 - INFO: Epoch: 87/200, Batch: 24/29, Batch_Loss_Train: 2.079
2024-06-21 00:52:38,916 - INFO: Epoch: 87/200, Batch: 25/29, Batch_Loss_Train: 2.509
2024-06-21 00:52:39,219 - INFO: Epoch: 87/200, Batch: 26/29, Batch_Loss_Train: 1.848
2024-06-21 00:52:39,612 - INFO: Epoch: 87/200, Batch: 27/29, Batch_Loss_Train: 2.986
2024-06-21 00:52:39,927 - INFO: Epoch: 87/200, Batch: 28/29, Batch_Loss_Train: 2.256
2024-06-21 00:52:40,148 - INFO: Epoch: 87/200, Batch: 29/29, Batch_Loss_Train: 2.030
2024-06-21 00:52:51,208 - INFO: 87/200 final results:
2024-06-21 00:52:51,208 - INFO: Training loss: 2.132.
2024-06-21 00:52:51,208 - INFO: Training MAE: 1.140.
2024-06-21 00:52:51,208 - INFO: Training MSE: 2.134.
2024-06-21 00:53:11,936 - INFO: Epoch: 87/200, Loss_train: 2.1320552908141037, Loss_val: 2.545324450936811
2024-06-21 00:53:11,936 - INFO: Best internal validation val_loss: 2.364 at epoch: 86.
2024-06-21 00:53:11,936 - INFO: Epoch 88/200...
2024-06-21 00:53:11,936 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:53:11,936 - INFO: Batch size: 32.
2024-06-21 00:53:11,940 - INFO: Dataset:
2024-06-21 00:53:11,940 - INFO: Batch size:
2024-06-21 00:53:11,940 - INFO: Number of workers:
2024-06-21 00:53:13,213 - INFO: Epoch: 88/200, Batch: 1/29, Batch_Loss_Train: 2.051
2024-06-21 00:53:13,525 - INFO: Epoch: 88/200, Batch: 2/29, Batch_Loss_Train: 2.162
2024-06-21 00:53:13,930 - INFO: Epoch: 88/200, Batch: 3/29, Batch_Loss_Train: 2.563
2024-06-21 00:53:14,252 - INFO: Epoch: 88/200, Batch: 4/29, Batch_Loss_Train: 2.561
2024-06-21 00:53:14,672 - INFO: Epoch: 88/200, Batch: 5/29, Batch_Loss_Train: 2.051
2024-06-21 00:53:14,974 - INFO: Epoch: 88/200, Batch: 6/29, Batch_Loss_Train: 1.425
2024-06-21 00:53:15,355 - INFO: Epoch: 88/200, Batch: 7/29, Batch_Loss_Train: 2.668
2024-06-21 00:53:15,672 - INFO: Epoch: 88/200, Batch: 8/29, Batch_Loss_Train: 2.404
2024-06-21 00:53:16,102 - INFO: Epoch: 88/200, Batch: 9/29, Batch_Loss_Train: 3.126
2024-06-21 00:53:16,400 - INFO: Epoch: 88/200, Batch: 10/29, Batch_Loss_Train: 2.925
2024-06-21 00:53:16,776 - INFO: Epoch: 88/200, Batch: 11/29, Batch_Loss_Train: 2.267
2024-06-21 00:53:17,095 - INFO: Epoch: 88/200, Batch: 12/29, Batch_Loss_Train: 2.682
2024-06-21 00:53:17,543 - INFO: Epoch: 88/200, Batch: 13/29, Batch_Loss_Train: 2.227
2024-06-21 00:53:17,853 - INFO: Epoch: 88/200, Batch: 14/29, Batch_Loss_Train: 2.040
2024-06-21 00:53:18,257 - INFO: Epoch: 88/200, Batch: 15/29, Batch_Loss_Train: 2.428
2024-06-21 00:53:18,575 - INFO: Epoch: 88/200, Batch: 16/29, Batch_Loss_Train: 1.933
2024-06-21 00:53:19,010 - INFO: Epoch: 88/200, Batch: 17/29, Batch_Loss_Train: 3.219
2024-06-21 00:53:19,315 - INFO: Epoch: 88/200, Batch: 18/29, Batch_Loss_Train: 3.226
2024-06-21 00:53:19,704 - INFO: Epoch: 88/200, Batch: 19/29, Batch_Loss_Train: 1.690
2024-06-21 00:53:20,019 - INFO: Epoch: 88/200, Batch: 20/29, Batch_Loss_Train: 2.232
2024-06-21 00:53:20,446 - INFO: Epoch: 88/200, Batch: 21/29, Batch_Loss_Train: 1.855
2024-06-21 00:53:20,754 - INFO: Epoch: 88/200, Batch: 22/29, Batch_Loss_Train: 2.276
2024-06-21 00:53:21,143 - INFO: Epoch: 88/200, Batch: 23/29, Batch_Loss_Train: 2.329
2024-06-21 00:53:21,464 - INFO: Epoch: 88/200, Batch: 24/29, Batch_Loss_Train: 2.416
2024-06-21 00:53:21,888 - INFO: Epoch: 88/200, Batch: 25/29, Batch_Loss_Train: 2.086
2024-06-21 00:53:22,188 - INFO: Epoch: 88/200, Batch: 26/29, Batch_Loss_Train: 3.210
2024-06-21 00:53:22,569 - INFO: Epoch: 88/200, Batch: 27/29, Batch_Loss_Train: 3.449
2024-06-21 00:53:22,882 - INFO: Epoch: 88/200, Batch: 28/29, Batch_Loss_Train: 2.125
2024-06-21 00:53:23,093 - INFO: Epoch: 88/200, Batch: 29/29, Batch_Loss_Train: 2.984
2024-06-21 00:53:34,113 - INFO: 88/200 final results:
2024-06-21 00:53:34,113 - INFO: Training loss: 2.435.
2024-06-21 00:53:34,113 - INFO: Training MAE: 1.207.
2024-06-21 00:53:34,114 - INFO: Training MSE: 2.424.
2024-06-21 00:53:54,820 - INFO: Epoch: 88/200, Loss_train: 2.434884416645971, Loss_val: 2.964116671989704
2024-06-21 00:53:54,820 - INFO: Best internal validation val_loss: 2.364 at epoch: 86.
2024-06-21 00:53:54,820 - INFO: Epoch 89/200...
2024-06-21 00:53:54,820 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:53:54,820 - INFO: Batch size: 32.
2024-06-21 00:53:54,824 - INFO: Dataset:
2024-06-21 00:53:54,824 - INFO: Batch size:
2024-06-21 00:53:54,824 - INFO: Number of workers:
2024-06-21 00:53:56,064 - INFO: Epoch: 89/200, Batch: 1/29, Batch_Loss_Train: 2.881
2024-06-21 00:53:56,388 - INFO: Epoch: 89/200, Batch: 2/29, Batch_Loss_Train: 2.160
2024-06-21 00:53:56,791 - INFO: Epoch: 89/200, Batch: 3/29, Batch_Loss_Train: 1.731
2024-06-21 00:53:57,114 - INFO: Epoch: 89/200, Batch: 4/29, Batch_Loss_Train: 2.389
2024-06-21 00:53:57,550 - INFO: Epoch: 89/200, Batch: 5/29, Batch_Loss_Train: 3.004
2024-06-21 00:53:57,857 - INFO: Epoch: 89/200, Batch: 6/29, Batch_Loss_Train: 2.111
2024-06-21 00:53:58,254 - INFO: Epoch: 89/200, Batch: 7/29, Batch_Loss_Train: 1.806
2024-06-21 00:53:58,576 - INFO: Epoch: 89/200, Batch: 8/29, Batch_Loss_Train: 2.088
2024-06-21 00:53:59,003 - INFO: Epoch: 89/200, Batch: 9/29, Batch_Loss_Train: 2.048
2024-06-21 00:53:59,301 - INFO: Epoch: 89/200, Batch: 10/29, Batch_Loss_Train: 2.077
2024-06-21 00:53:59,690 - INFO: Epoch: 89/200, Batch: 11/29, Batch_Loss_Train: 2.168
2024-06-21 00:54:00,010 - INFO: Epoch: 89/200, Batch: 12/29, Batch_Loss_Train: 1.652
2024-06-21 00:54:00,437 - INFO: Epoch: 89/200, Batch: 13/29, Batch_Loss_Train: 2.697
2024-06-21 00:54:00,744 - INFO: Epoch: 89/200, Batch: 14/29, Batch_Loss_Train: 2.374
2024-06-21 00:54:01,144 - INFO: Epoch: 89/200, Batch: 15/29, Batch_Loss_Train: 2.338
2024-06-21 00:54:01,459 - INFO: Epoch: 89/200, Batch: 16/29, Batch_Loss_Train: 2.146
2024-06-21 00:54:01,874 - INFO: Epoch: 89/200, Batch: 17/29, Batch_Loss_Train: 2.053
2024-06-21 00:54:02,177 - INFO: Epoch: 89/200, Batch: 18/29, Batch_Loss_Train: 2.766
2024-06-21 00:54:02,564 - INFO: Epoch: 89/200, Batch: 19/29, Batch_Loss_Train: 2.936
2024-06-21 00:54:02,877 - INFO: Epoch: 89/200, Batch: 20/29, Batch_Loss_Train: 1.831
2024-06-21 00:54:03,298 - INFO: Epoch: 89/200, Batch: 21/29, Batch_Loss_Train: 2.290
2024-06-21 00:54:03,604 - INFO: Epoch: 89/200, Batch: 22/29, Batch_Loss_Train: 2.223
2024-06-21 00:54:03,988 - INFO: Epoch: 89/200, Batch: 23/29, Batch_Loss_Train: 2.518
2024-06-21 00:54:04,306 - INFO: Epoch: 89/200, Batch: 24/29, Batch_Loss_Train: 2.365
2024-06-21 00:54:04,730 - INFO: Epoch: 89/200, Batch: 25/29, Batch_Loss_Train: 2.725
2024-06-21 00:54:05,031 - INFO: Epoch: 89/200, Batch: 26/29, Batch_Loss_Train: 1.815
2024-06-21 00:54:05,404 - INFO: Epoch: 89/200, Batch: 27/29, Batch_Loss_Train: 2.677
2024-06-21 00:54:05,718 - INFO: Epoch: 89/200, Batch: 28/29, Batch_Loss_Train: 2.138
2024-06-21 00:54:05,935 - INFO: Epoch: 89/200, Batch: 29/29, Batch_Loss_Train: 1.932
2024-06-21 00:54:17,012 - INFO: 89/200 final results:
2024-06-21 00:54:17,012 - INFO: Training loss: 2.274.
2024-06-21 00:54:17,012 - INFO: Training MAE: 1.176.
2024-06-21 00:54:17,012 - INFO: Training MSE: 2.281.
2024-06-21 00:54:37,493 - INFO: Epoch: 89/200, Loss_train: 2.273751682248609, Loss_val: 2.1724249576700143
2024-06-21 00:54:37,541 - INFO: Saved new best metric model for epoch 89.
2024-06-21 00:54:37,541 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:54:37,541 - INFO: Epoch 90/200...
2024-06-21 00:54:37,541 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:54:37,541 - INFO: Batch size: 32.
2024-06-21 00:54:37,545 - INFO: Dataset:
2024-06-21 00:54:37,545 - INFO: Batch size:
2024-06-21 00:54:37,546 - INFO: Number of workers:
2024-06-21 00:54:38,830 - INFO: Epoch: 90/200, Batch: 1/29, Batch_Loss_Train: 3.538
2024-06-21 00:54:39,142 - INFO: Epoch: 90/200, Batch: 2/29, Batch_Loss_Train: 2.184
2024-06-21 00:54:39,549 - INFO: Epoch: 90/200, Batch: 3/29, Batch_Loss_Train: 2.805
2024-06-21 00:54:39,873 - INFO: Epoch: 90/200, Batch: 4/29, Batch_Loss_Train: 1.671
2024-06-21 00:54:40,304 - INFO: Epoch: 90/200, Batch: 5/29, Batch_Loss_Train: 2.134
2024-06-21 00:54:40,609 - INFO: Epoch: 90/200, Batch: 6/29, Batch_Loss_Train: 2.015
2024-06-21 00:54:41,002 - INFO: Epoch: 90/200, Batch: 7/29, Batch_Loss_Train: 2.088
2024-06-21 00:54:41,321 - INFO: Epoch: 90/200, Batch: 8/29, Batch_Loss_Train: 2.117
2024-06-21 00:54:41,751 - INFO: Epoch: 90/200, Batch: 9/29, Batch_Loss_Train: 2.667
2024-06-21 00:54:42,050 - INFO: Epoch: 90/200, Batch: 10/29, Batch_Loss_Train: 2.125
2024-06-21 00:54:42,440 - INFO: Epoch: 90/200, Batch: 11/29, Batch_Loss_Train: 2.443
2024-06-21 00:54:42,760 - INFO: Epoch: 90/200, Batch: 12/29, Batch_Loss_Train: 1.996
2024-06-21 00:54:43,201 - INFO: Epoch: 90/200, Batch: 13/29, Batch_Loss_Train: 2.511
2024-06-21 00:54:43,509 - INFO: Epoch: 90/200, Batch: 14/29, Batch_Loss_Train: 3.941
2024-06-21 00:54:43,912 - INFO: Epoch: 90/200, Batch: 15/29, Batch_Loss_Train: 4.003
2024-06-21 00:54:44,228 - INFO: Epoch: 90/200, Batch: 16/29, Batch_Loss_Train: 1.665
2024-06-21 00:54:44,662 - INFO: Epoch: 90/200, Batch: 17/29, Batch_Loss_Train: 2.076
2024-06-21 00:54:44,965 - INFO: Epoch: 90/200, Batch: 18/29, Batch_Loss_Train: 2.954
2024-06-21 00:54:45,358 - INFO: Epoch: 90/200, Batch: 19/29, Batch_Loss_Train: 2.671
2024-06-21 00:54:45,669 - INFO: Epoch: 90/200, Batch: 20/29, Batch_Loss_Train: 2.436
2024-06-21 00:54:46,101 - INFO: Epoch: 90/200, Batch: 21/29, Batch_Loss_Train: 2.575
2024-06-21 00:54:46,407 - INFO: Epoch: 90/200, Batch: 22/29, Batch_Loss_Train: 2.835
2024-06-21 00:54:46,800 - INFO: Epoch: 90/200, Batch: 23/29, Batch_Loss_Train: 2.636
2024-06-21 00:54:47,118 - INFO: Epoch: 90/200, Batch: 24/29, Batch_Loss_Train: 1.980
2024-06-21 00:54:47,544 - INFO: Epoch: 90/200, Batch: 25/29, Batch_Loss_Train: 2.091
2024-06-21 00:54:47,845 - INFO: Epoch: 90/200, Batch: 26/29, Batch_Loss_Train: 1.894
2024-06-21 00:54:48,234 - INFO: Epoch: 90/200, Batch: 27/29, Batch_Loss_Train: 1.451
2024-06-21 00:54:48,548 - INFO: Epoch: 90/200, Batch: 28/29, Batch_Loss_Train: 1.523
2024-06-21 00:54:48,766 - INFO: Epoch: 90/200, Batch: 29/29, Batch_Loss_Train: 2.426
2024-06-21 00:54:59,760 - INFO: 90/200 final results:
2024-06-21 00:54:59,760 - INFO: Training loss: 2.395.
2024-06-21 00:54:59,761 - INFO: Training MAE: 1.205.
2024-06-21 00:54:59,761 - INFO: Training MSE: 2.394.
2024-06-21 00:55:20,151 - INFO: Epoch: 90/200, Loss_train: 2.3947991667122674, Loss_val: 2.3214898643822504
2024-06-21 00:55:20,151 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:55:20,151 - INFO: Epoch 91/200...
2024-06-21 00:55:20,151 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:55:20,151 - INFO: Batch size: 32.
2024-06-21 00:55:20,155 - INFO: Dataset:
2024-06-21 00:55:20,155 - INFO: Batch size:
2024-06-21 00:55:20,155 - INFO: Number of workers:
2024-06-21 00:55:21,387 - INFO: Epoch: 91/200, Batch: 1/29, Batch_Loss_Train: 2.612
2024-06-21 00:55:21,735 - INFO: Epoch: 91/200, Batch: 2/29, Batch_Loss_Train: 1.899
2024-06-21 00:55:22,132 - INFO: Epoch: 91/200, Batch: 3/29, Batch_Loss_Train: 2.499
2024-06-21 00:55:22,458 - INFO: Epoch: 91/200, Batch: 4/29, Batch_Loss_Train: 1.992
2024-06-21 00:55:22,874 - INFO: Epoch: 91/200, Batch: 5/29, Batch_Loss_Train: 2.238
2024-06-21 00:55:23,203 - INFO: Epoch: 91/200, Batch: 6/29, Batch_Loss_Train: 1.781
2024-06-21 00:55:23,600 - INFO: Epoch: 91/200, Batch: 7/29, Batch_Loss_Train: 2.160
2024-06-21 00:55:23,908 - INFO: Epoch: 91/200, Batch: 8/29, Batch_Loss_Train: 1.326
2024-06-21 00:55:24,325 - INFO: Epoch: 91/200, Batch: 9/29, Batch_Loss_Train: 2.376
2024-06-21 00:55:24,658 - INFO: Epoch: 91/200, Batch: 10/29, Batch_Loss_Train: 1.617
2024-06-21 00:55:25,052 - INFO: Epoch: 91/200, Batch: 11/29, Batch_Loss_Train: 2.951
2024-06-21 00:55:25,362 - INFO: Epoch: 91/200, Batch: 12/29, Batch_Loss_Train: 2.175
2024-06-21 00:55:25,800 - INFO: Epoch: 91/200, Batch: 13/29, Batch_Loss_Train: 4.266
2024-06-21 00:55:26,135 - INFO: Epoch: 91/200, Batch: 14/29, Batch_Loss_Train: 2.458
2024-06-21 00:55:26,543 - INFO: Epoch: 91/200, Batch: 15/29, Batch_Loss_Train: 2.954
2024-06-21 00:55:26,849 - INFO: Epoch: 91/200, Batch: 16/29, Batch_Loss_Train: 1.724
2024-06-21 00:55:27,275 - INFO: Epoch: 91/200, Batch: 17/29, Batch_Loss_Train: 2.376
2024-06-21 00:55:27,605 - INFO: Epoch: 91/200, Batch: 18/29, Batch_Loss_Train: 2.457
2024-06-21 00:55:28,000 - INFO: Epoch: 91/200, Batch: 19/29, Batch_Loss_Train: 2.469
2024-06-21 00:55:28,302 - INFO: Epoch: 91/200, Batch: 20/29, Batch_Loss_Train: 2.581
2024-06-21 00:55:28,727 - INFO: Epoch: 91/200, Batch: 21/29, Batch_Loss_Train: 2.858
2024-06-21 00:55:29,061 - INFO: Epoch: 91/200, Batch: 22/29, Batch_Loss_Train: 2.354
2024-06-21 00:55:29,452 - INFO: Epoch: 91/200, Batch: 23/29, Batch_Loss_Train: 2.557
2024-06-21 00:55:29,760 - INFO: Epoch: 91/200, Batch: 24/29, Batch_Loss_Train: 2.937
2024-06-21 00:55:30,173 - INFO: Epoch: 91/200, Batch: 25/29, Batch_Loss_Train: 2.576
2024-06-21 00:55:30,502 - INFO: Epoch: 91/200, Batch: 26/29, Batch_Loss_Train: 1.668
2024-06-21 00:55:30,892 - INFO: Epoch: 91/200, Batch: 27/29, Batch_Loss_Train: 1.941
2024-06-21 00:55:31,195 - INFO: Epoch: 91/200, Batch: 28/29, Batch_Loss_Train: 2.563
2024-06-21 00:55:31,408 - INFO: Epoch: 91/200, Batch: 29/29, Batch_Loss_Train: 2.343
2024-06-21 00:55:42,427 - INFO: 91/200 final results:
2024-06-21 00:55:42,427 - INFO: Training loss: 2.369.
2024-06-21 00:55:42,427 - INFO: Training MAE: 1.188.
2024-06-21 00:55:42,427 - INFO: Training MSE: 2.370.
2024-06-21 00:56:02,819 - INFO: Epoch: 91/200, Loss_train: 2.369281082317747, Loss_val: 2.782726230292485
2024-06-21 00:56:02,819 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:56:02,819 - INFO: Epoch 92/200...
2024-06-21 00:56:02,819 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:56:02,819 - INFO: Batch size: 32.
2024-06-21 00:56:02,823 - INFO: Dataset:
2024-06-21 00:56:02,823 - INFO: Batch size:
2024-06-21 00:56:02,823 - INFO: Number of workers:
2024-06-21 00:56:04,059 - INFO: Epoch: 92/200, Batch: 1/29, Batch_Loss_Train: 2.135
2024-06-21 00:56:04,373 - INFO: Epoch: 92/200, Batch: 2/29, Batch_Loss_Train: 2.273
2024-06-21 00:56:04,792 - INFO: Epoch: 92/200, Batch: 3/29, Batch_Loss_Train: 1.887
2024-06-21 00:56:05,118 - INFO: Epoch: 92/200, Batch: 4/29, Batch_Loss_Train: 2.677
2024-06-21 00:56:05,551 - INFO: Epoch: 92/200, Batch: 5/29, Batch_Loss_Train: 1.805
2024-06-21 00:56:05,858 - INFO: Epoch: 92/200, Batch: 6/29, Batch_Loss_Train: 2.267
2024-06-21 00:56:06,249 - INFO: Epoch: 92/200, Batch: 7/29, Batch_Loss_Train: 1.900
2024-06-21 00:56:06,570 - INFO: Epoch: 92/200, Batch: 8/29, Batch_Loss_Train: 1.955
2024-06-21 00:56:06,995 - INFO: Epoch: 92/200, Batch: 9/29, Batch_Loss_Train: 2.405
2024-06-21 00:56:07,296 - INFO: Epoch: 92/200, Batch: 10/29, Batch_Loss_Train: 3.144
2024-06-21 00:56:07,690 - INFO: Epoch: 92/200, Batch: 11/29, Batch_Loss_Train: 3.230
2024-06-21 00:56:08,012 - INFO: Epoch: 92/200, Batch: 12/29, Batch_Loss_Train: 2.278
2024-06-21 00:56:08,453 - INFO: Epoch: 92/200, Batch: 13/29, Batch_Loss_Train: 2.183
2024-06-21 00:56:08,763 - INFO: Epoch: 92/200, Batch: 14/29, Batch_Loss_Train: 1.981
2024-06-21 00:56:09,164 - INFO: Epoch: 92/200, Batch: 15/29, Batch_Loss_Train: 2.604
2024-06-21 00:56:09,482 - INFO: Epoch: 92/200, Batch: 16/29, Batch_Loss_Train: 3.421
2024-06-21 00:56:09,910 - INFO: Epoch: 92/200, Batch: 17/29, Batch_Loss_Train: 3.077
2024-06-21 00:56:10,215 - INFO: Epoch: 92/200, Batch: 18/29, Batch_Loss_Train: 3.124
2024-06-21 00:56:10,603 - INFO: Epoch: 92/200, Batch: 19/29, Batch_Loss_Train: 3.477
2024-06-21 00:56:10,917 - INFO: Epoch: 92/200, Batch: 20/29, Batch_Loss_Train: 1.463
2024-06-21 00:56:11,344 - INFO: Epoch: 92/200, Batch: 21/29, Batch_Loss_Train: 4.599
2024-06-21 00:56:11,652 - INFO: Epoch: 92/200, Batch: 22/29, Batch_Loss_Train: 3.410
2024-06-21 00:56:12,039 - INFO: Epoch: 92/200, Batch: 23/29, Batch_Loss_Train: 1.863
2024-06-21 00:56:12,359 - INFO: Epoch: 92/200, Batch: 24/29, Batch_Loss_Train: 2.661
2024-06-21 00:56:12,785 - INFO: Epoch: 92/200, Batch: 25/29, Batch_Loss_Train: 3.135
2024-06-21 00:56:13,085 - INFO: Epoch: 92/200, Batch: 26/29, Batch_Loss_Train: 2.557
2024-06-21 00:56:13,464 - INFO: Epoch: 92/200, Batch: 27/29, Batch_Loss_Train: 1.804
2024-06-21 00:56:13,776 - INFO: Epoch: 92/200, Batch: 28/29, Batch_Loss_Train: 2.457
2024-06-21 00:56:13,987 - INFO: Epoch: 92/200, Batch: 29/29, Batch_Loss_Train: 1.591
2024-06-21 00:56:24,955 - INFO: 92/200 final results:
2024-06-21 00:56:24,955 - INFO: Training loss: 2.530.
2024-06-21 00:56:24,955 - INFO: Training MAE: 1.222.
2024-06-21 00:56:24,955 - INFO: Training MSE: 2.548.
2024-06-21 00:56:45,596 - INFO: Epoch: 92/200, Loss_train: 2.529771299197756, Loss_val: 2.5583472046358833
2024-06-21 00:56:45,596 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:56:45,596 - INFO: Epoch 93/200...
2024-06-21 00:56:45,596 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:56:45,596 - INFO: Batch size: 32.
2024-06-21 00:56:45,600 - INFO: Dataset:
2024-06-21 00:56:45,600 - INFO: Batch size:
2024-06-21 00:56:45,600 - INFO: Number of workers:
2024-06-21 00:56:46,846 - INFO: Epoch: 93/200, Batch: 1/29, Batch_Loss_Train: 1.778
2024-06-21 00:56:47,171 - INFO: Epoch: 93/200, Batch: 2/29, Batch_Loss_Train: 2.617
2024-06-21 00:56:47,597 - INFO: Epoch: 93/200, Batch: 3/29, Batch_Loss_Train: 3.403
2024-06-21 00:56:47,923 - INFO: Epoch: 93/200, Batch: 4/29, Batch_Loss_Train: 1.916
2024-06-21 00:56:48,343 - INFO: Epoch: 93/200, Batch: 5/29, Batch_Loss_Train: 2.303
2024-06-21 00:56:48,650 - INFO: Epoch: 93/200, Batch: 6/29, Batch_Loss_Train: 2.462
2024-06-21 00:56:49,041 - INFO: Epoch: 93/200, Batch: 7/29, Batch_Loss_Train: 2.065
2024-06-21 00:56:49,361 - INFO: Epoch: 93/200, Batch: 8/29, Batch_Loss_Train: 2.311
2024-06-21 00:56:49,789 - INFO: Epoch: 93/200, Batch: 9/29, Batch_Loss_Train: 2.969
2024-06-21 00:56:50,097 - INFO: Epoch: 93/200, Batch: 10/29, Batch_Loss_Train: 2.020
2024-06-21 00:56:50,515 - INFO: Epoch: 93/200, Batch: 11/29, Batch_Loss_Train: 1.904
2024-06-21 00:56:50,834 - INFO: Epoch: 93/200, Batch: 12/29, Batch_Loss_Train: 2.424
2024-06-21 00:56:51,260 - INFO: Epoch: 93/200, Batch: 13/29, Batch_Loss_Train: 2.920
2024-06-21 00:56:51,565 - INFO: Epoch: 93/200, Batch: 14/29, Batch_Loss_Train: 1.745
2024-06-21 00:56:51,965 - INFO: Epoch: 93/200, Batch: 15/29, Batch_Loss_Train: 2.277
2024-06-21 00:56:52,279 - INFO: Epoch: 93/200, Batch: 16/29, Batch_Loss_Train: 1.916
2024-06-21 00:56:52,692 - INFO: Epoch: 93/200, Batch: 17/29, Batch_Loss_Train: 2.172
2024-06-21 00:56:52,993 - INFO: Epoch: 93/200, Batch: 18/29, Batch_Loss_Train: 3.423
2024-06-21 00:56:53,379 - INFO: Epoch: 93/200, Batch: 19/29, Batch_Loss_Train: 2.724
2024-06-21 00:56:53,693 - INFO: Epoch: 93/200, Batch: 20/29, Batch_Loss_Train: 2.710
2024-06-21 00:56:54,110 - INFO: Epoch: 93/200, Batch: 21/29, Batch_Loss_Train: 3.624
2024-06-21 00:56:54,417 - INFO: Epoch: 93/200, Batch: 22/29, Batch_Loss_Train: 2.118
2024-06-21 00:56:54,817 - INFO: Epoch: 93/200, Batch: 23/29, Batch_Loss_Train: 2.362
2024-06-21 00:56:55,134 - INFO: Epoch: 93/200, Batch: 24/29, Batch_Loss_Train: 2.533
2024-06-21 00:56:55,542 - INFO: Epoch: 93/200, Batch: 25/29, Batch_Loss_Train: 2.761
2024-06-21 00:56:55,841 - INFO: Epoch: 93/200, Batch: 26/29, Batch_Loss_Train: 1.817
2024-06-21 00:56:56,227 - INFO: Epoch: 93/200, Batch: 27/29, Batch_Loss_Train: 2.322
2024-06-21 00:56:56,539 - INFO: Epoch: 93/200, Batch: 28/29, Batch_Loss_Train: 1.693
2024-06-21 00:56:56,755 - INFO: Epoch: 93/200, Batch: 29/29, Batch_Loss_Train: 2.941
2024-06-21 00:57:07,801 - INFO: 93/200 final results:
2024-06-21 00:57:07,801 - INFO: Training loss: 2.422.
2024-06-21 00:57:07,801 - INFO: Training MAE: 1.192.
2024-06-21 00:57:07,801 - INFO: Training MSE: 2.411.
2024-06-21 00:57:28,184 - INFO: Epoch: 93/200, Loss_train: 2.421695384485968, Loss_val: 3.44562802232545
2024-06-21 00:57:28,184 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:57:28,185 - INFO: Epoch 94/200...
2024-06-21 00:57:28,185 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:57:28,185 - INFO: Batch size: 32.
2024-06-21 00:57:28,189 - INFO: Dataset:
2024-06-21 00:57:28,189 - INFO: Batch size:
2024-06-21 00:57:28,189 - INFO: Number of workers:
2024-06-21 00:57:29,452 - INFO: Epoch: 94/200, Batch: 1/29, Batch_Loss_Train: 2.666
2024-06-21 00:57:29,764 - INFO: Epoch: 94/200, Batch: 2/29, Batch_Loss_Train: 2.401
2024-06-21 00:57:30,160 - INFO: Epoch: 94/200, Batch: 3/29, Batch_Loss_Train: 2.615
2024-06-21 00:57:30,483 - INFO: Epoch: 94/200, Batch: 4/29, Batch_Loss_Train: 2.308
2024-06-21 00:57:30,902 - INFO: Epoch: 94/200, Batch: 5/29, Batch_Loss_Train: 2.490
2024-06-21 00:57:31,206 - INFO: Epoch: 94/200, Batch: 6/29, Batch_Loss_Train: 2.047
2024-06-21 00:57:31,594 - INFO: Epoch: 94/200, Batch: 7/29, Batch_Loss_Train: 2.263
2024-06-21 00:57:31,911 - INFO: Epoch: 94/200, Batch: 8/29, Batch_Loss_Train: 1.673
2024-06-21 00:57:32,336 - INFO: Epoch: 94/200, Batch: 9/29, Batch_Loss_Train: 2.046
2024-06-21 00:57:32,638 - INFO: Epoch: 94/200, Batch: 10/29, Batch_Loss_Train: 1.548
2024-06-21 00:57:33,033 - INFO: Epoch: 94/200, Batch: 11/29, Batch_Loss_Train: 2.533
2024-06-21 00:57:33,356 - INFO: Epoch: 94/200, Batch: 12/29, Batch_Loss_Train: 2.246
2024-06-21 00:57:33,806 - INFO: Epoch: 94/200, Batch: 13/29, Batch_Loss_Train: 1.944
2024-06-21 00:57:34,117 - INFO: Epoch: 94/200, Batch: 14/29, Batch_Loss_Train: 2.392
2024-06-21 00:57:34,524 - INFO: Epoch: 94/200, Batch: 15/29, Batch_Loss_Train: 3.014
2024-06-21 00:57:34,843 - INFO: Epoch: 94/200, Batch: 16/29, Batch_Loss_Train: 2.278
2024-06-21 00:57:35,281 - INFO: Epoch: 94/200, Batch: 17/29, Batch_Loss_Train: 1.875
2024-06-21 00:57:35,586 - INFO: Epoch: 94/200, Batch: 18/29, Batch_Loss_Train: 2.778
2024-06-21 00:57:35,981 - INFO: Epoch: 94/200, Batch: 19/29, Batch_Loss_Train: 2.412
2024-06-21 00:57:36,295 - INFO: Epoch: 94/200, Batch: 20/29, Batch_Loss_Train: 2.213
2024-06-21 00:57:36,732 - INFO: Epoch: 94/200, Batch: 21/29, Batch_Loss_Train: 1.800
2024-06-21 00:57:37,040 - INFO: Epoch: 94/200, Batch: 22/29, Batch_Loss_Train: 2.337
2024-06-21 00:57:37,429 - INFO: Epoch: 94/200, Batch: 23/29, Batch_Loss_Train: 1.998
2024-06-21 00:57:37,750 - INFO: Epoch: 94/200, Batch: 24/29, Batch_Loss_Train: 2.356
2024-06-21 00:57:38,180 - INFO: Epoch: 94/200, Batch: 25/29, Batch_Loss_Train: 1.916
2024-06-21 00:57:38,483 - INFO: Epoch: 94/200, Batch: 26/29, Batch_Loss_Train: 3.236
2024-06-21 00:57:38,861 - INFO: Epoch: 94/200, Batch: 27/29, Batch_Loss_Train: 2.494
2024-06-21 00:57:39,178 - INFO: Epoch: 94/200, Batch: 28/29, Batch_Loss_Train: 3.805
2024-06-21 00:57:39,400 - INFO: Epoch: 94/200, Batch: 29/29, Batch_Loss_Train: 3.296
2024-06-21 00:57:50,207 - INFO: 94/200 final results:
2024-06-21 00:57:50,207 - INFO: Training loss: 2.379.
2024-06-21 00:57:50,207 - INFO: Training MAE: 1.199.
2024-06-21 00:57:50,207 - INFO: Training MSE: 2.360.
2024-06-21 00:58:10,499 - INFO: Epoch: 94/200, Loss_train: 2.3785581218785254, Loss_val: 3.689620281087941
2024-06-21 00:58:10,499 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:58:10,499 - INFO: Epoch 95/200...
2024-06-21 00:58:10,499 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:58:10,499 - INFO: Batch size: 32.
2024-06-21 00:58:10,503 - INFO: Dataset:
2024-06-21 00:58:10,504 - INFO: Batch size:
2024-06-21 00:58:10,504 - INFO: Number of workers:
2024-06-21 00:58:11,762 - INFO: Epoch: 95/200, Batch: 1/29, Batch_Loss_Train: 3.597
2024-06-21 00:58:12,086 - INFO: Epoch: 95/200, Batch: 2/29, Batch_Loss_Train: 1.897
2024-06-21 00:58:12,485 - INFO: Epoch: 95/200, Batch: 3/29, Batch_Loss_Train: 2.230
2024-06-21 00:58:12,808 - INFO: Epoch: 95/200, Batch: 4/29, Batch_Loss_Train: 3.007
2024-06-21 00:58:13,238 - INFO: Epoch: 95/200, Batch: 5/29, Batch_Loss_Train: 2.048
2024-06-21 00:58:13,556 - INFO: Epoch: 95/200, Batch: 6/29, Batch_Loss_Train: 1.761
2024-06-21 00:58:13,951 - INFO: Epoch: 95/200, Batch: 7/29, Batch_Loss_Train: 2.070
2024-06-21 00:58:14,270 - INFO: Epoch: 95/200, Batch: 8/29, Batch_Loss_Train: 2.007
2024-06-21 00:58:14,696 - INFO: Epoch: 95/200, Batch: 9/29, Batch_Loss_Train: 2.331
2024-06-21 00:58:15,008 - INFO: Epoch: 95/200, Batch: 10/29, Batch_Loss_Train: 2.123
2024-06-21 00:58:15,403 - INFO: Epoch: 95/200, Batch: 11/29, Batch_Loss_Train: 1.801
2024-06-21 00:58:15,723 - INFO: Epoch: 95/200, Batch: 12/29, Batch_Loss_Train: 3.975
2024-06-21 00:58:16,157 - INFO: Epoch: 95/200, Batch: 13/29, Batch_Loss_Train: 2.970
2024-06-21 00:58:16,478 - INFO: Epoch: 95/200, Batch: 14/29, Batch_Loss_Train: 1.770
2024-06-21 00:58:16,880 - INFO: Epoch: 95/200, Batch: 15/29, Batch_Loss_Train: 2.277
2024-06-21 00:58:17,199 - INFO: Epoch: 95/200, Batch: 16/29, Batch_Loss_Train: 2.605
2024-06-21 00:58:17,626 - INFO: Epoch: 95/200, Batch: 17/29, Batch_Loss_Train: 2.297
2024-06-21 00:58:17,943 - INFO: Epoch: 95/200, Batch: 18/29, Batch_Loss_Train: 1.470
2024-06-21 00:58:18,339 - INFO: Epoch: 95/200, Batch: 19/29, Batch_Loss_Train: 2.591
2024-06-21 00:58:18,653 - INFO: Epoch: 95/200, Batch: 20/29, Batch_Loss_Train: 2.082
2024-06-21 00:58:19,072 - INFO: Epoch: 95/200, Batch: 21/29, Batch_Loss_Train: 2.450
2024-06-21 00:58:19,393 - INFO: Epoch: 95/200, Batch: 22/29, Batch_Loss_Train: 2.512
2024-06-21 00:58:19,792 - INFO: Epoch: 95/200, Batch: 23/29, Batch_Loss_Train: 2.403
2024-06-21 00:58:20,113 - INFO: Epoch: 95/200, Batch: 24/29, Batch_Loss_Train: 2.204
2024-06-21 00:58:20,527 - INFO: Epoch: 95/200, Batch: 25/29, Batch_Loss_Train: 2.553
2024-06-21 00:58:20,840 - INFO: Epoch: 95/200, Batch: 26/29, Batch_Loss_Train: 1.828
2024-06-21 00:58:21,227 - INFO: Epoch: 95/200, Batch: 27/29, Batch_Loss_Train: 1.962
2024-06-21 00:58:21,541 - INFO: Epoch: 95/200, Batch: 28/29, Batch_Loss_Train: 3.220
2024-06-21 00:58:21,757 - INFO: Epoch: 95/200, Batch: 29/29, Batch_Loss_Train: 2.965
2024-06-21 00:58:32,744 - INFO: 95/200 final results:
2024-06-21 00:58:32,744 - INFO: Training loss: 2.380.
2024-06-21 00:58:32,744 - INFO: Training MAE: 1.202.
2024-06-21 00:58:32,744 - INFO: Training MSE: 2.368.
2024-06-21 00:58:53,524 - INFO: Epoch: 95/200, Loss_train: 2.379677920505918, Loss_val: 3.1104167206534026
2024-06-21 00:58:53,524 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:58:53,524 - INFO: Epoch 96/200...
2024-06-21 00:58:53,524 - INFO: Learning rate: 0.00011019497700525854.
2024-06-21 00:58:53,524 - INFO: Batch size: 32.
2024-06-21 00:58:53,528 - INFO: Dataset:
2024-06-21 00:58:53,528 - INFO: Batch size:
2024-06-21 00:58:53,528 - INFO: Number of workers:
2024-06-21 00:58:54,785 - INFO: Epoch: 96/200, Batch: 1/29, Batch_Loss_Train: 2.750
2024-06-21 00:58:55,113 - INFO: Epoch: 96/200, Batch: 2/29, Batch_Loss_Train: 2.902
2024-06-21 00:58:55,537 - INFO: Epoch: 96/200, Batch: 3/29, Batch_Loss_Train: 1.813
2024-06-21 00:58:55,864 - INFO: Epoch: 96/200, Batch: 4/29, Batch_Loss_Train: 2.345
2024-06-21 00:58:56,291 - INFO: Epoch: 96/200, Batch: 5/29, Batch_Loss_Train: 2.121
2024-06-21 00:58:56,598 - INFO: Epoch: 96/200, Batch: 6/29, Batch_Loss_Train: 2.217
2024-06-21 00:58:57,008 - INFO: Epoch: 96/200, Batch: 7/29, Batch_Loss_Train: 2.286
2024-06-21 00:58:57,328 - INFO: Epoch: 96/200, Batch: 8/29, Batch_Loss_Train: 1.846
2024-06-21 00:58:57,743 - INFO: Epoch: 96/200, Batch: 9/29, Batch_Loss_Train: 1.461
2024-06-21 00:58:58,045 - INFO: Epoch: 96/200, Batch: 10/29, Batch_Loss_Train: 1.515
2024-06-21 00:58:58,451 - INFO: Epoch: 96/200, Batch: 11/29, Batch_Loss_Train: 1.956
2024-06-21 00:58:58,774 - INFO: Epoch: 96/200, Batch: 12/29, Batch_Loss_Train: 2.035
2024-06-21 00:58:59,211 - INFO: Epoch: 96/200, Batch: 13/29, Batch_Loss_Train: 1.559
2024-06-21 00:58:59,521 - INFO: Epoch: 96/200, Batch: 14/29, Batch_Loss_Train: 2.382
2024-06-21 00:58:59,942 - INFO: Epoch: 96/200, Batch: 15/29, Batch_Loss_Train: 2.450
2024-06-21 00:59:00,260 - INFO: Epoch: 96/200, Batch: 16/29, Batch_Loss_Train: 2.762
2024-06-21 00:59:00,685 - INFO: Epoch: 96/200, Batch: 17/29, Batch_Loss_Train: 3.428
2024-06-21 00:59:00,990 - INFO: Epoch: 96/200, Batch: 18/29, Batch_Loss_Train: 2.031
2024-06-21 00:59:01,399 - INFO: Epoch: 96/200, Batch: 19/29, Batch_Loss_Train: 2.087
2024-06-21 00:59:01,713 - INFO: Epoch: 96/200, Batch: 20/29, Batch_Loss_Train: 2.456
2024-06-21 00:59:02,137 - INFO: Epoch: 96/200, Batch: 21/29, Batch_Loss_Train: 3.022
2024-06-21 00:59:02,446 - INFO: Epoch: 96/200, Batch: 22/29, Batch_Loss_Train: 1.736
2024-06-21 00:59:02,859 - INFO: Epoch: 96/200, Batch: 23/29, Batch_Loss_Train: 3.060
2024-06-21 00:59:03,181 - INFO: Epoch: 96/200, Batch: 24/29, Batch_Loss_Train: 3.768
2024-06-21 00:59:03,598 - INFO: Epoch: 96/200, Batch: 25/29, Batch_Loss_Train: 2.097
2024-06-21 00:59:03,902 - INFO: Epoch: 96/200, Batch: 26/29, Batch_Loss_Train: 1.918
2024-06-21 00:59:04,305 - INFO: Epoch: 96/200, Batch: 27/29, Batch_Loss_Train: 2.829
2024-06-21 00:59:04,620 - INFO: Epoch: 96/200, Batch: 28/29, Batch_Loss_Train: 2.798
2024-06-21 00:59:04,843 - INFO: Epoch: 96/200, Batch: 29/29, Batch_Loss_Train: 2.266
2024-06-21 00:59:15,868 - INFO: 96/200 final results:
2024-06-21 00:59:15,869 - INFO: Training loss: 2.341.
2024-06-21 00:59:15,869 - INFO: Training MAE: 1.184.
2024-06-21 00:59:15,869 - INFO: Training MSE: 2.343.
2024-06-21 00:59:36,488 - INFO: Epoch: 96/200, Loss_train: 2.341201535586653, Loss_val: 2.8518550067112365
2024-06-21 00:59:36,489 - INFO: Best internal validation val_loss: 2.172 at epoch: 89.
2024-06-21 00:59:36,489 - INFO: Epoch 97/200...
2024-06-21 00:59:36,489 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 00:59:36,489 - INFO: Batch size: 32.
2024-06-21 00:59:36,493 - INFO: Dataset:
2024-06-21 00:59:36,493 - INFO: Batch size:
2024-06-21 00:59:36,493 - INFO: Number of workers:
2024-06-21 00:59:37,744 - INFO: Epoch: 97/200, Batch: 1/29, Batch_Loss_Train: 2.135
2024-06-21 00:59:38,057 - INFO: Epoch: 97/200, Batch: 2/29, Batch_Loss_Train: 1.896
2024-06-21 00:59:38,480 - INFO: Epoch: 97/200, Batch: 3/29, Batch_Loss_Train: 2.240
2024-06-21 00:59:38,807 - INFO: Epoch: 97/200, Batch: 4/29, Batch_Loss_Train: 1.868
2024-06-21 00:59:39,228 - INFO: Epoch: 97/200, Batch: 5/29, Batch_Loss_Train: 2.169
2024-06-21 00:59:39,535 - INFO: Epoch: 97/200, Batch: 6/29, Batch_Loss_Train: 2.067
2024-06-21 00:59:39,927 - INFO: Epoch: 97/200, Batch: 7/29, Batch_Loss_Train: 1.795
2024-06-21 00:59:40,247 - INFO: Epoch: 97/200, Batch: 8/29, Batch_Loss_Train: 2.050
2024-06-21 00:59:40,661 - INFO: Epoch: 97/200, Batch: 9/29, Batch_Loss_Train: 1.941
2024-06-21 00:59:40,960 - INFO: Epoch: 97/200, Batch: 10/29, Batch_Loss_Train: 2.038
2024-06-21 00:59:41,348 - INFO: Epoch: 97/200, Batch: 11/29, Batch_Loss_Train: 2.126
2024-06-21 00:59:41,668 - INFO: Epoch: 97/200, Batch: 12/29, Batch_Loss_Train: 1.007
2024-06-21 00:59:42,108 - INFO: Epoch: 97/200, Batch: 13/29, Batch_Loss_Train: 1.981
2024-06-21 00:59:42,418 - INFO: Epoch: 97/200, Batch: 14/29, Batch_Loss_Train: 1.926
2024-06-21 00:59:42,809 - INFO: Epoch: 97/200, Batch: 15/29, Batch_Loss_Train: 1.992
2024-06-21 00:59:43,128 - INFO: Epoch: 97/200, Batch: 16/29, Batch_Loss_Train: 1.592
2024-06-21 00:59:43,556 - INFO: Epoch: 97/200, Batch: 17/29, Batch_Loss_Train: 1.677
2024-06-21 00:59:43,861 - INFO: Epoch: 97/200, Batch: 18/29, Batch_Loss_Train: 1.870
2024-06-21 00:59:44,242 - INFO: Epoch: 97/200, Batch: 19/29, Batch_Loss_Train: 2.489
2024-06-21 00:59:44,558 - INFO: Epoch: 97/200, Batch: 20/29, Batch_Loss_Train: 1.603
2024-06-21 00:59:44,985 - INFO: Epoch: 97/200, Batch: 21/29, Batch_Loss_Train: 1.714
2024-06-21 00:59:45,293 - INFO: Epoch: 97/200, Batch: 22/29, Batch_Loss_Train: 1.837
2024-06-21 00:59:45,683 - INFO: Epoch: 97/200, Batch: 23/29, Batch_Loss_Train: 1.786
2024-06-21 00:59:46,004 - INFO: Epoch: 97/200, Batch: 24/29, Batch_Loss_Train: 1.319
2024-06-21 00:59:46,423 - INFO: Epoch: 97/200, Batch: 25/29, Batch_Loss_Train: 1.592
2024-06-21 00:59:46,723 - INFO: Epoch: 97/200, Batch: 26/29, Batch_Loss_Train: 1.897
2024-06-21 00:59:47,093 - INFO: Epoch: 97/200, Batch: 27/29, Batch_Loss_Train: 1.862
2024-06-21 00:59:47,406 - INFO: Epoch: 97/200, Batch: 28/29, Batch_Loss_Train: 2.205
2024-06-21 00:59:47,615 - INFO: Epoch: 97/200, Batch: 29/29, Batch_Loss_Train: 1.968
2024-06-21 00:59:58,380 - INFO: 97/200 final results:
2024-06-21 00:59:58,380 - INFO: Training loss: 1.884.
2024-06-21 00:59:58,380 - INFO: Training MAE: 1.068.
2024-06-21 00:59:58,380 - INFO: Training MSE: 1.882.
2024-06-21 01:00:19,112 - INFO: Epoch: 97/200, Loss_train: 1.884126613880026, Loss_val: 2.0762971475206573
2024-06-21 01:00:19,160 - INFO: Saved new best metric model for epoch 97.
2024-06-21 01:00:19,160 - INFO: Best internal validation val_loss: 2.076 at epoch: 97.
2024-06-21 01:00:19,160 - INFO: Epoch 98/200...
2024-06-21 01:00:19,160 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:00:19,160 - INFO: Batch size: 32.
2024-06-21 01:00:19,164 - INFO: Dataset:
2024-06-21 01:00:19,164 - INFO: Batch size:
2024-06-21 01:00:19,164 - INFO: Number of workers:
2024-06-21 01:00:20,436 - INFO: Epoch: 98/200, Batch: 1/29, Batch_Loss_Train: 1.747
2024-06-21 01:00:20,749 - INFO: Epoch: 98/200, Batch: 2/29, Batch_Loss_Train: 2.710
2024-06-21 01:00:21,164 - INFO: Epoch: 98/200, Batch: 3/29, Batch_Loss_Train: 1.543
2024-06-21 01:00:21,488 - INFO: Epoch: 98/200, Batch: 4/29, Batch_Loss_Train: 1.953
2024-06-21 01:00:21,911 - INFO: Epoch: 98/200, Batch: 5/29, Batch_Loss_Train: 1.951
2024-06-21 01:00:22,216 - INFO: Epoch: 98/200, Batch: 6/29, Batch_Loss_Train: 2.084
2024-06-21 01:00:22,618 - INFO: Epoch: 98/200, Batch: 7/29, Batch_Loss_Train: 1.589
2024-06-21 01:00:22,937 - INFO: Epoch: 98/200, Batch: 8/29, Batch_Loss_Train: 1.149
2024-06-21 01:00:23,371 - INFO: Epoch: 98/200, Batch: 9/29, Batch_Loss_Train: 1.870
2024-06-21 01:00:23,670 - INFO: Epoch: 98/200, Batch: 10/29, Batch_Loss_Train: 1.602
2024-06-21 01:00:24,072 - INFO: Epoch: 98/200, Batch: 11/29, Batch_Loss_Train: 1.973
2024-06-21 01:00:24,392 - INFO: Epoch: 98/200, Batch: 12/29, Batch_Loss_Train: 2.104
2024-06-21 01:00:24,828 - INFO: Epoch: 98/200, Batch: 13/29, Batch_Loss_Train: 2.063
2024-06-21 01:00:25,137 - INFO: Epoch: 98/200, Batch: 14/29, Batch_Loss_Train: 2.007
2024-06-21 01:00:25,551 - INFO: Epoch: 98/200, Batch: 15/29, Batch_Loss_Train: 1.574
2024-06-21 01:00:25,869 - INFO: Epoch: 98/200, Batch: 16/29, Batch_Loss_Train: 2.124
2024-06-21 01:00:26,278 - INFO: Epoch: 98/200, Batch: 17/29, Batch_Loss_Train: 1.931
2024-06-21 01:00:26,583 - INFO: Epoch: 98/200, Batch: 18/29, Batch_Loss_Train: 2.157
2024-06-21 01:00:26,984 - INFO: Epoch: 98/200, Batch: 19/29, Batch_Loss_Train: 2.035
2024-06-21 01:00:27,297 - INFO: Epoch: 98/200, Batch: 20/29, Batch_Loss_Train: 1.785
2024-06-21 01:00:27,705 - INFO: Epoch: 98/200, Batch: 21/29, Batch_Loss_Train: 1.699
2024-06-21 01:00:28,012 - INFO: Epoch: 98/200, Batch: 22/29, Batch_Loss_Train: 1.949
2024-06-21 01:00:28,420 - INFO: Epoch: 98/200, Batch: 23/29, Batch_Loss_Train: 1.504
2024-06-21 01:00:28,741 - INFO: Epoch: 98/200, Batch: 24/29, Batch_Loss_Train: 1.773
2024-06-21 01:00:29,158 - INFO: Epoch: 98/200, Batch: 25/29, Batch_Loss_Train: 1.690
2024-06-21 01:00:29,461 - INFO: Epoch: 98/200, Batch: 26/29, Batch_Loss_Train: 2.317
2024-06-21 01:00:29,864 - INFO: Epoch: 98/200, Batch: 27/29, Batch_Loss_Train: 2.545
2024-06-21 01:00:30,179 - INFO: Epoch: 98/200, Batch: 28/29, Batch_Loss_Train: 1.705
2024-06-21 01:00:30,401 - INFO: Epoch: 98/200, Batch: 29/29, Batch_Loss_Train: 0.952
2024-06-21 01:00:41,457 - INFO: 98/200 final results:
2024-06-21 01:00:41,457 - INFO: Training loss: 1.865.
2024-06-21 01:00:41,457 - INFO: Training MAE: 1.063.
2024-06-21 01:00:41,457 - INFO: Training MSE: 1.883.
2024-06-21 01:01:02,227 - INFO: Epoch: 98/200, Loss_train: 1.8650345144600704, Loss_val: 2.1651432945810516
2024-06-21 01:01:02,227 - INFO: Best internal validation val_loss: 2.076 at epoch: 97.
2024-06-21 01:01:02,227 - INFO: Epoch 99/200...
2024-06-21 01:01:02,227 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:01:02,227 - INFO: Batch size: 32.
2024-06-21 01:01:02,231 - INFO: Dataset:
2024-06-21 01:01:02,231 - INFO: Batch size:
2024-06-21 01:01:02,232 - INFO: Number of workers:
2024-06-21 01:01:03,527 - INFO: Epoch: 99/200, Batch: 1/29, Batch_Loss_Train: 2.130
2024-06-21 01:01:03,838 - INFO: Epoch: 99/200, Batch: 2/29, Batch_Loss_Train: 2.959
2024-06-21 01:01:04,242 - INFO: Epoch: 99/200, Batch: 3/29, Batch_Loss_Train: 2.339
2024-06-21 01:01:04,567 - INFO: Epoch: 99/200, Batch: 4/29, Batch_Loss_Train: 1.953
2024-06-21 01:01:04,999 - INFO: Epoch: 99/200, Batch: 5/29, Batch_Loss_Train: 2.177
2024-06-21 01:01:05,303 - INFO: Epoch: 99/200, Batch: 6/29, Batch_Loss_Train: 2.287
2024-06-21 01:01:05,695 - INFO: Epoch: 99/200, Batch: 7/29, Batch_Loss_Train: 1.797
2024-06-21 01:01:06,013 - INFO: Epoch: 99/200, Batch: 8/29, Batch_Loss_Train: 1.596
2024-06-21 01:01:06,440 - INFO: Epoch: 99/200, Batch: 9/29, Batch_Loss_Train: 1.524
2024-06-21 01:01:06,738 - INFO: Epoch: 99/200, Batch: 10/29, Batch_Loss_Train: 1.544
2024-06-21 01:01:07,115 - INFO: Epoch: 99/200, Batch: 11/29, Batch_Loss_Train: 2.203
2024-06-21 01:01:07,435 - INFO: Epoch: 99/200, Batch: 12/29, Batch_Loss_Train: 2.471
2024-06-21 01:01:07,862 - INFO: Epoch: 99/200, Batch: 13/29, Batch_Loss_Train: 2.431
2024-06-21 01:01:08,168 - INFO: Epoch: 99/200, Batch: 14/29, Batch_Loss_Train: 1.761
2024-06-21 01:01:08,558 - INFO: Epoch: 99/200, Batch: 15/29, Batch_Loss_Train: 1.593
2024-06-21 01:01:08,873 - INFO: Epoch: 99/200, Batch: 16/29, Batch_Loss_Train: 2.149
2024-06-21 01:01:09,290 - INFO: Epoch: 99/200, Batch: 17/29, Batch_Loss_Train: 3.288
2024-06-21 01:01:09,591 - INFO: Epoch: 99/200, Batch: 18/29, Batch_Loss_Train: 2.445
2024-06-21 01:01:09,968 - INFO: Epoch: 99/200, Batch: 19/29, Batch_Loss_Train: 1.836
2024-06-21 01:01:10,280 - INFO: Epoch: 99/200, Batch: 20/29, Batch_Loss_Train: 2.250
2024-06-21 01:01:10,694 - INFO: Epoch: 99/200, Batch: 21/29, Batch_Loss_Train: 1.511
2024-06-21 01:01:11,000 - INFO: Epoch: 99/200, Batch: 22/29, Batch_Loss_Train: 2.343
2024-06-21 01:01:11,384 - INFO: Epoch: 99/200, Batch: 23/29, Batch_Loss_Train: 1.972
2024-06-21 01:01:11,701 - INFO: Epoch: 99/200, Batch: 24/29, Batch_Loss_Train: 1.329
2024-06-21 01:01:12,119 - INFO: Epoch: 99/200, Batch: 25/29, Batch_Loss_Train: 1.777
2024-06-21 01:01:12,419 - INFO: Epoch: 99/200, Batch: 26/29, Batch_Loss_Train: 1.979
2024-06-21 01:01:12,792 - INFO: Epoch: 99/200, Batch: 27/29, Batch_Loss_Train: 2.034
2024-06-21 01:01:13,106 - INFO: Epoch: 99/200, Batch: 28/29, Batch_Loss_Train: 1.864
2024-06-21 01:01:13,319 - INFO: Epoch: 99/200, Batch: 29/29, Batch_Loss_Train: 1.301
2024-06-21 01:01:24,327 - INFO: 99/200 final results:
2024-06-21 01:01:24,327 - INFO: Training loss: 2.029.
2024-06-21 01:01:24,327 - INFO: Training MAE: 1.110.
2024-06-21 01:01:24,327 - INFO: Training MSE: 2.043.
2024-06-21 01:01:45,203 - INFO: Epoch: 99/200, Loss_train: 2.0290793838172125, Loss_val: 2.1106547117233276
2024-06-21 01:01:45,203 - INFO: Best internal validation val_loss: 2.076 at epoch: 97.
2024-06-21 01:01:45,203 - INFO: Epoch 100/200...
2024-06-21 01:01:45,203 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:01:45,203 - INFO: Batch size: 32.
2024-06-21 01:01:45,207 - INFO: Dataset:
2024-06-21 01:01:45,207 - INFO: Batch size:
2024-06-21 01:01:45,207 - INFO: Number of workers:
2024-06-21 01:01:46,464 - INFO: Epoch: 100/200, Batch: 1/29, Batch_Loss_Train: 2.075
2024-06-21 01:01:46,790 - INFO: Epoch: 100/200, Batch: 2/29, Batch_Loss_Train: 1.671
2024-06-21 01:01:47,192 - INFO: Epoch: 100/200, Batch: 3/29, Batch_Loss_Train: 2.485
2024-06-21 01:01:47,516 - INFO: Epoch: 100/200, Batch: 4/29, Batch_Loss_Train: 1.658
2024-06-21 01:01:47,931 - INFO: Epoch: 100/200, Batch: 5/29, Batch_Loss_Train: 1.537
2024-06-21 01:01:48,247 - INFO: Epoch: 100/200, Batch: 6/29, Batch_Loss_Train: 1.613
2024-06-21 01:01:48,628 - INFO: Epoch: 100/200, Batch: 7/29, Batch_Loss_Train: 2.182
2024-06-21 01:01:48,946 - INFO: Epoch: 100/200, Batch: 8/29, Batch_Loss_Train: 1.449
2024-06-21 01:01:49,360 - INFO: Epoch: 100/200, Batch: 9/29, Batch_Loss_Train: 2.063
2024-06-21 01:01:49,670 - INFO: Epoch: 100/200, Batch: 10/29, Batch_Loss_Train: 1.877
2024-06-21 01:01:50,058 - INFO: Epoch: 100/200, Batch: 11/29, Batch_Loss_Train: 2.314
2024-06-21 01:01:50,377 - INFO: Epoch: 100/200, Batch: 12/29, Batch_Loss_Train: 1.819
2024-06-21 01:01:50,803 - INFO: Epoch: 100/200, Batch: 13/29, Batch_Loss_Train: 1.480
2024-06-21 01:01:51,122 - INFO: Epoch: 100/200, Batch: 14/29, Batch_Loss_Train: 1.614
2024-06-21 01:01:51,523 - INFO: Epoch: 100/200, Batch: 15/29, Batch_Loss_Train: 1.819
2024-06-21 01:01:51,838 - INFO: Epoch: 100/200, Batch: 16/29, Batch_Loss_Train: 2.260
2024-06-21 01:01:52,257 - INFO: Epoch: 100/200, Batch: 17/29, Batch_Loss_Train: 1.852
2024-06-21 01:01:52,572 - INFO: Epoch: 100/200, Batch: 18/29, Batch_Loss_Train: 2.304
2024-06-21 01:01:52,954 - INFO: Epoch: 100/200, Batch: 19/29, Batch_Loss_Train: 1.533
2024-06-21 01:01:53,266 - INFO: Epoch: 100/200, Batch: 20/29, Batch_Loss_Train: 2.141
2024-06-21 01:01:53,684 - INFO: Epoch: 100/200, Batch: 21/29, Batch_Loss_Train: 1.803
2024-06-21 01:01:54,004 - INFO: Epoch: 100/200, Batch: 22/29, Batch_Loss_Train: 1.385
2024-06-21 01:01:54,397 - INFO: Epoch: 100/200, Batch: 23/29, Batch_Loss_Train: 2.344
2024-06-21 01:01:54,718 - INFO: Epoch: 100/200, Batch: 24/29, Batch_Loss_Train: 1.604
2024-06-21 01:01:55,129 - INFO: Epoch: 100/200, Batch: 25/29, Batch_Loss_Train: 1.342
2024-06-21 01:01:55,445 - INFO: Epoch: 100/200, Batch: 26/29, Batch_Loss_Train: 2.134
2024-06-21 01:01:55,835 - INFO: Epoch: 100/200, Batch: 27/29, Batch_Loss_Train: 1.646
2024-06-21 01:01:56,152 - INFO: Epoch: 100/200, Batch: 28/29, Batch_Loss_Train: 2.089
2024-06-21 01:01:56,377 - INFO: Epoch: 100/200, Batch: 29/29, Batch_Loss_Train: 2.043
2024-06-21 01:02:07,220 - INFO: 100/200 final results:
2024-06-21 01:02:07,220 - INFO: Training loss: 1.867.
2024-06-21 01:02:07,220 - INFO: Training MAE: 1.059.
2024-06-21 01:02:07,220 - INFO: Training MSE: 1.863.
2024-06-21 01:02:27,586 - INFO: Epoch: 100/200, Loss_train: 1.8667681217193604, Loss_val: 2.003593457156214
2024-06-21 01:02:27,631 - INFO: Saved new best metric model for epoch 100.
2024-06-21 01:02:27,632 - INFO: Best internal validation val_loss: 2.004 at epoch: 100.
2024-06-21 01:02:27,632 - INFO: Epoch 101/200...
2024-06-21 01:02:27,632 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:02:27,632 - INFO: Batch size: 32.
2024-06-21 01:02:27,636 - INFO: Dataset:
2024-06-21 01:02:27,636 - INFO: Batch size:
2024-06-21 01:02:27,636 - INFO: Number of workers:
2024-06-21 01:02:28,859 - INFO: Epoch: 101/200, Batch: 1/29, Batch_Loss_Train: 1.676
2024-06-21 01:02:29,197 - INFO: Epoch: 101/200, Batch: 2/29, Batch_Loss_Train: 1.849
2024-06-21 01:02:29,590 - INFO: Epoch: 101/200, Batch: 3/29, Batch_Loss_Train: 1.573
2024-06-21 01:02:29,913 - INFO: Epoch: 101/200, Batch: 4/29, Batch_Loss_Train: 1.899
2024-06-21 01:02:30,307 - INFO: Epoch: 101/200, Batch: 5/29, Batch_Loss_Train: 2.727
2024-06-21 01:02:30,636 - INFO: Epoch: 101/200, Batch: 6/29, Batch_Loss_Train: 1.447
2024-06-21 01:02:31,011 - INFO: Epoch: 101/200, Batch: 7/29, Batch_Loss_Train: 2.193
2024-06-21 01:02:31,327 - INFO: Epoch: 101/200, Batch: 8/29, Batch_Loss_Train: 1.409
2024-06-21 01:02:31,708 - INFO: Epoch: 101/200, Batch: 9/29, Batch_Loss_Train: 2.270
2024-06-21 01:02:32,038 - INFO: Epoch: 101/200, Batch: 10/29, Batch_Loss_Train: 2.079
2024-06-21 01:02:32,412 - INFO: Epoch: 101/200, Batch: 11/29, Batch_Loss_Train: 1.689
2024-06-21 01:02:32,732 - INFO: Epoch: 101/200, Batch: 12/29, Batch_Loss_Train: 2.139
2024-06-21 01:02:33,133 - INFO: Epoch: 101/200, Batch: 13/29, Batch_Loss_Train: 1.901
2024-06-21 01:02:33,465 - INFO: Epoch: 101/200, Batch: 14/29, Batch_Loss_Train: 1.606
2024-06-21 01:02:33,850 - INFO: Epoch: 101/200, Batch: 15/29, Batch_Loss_Train: 1.889
2024-06-21 01:02:34,165 - INFO: Epoch: 101/200, Batch: 16/29, Batch_Loss_Train: 1.641
2024-06-21 01:02:34,555 - INFO: Epoch: 101/200, Batch: 17/29, Batch_Loss_Train: 1.656
2024-06-21 01:02:34,882 - INFO: Epoch: 101/200, Batch: 18/29, Batch_Loss_Train: 2.259
2024-06-21 01:02:35,256 - INFO: Epoch: 101/200, Batch: 19/29, Batch_Loss_Train: 1.624
2024-06-21 01:02:35,568 - INFO: Epoch: 101/200, Batch: 20/29, Batch_Loss_Train: 1.772
2024-06-21 01:02:35,958 - INFO: Epoch: 101/200, Batch: 21/29, Batch_Loss_Train: 1.650
2024-06-21 01:02:36,287 - INFO: Epoch: 101/200, Batch: 22/29, Batch_Loss_Train: 2.421
2024-06-21 01:02:36,669 - INFO: Epoch: 101/200, Batch: 23/29, Batch_Loss_Train: 1.765
2024-06-21 01:02:36,987 - INFO: Epoch: 101/200, Batch: 24/29, Batch_Loss_Train: 1.655
2024-06-21 01:02:37,377 - INFO: Epoch: 101/200, Batch: 25/29, Batch_Loss_Train: 2.608
2024-06-21 01:02:37,701 - INFO: Epoch: 101/200, Batch: 26/29, Batch_Loss_Train: 2.050
2024-06-21 01:02:38,074 - INFO: Epoch: 101/200, Batch: 27/29, Batch_Loss_Train: 2.171
2024-06-21 01:02:38,387 - INFO: Epoch: 101/200, Batch: 28/29, Batch_Loss_Train: 1.960
2024-06-21 01:02:38,596 - INFO: Epoch: 101/200, Batch: 29/29, Batch_Loss_Train: 2.523
2024-06-21 01:02:49,520 - INFO: 101/200 final results:
2024-06-21 01:02:49,521 - INFO: Training loss: 1.934.
2024-06-21 01:02:49,521 - INFO: Training MAE: 1.071.
2024-06-21 01:02:49,521 - INFO: Training MSE: 1.923.
2024-06-21 01:03:09,786 - INFO: Epoch: 101/200, Loss_train: 1.9344206185176456, Loss_val: 2.5519824562401605
2024-06-21 01:03:09,786 - INFO: Best internal validation val_loss: 2.004 at epoch: 100.
2024-06-21 01:03:09,786 - INFO: Epoch 102/200...
2024-06-21 01:03:09,786 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:03:09,786 - INFO: Batch size: 32.
2024-06-21 01:03:09,790 - INFO: Dataset:
2024-06-21 01:03:09,790 - INFO: Batch size:
2024-06-21 01:03:09,790 - INFO: Number of workers:
2024-06-21 01:03:11,054 - INFO: Epoch: 102/200, Batch: 1/29, Batch_Loss_Train: 2.411
2024-06-21 01:03:11,366 - INFO: Epoch: 102/200, Batch: 2/29, Batch_Loss_Train: 1.754
2024-06-21 01:03:11,772 - INFO: Epoch: 102/200, Batch: 3/29, Batch_Loss_Train: 2.908
2024-06-21 01:03:12,082 - INFO: Epoch: 102/200, Batch: 4/29, Batch_Loss_Train: 2.075
2024-06-21 01:03:12,521 - INFO: Epoch: 102/200, Batch: 5/29, Batch_Loss_Train: 2.002
2024-06-21 01:03:12,825 - INFO: Epoch: 102/200, Batch: 6/29, Batch_Loss_Train: 1.848
2024-06-21 01:03:13,217 - INFO: Epoch: 102/200, Batch: 7/29, Batch_Loss_Train: 2.257
2024-06-21 01:03:13,521 - INFO: Epoch: 102/200, Batch: 8/29, Batch_Loss_Train: 1.444
2024-06-21 01:03:13,963 - INFO: Epoch: 102/200, Batch: 9/29, Batch_Loss_Train: 1.942
2024-06-21 01:03:14,265 - INFO: Epoch: 102/200, Batch: 10/29, Batch_Loss_Train: 2.088
2024-06-21 01:03:14,646 - INFO: Epoch: 102/200, Batch: 11/29, Batch_Loss_Train: 1.371
2024-06-21 01:03:14,957 - INFO: Epoch: 102/200, Batch: 12/29, Batch_Loss_Train: 1.813
2024-06-21 01:03:15,414 - INFO: Epoch: 102/200, Batch: 13/29, Batch_Loss_Train: 1.944
2024-06-21 01:03:15,723 - INFO: Epoch: 102/200, Batch: 14/29, Batch_Loss_Train: 1.886
2024-06-21 01:03:16,130 - INFO: Epoch: 102/200, Batch: 15/29, Batch_Loss_Train: 2.259
2024-06-21 01:03:16,436 - INFO: Epoch: 102/200, Batch: 16/29, Batch_Loss_Train: 2.196
2024-06-21 01:03:16,883 - INFO: Epoch: 102/200, Batch: 17/29, Batch_Loss_Train: 2.173
2024-06-21 01:03:17,189 - INFO: Epoch: 102/200, Batch: 18/29, Batch_Loss_Train: 1.412
2024-06-21 01:03:17,585 - INFO: Epoch: 102/200, Batch: 19/29, Batch_Loss_Train: 1.981
2024-06-21 01:03:17,886 - INFO: Epoch: 102/200, Batch: 20/29, Batch_Loss_Train: 2.047
2024-06-21 01:03:18,330 - INFO: Epoch: 102/200, Batch: 21/29, Batch_Loss_Train: 1.662
2024-06-21 01:03:18,638 - INFO: Epoch: 102/200, Batch: 22/29, Batch_Loss_Train: 1.352
2024-06-21 01:03:19,037 - INFO: Epoch: 102/200, Batch: 23/29, Batch_Loss_Train: 1.493
2024-06-21 01:03:19,346 - INFO: Epoch: 102/200, Batch: 24/29, Batch_Loss_Train: 1.699
2024-06-21 01:03:19,784 - INFO: Epoch: 102/200, Batch: 25/29, Batch_Loss_Train: 2.498
2024-06-21 01:03:20,085 - INFO: Epoch: 102/200, Batch: 26/29, Batch_Loss_Train: 1.971
2024-06-21 01:03:20,466 - INFO: Epoch: 102/200, Batch: 27/29, Batch_Loss_Train: 1.638
2024-06-21 01:03:20,767 - INFO: Epoch: 102/200, Batch: 28/29, Batch_Loss_Train: 2.213
2024-06-21 01:03:20,989 - INFO: Epoch: 102/200, Batch: 29/29, Batch_Loss_Train: 1.846
2024-06-21 01:03:31,963 - INFO: 102/200 final results:
2024-06-21 01:03:31,963 - INFO: Training loss: 1.937.
2024-06-21 01:03:31,963 - INFO: Training MAE: 1.085.
2024-06-21 01:03:31,963 - INFO: Training MSE: 1.939.
2024-06-21 01:03:52,557 - INFO: Epoch: 102/200, Loss_train: 1.937401298818917, Loss_val: 2.0175335181170495
2024-06-21 01:03:52,557 - INFO: Best internal validation val_loss: 2.004 at epoch: 100.
2024-06-21 01:03:52,557 - INFO: Epoch 103/200...
2024-06-21 01:03:52,557 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:03:52,557 - INFO: Batch size: 32.
2024-06-21 01:03:52,561 - INFO: Dataset:
2024-06-21 01:03:52,561 - INFO: Batch size:
2024-06-21 01:03:52,561 - INFO: Number of workers:
2024-06-21 01:03:53,830 - INFO: Epoch: 103/200, Batch: 1/29, Batch_Loss_Train: 2.327
2024-06-21 01:03:54,142 - INFO: Epoch: 103/200, Batch: 2/29, Batch_Loss_Train: 1.896
2024-06-21 01:03:54,549 - INFO: Epoch: 103/200, Batch: 3/29, Batch_Loss_Train: 2.354
2024-06-21 01:03:54,873 - INFO: Epoch: 103/200, Batch: 4/29, Batch_Loss_Train: 1.850
2024-06-21 01:03:55,322 - INFO: Epoch: 103/200, Batch: 5/29, Batch_Loss_Train: 2.324
2024-06-21 01:03:55,626 - INFO: Epoch: 103/200, Batch: 6/29, Batch_Loss_Train: 1.720
2024-06-21 01:03:56,017 - INFO: Epoch: 103/200, Batch: 7/29, Batch_Loss_Train: 1.220
2024-06-21 01:03:56,322 - INFO: Epoch: 103/200, Batch: 8/29, Batch_Loss_Train: 1.399
2024-06-21 01:03:56,772 - INFO: Epoch: 103/200, Batch: 9/29, Batch_Loss_Train: 1.641
2024-06-21 01:03:57,071 - INFO: Epoch: 103/200, Batch: 10/29, Batch_Loss_Train: 2.046
2024-06-21 01:03:57,457 - INFO: Epoch: 103/200, Batch: 11/29, Batch_Loss_Train: 2.261
2024-06-21 01:03:57,764 - INFO: Epoch: 103/200, Batch: 12/29, Batch_Loss_Train: 2.100
2024-06-21 01:03:58,219 - INFO: Epoch: 103/200, Batch: 13/29, Batch_Loss_Train: 2.100
2024-06-21 01:03:58,526 - INFO: Epoch: 103/200, Batch: 14/29, Batch_Loss_Train: 2.186
2024-06-21 01:03:58,928 - INFO: Epoch: 103/200, Batch: 15/29, Batch_Loss_Train: 1.163
2024-06-21 01:03:59,232 - INFO: Epoch: 103/200, Batch: 16/29, Batch_Loss_Train: 1.863
2024-06-21 01:03:59,677 - INFO: Epoch: 103/200, Batch: 17/29, Batch_Loss_Train: 2.617
2024-06-21 01:03:59,980 - INFO: Epoch: 103/200, Batch: 18/29, Batch_Loss_Train: 1.297
2024-06-21 01:04:00,369 - INFO: Epoch: 103/200, Batch: 19/29, Batch_Loss_Train: 1.616
2024-06-21 01:04:00,668 - INFO: Epoch: 103/200, Batch: 20/29, Batch_Loss_Train: 1.795
2024-06-21 01:04:01,116 - INFO: Epoch: 103/200, Batch: 21/29, Batch_Loss_Train: 1.588
2024-06-21 01:04:01,424 - INFO: Epoch: 103/200, Batch: 22/29, Batch_Loss_Train: 1.100
2024-06-21 01:04:01,825 - INFO: Epoch: 103/200, Batch: 23/29, Batch_Loss_Train: 1.756
2024-06-21 01:04:02,134 - INFO: Epoch: 103/200, Batch: 24/29, Batch_Loss_Train: 1.951
2024-06-21 01:04:02,576 - INFO: Epoch: 103/200, Batch: 25/29, Batch_Loss_Train: 1.756
2024-06-21 01:04:02,880 - INFO: Epoch: 103/200, Batch: 26/29, Batch_Loss_Train: 1.708
2024-06-21 01:04:03,256 - INFO: Epoch: 103/200, Batch: 27/29, Batch_Loss_Train: 1.511
2024-06-21 01:04:03,559 - INFO: Epoch: 103/200, Batch: 28/29, Batch_Loss_Train: 1.452
2024-06-21 01:04:03,773 - INFO: Epoch: 103/200, Batch: 29/29, Batch_Loss_Train: 1.196
2024-06-21 01:04:14,807 - INFO: 103/200 final results:
2024-06-21 01:04:14,808 - INFO: Training loss: 1.786.
2024-06-21 01:04:14,808 - INFO: Training MAE: 1.039.
2024-06-21 01:04:14,808 - INFO: Training MSE: 1.798.
2024-06-21 01:04:34,882 - INFO: Epoch: 103/200, Loss_train: 1.7859729281787216, Loss_val: 2.0165943092313308
2024-06-21 01:04:34,882 - INFO: Best internal validation val_loss: 2.004 at epoch: 100.
2024-06-21 01:04:34,882 - INFO: Epoch 104/200...
2024-06-21 01:04:34,882 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:04:34,882 - INFO: Batch size: 32.
2024-06-21 01:04:34,886 - INFO: Dataset:
2024-06-21 01:04:34,886 - INFO: Batch size:
2024-06-21 01:04:34,886 - INFO: Number of workers:
2024-06-21 01:04:36,148 - INFO: Epoch: 104/200, Batch: 1/29, Batch_Loss_Train: 1.716
2024-06-21 01:04:36,459 - INFO: Epoch: 104/200, Batch: 2/29, Batch_Loss_Train: 1.189
2024-06-21 01:04:36,878 - INFO: Epoch: 104/200, Batch: 3/29, Batch_Loss_Train: 1.710
2024-06-21 01:04:37,201 - INFO: Epoch: 104/200, Batch: 4/29, Batch_Loss_Train: 1.589
2024-06-21 01:04:37,614 - INFO: Epoch: 104/200, Batch: 5/29, Batch_Loss_Train: 1.597
2024-06-21 01:04:37,918 - INFO: Epoch: 104/200, Batch: 6/29, Batch_Loss_Train: 1.865
2024-06-21 01:04:38,327 - INFO: Epoch: 104/200, Batch: 7/29, Batch_Loss_Train: 2.368
2024-06-21 01:04:38,644 - INFO: Epoch: 104/200, Batch: 8/29, Batch_Loss_Train: 1.533
2024-06-21 01:04:39,059 - INFO: Epoch: 104/200, Batch: 9/29, Batch_Loss_Train: 1.369
2024-06-21 01:04:39,357 - INFO: Epoch: 104/200, Batch: 10/29, Batch_Loss_Train: 2.016
2024-06-21 01:04:39,753 - INFO: Epoch: 104/200, Batch: 11/29, Batch_Loss_Train: 2.410
2024-06-21 01:04:40,076 - INFO: Epoch: 104/200, Batch: 12/29, Batch_Loss_Train: 1.998
2024-06-21 01:04:40,501 - INFO: Epoch: 104/200, Batch: 13/29, Batch_Loss_Train: 1.742
2024-06-21 01:04:40,811 - INFO: Epoch: 104/200, Batch: 14/29, Batch_Loss_Train: 2.351
2024-06-21 01:04:41,221 - INFO: Epoch: 104/200, Batch: 15/29, Batch_Loss_Train: 2.656
2024-06-21 01:04:41,539 - INFO: Epoch: 104/200, Batch: 16/29, Batch_Loss_Train: 2.178
2024-06-21 01:04:41,964 - INFO: Epoch: 104/200, Batch: 17/29, Batch_Loss_Train: 1.872
2024-06-21 01:04:42,269 - INFO: Epoch: 104/200, Batch: 18/29, Batch_Loss_Train: 2.398
2024-06-21 01:04:42,673 - INFO: Epoch: 104/200, Batch: 19/29, Batch_Loss_Train: 1.652
2024-06-21 01:04:42,987 - INFO: Epoch: 104/200, Batch: 20/29, Batch_Loss_Train: 1.524
2024-06-21 01:04:43,403 - INFO: Epoch: 104/200, Batch: 21/29, Batch_Loss_Train: 1.526
2024-06-21 01:04:43,709 - INFO: Epoch: 104/200, Batch: 22/29, Batch_Loss_Train: 1.390
2024-06-21 01:04:44,116 - INFO: Epoch: 104/200, Batch: 23/29, Batch_Loss_Train: 2.833
2024-06-21 01:04:44,434 - INFO: Epoch: 104/200, Batch: 24/29, Batch_Loss_Train: 2.774
2024-06-21 01:04:44,843 - INFO: Epoch: 104/200, Batch: 25/29, Batch_Loss_Train: 2.050
2024-06-21 01:04:45,143 - INFO: Epoch: 104/200, Batch: 26/29, Batch_Loss_Train: 2.278
2024-06-21 01:04:45,540 - INFO: Epoch: 104/200, Batch: 27/29, Batch_Loss_Train: 1.869
2024-06-21 01:04:45,856 - INFO: Epoch: 104/200, Batch: 28/29, Batch_Loss_Train: 1.776
2024-06-21 01:04:46,080 - INFO: Epoch: 104/200, Batch: 29/29, Batch_Loss_Train: 2.062
2024-06-21 01:04:57,245 - INFO: 104/200 final results:
2024-06-21 01:04:57,245 - INFO: Training loss: 1.941.
2024-06-21 01:04:57,245 - INFO: Training MAE: 1.071.
2024-06-21 01:04:57,245 - INFO: Training MSE: 1.939.
2024-06-21 01:05:17,834 - INFO: Epoch: 104/200, Loss_train: 1.941115712297374, Loss_val: 2.3997488412363777
2024-06-21 01:05:17,834 - INFO: Best internal validation val_loss: 2.004 at epoch: 100.
2024-06-21 01:05:17,834 - INFO: Epoch 105/200...
2024-06-21 01:05:17,834 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:05:17,834 - INFO: Batch size: 32.
2024-06-21 01:05:17,838 - INFO: Dataset:
2024-06-21 01:05:17,838 - INFO: Batch size:
2024-06-21 01:05:17,838 - INFO: Number of workers:
2024-06-21 01:05:19,109 - INFO: Epoch: 105/200, Batch: 1/29, Batch_Loss_Train: 2.437
2024-06-21 01:05:19,421 - INFO: Epoch: 105/200, Batch: 2/29, Batch_Loss_Train: 2.040
2024-06-21 01:05:19,828 - INFO: Epoch: 105/200, Batch: 3/29, Batch_Loss_Train: 1.349
2024-06-21 01:05:20,150 - INFO: Epoch: 105/200, Batch: 4/29, Batch_Loss_Train: 2.780
2024-06-21 01:05:20,589 - INFO: Epoch: 105/200, Batch: 5/29, Batch_Loss_Train: 2.242
2024-06-21 01:05:20,897 - INFO: Epoch: 105/200, Batch: 6/29, Batch_Loss_Train: 2.437
2024-06-21 01:05:21,292 - INFO: Epoch: 105/200, Batch: 7/29, Batch_Loss_Train: 2.709
2024-06-21 01:05:21,614 - INFO: Epoch: 105/200, Batch: 8/29, Batch_Loss_Train: 2.077
2024-06-21 01:05:22,054 - INFO: Epoch: 105/200, Batch: 9/29, Batch_Loss_Train: 2.256
2024-06-21 01:05:22,357 - INFO: Epoch: 105/200, Batch: 10/29, Batch_Loss_Train: 2.073
2024-06-21 01:05:22,758 - INFO: Epoch: 105/200, Batch: 11/29, Batch_Loss_Train: 1.944
2024-06-21 01:05:23,081 - INFO: Epoch: 105/200, Batch: 12/29, Batch_Loss_Train: 1.614
2024-06-21 01:05:23,525 - INFO: Epoch: 105/200, Batch: 13/29, Batch_Loss_Train: 1.714
2024-06-21 01:05:23,833 - INFO: Epoch: 105/200, Batch: 14/29, Batch_Loss_Train: 1.662
2024-06-21 01:05:24,238 - INFO: Epoch: 105/200, Batch: 15/29, Batch_Loss_Train: 1.514
2024-06-21 01:05:24,553 - INFO: Epoch: 105/200, Batch: 16/29, Batch_Loss_Train: 1.234
2024-06-21 01:05:24,982 - INFO: Epoch: 105/200, Batch: 17/29, Batch_Loss_Train: 1.825
2024-06-21 01:05:25,285 - INFO: Epoch: 105/200, Batch: 18/29, Batch_Loss_Train: 1.880
2024-06-21 01:05:25,677 - INFO: Epoch: 105/200, Batch: 19/29, Batch_Loss_Train: 1.790
2024-06-21 01:05:25,989 - INFO: Epoch: 105/200, Batch: 20/29, Batch_Loss_Train: 1.496
2024-06-21 01:05:26,424 - INFO: Epoch: 105/200, Batch: 21/29, Batch_Loss_Train: 1.507
2024-06-21 01:05:26,733 - INFO: Epoch: 105/200, Batch: 22/29, Batch_Loss_Train: 1.446
2024-06-21 01:05:27,134 - INFO: Epoch: 105/200, Batch: 23/29, Batch_Loss_Train: 2.125
2024-06-21 01:05:27,456 - INFO: Epoch: 105/200, Batch: 24/29, Batch_Loss_Train: 1.688
2024-06-21 01:05:27,876 - INFO: Epoch: 105/200, Batch: 25/29, Batch_Loss_Train: 1.523
2024-06-21 01:05:28,177 - INFO: Epoch: 105/200, Batch: 26/29, Batch_Loss_Train: 2.589
2024-06-21 01:05:28,555 - INFO: Epoch: 105/200, Batch: 27/29, Batch_Loss_Train: 2.653
2024-06-21 01:05:28,869 - INFO: Epoch: 105/200, Batch: 28/29, Batch_Loss_Train: 2.497
2024-06-21 01:05:29,086 - INFO: Epoch: 105/200, Batch: 29/29, Batch_Loss_Train: 1.599
2024-06-21 01:05:40,081 - INFO: 105/200 final results:
2024-06-21 01:05:40,081 - INFO: Training loss: 1.955.
2024-06-21 01:05:40,081 - INFO: Training MAE: 1.089.
2024-06-21 01:05:40,081 - INFO: Training MSE: 1.962.
2024-06-21 01:06:00,768 - INFO: Epoch: 105/200, Loss_train: 1.9552642600289707, Loss_val: 1.9879736489263073
2024-06-21 01:06:00,817 - INFO: Saved new best metric model for epoch 105.
2024-06-21 01:06:00,817 - INFO: Best internal validation val_loss: 1.988 at epoch: 105.
2024-06-21 01:06:00,817 - INFO: Epoch 106/200...
2024-06-21 01:06:00,817 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:06:00,817 - INFO: Batch size: 32.
2024-06-21 01:06:00,821 - INFO: Dataset:
2024-06-21 01:06:00,821 - INFO: Batch size:
2024-06-21 01:06:00,821 - INFO: Number of workers:
2024-06-21 01:06:02,060 - INFO: Epoch: 106/200, Batch: 1/29, Batch_Loss_Train: 2.366
2024-06-21 01:06:02,385 - INFO: Epoch: 106/200, Batch: 2/29, Batch_Loss_Train: 1.787
2024-06-21 01:06:02,804 - INFO: Epoch: 106/200, Batch: 3/29, Batch_Loss_Train: 1.962
2024-06-21 01:06:03,127 - INFO: Epoch: 106/200, Batch: 4/29, Batch_Loss_Train: 1.598
2024-06-21 01:06:03,544 - INFO: Epoch: 106/200, Batch: 5/29, Batch_Loss_Train: 1.897
2024-06-21 01:06:03,848 - INFO: Epoch: 106/200, Batch: 6/29, Batch_Loss_Train: 1.828
2024-06-21 01:06:04,256 - INFO: Epoch: 106/200, Batch: 7/29, Batch_Loss_Train: 1.644
2024-06-21 01:06:04,577 - INFO: Epoch: 106/200, Batch: 8/29, Batch_Loss_Train: 1.261
2024-06-21 01:06:04,999 - INFO: Epoch: 106/200, Batch: 9/29, Batch_Loss_Train: 1.806
2024-06-21 01:06:05,301 - INFO: Epoch: 106/200, Batch: 10/29, Batch_Loss_Train: 1.875
2024-06-21 01:06:05,706 - INFO: Epoch: 106/200, Batch: 11/29, Batch_Loss_Train: 1.869
2024-06-21 01:06:06,029 - INFO: Epoch: 106/200, Batch: 12/29, Batch_Loss_Train: 1.693
2024-06-21 01:06:06,462 - INFO: Epoch: 106/200, Batch: 13/29, Batch_Loss_Train: 1.954
2024-06-21 01:06:06,772 - INFO: Epoch: 106/200, Batch: 14/29, Batch_Loss_Train: 2.200
2024-06-21 01:06:07,189 - INFO: Epoch: 106/200, Batch: 15/29, Batch_Loss_Train: 2.135
2024-06-21 01:06:07,506 - INFO: Epoch: 106/200, Batch: 16/29, Batch_Loss_Train: 2.044
2024-06-21 01:06:07,931 - INFO: Epoch: 106/200, Batch: 17/29, Batch_Loss_Train: 1.852
2024-06-21 01:06:08,235 - INFO: Epoch: 106/200, Batch: 18/29, Batch_Loss_Train: 2.972
2024-06-21 01:06:08,637 - INFO: Epoch: 106/200, Batch: 19/29, Batch_Loss_Train: 1.898
2024-06-21 01:06:08,951 - INFO: Epoch: 106/200, Batch: 20/29, Batch_Loss_Train: 2.314
2024-06-21 01:06:09,371 - INFO: Epoch: 106/200, Batch: 21/29, Batch_Loss_Train: 1.741
2024-06-21 01:06:09,680 - INFO: Epoch: 106/200, Batch: 22/29, Batch_Loss_Train: 1.958
2024-06-21 01:06:10,085 - INFO: Epoch: 106/200, Batch: 23/29, Batch_Loss_Train: 1.480
2024-06-21 01:06:10,406 - INFO: Epoch: 106/200, Batch: 24/29, Batch_Loss_Train: 1.230
2024-06-21 01:06:10,810 - INFO: Epoch: 106/200, Batch: 25/29, Batch_Loss_Train: 2.165
2024-06-21 01:06:11,110 - INFO: Epoch: 106/200, Batch: 26/29, Batch_Loss_Train: 2.098
2024-06-21 01:06:11,506 - INFO: Epoch: 106/200, Batch: 27/29, Batch_Loss_Train: 1.791
2024-06-21 01:06:11,822 - INFO: Epoch: 106/200, Batch: 28/29, Batch_Loss_Train: 1.520
2024-06-21 01:06:12,034 - INFO: Epoch: 106/200, Batch: 29/29, Batch_Loss_Train: 1.476
2024-06-21 01:06:23,093 - INFO: 106/200 final results:
2024-06-21 01:06:23,093 - INFO: Training loss: 1.876.
2024-06-21 01:06:23,093 - INFO: Training MAE: 1.064.
2024-06-21 01:06:23,093 - INFO: Training MSE: 1.884.
2024-06-21 01:06:43,813 - INFO: Epoch: 106/200, Loss_train: 1.8763214884133175, Loss_val: 2.3098076191441765
2024-06-21 01:06:43,813 - INFO: Best internal validation val_loss: 1.988 at epoch: 105.
2024-06-21 01:06:43,813 - INFO: Epoch 107/200...
2024-06-21 01:06:43,814 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:06:43,814 - INFO: Batch size: 32.
2024-06-21 01:06:43,817 - INFO: Dataset:
2024-06-21 01:06:43,818 - INFO: Batch size:
2024-06-21 01:06:43,818 - INFO: Number of workers:
2024-06-21 01:06:45,079 - INFO: Epoch: 107/200, Batch: 1/29, Batch_Loss_Train: 2.741
2024-06-21 01:06:45,392 - INFO: Epoch: 107/200, Batch: 2/29, Batch_Loss_Train: 2.238
2024-06-21 01:06:45,812 - INFO: Epoch: 107/200, Batch: 3/29, Batch_Loss_Train: 1.879
2024-06-21 01:06:46,139 - INFO: Epoch: 107/200, Batch: 4/29, Batch_Loss_Train: 1.810
2024-06-21 01:06:46,578 - INFO: Epoch: 107/200, Batch: 5/29, Batch_Loss_Train: 1.639
2024-06-21 01:06:46,884 - INFO: Epoch: 107/200, Batch: 6/29, Batch_Loss_Train: 1.917
2024-06-21 01:06:47,276 - INFO: Epoch: 107/200, Batch: 7/29, Batch_Loss_Train: 2.252
2024-06-21 01:06:47,596 - INFO: Epoch: 107/200, Batch: 8/29, Batch_Loss_Train: 2.144
2024-06-21 01:06:48,025 - INFO: Epoch: 107/200, Batch: 9/29, Batch_Loss_Train: 2.286
2024-06-21 01:06:48,326 - INFO: Epoch: 107/200, Batch: 10/29, Batch_Loss_Train: 2.286
2024-06-21 01:06:48,722 - INFO: Epoch: 107/200, Batch: 11/29, Batch_Loss_Train: 1.980
2024-06-21 01:06:49,047 - INFO: Epoch: 107/200, Batch: 12/29, Batch_Loss_Train: 2.214
2024-06-21 01:06:49,493 - INFO: Epoch: 107/200, Batch: 13/29, Batch_Loss_Train: 2.262
2024-06-21 01:06:49,802 - INFO: Epoch: 107/200, Batch: 14/29, Batch_Loss_Train: 1.991
2024-06-21 01:06:50,210 - INFO: Epoch: 107/200, Batch: 15/29, Batch_Loss_Train: 1.187
2024-06-21 01:06:50,528 - INFO: Epoch: 107/200, Batch: 16/29, Batch_Loss_Train: 1.486
2024-06-21 01:06:50,962 - INFO: Epoch: 107/200, Batch: 17/29, Batch_Loss_Train: 1.949
2024-06-21 01:06:51,267 - INFO: Epoch: 107/200, Batch: 18/29, Batch_Loss_Train: 1.632
2024-06-21 01:06:51,656 - INFO: Epoch: 107/200, Batch: 19/29, Batch_Loss_Train: 1.382
2024-06-21 01:06:51,972 - INFO: Epoch: 107/200, Batch: 20/29, Batch_Loss_Train: 1.626
2024-06-21 01:06:52,406 - INFO: Epoch: 107/200, Batch: 21/29, Batch_Loss_Train: 1.756
2024-06-21 01:06:52,715 - INFO: Epoch: 107/200, Batch: 22/29, Batch_Loss_Train: 1.388
2024-06-21 01:06:53,103 - INFO: Epoch: 107/200, Batch: 23/29, Batch_Loss_Train: 2.351
2024-06-21 01:06:53,424 - INFO: Epoch: 107/200, Batch: 24/29, Batch_Loss_Train: 2.612
2024-06-21 01:06:53,843 - INFO: Epoch: 107/200, Batch: 25/29, Batch_Loss_Train: 1.882
2024-06-21 01:06:54,146 - INFO: Epoch: 107/200, Batch: 26/29, Batch_Loss_Train: 2.190
2024-06-21 01:06:54,530 - INFO: Epoch: 107/200, Batch: 27/29, Batch_Loss_Train: 2.391
2024-06-21 01:06:54,853 - INFO: Epoch: 107/200, Batch: 28/29, Batch_Loss_Train: 1.718
2024-06-21 01:06:55,084 - INFO: Epoch: 107/200, Batch: 29/29, Batch_Loss_Train: 2.207
2024-06-21 01:07:06,002 - INFO: 107/200 final results:
2024-06-21 01:07:06,002 - INFO: Training loss: 1.979.
2024-06-21 01:07:06,002 - INFO: Training MAE: 1.090.
2024-06-21 01:07:06,002 - INFO: Training MSE: 1.975.
2024-06-21 01:07:26,084 - INFO: Epoch: 107/200, Loss_train: 1.9792203122171863, Loss_val: 2.077275968831161
2024-06-21 01:07:26,084 - INFO: Best internal validation val_loss: 1.988 at epoch: 105.
2024-06-21 01:07:26,084 - INFO: Epoch 108/200...
2024-06-21 01:07:26,084 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:07:26,084 - INFO: Batch size: 32.
2024-06-21 01:07:26,088 - INFO: Dataset:
2024-06-21 01:07:26,088 - INFO: Batch size:
2024-06-21 01:07:26,088 - INFO: Number of workers:
2024-06-21 01:07:27,332 - INFO: Epoch: 108/200, Batch: 1/29, Batch_Loss_Train: 1.489
2024-06-21 01:07:27,644 - INFO: Epoch: 108/200, Batch: 2/29, Batch_Loss_Train: 2.239
2024-06-21 01:07:28,063 - INFO: Epoch: 108/200, Batch: 3/29, Batch_Loss_Train: 2.363
2024-06-21 01:07:28,387 - INFO: Epoch: 108/200, Batch: 4/29, Batch_Loss_Train: 2.258
2024-06-21 01:07:28,806 - INFO: Epoch: 108/200, Batch: 5/29, Batch_Loss_Train: 1.598
2024-06-21 01:07:29,122 - INFO: Epoch: 108/200, Batch: 6/29, Batch_Loss_Train: 2.385
2024-06-21 01:07:29,514 - INFO: Epoch: 108/200, Batch: 7/29, Batch_Loss_Train: 2.028
2024-06-21 01:07:29,831 - INFO: Epoch: 108/200, Batch: 8/29, Batch_Loss_Train: 1.406
2024-06-21 01:07:30,264 - INFO: Epoch: 108/200, Batch: 9/29, Batch_Loss_Train: 1.896
2024-06-21 01:07:30,575 - INFO: Epoch: 108/200, Batch: 10/29, Batch_Loss_Train: 2.182
2024-06-21 01:07:30,971 - INFO: Epoch: 108/200, Batch: 11/29, Batch_Loss_Train: 2.030
2024-06-21 01:07:31,290 - INFO: Epoch: 108/200, Batch: 12/29, Batch_Loss_Train: 1.307
2024-06-21 01:07:31,730 - INFO: Epoch: 108/200, Batch: 13/29, Batch_Loss_Train: 1.664
2024-06-21 01:07:32,037 - INFO: Epoch: 108/200, Batch: 14/29, Batch_Loss_Train: 1.717
2024-06-21 01:07:32,441 - INFO: Epoch: 108/200, Batch: 15/29, Batch_Loss_Train: 1.678
2024-06-21 01:07:32,756 - INFO: Epoch: 108/200, Batch: 16/29, Batch_Loss_Train: 1.411
2024-06-21 01:07:33,187 - INFO: Epoch: 108/200, Batch: 17/29, Batch_Loss_Train: 1.485
2024-06-21 01:07:33,489 - INFO: Epoch: 108/200, Batch: 18/29, Batch_Loss_Train: 1.450
2024-06-21 01:07:33,879 - INFO: Epoch: 108/200, Batch: 19/29, Batch_Loss_Train: 1.892
2024-06-21 01:07:34,190 - INFO: Epoch: 108/200, Batch: 20/29, Batch_Loss_Train: 2.106
2024-06-21 01:07:34,617 - INFO: Epoch: 108/200, Batch: 21/29, Batch_Loss_Train: 2.029
2024-06-21 01:07:34,923 - INFO: Epoch: 108/200, Batch: 22/29, Batch_Loss_Train: 1.537
2024-06-21 01:07:35,320 - INFO: Epoch: 108/200, Batch: 23/29, Batch_Loss_Train: 1.822
2024-06-21 01:07:35,638 - INFO: Epoch: 108/200, Batch: 24/29, Batch_Loss_Train: 1.814
2024-06-21 01:07:36,064 - INFO: Epoch: 108/200, Batch: 25/29, Batch_Loss_Train: 1.699
2024-06-21 01:07:36,364 - INFO: Epoch: 108/200, Batch: 26/29, Batch_Loss_Train: 1.827
2024-06-21 01:07:36,750 - INFO: Epoch: 108/200, Batch: 27/29, Batch_Loss_Train: 2.009
2024-06-21 01:07:37,064 - INFO: Epoch: 108/200, Batch: 28/29, Batch_Loss_Train: 2.175
2024-06-21 01:07:37,286 - INFO: Epoch: 108/200, Batch: 29/29, Batch_Loss_Train: 1.025
2024-06-21 01:07:48,200 - INFO: 108/200 final results:
2024-06-21 01:07:48,200 - INFO: Training loss: 1.811.
2024-06-21 01:07:48,200 - INFO: Training MAE: 1.044.
2024-06-21 01:07:48,200 - INFO: Training MSE: 1.827.
2024-06-21 01:08:09,071 - INFO: Epoch: 108/200, Loss_train: 1.810998587772764, Loss_val: 1.8301993978434596
2024-06-21 01:08:09,120 - INFO: Saved new best metric model for epoch 108.
2024-06-21 01:08:09,120 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:08:09,120 - INFO: Epoch 109/200...
2024-06-21 01:08:09,120 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:08:09,120 - INFO: Batch size: 32.
2024-06-21 01:08:09,124 - INFO: Dataset:
2024-06-21 01:08:09,125 - INFO: Batch size:
2024-06-21 01:08:09,125 - INFO: Number of workers:
2024-06-21 01:08:10,396 - INFO: Epoch: 109/200, Batch: 1/29, Batch_Loss_Train: 1.987
2024-06-21 01:08:10,723 - INFO: Epoch: 109/200, Batch: 2/29, Batch_Loss_Train: 1.807
2024-06-21 01:08:11,129 - INFO: Epoch: 109/200, Batch: 3/29, Batch_Loss_Train: 2.010
2024-06-21 01:08:11,453 - INFO: Epoch: 109/200, Batch: 4/29, Batch_Loss_Train: 1.526
2024-06-21 01:08:11,879 - INFO: Epoch: 109/200, Batch: 5/29, Batch_Loss_Train: 1.911
2024-06-21 01:08:12,186 - INFO: Epoch: 109/200, Batch: 6/29, Batch_Loss_Train: 1.718
2024-06-21 01:08:12,594 - INFO: Epoch: 109/200, Batch: 7/29, Batch_Loss_Train: 1.744
2024-06-21 01:08:12,915 - INFO: Epoch: 109/200, Batch: 8/29, Batch_Loss_Train: 1.851
2024-06-21 01:08:13,335 - INFO: Epoch: 109/200, Batch: 9/29, Batch_Loss_Train: 1.337
2024-06-21 01:08:13,637 - INFO: Epoch: 109/200, Batch: 10/29, Batch_Loss_Train: 2.292
2024-06-21 01:08:14,048 - INFO: Epoch: 109/200, Batch: 11/29, Batch_Loss_Train: 2.871
2024-06-21 01:08:14,371 - INFO: Epoch: 109/200, Batch: 12/29, Batch_Loss_Train: 1.719
2024-06-21 01:08:14,815 - INFO: Epoch: 109/200, Batch: 13/29, Batch_Loss_Train: 2.112
2024-06-21 01:08:15,125 - INFO: Epoch: 109/200, Batch: 14/29, Batch_Loss_Train: 1.640
2024-06-21 01:08:15,532 - INFO: Epoch: 109/200, Batch: 15/29, Batch_Loss_Train: 2.439
2024-06-21 01:08:15,851 - INFO: Epoch: 109/200, Batch: 16/29, Batch_Loss_Train: 2.426
2024-06-21 01:08:16,288 - INFO: Epoch: 109/200, Batch: 17/29, Batch_Loss_Train: 1.840
2024-06-21 01:08:16,594 - INFO: Epoch: 109/200, Batch: 18/29, Batch_Loss_Train: 1.384
2024-06-21 01:08:16,989 - INFO: Epoch: 109/200, Batch: 19/29, Batch_Loss_Train: 2.048
2024-06-21 01:08:17,304 - INFO: Epoch: 109/200, Batch: 20/29, Batch_Loss_Train: 2.628
2024-06-21 01:08:17,739 - INFO: Epoch: 109/200, Batch: 21/29, Batch_Loss_Train: 1.630
2024-06-21 01:08:18,048 - INFO: Epoch: 109/200, Batch: 22/29, Batch_Loss_Train: 2.050
2024-06-21 01:08:18,445 - INFO: Epoch: 109/200, Batch: 23/29, Batch_Loss_Train: 1.734
2024-06-21 01:08:18,767 - INFO: Epoch: 109/200, Batch: 24/29, Batch_Loss_Train: 1.563
2024-06-21 01:08:19,193 - INFO: Epoch: 109/200, Batch: 25/29, Batch_Loss_Train: 1.698
2024-06-21 01:08:19,495 - INFO: Epoch: 109/200, Batch: 26/29, Batch_Loss_Train: 1.454
2024-06-21 01:08:19,884 - INFO: Epoch: 109/200, Batch: 27/29, Batch_Loss_Train: 2.303
2024-06-21 01:08:20,199 - INFO: Epoch: 109/200, Batch: 28/29, Batch_Loss_Train: 1.978
2024-06-21 01:08:20,421 - INFO: Epoch: 109/200, Batch: 29/29, Batch_Loss_Train: 2.660
2024-06-21 01:08:31,421 - INFO: 109/200 final results:
2024-06-21 01:08:31,421 - INFO: Training loss: 1.943.
2024-06-21 01:08:31,421 - INFO: Training MAE: 1.070.
2024-06-21 01:08:31,421 - INFO: Training MSE: 1.929.
2024-06-21 01:08:51,930 - INFO: Epoch: 109/200, Loss_train: 1.9434536695480347, Loss_val: 2.69306612425837
2024-06-21 01:08:51,930 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:08:51,930 - INFO: Epoch 110/200...
2024-06-21 01:08:51,930 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:08:51,930 - INFO: Batch size: 32.
2024-06-21 01:08:51,934 - INFO: Dataset:
2024-06-21 01:08:51,935 - INFO: Batch size:
2024-06-21 01:08:51,935 - INFO: Number of workers:
2024-06-21 01:08:53,207 - INFO: Epoch: 110/200, Batch: 1/29, Batch_Loss_Train: 2.082
2024-06-21 01:08:53,522 - INFO: Epoch: 110/200, Batch: 2/29, Batch_Loss_Train: 1.572
2024-06-21 01:08:53,934 - INFO: Epoch: 110/200, Batch: 3/29, Batch_Loss_Train: 1.742
2024-06-21 01:08:54,260 - INFO: Epoch: 110/200, Batch: 4/29, Batch_Loss_Train: 1.925
2024-06-21 01:08:54,684 - INFO: Epoch: 110/200, Batch: 5/29, Batch_Loss_Train: 1.333
2024-06-21 01:08:54,992 - INFO: Epoch: 110/200, Batch: 6/29, Batch_Loss_Train: 1.515
2024-06-21 01:08:55,409 - INFO: Epoch: 110/200, Batch: 7/29, Batch_Loss_Train: 1.444
2024-06-21 01:08:55,715 - INFO: Epoch: 110/200, Batch: 8/29, Batch_Loss_Train: 1.755
2024-06-21 01:08:56,129 - INFO: Epoch: 110/200, Batch: 9/29, Batch_Loss_Train: 2.232
2024-06-21 01:08:56,429 - INFO: Epoch: 110/200, Batch: 10/29, Batch_Loss_Train: 1.781
2024-06-21 01:08:56,854 - INFO: Epoch: 110/200, Batch: 11/29, Batch_Loss_Train: 1.433
2024-06-21 01:08:57,163 - INFO: Epoch: 110/200, Batch: 12/29, Batch_Loss_Train: 2.081
2024-06-21 01:08:57,595 - INFO: Epoch: 110/200, Batch: 13/29, Batch_Loss_Train: 1.523
2024-06-21 01:08:57,904 - INFO: Epoch: 110/200, Batch: 14/29, Batch_Loss_Train: 2.173
2024-06-21 01:08:58,345 - INFO: Epoch: 110/200, Batch: 15/29, Batch_Loss_Train: 1.592
2024-06-21 01:08:58,650 - INFO: Epoch: 110/200, Batch: 16/29, Batch_Loss_Train: 1.859
2024-06-21 01:08:59,060 - INFO: Epoch: 110/200, Batch: 17/29, Batch_Loss_Train: 1.086
2024-06-21 01:08:59,366 - INFO: Epoch: 110/200, Batch: 18/29, Batch_Loss_Train: 1.616
2024-06-21 01:08:59,793 - INFO: Epoch: 110/200, Batch: 19/29, Batch_Loss_Train: 2.120
2024-06-21 01:09:00,093 - INFO: Epoch: 110/200, Batch: 20/29, Batch_Loss_Train: 1.232
2024-06-21 01:09:00,502 - INFO: Epoch: 110/200, Batch: 21/29, Batch_Loss_Train: 2.275
2024-06-21 01:09:00,810 - INFO: Epoch: 110/200, Batch: 22/29, Batch_Loss_Train: 2.615
2024-06-21 01:09:01,232 - INFO: Epoch: 110/200, Batch: 23/29, Batch_Loss_Train: 1.788
2024-06-21 01:09:01,539 - INFO: Epoch: 110/200, Batch: 24/29, Batch_Loss_Train: 2.055
2024-06-21 01:09:01,941 - INFO: Epoch: 110/200, Batch: 25/29, Batch_Loss_Train: 2.402
2024-06-21 01:09:02,244 - INFO: Epoch: 110/200, Batch: 26/29, Batch_Loss_Train: 2.014
2024-06-21 01:09:02,656 - INFO: Epoch: 110/200, Batch: 27/29, Batch_Loss_Train: 1.910
2024-06-21 01:09:02,958 - INFO: Epoch: 110/200, Batch: 28/29, Batch_Loss_Train: 2.004
2024-06-21 01:09:03,171 - INFO: Epoch: 110/200, Batch: 29/29, Batch_Loss_Train: 2.171
2024-06-21 01:09:14,140 - INFO: 110/200 final results:
2024-06-21 01:09:14,140 - INFO: Training loss: 1.839.
2024-06-21 01:09:14,140 - INFO: Training MAE: 1.046.
2024-06-21 01:09:14,140 - INFO: Training MSE: 1.832.
2024-06-21 01:09:34,549 - INFO: Epoch: 110/200, Loss_train: 1.8390225172042847, Loss_val: 1.9183867142118256
2024-06-21 01:09:34,549 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:09:34,549 - INFO: Epoch 111/200...
2024-06-21 01:09:34,549 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:09:34,550 - INFO: Batch size: 32.
2024-06-21 01:09:34,554 - INFO: Dataset:
2024-06-21 01:09:34,554 - INFO: Batch size:
2024-06-21 01:09:34,554 - INFO: Number of workers:
2024-06-21 01:09:35,799 - INFO: Epoch: 111/200, Batch: 1/29, Batch_Loss_Train: 1.480
2024-06-21 01:09:36,110 - INFO: Epoch: 111/200, Batch: 2/29, Batch_Loss_Train: 1.632
2024-06-21 01:09:36,527 - INFO: Epoch: 111/200, Batch: 3/29, Batch_Loss_Train: 2.119
2024-06-21 01:09:36,853 - INFO: Epoch: 111/200, Batch: 4/29, Batch_Loss_Train: 1.817
2024-06-21 01:09:37,281 - INFO: Epoch: 111/200, Batch: 5/29, Batch_Loss_Train: 1.779
2024-06-21 01:09:37,587 - INFO: Epoch: 111/200, Batch: 6/29, Batch_Loss_Train: 2.277
2024-06-21 01:09:37,971 - INFO: Epoch: 111/200, Batch: 7/29, Batch_Loss_Train: 1.940
2024-06-21 01:09:38,292 - INFO: Epoch: 111/200, Batch: 8/29, Batch_Loss_Train: 1.332
2024-06-21 01:09:38,729 - INFO: Epoch: 111/200, Batch: 9/29, Batch_Loss_Train: 1.536
2024-06-21 01:09:39,029 - INFO: Epoch: 111/200, Batch: 10/29, Batch_Loss_Train: 2.262
2024-06-21 01:09:39,423 - INFO: Epoch: 111/200, Batch: 11/29, Batch_Loss_Train: 1.807
2024-06-21 01:09:39,743 - INFO: Epoch: 111/200, Batch: 12/29, Batch_Loss_Train: 1.434
2024-06-21 01:09:40,175 - INFO: Epoch: 111/200, Batch: 13/29, Batch_Loss_Train: 1.607
2024-06-21 01:09:40,481 - INFO: Epoch: 111/200, Batch: 14/29, Batch_Loss_Train: 1.162
2024-06-21 01:09:40,883 - INFO: Epoch: 111/200, Batch: 15/29, Batch_Loss_Train: 2.078
2024-06-21 01:09:41,199 - INFO: Epoch: 111/200, Batch: 16/29, Batch_Loss_Train: 1.689
2024-06-21 01:09:41,635 - INFO: Epoch: 111/200, Batch: 17/29, Batch_Loss_Train: 1.975
2024-06-21 01:09:41,940 - INFO: Epoch: 111/200, Batch: 18/29, Batch_Loss_Train: 1.758
2024-06-21 01:09:42,334 - INFO: Epoch: 111/200, Batch: 19/29, Batch_Loss_Train: 2.545
2024-06-21 01:09:42,649 - INFO: Epoch: 111/200, Batch: 20/29, Batch_Loss_Train: 2.462
2024-06-21 01:09:43,071 - INFO: Epoch: 111/200, Batch: 21/29, Batch_Loss_Train: 2.318
2024-06-21 01:09:43,379 - INFO: Epoch: 111/200, Batch: 22/29, Batch_Loss_Train: 1.794
2024-06-21 01:09:43,775 - INFO: Epoch: 111/200, Batch: 23/29, Batch_Loss_Train: 2.464
2024-06-21 01:09:44,097 - INFO: Epoch: 111/200, Batch: 24/29, Batch_Loss_Train: 1.559
2024-06-21 01:09:44,525 - INFO: Epoch: 111/200, Batch: 25/29, Batch_Loss_Train: 1.339
2024-06-21 01:09:44,825 - INFO: Epoch: 111/200, Batch: 26/29, Batch_Loss_Train: 1.532
2024-06-21 01:09:45,208 - INFO: Epoch: 111/200, Batch: 27/29, Batch_Loss_Train: 2.602
2024-06-21 01:09:45,521 - INFO: Epoch: 111/200, Batch: 28/29, Batch_Loss_Train: 2.751
2024-06-21 01:09:45,741 - INFO: Epoch: 111/200, Batch: 29/29, Batch_Loss_Train: 2.070
2024-06-21 01:09:56,715 - INFO: 111/200 final results:
2024-06-21 01:09:56,716 - INFO: Training loss: 1.901.
2024-06-21 01:09:56,716 - INFO: Training MAE: 1.055.
2024-06-21 01:09:56,716 - INFO: Training MSE: 1.897.
2024-06-21 01:10:17,120 - INFO: Epoch: 111/200, Loss_train: 1.9006045029081147, Loss_val: 2.0136734576060853
2024-06-21 01:10:17,120 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:10:17,120 - INFO: Epoch 112/200...
2024-06-21 01:10:17,120 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:10:17,120 - INFO: Batch size: 32.
2024-06-21 01:10:17,124 - INFO: Dataset:
2024-06-21 01:10:17,124 - INFO: Batch size:
2024-06-21 01:10:17,124 - INFO: Number of workers:
2024-06-21 01:10:18,389 - INFO: Epoch: 112/200, Batch: 1/29, Batch_Loss_Train: 2.255
2024-06-21 01:10:18,701 - INFO: Epoch: 112/200, Batch: 2/29, Batch_Loss_Train: 2.538
2024-06-21 01:10:19,105 - INFO: Epoch: 112/200, Batch: 3/29, Batch_Loss_Train: 2.509
2024-06-21 01:10:19,428 - INFO: Epoch: 112/200, Batch: 4/29, Batch_Loss_Train: 1.910
2024-06-21 01:10:19,873 - INFO: Epoch: 112/200, Batch: 5/29, Batch_Loss_Train: 1.477
2024-06-21 01:10:20,180 - INFO: Epoch: 112/200, Batch: 6/29, Batch_Loss_Train: 1.711
2024-06-21 01:10:20,572 - INFO: Epoch: 112/200, Batch: 7/29, Batch_Loss_Train: 1.830
2024-06-21 01:10:20,879 - INFO: Epoch: 112/200, Batch: 8/29, Batch_Loss_Train: 1.590
2024-06-21 01:10:21,622 - INFO: Epoch: 112/200, Batch: 9/29, Batch_Loss_Train: 2.237
2024-06-21 01:10:21,923 - INFO: Epoch: 112/200, Batch: 10/29, Batch_Loss_Train: 2.146
2024-06-21 01:10:22,313 - INFO: Epoch: 112/200, Batch: 11/29, Batch_Loss_Train: 1.454
2024-06-21 01:10:22,623 - INFO: Epoch: 112/200, Batch: 12/29, Batch_Loss_Train: 1.263
2024-06-21 01:10:23,080 - INFO: Epoch: 112/200, Batch: 13/29, Batch_Loss_Train: 1.309
2024-06-21 01:10:23,389 - INFO: Epoch: 112/200, Batch: 14/29, Batch_Loss_Train: 1.560
2024-06-21 01:10:23,791 - INFO: Epoch: 112/200, Batch: 15/29, Batch_Loss_Train: 2.097
2024-06-21 01:10:24,093 - INFO: Epoch: 112/200, Batch: 16/29, Batch_Loss_Train: 2.005
2024-06-21 01:10:24,536 - INFO: Epoch: 112/200, Batch: 17/29, Batch_Loss_Train: 1.545
2024-06-21 01:10:24,842 - INFO: Epoch: 112/200, Batch: 18/29, Batch_Loss_Train: 1.466
2024-06-21 01:10:25,230 - INFO: Epoch: 112/200, Batch: 19/29, Batch_Loss_Train: 1.764
2024-06-21 01:10:25,533 - INFO: Epoch: 112/200, Batch: 20/29, Batch_Loss_Train: 2.010
2024-06-21 01:10:25,971 - INFO: Epoch: 112/200, Batch: 21/29, Batch_Loss_Train: 1.832
2024-06-21 01:10:26,280 - INFO: Epoch: 112/200, Batch: 22/29, Batch_Loss_Train: 1.938
2024-06-21 01:10:26,681 - INFO: Epoch: 112/200, Batch: 23/29, Batch_Loss_Train: 1.651
2024-06-21 01:10:26,991 - INFO: Epoch: 112/200, Batch: 24/29, Batch_Loss_Train: 2.261
2024-06-21 01:10:27,433 - INFO: Epoch: 112/200, Batch: 25/29, Batch_Loss_Train: 2.562
2024-06-21 01:10:27,737 - INFO: Epoch: 112/200, Batch: 26/29, Batch_Loss_Train: 1.638
2024-06-21 01:10:28,121 - INFO: Epoch: 112/200, Batch: 27/29, Batch_Loss_Train: 2.155
2024-06-21 01:10:28,426 - INFO: Epoch: 112/200, Batch: 28/29, Batch_Loss_Train: 1.664
2024-06-21 01:10:28,649 - INFO: Epoch: 112/200, Batch: 29/29, Batch_Loss_Train: 1.005
2024-06-21 01:10:39,392 - INFO: 112/200 final results:
2024-06-21 01:10:39,392 - INFO: Training loss: 1.841.
2024-06-21 01:10:39,392 - INFO: Training MAE: 1.043.
2024-06-21 01:10:39,392 - INFO: Training MSE: 1.857.
2024-06-21 01:11:00,056 - INFO: Epoch: 112/200, Loss_train: 1.8407125103062596, Loss_val: 2.0485895995436043
2024-06-21 01:11:00,056 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:11:00,056 - INFO: Epoch 113/200...
2024-06-21 01:11:00,056 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:11:00,056 - INFO: Batch size: 32.
2024-06-21 01:11:00,060 - INFO: Dataset:
2024-06-21 01:11:00,061 - INFO: Batch size:
2024-06-21 01:11:00,061 - INFO: Number of workers:
2024-06-21 01:11:01,314 - INFO: Epoch: 113/200, Batch: 1/29, Batch_Loss_Train: 1.415
2024-06-21 01:11:01,663 - INFO: Epoch: 113/200, Batch: 2/29, Batch_Loss_Train: 1.747
2024-06-21 01:11:02,068 - INFO: Epoch: 113/200, Batch: 3/29, Batch_Loss_Train: 1.892
2024-06-21 01:11:02,396 - INFO: Epoch: 113/200, Batch: 4/29, Batch_Loss_Train: 1.999
2024-06-21 01:11:02,814 - INFO: Epoch: 113/200, Batch: 5/29, Batch_Loss_Train: 1.820
2024-06-21 01:11:03,146 - INFO: Epoch: 113/200, Batch: 6/29, Batch_Loss_Train: 2.200
2024-06-21 01:11:03,540 - INFO: Epoch: 113/200, Batch: 7/29, Batch_Loss_Train: 1.934
2024-06-21 01:11:03,849 - INFO: Epoch: 113/200, Batch: 8/29, Batch_Loss_Train: 1.778
2024-06-21 01:11:04,261 - INFO: Epoch: 113/200, Batch: 9/29, Batch_Loss_Train: 1.934
2024-06-21 01:11:04,588 - INFO: Epoch: 113/200, Batch: 10/29, Batch_Loss_Train: 1.207
2024-06-21 01:11:04,982 - INFO: Epoch: 113/200, Batch: 11/29, Batch_Loss_Train: 1.994
2024-06-21 01:11:05,292 - INFO: Epoch: 113/200, Batch: 12/29, Batch_Loss_Train: 1.579
2024-06-21 01:11:05,714 - INFO: Epoch: 113/200, Batch: 13/29, Batch_Loss_Train: 1.985
2024-06-21 01:11:06,046 - INFO: Epoch: 113/200, Batch: 14/29, Batch_Loss_Train: 2.215
2024-06-21 01:11:06,449 - INFO: Epoch: 113/200, Batch: 15/29, Batch_Loss_Train: 2.063
2024-06-21 01:11:06,753 - INFO: Epoch: 113/200, Batch: 16/29, Batch_Loss_Train: 1.562
2024-06-21 01:11:07,165 - INFO: Epoch: 113/200, Batch: 17/29, Batch_Loss_Train: 1.393
2024-06-21 01:11:07,493 - INFO: Epoch: 113/200, Batch: 18/29, Batch_Loss_Train: 1.639
2024-06-21 01:11:07,881 - INFO: Epoch: 113/200, Batch: 19/29, Batch_Loss_Train: 2.009
2024-06-21 01:11:08,181 - INFO: Epoch: 113/200, Batch: 20/29, Batch_Loss_Train: 1.438
2024-06-21 01:11:08,581 - INFO: Epoch: 113/200, Batch: 21/29, Batch_Loss_Train: 1.552
2024-06-21 01:11:08,913 - INFO: Epoch: 113/200, Batch: 22/29, Batch_Loss_Train: 1.440
2024-06-21 01:11:09,300 - INFO: Epoch: 113/200, Batch: 23/29, Batch_Loss_Train: 2.159
2024-06-21 01:11:09,605 - INFO: Epoch: 113/200, Batch: 24/29, Batch_Loss_Train: 1.901
2024-06-21 01:11:10,013 - INFO: Epoch: 113/200, Batch: 25/29, Batch_Loss_Train: 2.416
2024-06-21 01:11:10,340 - INFO: Epoch: 113/200, Batch: 26/29, Batch_Loss_Train: 1.575
2024-06-21 01:11:10,722 - INFO: Epoch: 113/200, Batch: 27/29, Batch_Loss_Train: 1.914
2024-06-21 01:11:11,025 - INFO: Epoch: 113/200, Batch: 28/29, Batch_Loss_Train: 1.853
2024-06-21 01:11:11,248 - INFO: Epoch: 113/200, Batch: 29/29, Batch_Loss_Train: 1.628
2024-06-21 01:11:22,394 - INFO: 113/200 final results:
2024-06-21 01:11:22,394 - INFO: Training loss: 1.801.
2024-06-21 01:11:22,394 - INFO: Training MAE: 1.035.
2024-06-21 01:11:22,394 - INFO: Training MSE: 1.805.
2024-06-21 01:11:42,861 - INFO: Epoch: 113/200, Loss_train: 1.8013715661805252, Loss_val: 2.0436274902573945
2024-06-21 01:11:42,861 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:11:42,861 - INFO: Epoch 114/200...
2024-06-21 01:11:42,861 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:11:42,861 - INFO: Batch size: 32.
2024-06-21 01:11:42,865 - INFO: Dataset:
2024-06-21 01:11:42,865 - INFO: Batch size:
2024-06-21 01:11:42,865 - INFO: Number of workers:
2024-06-21 01:11:44,114 - INFO: Epoch: 114/200, Batch: 1/29, Batch_Loss_Train: 1.763
2024-06-21 01:11:44,425 - INFO: Epoch: 114/200, Batch: 2/29, Batch_Loss_Train: 1.341
2024-06-21 01:11:44,834 - INFO: Epoch: 114/200, Batch: 3/29, Batch_Loss_Train: 1.556
2024-06-21 01:11:45,157 - INFO: Epoch: 114/200, Batch: 4/29, Batch_Loss_Train: 1.534
2024-06-21 01:11:45,572 - INFO: Epoch: 114/200, Batch: 5/29, Batch_Loss_Train: 2.078
2024-06-21 01:11:45,876 - INFO: Epoch: 114/200, Batch: 6/29, Batch_Loss_Train: 1.553
2024-06-21 01:11:46,270 - INFO: Epoch: 114/200, Batch: 7/29, Batch_Loss_Train: 2.242
2024-06-21 01:11:46,586 - INFO: Epoch: 114/200, Batch: 8/29, Batch_Loss_Train: 2.658
2024-06-21 01:11:47,000 - INFO: Epoch: 114/200, Batch: 9/29, Batch_Loss_Train: 1.464
2024-06-21 01:11:47,298 - INFO: Epoch: 114/200, Batch: 10/29, Batch_Loss_Train: 1.381
2024-06-21 01:11:47,692 - INFO: Epoch: 114/200, Batch: 11/29, Batch_Loss_Train: 1.911
2024-06-21 01:11:48,010 - INFO: Epoch: 114/200, Batch: 12/29, Batch_Loss_Train: 1.501
2024-06-21 01:11:48,437 - INFO: Epoch: 114/200, Batch: 13/29, Batch_Loss_Train: 1.940
2024-06-21 01:11:48,744 - INFO: Epoch: 114/200, Batch: 14/29, Batch_Loss_Train: 2.418
2024-06-21 01:11:49,149 - INFO: Epoch: 114/200, Batch: 15/29, Batch_Loss_Train: 2.266
2024-06-21 01:11:49,462 - INFO: Epoch: 114/200, Batch: 16/29, Batch_Loss_Train: 1.375
2024-06-21 01:11:49,880 - INFO: Epoch: 114/200, Batch: 17/29, Batch_Loss_Train: 2.044
2024-06-21 01:11:50,182 - INFO: Epoch: 114/200, Batch: 18/29, Batch_Loss_Train: 1.533
2024-06-21 01:11:50,579 - INFO: Epoch: 114/200, Batch: 19/29, Batch_Loss_Train: 2.836
2024-06-21 01:11:50,889 - INFO: Epoch: 114/200, Batch: 20/29, Batch_Loss_Train: 1.606
2024-06-21 01:11:51,300 - INFO: Epoch: 114/200, Batch: 21/29, Batch_Loss_Train: 1.620
2024-06-21 01:11:51,607 - INFO: Epoch: 114/200, Batch: 22/29, Batch_Loss_Train: 2.352
2024-06-21 01:11:52,018 - INFO: Epoch: 114/200, Batch: 23/29, Batch_Loss_Train: 1.349
2024-06-21 01:11:52,339 - INFO: Epoch: 114/200, Batch: 24/29, Batch_Loss_Train: 1.115
2024-06-21 01:11:52,757 - INFO: Epoch: 114/200, Batch: 25/29, Batch_Loss_Train: 1.978
2024-06-21 01:11:53,060 - INFO: Epoch: 114/200, Batch: 26/29, Batch_Loss_Train: 2.061
2024-06-21 01:11:53,455 - INFO: Epoch: 114/200, Batch: 27/29, Batch_Loss_Train: 3.199
2024-06-21 01:11:53,771 - INFO: Epoch: 114/200, Batch: 28/29, Batch_Loss_Train: 1.900
2024-06-21 01:11:53,989 - INFO: Epoch: 114/200, Batch: 29/29, Batch_Loss_Train: 1.964
2024-06-21 01:12:04,977 - INFO: 114/200 final results:
2024-06-21 01:12:04,977 - INFO: Training loss: 1.881.
2024-06-21 01:12:04,977 - INFO: Training MAE: 1.068.
2024-06-21 01:12:04,977 - INFO: Training MSE: 1.879.
2024-06-21 01:12:25,628 - INFO: Epoch: 114/200, Loss_train: 1.8806096192063957, Loss_val: 1.9432731640749965
2024-06-21 01:12:25,628 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:12:25,628 - INFO: Epoch 115/200...
2024-06-21 01:12:25,628 - INFO: Learning rate: 5.509748850262927e-05.
2024-06-21 01:12:25,628 - INFO: Batch size: 32.
2024-06-21 01:12:25,632 - INFO: Dataset:
2024-06-21 01:12:25,632 - INFO: Batch size:
2024-06-21 01:12:25,632 - INFO: Number of workers:
2024-06-21 01:12:26,898 - INFO: Epoch: 115/200, Batch: 1/29, Batch_Loss_Train: 1.973
2024-06-21 01:12:27,212 - INFO: Epoch: 115/200, Batch: 2/29, Batch_Loss_Train: 1.409
2024-06-21 01:12:27,617 - INFO: Epoch: 115/200, Batch: 3/29, Batch_Loss_Train: 1.642
2024-06-21 01:12:27,942 - INFO: Epoch: 115/200, Batch: 4/29, Batch_Loss_Train: 2.152
2024-06-21 01:12:28,373 - INFO: Epoch: 115/200, Batch: 5/29, Batch_Loss_Train: 1.772
2024-06-21 01:12:28,678 - INFO: Epoch: 115/200, Batch: 6/29, Batch_Loss_Train: 1.391
2024-06-21 01:12:29,066 - INFO: Epoch: 115/200, Batch: 7/29, Batch_Loss_Train: 1.193
2024-06-21 01:12:29,384 - INFO: Epoch: 115/200, Batch: 8/29, Batch_Loss_Train: 1.326
2024-06-21 01:12:29,815 - INFO: Epoch: 115/200, Batch: 9/29, Batch_Loss_Train: 1.106
2024-06-21 01:12:30,116 - INFO: Epoch: 115/200, Batch: 10/29, Batch_Loss_Train: 1.857
2024-06-21 01:12:30,508 - INFO: Epoch: 115/200, Batch: 11/29, Batch_Loss_Train: 1.690
2024-06-21 01:12:30,831 - INFO: Epoch: 115/200, Batch: 12/29, Batch_Loss_Train: 1.363
2024-06-21 01:12:31,271 - INFO: Epoch: 115/200, Batch: 13/29, Batch_Loss_Train: 1.503
2024-06-21 01:12:31,581 - INFO: Epoch: 115/200, Batch: 14/29, Batch_Loss_Train: 1.383
2024-06-21 01:12:31,982 - INFO: Epoch: 115/200, Batch: 15/29, Batch_Loss_Train: 1.400
2024-06-21 01:12:32,301 - INFO: Epoch: 115/200, Batch: 16/29, Batch_Loss_Train: 1.607
2024-06-21 01:12:32,731 - INFO: Epoch: 115/200, Batch: 17/29, Batch_Loss_Train: 2.136
2024-06-21 01:12:33,036 - INFO: Epoch: 115/200, Batch: 18/29, Batch_Loss_Train: 2.030
2024-06-21 01:12:33,424 - INFO: Epoch: 115/200, Batch: 19/29, Batch_Loss_Train: 1.855
2024-06-21 01:12:33,739 - INFO: Epoch: 115/200, Batch: 20/29, Batch_Loss_Train: 1.788
2024-06-21 01:12:34,162 - INFO: Epoch: 115/200, Batch: 21/29, Batch_Loss_Train: 2.114
2024-06-21 01:12:34,468 - INFO: Epoch: 115/200, Batch: 22/29, Batch_Loss_Train: 2.157
2024-06-21 01:12:34,855 - INFO: Epoch: 115/200, Batch: 23/29, Batch_Loss_Train: 2.220
2024-06-21 01:12:35,173 - INFO: Epoch: 115/200, Batch: 24/29, Batch_Loss_Train: 2.123
2024-06-21 01:12:35,592 - INFO: Epoch: 115/200, Batch: 25/29, Batch_Loss_Train: 1.576
2024-06-21 01:12:35,893 - INFO: Epoch: 115/200, Batch: 26/29, Batch_Loss_Train: 1.806
2024-06-21 01:12:36,264 - INFO: Epoch: 115/200, Batch: 27/29, Batch_Loss_Train: 1.793
2024-06-21 01:12:36,577 - INFO: Epoch: 115/200, Batch: 28/29, Batch_Loss_Train: 2.411
2024-06-21 01:12:36,788 - INFO: Epoch: 115/200, Batch: 29/29, Batch_Loss_Train: 2.683
2024-06-21 01:12:47,731 - INFO: 115/200 final results:
2024-06-21 01:12:47,731 - INFO: Training loss: 1.774.
2024-06-21 01:12:47,731 - INFO: Training MAE: 1.032.
2024-06-21 01:12:47,731 - INFO: Training MSE: 1.756.
2024-06-21 01:13:08,067 - INFO: Epoch: 115/200, Loss_train: 1.7744408105981762, Loss_val: 2.1855940346060128
2024-06-21 01:13:08,067 - INFO: Best internal validation val_loss: 1.830 at epoch: 108.
2024-06-21 01:13:08,067 - INFO: Epoch 116/200...
2024-06-21 01:13:08,067 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:13:08,067 - INFO: Batch size: 32.
2024-06-21 01:13:08,071 - INFO: Dataset:
2024-06-21 01:13:08,071 - INFO: Batch size:
2024-06-21 01:13:08,071 - INFO: Number of workers:
2024-06-21 01:13:09,314 - INFO: Epoch: 116/200, Batch: 1/29, Batch_Loss_Train: 1.763
2024-06-21 01:13:09,639 - INFO: Epoch: 116/200, Batch: 2/29, Batch_Loss_Train: 1.303
2024-06-21 01:13:10,037 - INFO: Epoch: 116/200, Batch: 3/29, Batch_Loss_Train: 1.773
2024-06-21 01:13:10,361 - INFO: Epoch: 116/200, Batch: 4/29, Batch_Loss_Train: 1.707
2024-06-21 01:13:10,784 - INFO: Epoch: 116/200, Batch: 5/29, Batch_Loss_Train: 1.862
2024-06-21 01:13:11,101 - INFO: Epoch: 116/200, Batch: 6/29, Batch_Loss_Train: 2.093
2024-06-21 01:13:11,489 - INFO: Epoch: 116/200, Batch: 7/29, Batch_Loss_Train: 1.695
2024-06-21 01:13:11,807 - INFO: Epoch: 116/200, Batch: 8/29, Batch_Loss_Train: 0.843
2024-06-21 01:13:12,223 - INFO: Epoch: 116/200, Batch: 9/29, Batch_Loss_Train: 1.842
2024-06-21 01:13:12,533 - INFO: Epoch: 116/200, Batch: 10/29, Batch_Loss_Train: 1.573
2024-06-21 01:13:12,921 - INFO: Epoch: 116/200, Batch: 11/29, Batch_Loss_Train: 1.973
2024-06-21 01:13:13,240 - INFO: Epoch: 116/200, Batch: 12/29, Batch_Loss_Train: 1.488
2024-06-21 01:13:13,673 - INFO: Epoch: 116/200, Batch: 13/29, Batch_Loss_Train: 1.624
2024-06-21 01:13:13,992 - INFO: Epoch: 116/200, Batch: 14/29, Batch_Loss_Train: 1.474
2024-06-21 01:13:14,390 - INFO: Epoch: 116/200, Batch: 15/29, Batch_Loss_Train: 1.443
2024-06-21 01:13:14,706 - INFO: Epoch: 116/200, Batch: 16/29, Batch_Loss_Train: 1.686
2024-06-21 01:13:15,128 - INFO: Epoch: 116/200, Batch: 17/29, Batch_Loss_Train: 2.148
2024-06-21 01:13:15,443 - INFO: Epoch: 116/200, Batch: 18/29, Batch_Loss_Train: 2.083
2024-06-21 01:13:15,828 - INFO: Epoch: 116/200, Batch: 19/29, Batch_Loss_Train: 1.528
2024-06-21 01:13:16,142 - INFO: Epoch: 116/200, Batch: 20/29, Batch_Loss_Train: 1.080
2024-06-21 01:13:16,564 - INFO: Epoch: 116/200, Batch: 21/29, Batch_Loss_Train: 1.524
2024-06-21 01:13:16,886 - INFO: Epoch: 116/200, Batch: 22/29, Batch_Loss_Train: 1.961
2024-06-21 01:13:17,287 - INFO: Epoch: 116/200, Batch: 23/29, Batch_Loss_Train: 1.283
2024-06-21 01:13:17,608 - INFO: Epoch: 116/200, Batch: 24/29, Batch_Loss_Train: 1.420
2024-06-21 01:13:18,027 - INFO: Epoch: 116/200, Batch: 25/29, Batch_Loss_Train: 1.057
2024-06-21 01:13:18,343 - INFO: Epoch: 116/200, Batch: 26/29, Batch_Loss_Train: 1.706
2024-06-21 01:13:18,736 - INFO: Epoch: 116/200, Batch: 27/29, Batch_Loss_Train: 1.959
2024-06-21 01:13:19,053 - INFO: Epoch: 116/200, Batch: 28/29, Batch_Loss_Train: 1.440
2024-06-21 01:13:19,278 - INFO: Epoch: 116/200, Batch: 29/29, Batch_Loss_Train: 1.157
2024-06-21 01:13:30,290 - INFO: 116/200 final results:
2024-06-21 01:13:30,290 - INFO: Training loss: 1.603.
2024-06-21 01:13:30,290 - INFO: Training MAE: 0.980.
2024-06-21 01:13:30,290 - INFO: Training MSE: 1.612.
2024-06-21 01:13:50,728 - INFO: Epoch: 116/200, Loss_train: 1.6031087102561161, Loss_val: 1.7114246775364053
2024-06-21 01:13:50,776 - INFO: Saved new best metric model for epoch 116.
2024-06-21 01:13:50,776 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:13:50,776 - INFO: Epoch 117/200...
2024-06-21 01:13:50,776 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:13:50,776 - INFO: Batch size: 32.
2024-06-21 01:13:50,780 - INFO: Dataset:
2024-06-21 01:13:50,780 - INFO: Batch size:
2024-06-21 01:13:50,780 - INFO: Number of workers:
2024-06-21 01:13:52,034 - INFO: Epoch: 117/200, Batch: 1/29, Batch_Loss_Train: 2.047
2024-06-21 01:13:52,346 - INFO: Epoch: 117/200, Batch: 2/29, Batch_Loss_Train: 1.310
2024-06-21 01:13:52,767 - INFO: Epoch: 117/200, Batch: 3/29, Batch_Loss_Train: 1.481
2024-06-21 01:13:53,091 - INFO: Epoch: 117/200, Batch: 4/29, Batch_Loss_Train: 2.116
2024-06-21 01:13:53,509 - INFO: Epoch: 117/200, Batch: 5/29, Batch_Loss_Train: 1.607
2024-06-21 01:13:53,815 - INFO: Epoch: 117/200, Batch: 6/29, Batch_Loss_Train: 1.386
2024-06-21 01:13:54,208 - INFO: Epoch: 117/200, Batch: 7/29, Batch_Loss_Train: 1.085
2024-06-21 01:13:54,529 - INFO: Epoch: 117/200, Batch: 8/29, Batch_Loss_Train: 1.786
2024-06-21 01:13:54,945 - INFO: Epoch: 117/200, Batch: 9/29, Batch_Loss_Train: 1.290
2024-06-21 01:13:55,247 - INFO: Epoch: 117/200, Batch: 10/29, Batch_Loss_Train: 1.713
2024-06-21 01:13:55,661 - INFO: Epoch: 117/200, Batch: 11/29, Batch_Loss_Train: 1.385
2024-06-21 01:13:55,983 - INFO: Epoch: 117/200, Batch: 12/29, Batch_Loss_Train: 1.498
2024-06-21 01:13:56,415 - INFO: Epoch: 117/200, Batch: 13/29, Batch_Loss_Train: 1.387
2024-06-21 01:13:56,725 - INFO: Epoch: 117/200, Batch: 14/29, Batch_Loss_Train: 2.114
2024-06-21 01:13:57,146 - INFO: Epoch: 117/200, Batch: 15/29, Batch_Loss_Train: 1.642
2024-06-21 01:13:57,464 - INFO: Epoch: 117/200, Batch: 16/29, Batch_Loss_Train: 1.247
2024-06-21 01:13:57,888 - INFO: Epoch: 117/200, Batch: 17/29, Batch_Loss_Train: 1.855
2024-06-21 01:13:58,193 - INFO: Epoch: 117/200, Batch: 18/29, Batch_Loss_Train: 1.511
2024-06-21 01:13:58,602 - INFO: Epoch: 117/200, Batch: 19/29, Batch_Loss_Train: 1.416
2024-06-21 01:13:58,917 - INFO: Epoch: 117/200, Batch: 20/29, Batch_Loss_Train: 1.857
2024-06-21 01:13:59,335 - INFO: Epoch: 117/200, Batch: 21/29, Batch_Loss_Train: 1.192
2024-06-21 01:13:59,645 - INFO: Epoch: 117/200, Batch: 22/29, Batch_Loss_Train: 1.250
2024-06-21 01:14:00,059 - INFO: Epoch: 117/200, Batch: 23/29, Batch_Loss_Train: 1.608
2024-06-21 01:14:00,380 - INFO: Epoch: 117/200, Batch: 24/29, Batch_Loss_Train: 1.643
2024-06-21 01:14:00,799 - INFO: Epoch: 117/200, Batch: 25/29, Batch_Loss_Train: 1.072
2024-06-21 01:14:01,103 - INFO: Epoch: 117/200, Batch: 26/29, Batch_Loss_Train: 1.968
2024-06-21 01:14:01,506 - INFO: Epoch: 117/200, Batch: 27/29, Batch_Loss_Train: 2.366
2024-06-21 01:14:01,822 - INFO: Epoch: 117/200, Batch: 28/29, Batch_Loss_Train: 1.839
2024-06-21 01:14:02,046 - INFO: Epoch: 117/200, Batch: 29/29, Batch_Loss_Train: 1.099
2024-06-21 01:14:13,104 - INFO: 117/200 final results:
2024-06-21 01:14:13,105 - INFO: Training loss: 1.578.
2024-06-21 01:14:13,105 - INFO: Training MAE: 0.977.
2024-06-21 01:14:13,105 - INFO: Training MSE: 1.588.
2024-06-21 01:14:33,439 - INFO: Epoch: 117/200, Loss_train: 1.5783085206459309, Loss_val: 2.0102449306126298
2024-06-21 01:14:33,439 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:14:33,439 - INFO: Epoch 118/200...
2024-06-21 01:14:33,439 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:14:33,439 - INFO: Batch size: 32.
2024-06-21 01:14:33,443 - INFO: Dataset:
2024-06-21 01:14:33,444 - INFO: Batch size:
2024-06-21 01:14:33,444 - INFO: Number of workers:
2024-06-21 01:14:34,708 - INFO: Epoch: 118/200, Batch: 1/29, Batch_Loss_Train: 1.867
2024-06-21 01:14:35,021 - INFO: Epoch: 118/200, Batch: 2/29, Batch_Loss_Train: 2.121
2024-06-21 01:14:35,423 - INFO: Epoch: 118/200, Batch: 3/29, Batch_Loss_Train: 1.346
2024-06-21 01:14:35,749 - INFO: Epoch: 118/200, Batch: 4/29, Batch_Loss_Train: 2.080
2024-06-21 01:14:36,179 - INFO: Epoch: 118/200, Batch: 5/29, Batch_Loss_Train: 2.565
2024-06-21 01:14:36,484 - INFO: Epoch: 118/200, Batch: 6/29, Batch_Loss_Train: 1.472
2024-06-21 01:14:36,873 - INFO: Epoch: 118/200, Batch: 7/29, Batch_Loss_Train: 1.782
2024-06-21 01:14:37,192 - INFO: Epoch: 118/200, Batch: 8/29, Batch_Loss_Train: 1.754
2024-06-21 01:14:37,624 - INFO: Epoch: 118/200, Batch: 9/29, Batch_Loss_Train: 1.604
2024-06-21 01:14:37,924 - INFO: Epoch: 118/200, Batch: 10/29, Batch_Loss_Train: 1.490
2024-06-21 01:14:38,314 - INFO: Epoch: 118/200, Batch: 11/29, Batch_Loss_Train: 1.364
2024-06-21 01:14:38,635 - INFO: Epoch: 118/200, Batch: 12/29, Batch_Loss_Train: 1.290
2024-06-21 01:14:39,075 - INFO: Epoch: 118/200, Batch: 13/29, Batch_Loss_Train: 1.147
2024-06-21 01:14:39,383 - INFO: Epoch: 118/200, Batch: 14/29, Batch_Loss_Train: 1.552
2024-06-21 01:14:39,784 - INFO: Epoch: 118/200, Batch: 15/29, Batch_Loss_Train: 1.765
2024-06-21 01:14:40,101 - INFO: Epoch: 118/200, Batch: 16/29, Batch_Loss_Train: 1.334
2024-06-21 01:14:40,533 - INFO: Epoch: 118/200, Batch: 17/29, Batch_Loss_Train: 1.870
2024-06-21 01:14:40,837 - INFO: Epoch: 118/200, Batch: 18/29, Batch_Loss_Train: 1.119
2024-06-21 01:14:41,224 - INFO: Epoch: 118/200, Batch: 19/29, Batch_Loss_Train: 1.454
2024-06-21 01:14:41,536 - INFO: Epoch: 118/200, Batch: 20/29, Batch_Loss_Train: 1.749
2024-06-21 01:14:41,964 - INFO: Epoch: 118/200, Batch: 21/29, Batch_Loss_Train: 1.684
2024-06-21 01:14:42,271 - INFO: Epoch: 118/200, Batch: 22/29, Batch_Loss_Train: 1.673
2024-06-21 01:14:42,669 - INFO: Epoch: 118/200, Batch: 23/29, Batch_Loss_Train: 1.359
2024-06-21 01:14:42,989 - INFO: Epoch: 118/200, Batch: 24/29, Batch_Loss_Train: 2.121
2024-06-21 01:14:43,416 - INFO: Epoch: 118/200, Batch: 25/29, Batch_Loss_Train: 1.766
2024-06-21 01:14:43,718 - INFO: Epoch: 118/200, Batch: 26/29, Batch_Loss_Train: 1.441
2024-06-21 01:14:44,107 - INFO: Epoch: 118/200, Batch: 27/29, Batch_Loss_Train: 1.300
2024-06-21 01:14:44,422 - INFO: Epoch: 118/200, Batch: 28/29, Batch_Loss_Train: 1.656
2024-06-21 01:14:44,645 - INFO: Epoch: 118/200, Batch: 29/29, Batch_Loss_Train: 1.690
2024-06-21 01:14:55,635 - INFO: 118/200 final results:
2024-06-21 01:14:55,636 - INFO: Training loss: 1.635.
2024-06-21 01:14:55,636 - INFO: Training MAE: 0.994.
2024-06-21 01:14:55,636 - INFO: Training MSE: 1.634.
2024-06-21 01:15:16,034 - INFO: Epoch: 118/200, Loss_train: 1.634949412839166, Loss_val: 1.8296859983740181
2024-06-21 01:15:16,034 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:15:16,034 - INFO: Epoch 119/200...
2024-06-21 01:15:16,034 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:15:16,034 - INFO: Batch size: 32.
2024-06-21 01:15:16,038 - INFO: Dataset:
2024-06-21 01:15:16,038 - INFO: Batch size:
2024-06-21 01:15:16,038 - INFO: Number of workers:
2024-06-21 01:15:17,310 - INFO: Epoch: 119/200, Batch: 1/29, Batch_Loss_Train: 1.672
2024-06-21 01:15:17,622 - INFO: Epoch: 119/200, Batch: 2/29, Batch_Loss_Train: 1.721
2024-06-21 01:15:18,012 - INFO: Epoch: 119/200, Batch: 3/29, Batch_Loss_Train: 1.461
2024-06-21 01:15:18,335 - INFO: Epoch: 119/200, Batch: 4/29, Batch_Loss_Train: 1.635
2024-06-21 01:15:18,775 - INFO: Epoch: 119/200, Batch: 5/29, Batch_Loss_Train: 1.533
2024-06-21 01:15:19,079 - INFO: Epoch: 119/200, Batch: 6/29, Batch_Loss_Train: 1.924
2024-06-21 01:15:19,455 - INFO: Epoch: 119/200, Batch: 7/29, Batch_Loss_Train: 0.993
2024-06-21 01:15:19,758 - INFO: Epoch: 119/200, Batch: 8/29, Batch_Loss_Train: 1.391
2024-06-21 01:15:20,195 - INFO: Epoch: 119/200, Batch: 9/29, Batch_Loss_Train: 1.887
2024-06-21 01:15:20,494 - INFO: Epoch: 119/200, Batch: 10/29, Batch_Loss_Train: 1.966
2024-06-21 01:15:20,872 - INFO: Epoch: 119/200, Batch: 11/29, Batch_Loss_Train: 1.515
2024-06-21 01:15:21,178 - INFO: Epoch: 119/200, Batch: 12/29, Batch_Loss_Train: 1.335
2024-06-21 01:15:21,628 - INFO: Epoch: 119/200, Batch: 13/29, Batch_Loss_Train: 2.561
2024-06-21 01:15:21,934 - INFO: Epoch: 119/200, Batch: 14/29, Batch_Loss_Train: 1.319
2024-06-21 01:15:22,327 - INFO: Epoch: 119/200, Batch: 15/29, Batch_Loss_Train: 1.785
2024-06-21 01:15:22,629 - INFO: Epoch: 119/200, Batch: 16/29, Batch_Loss_Train: 1.833
2024-06-21 01:15:23,067 - INFO: Epoch: 119/200, Batch: 17/29, Batch_Loss_Train: 1.605
2024-06-21 01:15:23,369 - INFO: Epoch: 119/200, Batch: 18/29, Batch_Loss_Train: 1.696
2024-06-21 01:15:23,746 - INFO: Epoch: 119/200, Batch: 19/29, Batch_Loss_Train: 1.908
2024-06-21 01:15:24,044 - INFO: Epoch: 119/200, Batch: 20/29, Batch_Loss_Train: 1.654
2024-06-21 01:15:24,480 - INFO: Epoch: 119/200, Batch: 21/29, Batch_Loss_Train: 1.520
2024-06-21 01:15:24,784 - INFO: Epoch: 119/200, Batch: 22/29, Batch_Loss_Train: 1.408
2024-06-21 01:15:25,167 - INFO: Epoch: 119/200, Batch: 23/29, Batch_Loss_Train: 1.855
2024-06-21 01:15:25,472 - INFO: Epoch: 119/200, Batch: 24/29, Batch_Loss_Train: 1.283
2024-06-21 01:15:25,900 - INFO: Epoch: 119/200, Batch: 25/29, Batch_Loss_Train: 1.874
2024-06-21 01:15:26,200 - INFO: Epoch: 119/200, Batch: 26/29, Batch_Loss_Train: 1.433
2024-06-21 01:15:26,573 - INFO: Epoch: 119/200, Batch: 27/29, Batch_Loss_Train: 1.373
2024-06-21 01:15:26,872 - INFO: Epoch: 119/200, Batch: 28/29, Batch_Loss_Train: 1.462
2024-06-21 01:15:27,082 - INFO: Epoch: 119/200, Batch: 29/29, Batch_Loss_Train: 1.347
2024-06-21 01:15:38,077 - INFO: 119/200 final results:
2024-06-21 01:15:38,077 - INFO: Training loss: 1.619.
2024-06-21 01:15:38,077 - INFO: Training MAE: 0.988.
2024-06-21 01:15:38,077 - INFO: Training MSE: 1.624.
2024-06-21 01:15:58,543 - INFO: Epoch: 119/200, Loss_train: 1.6189079264114643, Loss_val: 1.7425538712534412
2024-06-21 01:15:58,543 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:15:58,543 - INFO: Epoch 120/200...
2024-06-21 01:15:58,543 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:15:58,543 - INFO: Batch size: 32.
2024-06-21 01:15:58,548 - INFO: Dataset:
2024-06-21 01:15:58,548 - INFO: Batch size:
2024-06-21 01:15:58,548 - INFO: Number of workers:
2024-06-21 01:15:59,827 - INFO: Epoch: 120/200, Batch: 1/29, Batch_Loss_Train: 1.846
2024-06-21 01:16:00,141 - INFO: Epoch: 120/200, Batch: 2/29, Batch_Loss_Train: 1.460
2024-06-21 01:16:00,536 - INFO: Epoch: 120/200, Batch: 3/29, Batch_Loss_Train: 1.592
2024-06-21 01:16:00,862 - INFO: Epoch: 120/200, Batch: 4/29, Batch_Loss_Train: 1.399
2024-06-21 01:16:01,314 - INFO: Epoch: 120/200, Batch: 5/29, Batch_Loss_Train: 1.570
2024-06-21 01:16:01,622 - INFO: Epoch: 120/200, Batch: 6/29, Batch_Loss_Train: 2.012
2024-06-21 01:16:02,019 - INFO: Epoch: 120/200, Batch: 7/29, Batch_Loss_Train: 1.486
2024-06-21 01:16:02,327 - INFO: Epoch: 120/200, Batch: 8/29, Batch_Loss_Train: 1.217
2024-06-21 01:16:02,779 - INFO: Epoch: 120/200, Batch: 9/29, Batch_Loss_Train: 1.539
2024-06-21 01:16:03,078 - INFO: Epoch: 120/200, Batch: 10/29, Batch_Loss_Train: 1.602
2024-06-21 01:16:03,477 - INFO: Epoch: 120/200, Batch: 11/29, Batch_Loss_Train: 1.246
2024-06-21 01:16:03,785 - INFO: Epoch: 120/200, Batch: 12/29, Batch_Loss_Train: 1.907
2024-06-21 01:16:04,245 - INFO: Epoch: 120/200, Batch: 13/29, Batch_Loss_Train: 1.326
2024-06-21 01:16:04,554 - INFO: Epoch: 120/200, Batch: 14/29, Batch_Loss_Train: 1.728
2024-06-21 01:16:04,961 - INFO: Epoch: 120/200, Batch: 15/29, Batch_Loss_Train: 1.659
2024-06-21 01:16:05,266 - INFO: Epoch: 120/200, Batch: 16/29, Batch_Loss_Train: 1.520
2024-06-21 01:16:05,713 - INFO: Epoch: 120/200, Batch: 17/29, Batch_Loss_Train: 1.588
2024-06-21 01:16:06,017 - INFO: Epoch: 120/200, Batch: 18/29, Batch_Loss_Train: 1.767
2024-06-21 01:16:06,413 - INFO: Epoch: 120/200, Batch: 19/29, Batch_Loss_Train: 2.082
2024-06-21 01:16:06,714 - INFO: Epoch: 120/200, Batch: 20/29, Batch_Loss_Train: 2.117
2024-06-21 01:16:07,165 - INFO: Epoch: 120/200, Batch: 21/29, Batch_Loss_Train: 1.980
2024-06-21 01:16:07,474 - INFO: Epoch: 120/200, Batch: 22/29, Batch_Loss_Train: 1.617
2024-06-21 01:16:07,874 - INFO: Epoch: 120/200, Batch: 23/29, Batch_Loss_Train: 1.578
2024-06-21 01:16:08,184 - INFO: Epoch: 120/200, Batch: 24/29, Batch_Loss_Train: 1.302
2024-06-21 01:16:08,625 - INFO: Epoch: 120/200, Batch: 25/29, Batch_Loss_Train: 1.456
2024-06-21 01:16:08,929 - INFO: Epoch: 120/200, Batch: 26/29, Batch_Loss_Train: 1.763
2024-06-21 01:16:09,320 - INFO: Epoch: 120/200, Batch: 27/29, Batch_Loss_Train: 1.546
2024-06-21 01:16:09,624 - INFO: Epoch: 120/200, Batch: 28/29, Batch_Loss_Train: 1.916
2024-06-21 01:16:09,847 - INFO: Epoch: 120/200, Batch: 29/29, Batch_Loss_Train: 1.641
2024-06-21 01:16:20,908 - INFO: 120/200 final results:
2024-06-21 01:16:20,908 - INFO: Training loss: 1.637.
2024-06-21 01:16:20,908 - INFO: Training MAE: 0.995.
2024-06-21 01:16:20,908 - INFO: Training MSE: 1.636.
2024-06-21 01:16:41,277 - INFO: Epoch: 120/200, Loss_train: 1.6365594082865222, Loss_val: 1.9029599058217015
2024-06-21 01:16:41,278 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:16:41,278 - INFO: Epoch 121/200...
2024-06-21 01:16:41,278 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:16:41,278 - INFO: Batch size: 32.
2024-06-21 01:16:41,281 - INFO: Dataset:
2024-06-21 01:16:41,282 - INFO: Batch size:
2024-06-21 01:16:41,282 - INFO: Number of workers:
2024-06-21 01:16:42,526 - INFO: Epoch: 121/200, Batch: 1/29, Batch_Loss_Train: 1.384
2024-06-21 01:16:42,851 - INFO: Epoch: 121/200, Batch: 2/29, Batch_Loss_Train: 1.956
2024-06-21 01:16:43,255 - INFO: Epoch: 121/200, Batch: 3/29, Batch_Loss_Train: 1.733
2024-06-21 01:16:43,578 - INFO: Epoch: 121/200, Batch: 4/29, Batch_Loss_Train: 1.412
2024-06-21 01:16:43,992 - INFO: Epoch: 121/200, Batch: 5/29, Batch_Loss_Train: 1.311
2024-06-21 01:16:44,296 - INFO: Epoch: 121/200, Batch: 6/29, Batch_Loss_Train: 2.440
2024-06-21 01:16:44,683 - INFO: Epoch: 121/200, Batch: 7/29, Batch_Loss_Train: 1.918
2024-06-21 01:16:44,998 - INFO: Epoch: 121/200, Batch: 8/29, Batch_Loss_Train: 2.105
2024-06-21 01:16:45,406 - INFO: Epoch: 121/200, Batch: 9/29, Batch_Loss_Train: 1.792
2024-06-21 01:16:45,704 - INFO: Epoch: 121/200, Batch: 10/29, Batch_Loss_Train: 1.434
2024-06-21 01:16:46,095 - INFO: Epoch: 121/200, Batch: 11/29, Batch_Loss_Train: 1.627
2024-06-21 01:16:46,412 - INFO: Epoch: 121/200, Batch: 12/29, Batch_Loss_Train: 1.665
2024-06-21 01:16:46,839 - INFO: Epoch: 121/200, Batch: 13/29, Batch_Loss_Train: 1.875
2024-06-21 01:16:47,149 - INFO: Epoch: 121/200, Batch: 14/29, Batch_Loss_Train: 1.849
2024-06-21 01:16:47,555 - INFO: Epoch: 121/200, Batch: 15/29, Batch_Loss_Train: 1.654
2024-06-21 01:16:47,872 - INFO: Epoch: 121/200, Batch: 16/29, Batch_Loss_Train: 1.218
2024-06-21 01:16:48,292 - INFO: Epoch: 121/200, Batch: 17/29, Batch_Loss_Train: 1.773
2024-06-21 01:16:48,597 - INFO: Epoch: 121/200, Batch: 18/29, Batch_Loss_Train: 1.613
2024-06-21 01:16:48,995 - INFO: Epoch: 121/200, Batch: 19/29, Batch_Loss_Train: 1.303
2024-06-21 01:16:49,309 - INFO: Epoch: 121/200, Batch: 20/29, Batch_Loss_Train: 1.449
2024-06-21 01:16:49,723 - INFO: Epoch: 121/200, Batch: 21/29, Batch_Loss_Train: 1.445
2024-06-21 01:16:50,031 - INFO: Epoch: 121/200, Batch: 22/29, Batch_Loss_Train: 1.625
2024-06-21 01:16:50,437 - INFO: Epoch: 121/200, Batch: 23/29, Batch_Loss_Train: 1.476
2024-06-21 01:16:50,757 - INFO: Epoch: 121/200, Batch: 24/29, Batch_Loss_Train: 1.326
2024-06-21 01:16:51,163 - INFO: Epoch: 121/200, Batch: 25/29, Batch_Loss_Train: 1.672
2024-06-21 01:16:51,464 - INFO: Epoch: 121/200, Batch: 26/29, Batch_Loss_Train: 1.640
2024-06-21 01:16:51,856 - INFO: Epoch: 121/200, Batch: 27/29, Batch_Loss_Train: 1.244
2024-06-21 01:16:52,169 - INFO: Epoch: 121/200, Batch: 28/29, Batch_Loss_Train: 1.920
2024-06-21 01:16:52,382 - INFO: Epoch: 121/200, Batch: 29/29, Batch_Loss_Train: 1.745
2024-06-21 01:17:03,372 - INFO: 121/200 final results:
2024-06-21 01:17:03,373 - INFO: Training loss: 1.642.
2024-06-21 01:17:03,373 - INFO: Training MAE: 0.998.
2024-06-21 01:17:03,373 - INFO: Training MSE: 1.639.
2024-06-21 01:17:24,108 - INFO: Epoch: 121/200, Loss_train: 1.6415231680047924, Loss_val: 1.7836860952706173
2024-06-21 01:17:24,108 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:17:24,108 - INFO: Epoch 122/200...
2024-06-21 01:17:24,108 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:17:24,108 - INFO: Batch size: 32.
2024-06-21 01:17:24,112 - INFO: Dataset:
2024-06-21 01:17:24,112 - INFO: Batch size:
2024-06-21 01:17:24,112 - INFO: Number of workers:
2024-06-21 01:17:25,369 - INFO: Epoch: 122/200, Batch: 1/29, Batch_Loss_Train: 2.877
2024-06-21 01:17:25,682 - INFO: Epoch: 122/200, Batch: 2/29, Batch_Loss_Train: 1.507
2024-06-21 01:17:26,105 - INFO: Epoch: 122/200, Batch: 3/29, Batch_Loss_Train: 1.858
2024-06-21 01:17:26,430 - INFO: Epoch: 122/200, Batch: 4/29, Batch_Loss_Train: 1.862
2024-06-21 01:17:26,856 - INFO: Epoch: 122/200, Batch: 5/29, Batch_Loss_Train: 1.623
2024-06-21 01:17:27,162 - INFO: Epoch: 122/200, Batch: 6/29, Batch_Loss_Train: 1.594
2024-06-21 01:17:27,571 - INFO: Epoch: 122/200, Batch: 7/29, Batch_Loss_Train: 1.551
2024-06-21 01:17:27,889 - INFO: Epoch: 122/200, Batch: 8/29, Batch_Loss_Train: 1.315
2024-06-21 01:17:28,306 - INFO: Epoch: 122/200, Batch: 9/29, Batch_Loss_Train: 1.714
2024-06-21 01:17:28,606 - INFO: Epoch: 122/200, Batch: 10/29, Batch_Loss_Train: 1.579
2024-06-21 01:17:29,014 - INFO: Epoch: 122/200, Batch: 11/29, Batch_Loss_Train: 2.024
2024-06-21 01:17:29,335 - INFO: Epoch: 122/200, Batch: 12/29, Batch_Loss_Train: 1.464
2024-06-21 01:17:29,771 - INFO: Epoch: 122/200, Batch: 13/29, Batch_Loss_Train: 1.986
2024-06-21 01:17:30,080 - INFO: Epoch: 122/200, Batch: 14/29, Batch_Loss_Train: 1.308
2024-06-21 01:17:30,499 - INFO: Epoch: 122/200, Batch: 15/29, Batch_Loss_Train: 1.319
2024-06-21 01:17:30,816 - INFO: Epoch: 122/200, Batch: 16/29, Batch_Loss_Train: 1.676
2024-06-21 01:17:31,241 - INFO: Epoch: 122/200, Batch: 17/29, Batch_Loss_Train: 1.577
2024-06-21 01:17:31,545 - INFO: Epoch: 122/200, Batch: 18/29, Batch_Loss_Train: 1.912
2024-06-21 01:17:31,953 - INFO: Epoch: 122/200, Batch: 19/29, Batch_Loss_Train: 1.858
2024-06-21 01:17:32,266 - INFO: Epoch: 122/200, Batch: 20/29, Batch_Loss_Train: 2.067
2024-06-21 01:17:32,687 - INFO: Epoch: 122/200, Batch: 21/29, Batch_Loss_Train: 1.412
2024-06-21 01:17:32,995 - INFO: Epoch: 122/200, Batch: 22/29, Batch_Loss_Train: 1.387
2024-06-21 01:17:33,407 - INFO: Epoch: 122/200, Batch: 23/29, Batch_Loss_Train: 2.354
2024-06-21 01:17:33,727 - INFO: Epoch: 122/200, Batch: 24/29, Batch_Loss_Train: 1.599
2024-06-21 01:17:34,144 - INFO: Epoch: 122/200, Batch: 25/29, Batch_Loss_Train: 1.964
2024-06-21 01:17:34,447 - INFO: Epoch: 122/200, Batch: 26/29, Batch_Loss_Train: 1.594
2024-06-21 01:17:34,847 - INFO: Epoch: 122/200, Batch: 27/29, Batch_Loss_Train: 1.763
2024-06-21 01:17:35,163 - INFO: Epoch: 122/200, Batch: 28/29, Batch_Loss_Train: 1.326
2024-06-21 01:17:35,385 - INFO: Epoch: 122/200, Batch: 29/29, Batch_Loss_Train: 1.342
2024-06-21 01:17:46,375 - INFO: 122/200 final results:
2024-06-21 01:17:46,375 - INFO: Training loss: 1.704.
2024-06-21 01:17:46,375 - INFO: Training MAE: 1.012.
2024-06-21 01:17:46,375 - INFO: Training MSE: 1.711.
2024-06-21 01:18:06,691 - INFO: Epoch: 122/200, Loss_train: 1.7038927160460373, Loss_val: 1.7704004933094155
2024-06-21 01:18:06,691 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:18:06,691 - INFO: Epoch 123/200...
2024-06-21 01:18:06,691 - INFO: Learning rate: 2.7548744251314635e-05.
2024-06-21 01:18:06,691 - INFO: Batch size: 32.
2024-06-21 01:18:06,695 - INFO: Dataset:
2024-06-21 01:18:06,695 - INFO: Batch size:
2024-06-21 01:18:06,695 - INFO: Number of workers:
2024-06-21 01:18:07,933 - INFO: Epoch: 123/200, Batch: 1/29, Batch_Loss_Train: 1.450
2024-06-21 01:18:08,247 - INFO: Epoch: 123/200, Batch: 2/29, Batch_Loss_Train: 2.033
2024-06-21 01:18:08,689 - INFO: Epoch: 123/200, Batch: 3/29, Batch_Loss_Train: 1.813
2024-06-21 01:18:09,018 - INFO: Epoch: 123/200, Batch: 4/29, Batch_Loss_Train: 1.622
2024-06-21 01:18:09,438 - INFO: Epoch: 123/200, Batch: 5/29, Batch_Loss_Train: 1.667
2024-06-21 01:18:09,745 - INFO: Epoch: 123/200, Batch: 6/29, Batch_Loss_Train: 1.171
2024-06-21 01:18:10,155 - INFO: Epoch: 123/200, Batch: 7/29, Batch_Loss_Train: 1.736
2024-06-21 01:18:10,475 - INFO: Epoch: 123/200, Batch: 8/29, Batch_Loss_Train: 1.804
2024-06-21 01:18:10,893 - INFO: Epoch: 123/200, Batch: 9/29, Batch_Loss_Train: 1.507
2024-06-21 01:18:11,194 - INFO: Epoch: 123/200, Batch: 10/29, Batch_Loss_Train: 1.365
2024-06-21 01:18:11,600 - INFO: Epoch: 123/200, Batch: 11/29, Batch_Loss_Train: 1.800
2024-06-21 01:18:11,920 - INFO: Epoch: 123/200, Batch: 12/29, Batch_Loss_Train: 1.171
2024-06-21 01:18:12,351 - INFO: Epoch: 123/200, Batch: 13/29, Batch_Loss_Train: 1.771
2024-06-21 01:18:12,660 - INFO: Epoch: 123/200, Batch: 14/29, Batch_Loss_Train: 1.661
2024-06-21 01:18:13,076 - INFO: Epoch: 123/200, Batch: 15/29, Batch_Loss_Train: 1.644
2024-06-21 01:18:13,393 - INFO: Epoch: 123/200, Batch: 16/29, Batch_Loss_Train: 1.373
2024-06-21 01:18:13,814 - INFO: Epoch: 123/200, Batch: 17/29, Batch_Loss_Train: 1.677
2024-06-21 01:18:14,119 - INFO: Epoch: 123/200, Batch: 18/29, Batch_Loss_Train: 1.538
2024-06-21 01:18:14,524 - INFO: Epoch: 123/200, Batch: 19/29, Batch_Loss_Train: 1.533
2024-06-21 01:18:14,838 - INFO: Epoch: 123/200, Batch: 20/29, Batch_Loss_Train: 2.281
2024-06-21 01:18:15,255 - INFO: Epoch: 123/200, Batch: 21/29, Batch_Loss_Train: 1.300
2024-06-21 01:18:15,564 - INFO: Epoch: 123/200, Batch: 22/29, Batch_Loss_Train: 1.761
2024-06-21 01:18:15,969 - INFO: Epoch: 123/200, Batch: 23/29, Batch_Loss_Train: 1.535
2024-06-21 01:18:16,289 - INFO: Epoch: 123/200, Batch: 24/29, Batch_Loss_Train: 1.277
2024-06-21 01:18:16,701 - INFO: Epoch: 123/200, Batch: 25/29, Batch_Loss_Train: 1.603
2024-06-21 01:18:17,004 - INFO: Epoch: 123/200, Batch: 26/29, Batch_Loss_Train: 1.889
2024-06-21 01:18:17,394 - INFO: Epoch: 123/200, Batch: 27/29, Batch_Loss_Train: 1.420
2024-06-21 01:18:17,710 - INFO: Epoch: 123/200, Batch: 28/29, Batch_Loss_Train: 1.557
2024-06-21 01:18:17,932 - INFO: Epoch: 123/200, Batch: 29/29, Batch_Loss_Train: 1.950
2024-06-21 01:18:28,699 - INFO: 123/200 final results:
2024-06-21 01:18:28,700 - INFO: Training loss: 1.618.
2024-06-21 01:18:28,700 - INFO: Training MAE: 0.986.
2024-06-21 01:18:28,700 - INFO: Training MSE: 1.611.
2024-06-21 01:18:49,103 - INFO: Epoch: 123/200, Loss_train: 1.6175275260004505, Loss_val: 1.8460613715237584
2024-06-21 01:18:49,103 - INFO: Best internal validation val_loss: 1.711 at epoch: 116.
2024-06-21 01:18:49,103 - INFO: Epoch 124/200...
2024-06-21 01:18:49,103 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:18:49,103 - INFO: Batch size: 32.
2024-06-21 01:18:49,107 - INFO: Dataset:
2024-06-21 01:18:49,107 - INFO: Batch size:
2024-06-21 01:18:49,107 - INFO: Number of workers:
2024-06-21 01:18:50,372 - INFO: Epoch: 124/200, Batch: 1/29, Batch_Loss_Train: 1.796
2024-06-21 01:18:50,686 - INFO: Epoch: 124/200, Batch: 2/29, Batch_Loss_Train: 1.746
2024-06-21 01:18:51,108 - INFO: Epoch: 124/200, Batch: 3/29, Batch_Loss_Train: 1.573
2024-06-21 01:18:51,434 - INFO: Epoch: 124/200, Batch: 4/29, Batch_Loss_Train: 2.314
2024-06-21 01:18:51,855 - INFO: Epoch: 124/200, Batch: 5/29, Batch_Loss_Train: 1.476
2024-06-21 01:18:52,163 - INFO: Epoch: 124/200, Batch: 6/29, Batch_Loss_Train: 2.274
2024-06-21 01:18:52,571 - INFO: Epoch: 124/200, Batch: 7/29, Batch_Loss_Train: 1.372
2024-06-21 01:18:52,892 - INFO: Epoch: 124/200, Batch: 8/29, Batch_Loss_Train: 1.055
2024-06-21 01:18:53,311 - INFO: Epoch: 124/200, Batch: 9/29, Batch_Loss_Train: 1.544
2024-06-21 01:18:53,612 - INFO: Epoch: 124/200, Batch: 10/29, Batch_Loss_Train: 1.305
2024-06-21 01:18:54,021 - INFO: Epoch: 124/200, Batch: 11/29, Batch_Loss_Train: 1.442
2024-06-21 01:18:54,345 - INFO: Epoch: 124/200, Batch: 12/29, Batch_Loss_Train: 1.639
2024-06-21 01:18:54,781 - INFO: Epoch: 124/200, Batch: 13/29, Batch_Loss_Train: 2.331
2024-06-21 01:18:55,092 - INFO: Epoch: 124/200, Batch: 14/29, Batch_Loss_Train: 1.284
2024-06-21 01:18:55,512 - INFO: Epoch: 124/200, Batch: 15/29, Batch_Loss_Train: 1.893
2024-06-21 01:18:55,830 - INFO: Epoch: 124/200, Batch: 16/29, Batch_Loss_Train: 1.305
2024-06-21 01:18:56,246 - INFO: Epoch: 124/200, Batch: 17/29, Batch_Loss_Train: 1.567
2024-06-21 01:18:56,553 - INFO: Epoch: 124/200, Batch: 18/29, Batch_Loss_Train: 1.417
2024-06-21 01:18:56,960 - INFO: Epoch: 124/200, Batch: 19/29, Batch_Loss_Train: 1.515
2024-06-21 01:18:57,273 - INFO: Epoch: 124/200, Batch: 20/29, Batch_Loss_Train: 1.365
2024-06-21 01:18:57,689 - INFO: Epoch: 124/200, Batch: 21/29, Batch_Loss_Train: 1.151
2024-06-21 01:18:57,995 - INFO: Epoch: 124/200, Batch: 22/29, Batch_Loss_Train: 1.807
2024-06-21 01:18:58,392 - INFO: Epoch: 124/200, Batch: 23/29, Batch_Loss_Train: 1.679
2024-06-21 01:18:58,710 - INFO: Epoch: 124/200, Batch: 24/29, Batch_Loss_Train: 1.617
2024-06-21 01:18:59,124 - INFO: Epoch: 124/200, Batch: 25/29, Batch_Loss_Train: 1.614
2024-06-21 01:18:59,425 - INFO: Epoch: 124/200, Batch: 26/29, Batch_Loss_Train: 1.643
2024-06-21 01:18:59,808 - INFO: Epoch: 124/200, Batch: 27/29, Batch_Loss_Train: 1.652
2024-06-21 01:19:00,120 - INFO: Epoch: 124/200, Batch: 28/29, Batch_Loss_Train: 1.481
2024-06-21 01:19:00,334 - INFO: Epoch: 124/200, Batch: 29/29, Batch_Loss_Train: 1.801
2024-06-21 01:19:11,350 - INFO: 124/200 final results:
2024-06-21 01:19:11,350 - INFO: Training loss: 1.609.
2024-06-21 01:19:11,351 - INFO: Training MAE: 0.989.
2024-06-21 01:19:11,351 - INFO: Training MSE: 1.605.
2024-06-21 01:19:32,121 - INFO: Epoch: 124/200, Loss_train: 1.6087921898940514, Loss_val: 1.6665223651918872
2024-06-21 01:19:32,169 - INFO: Saved new best metric model for epoch 124.
2024-06-21 01:19:32,169 - INFO: Best internal validation val_loss: 1.667 at epoch: 124.
2024-06-21 01:19:32,169 - INFO: Epoch 125/200...
2024-06-21 01:19:32,169 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:19:32,169 - INFO: Batch size: 32.
2024-06-21 01:19:32,174 - INFO: Dataset:
2024-06-21 01:19:32,174 - INFO: Batch size:
2024-06-21 01:19:32,174 - INFO: Number of workers:
2024-06-21 01:19:33,455 - INFO: Epoch: 125/200, Batch: 1/29, Batch_Loss_Train: 1.393
2024-06-21 01:19:33,769 - INFO: Epoch: 125/200, Batch: 2/29, Batch_Loss_Train: 1.540
2024-06-21 01:19:34,166 - INFO: Epoch: 125/200, Batch: 3/29, Batch_Loss_Train: 1.573
2024-06-21 01:19:34,490 - INFO: Epoch: 125/200, Batch: 4/29, Batch_Loss_Train: 1.752
2024-06-21 01:19:34,916 - INFO: Epoch: 125/200, Batch: 5/29, Batch_Loss_Train: 1.938
2024-06-21 01:19:35,221 - INFO: Epoch: 125/200, Batch: 6/29, Batch_Loss_Train: 1.655
2024-06-21 01:19:35,614 - INFO: Epoch: 125/200, Batch: 7/29, Batch_Loss_Train: 1.613
2024-06-21 01:19:35,932 - INFO: Epoch: 125/200, Batch: 8/29, Batch_Loss_Train: 1.462
2024-06-21 01:19:36,360 - INFO: Epoch: 125/200, Batch: 9/29, Batch_Loss_Train: 1.586
2024-06-21 01:19:36,659 - INFO: Epoch: 125/200, Batch: 10/29, Batch_Loss_Train: 1.583
2024-06-21 01:19:37,051 - INFO: Epoch: 125/200, Batch: 11/29, Batch_Loss_Train: 1.662
2024-06-21 01:19:37,371 - INFO: Epoch: 125/200, Batch: 12/29, Batch_Loss_Train: 2.037
2024-06-21 01:19:37,806 - INFO: Epoch: 125/200, Batch: 13/29, Batch_Loss_Train: 1.726
2024-06-21 01:19:38,114 - INFO: Epoch: 125/200, Batch: 14/29, Batch_Loss_Train: 1.522
2024-06-21 01:19:38,517 - INFO: Epoch: 125/200, Batch: 15/29, Batch_Loss_Train: 1.471
2024-06-21 01:19:38,834 - INFO: Epoch: 125/200, Batch: 16/29, Batch_Loss_Train: 2.188
2024-06-21 01:19:39,257 - INFO: Epoch: 125/200, Batch: 17/29, Batch_Loss_Train: 1.901
2024-06-21 01:19:39,560 - INFO: Epoch: 125/200, Batch: 18/29, Batch_Loss_Train: 1.537
2024-06-21 01:19:39,951 - INFO: Epoch: 125/200, Batch: 19/29, Batch_Loss_Train: 1.471
2024-06-21 01:19:40,262 - INFO: Epoch: 125/200, Batch: 20/29, Batch_Loss_Train: 1.380
2024-06-21 01:19:40,689 - INFO: Epoch: 125/200, Batch: 21/29, Batch_Loss_Train: 1.881
2024-06-21 01:19:40,995 - INFO: Epoch: 125/200, Batch: 22/29, Batch_Loss_Train: 1.102
2024-06-21 01:19:41,386 - INFO: Epoch: 125/200, Batch: 23/29, Batch_Loss_Train: 1.688
2024-06-21 01:19:41,704 - INFO: Epoch: 125/200, Batch: 24/29, Batch_Loss_Train: 1.457
2024-06-21 01:19:42,127 - INFO: Epoch: 125/200, Batch: 25/29, Batch_Loss_Train: 1.327
2024-06-21 01:19:42,428 - INFO: Epoch: 125/200, Batch: 26/29, Batch_Loss_Train: 1.727
2024-06-21 01:19:42,815 - INFO: Epoch: 125/200, Batch: 27/29, Batch_Loss_Train: 1.550
2024-06-21 01:19:43,129 - INFO: Epoch: 125/200, Batch: 28/29, Batch_Loss_Train: 1.224
2024-06-21 01:19:43,351 - INFO: Epoch: 125/200, Batch: 29/29, Batch_Loss_Train: 1.488
2024-06-21 01:19:54,366 - INFO: 125/200 final results:
2024-06-21 01:19:54,366 - INFO: Training loss: 1.601.
2024-06-21 01:19:54,366 - INFO: Training MAE: 0.982.
2024-06-21 01:19:54,366 - INFO: Training MSE: 1.603.
2024-06-21 01:20:14,804 - INFO: Epoch: 125/200, Loss_train: 1.6011384725570679, Loss_val: 1.6619363899888664
2024-06-21 01:20:14,856 - INFO: Saved new best metric model for epoch 125.
2024-06-21 01:20:14,856 - INFO: Best internal validation val_loss: 1.662 at epoch: 125.
2024-06-21 01:20:14,856 - INFO: Epoch 126/200...
2024-06-21 01:20:14,856 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:20:14,856 - INFO: Batch size: 32.
2024-06-21 01:20:14,861 - INFO: Dataset:
2024-06-21 01:20:14,861 - INFO: Batch size:
2024-06-21 01:20:14,861 - INFO: Number of workers:
2024-06-21 01:20:16,174 - INFO: Epoch: 126/200, Batch: 1/29, Batch_Loss_Train: 1.418
2024-06-21 01:20:16,487 - INFO: Epoch: 126/200, Batch: 2/29, Batch_Loss_Train: 1.428
2024-06-21 01:20:16,909 - INFO: Epoch: 126/200, Batch: 3/29, Batch_Loss_Train: 1.332
2024-06-21 01:20:17,233 - INFO: Epoch: 126/200, Batch: 4/29, Batch_Loss_Train: 1.668
2024-06-21 01:20:17,656 - INFO: Epoch: 126/200, Batch: 5/29, Batch_Loss_Train: 1.585
2024-06-21 01:20:17,962 - INFO: Epoch: 126/200, Batch: 6/29, Batch_Loss_Train: 1.750
2024-06-21 01:20:18,369 - INFO: Epoch: 126/200, Batch: 7/29, Batch_Loss_Train: 1.442
2024-06-21 01:20:18,689 - INFO: Epoch: 126/200, Batch: 8/29, Batch_Loss_Train: 1.316
2024-06-21 01:20:19,104 - INFO: Epoch: 126/200, Batch: 9/29, Batch_Loss_Train: 1.529
2024-06-21 01:20:19,404 - INFO: Epoch: 126/200, Batch: 10/29, Batch_Loss_Train: 1.291
2024-06-21 01:20:19,808 - INFO: Epoch: 126/200, Batch: 11/29, Batch_Loss_Train: 1.847
2024-06-21 01:20:20,128 - INFO: Epoch: 126/200, Batch: 12/29, Batch_Loss_Train: 1.739
2024-06-21 01:20:20,561 - INFO: Epoch: 126/200, Batch: 13/29, Batch_Loss_Train: 1.550
2024-06-21 01:20:20,870 - INFO: Epoch: 126/200, Batch: 14/29, Batch_Loss_Train: 1.405
2024-06-21 01:20:21,287 - INFO: Epoch: 126/200, Batch: 15/29, Batch_Loss_Train: 1.481
2024-06-21 01:20:21,604 - INFO: Epoch: 126/200, Batch: 16/29, Batch_Loss_Train: 1.149
2024-06-21 01:20:22,028 - INFO: Epoch: 126/200, Batch: 17/29, Batch_Loss_Train: 1.126
2024-06-21 01:20:22,332 - INFO: Epoch: 126/200, Batch: 18/29, Batch_Loss_Train: 1.084
2024-06-21 01:20:22,738 - INFO: Epoch: 126/200, Batch: 19/29, Batch_Loss_Train: 1.691
2024-06-21 01:20:23,051 - INFO: Epoch: 126/200, Batch: 20/29, Batch_Loss_Train: 1.851
2024-06-21 01:20:23,472 - INFO: Epoch: 126/200, Batch: 21/29, Batch_Loss_Train: 1.141
2024-06-21 01:20:23,779 - INFO: Epoch: 126/200, Batch: 22/29, Batch_Loss_Train: 1.191
2024-06-21 01:20:24,191 - INFO: Epoch: 126/200, Batch: 23/29, Batch_Loss_Train: 1.194
2024-06-21 01:20:24,511 - INFO: Epoch: 126/200, Batch: 24/29, Batch_Loss_Train: 1.093
2024-06-21 01:20:24,922 - INFO: Epoch: 126/200, Batch: 25/29, Batch_Loss_Train: 1.499
2024-06-21 01:20:25,224 - INFO: Epoch: 126/200, Batch: 26/29, Batch_Loss_Train: 1.616
2024-06-21 01:20:25,619 - INFO: Epoch: 126/200, Batch: 27/29, Batch_Loss_Train: 1.414
2024-06-21 01:20:25,933 - INFO: Epoch: 126/200, Batch: 28/29, Batch_Loss_Train: 1.180
2024-06-21 01:20:26,153 - INFO: Epoch: 126/200, Batch: 29/29, Batch_Loss_Train: 1.601
2024-06-21 01:20:37,178 - INFO: 126/200 final results:
2024-06-21 01:20:37,179 - INFO: Training loss: 1.435.
2024-06-21 01:20:37,179 - INFO: Training MAE: 0.932.
2024-06-21 01:20:37,179 - INFO: Training MSE: 1.431.
2024-06-21 01:20:57,474 - INFO: Epoch: 126/200, Loss_train: 1.4347541989951298, Loss_val: 1.65387178906079
2024-06-21 01:20:57,523 - INFO: Saved new best metric model for epoch 126.
2024-06-21 01:20:57,523 - INFO: Best internal validation val_loss: 1.654 at epoch: 126.
2024-06-21 01:20:57,523 - INFO: Epoch 127/200...
2024-06-21 01:20:57,523 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:20:57,523 - INFO: Batch size: 32.
2024-06-21 01:20:57,527 - INFO: Dataset:
2024-06-21 01:20:57,527 - INFO: Batch size:
2024-06-21 01:20:57,528 - INFO: Number of workers:
2024-06-21 01:20:58,768 - INFO: Epoch: 127/200, Batch: 1/29, Batch_Loss_Train: 1.690
2024-06-21 01:20:59,093 - INFO: Epoch: 127/200, Batch: 2/29, Batch_Loss_Train: 1.814
2024-06-21 01:20:59,508 - INFO: Epoch: 127/200, Batch: 3/29, Batch_Loss_Train: 1.614
2024-06-21 01:20:59,835 - INFO: Epoch: 127/200, Batch: 4/29, Batch_Loss_Train: 1.290
2024-06-21 01:21:00,249 - INFO: Epoch: 127/200, Batch: 5/29, Batch_Loss_Train: 1.250
2024-06-21 01:21:00,569 - INFO: Epoch: 127/200, Batch: 6/29, Batch_Loss_Train: 1.311
2024-06-21 01:21:00,971 - INFO: Epoch: 127/200, Batch: 7/29, Batch_Loss_Train: 1.812
2024-06-21 01:21:01,291 - INFO: Epoch: 127/200, Batch: 8/29, Batch_Loss_Train: 1.707
2024-06-21 01:21:01,690 - INFO: Epoch: 127/200, Batch: 9/29, Batch_Loss_Train: 1.528
2024-06-21 01:21:02,004 - INFO: Epoch: 127/200, Batch: 10/29, Batch_Loss_Train: 1.348
2024-06-21 01:21:02,414 - INFO: Epoch: 127/200, Batch: 11/29, Batch_Loss_Train: 1.381
2024-06-21 01:21:02,737 - INFO: Epoch: 127/200, Batch: 12/29, Batch_Loss_Train: 1.737
2024-06-21 01:21:03,153 - INFO: Epoch: 127/200, Batch: 13/29, Batch_Loss_Train: 1.206
2024-06-21 01:21:03,475 - INFO: Epoch: 127/200, Batch: 14/29, Batch_Loss_Train: 1.146
2024-06-21 01:21:03,890 - INFO: Epoch: 127/200, Batch: 15/29, Batch_Loss_Train: 1.935
2024-06-21 01:21:04,209 - INFO: Epoch: 127/200, Batch: 16/29, Batch_Loss_Train: 1.804
2024-06-21 01:21:04,622 - INFO: Epoch: 127/200, Batch: 17/29, Batch_Loss_Train: 1.226
2024-06-21 01:21:04,941 - INFO: Epoch: 127/200, Batch: 18/29, Batch_Loss_Train: 1.168
2024-06-21 01:21:05,341 - INFO: Epoch: 127/200, Batch: 19/29, Batch_Loss_Train: 1.828
2024-06-21 01:21:05,656 - INFO: Epoch: 127/200, Batch: 20/29, Batch_Loss_Train: 1.381
2024-06-21 01:21:06,064 - INFO: Epoch: 127/200, Batch: 21/29, Batch_Loss_Train: 1.789
2024-06-21 01:21:06,384 - INFO: Epoch: 127/200, Batch: 22/29, Batch_Loss_Train: 1.784
2024-06-21 01:21:06,796 - INFO: Epoch: 127/200, Batch: 23/29, Batch_Loss_Train: 1.545
2024-06-21 01:21:07,118 - INFO: Epoch: 127/200, Batch: 24/29, Batch_Loss_Train: 1.546
2024-06-21 01:21:07,523 - INFO: Epoch: 127/200, Batch: 25/29, Batch_Loss_Train: 1.450
2024-06-21 01:21:07,839 - INFO: Epoch: 127/200, Batch: 26/29, Batch_Loss_Train: 1.407
2024-06-21 01:21:08,237 - INFO: Epoch: 127/200, Batch: 27/29, Batch_Loss_Train: 2.093
2024-06-21 01:21:08,553 - INFO: Epoch: 127/200, Batch: 28/29, Batch_Loss_Train: 1.807
2024-06-21 01:21:08,775 - INFO: Epoch: 127/200, Batch: 29/29, Batch_Loss_Train: 1.830
2024-06-21 01:21:19,800 - INFO: 127/200 final results:
2024-06-21 01:21:19,800 - INFO: Training loss: 1.567.
2024-06-21 01:21:19,800 - INFO: Training MAE: 0.976.
2024-06-21 01:21:19,800 - INFO: Training MSE: 1.561.
2024-06-21 01:21:40,238 - INFO: Epoch: 127/200, Loss_train: 1.5665119639758407, Loss_val: 1.67335241416405
2024-06-21 01:21:40,238 - INFO: Best internal validation val_loss: 1.654 at epoch: 126.
2024-06-21 01:21:40,238 - INFO: Epoch 128/200...
2024-06-21 01:21:40,238 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:21:40,238 - INFO: Batch size: 32.
2024-06-21 01:21:40,242 - INFO: Dataset:
2024-06-21 01:21:40,242 - INFO: Batch size:
2024-06-21 01:21:40,242 - INFO: Number of workers:
2024-06-21 01:21:41,500 - INFO: Epoch: 128/200, Batch: 1/29, Batch_Loss_Train: 1.779
2024-06-21 01:21:41,826 - INFO: Epoch: 128/200, Batch: 2/29, Batch_Loss_Train: 1.130
2024-06-21 01:21:42,230 - INFO: Epoch: 128/200, Batch: 3/29, Batch_Loss_Train: 1.531
2024-06-21 01:21:42,555 - INFO: Epoch: 128/200, Batch: 4/29, Batch_Loss_Train: 1.052
2024-06-21 01:21:42,987 - INFO: Epoch: 128/200, Batch: 5/29, Batch_Loss_Train: 1.451
2024-06-21 01:21:43,292 - INFO: Epoch: 128/200, Batch: 6/29, Batch_Loss_Train: 1.050
2024-06-21 01:21:43,682 - INFO: Epoch: 128/200, Batch: 7/29, Batch_Loss_Train: 1.392
2024-06-21 01:21:43,999 - INFO: Epoch: 128/200, Batch: 8/29, Batch_Loss_Train: 1.003
2024-06-21 01:21:44,438 - INFO: Epoch: 128/200, Batch: 9/29, Batch_Loss_Train: 2.040
2024-06-21 01:21:44,739 - INFO: Epoch: 128/200, Batch: 10/29, Batch_Loss_Train: 1.326
2024-06-21 01:21:45,132 - INFO: Epoch: 128/200, Batch: 11/29, Batch_Loss_Train: 1.339
2024-06-21 01:21:45,455 - INFO: Epoch: 128/200, Batch: 12/29, Batch_Loss_Train: 1.408
2024-06-21 01:21:45,899 - INFO: Epoch: 128/200, Batch: 13/29, Batch_Loss_Train: 1.838
2024-06-21 01:21:46,207 - INFO: Epoch: 128/200, Batch: 14/29, Batch_Loss_Train: 1.511
2024-06-21 01:21:46,612 - INFO: Epoch: 128/200, Batch: 15/29, Batch_Loss_Train: 1.344
2024-06-21 01:21:46,928 - INFO: Epoch: 128/200, Batch: 16/29, Batch_Loss_Train: 1.782
2024-06-21 01:21:47,362 - INFO: Epoch: 128/200, Batch: 17/29, Batch_Loss_Train: 1.478
2024-06-21 01:21:47,668 - INFO: Epoch: 128/200, Batch: 18/29, Batch_Loss_Train: 1.890
2024-06-21 01:21:48,063 - INFO: Epoch: 128/200, Batch: 19/29, Batch_Loss_Train: 1.364
2024-06-21 01:21:48,375 - INFO: Epoch: 128/200, Batch: 20/29, Batch_Loss_Train: 1.194
2024-06-21 01:21:48,807 - INFO: Epoch: 128/200, Batch: 21/29, Batch_Loss_Train: 1.671
2024-06-21 01:21:49,113 - INFO: Epoch: 128/200, Batch: 22/29, Batch_Loss_Train: 1.667
2024-06-21 01:21:49,502 - INFO: Epoch: 128/200, Batch: 23/29, Batch_Loss_Train: 1.586
2024-06-21 01:21:49,820 - INFO: Epoch: 128/200, Batch: 24/29, Batch_Loss_Train: 1.686
2024-06-21 01:21:50,243 - INFO: Epoch: 128/200, Batch: 25/29, Batch_Loss_Train: 1.537
2024-06-21 01:21:50,546 - INFO: Epoch: 128/200, Batch: 26/29, Batch_Loss_Train: 2.130
2024-06-21 01:21:50,932 - INFO: Epoch: 128/200, Batch: 27/29, Batch_Loss_Train: 1.523
2024-06-21 01:21:51,254 - INFO: Epoch: 128/200, Batch: 28/29, Batch_Loss_Train: 1.728
2024-06-21 01:21:51,487 - INFO: Epoch: 128/200, Batch: 29/29, Batch_Loss_Train: 1.027
2024-06-21 01:22:02,604 - INFO: 128/200 final results:
2024-06-21 01:22:02,604 - INFO: Training loss: 1.499.
2024-06-21 01:22:02,604 - INFO: Training MAE: 0.954.
2024-06-21 01:22:02,604 - INFO: Training MSE: 1.508.
2024-06-21 01:22:23,165 - INFO: Epoch: 128/200, Loss_train: 1.4985594831663986, Loss_val: 1.721544477446326
2024-06-21 01:22:23,165 - INFO: Best internal validation val_loss: 1.654 at epoch: 126.
2024-06-21 01:22:23,166 - INFO: Epoch 129/200...
2024-06-21 01:22:23,166 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:22:23,166 - INFO: Batch size: 32.
2024-06-21 01:22:23,170 - INFO: Dataset:
2024-06-21 01:22:23,170 - INFO: Batch size:
2024-06-21 01:22:23,170 - INFO: Number of workers:
2024-06-21 01:22:24,508 - INFO: Epoch: 129/200, Batch: 1/29, Batch_Loss_Train: 1.368
2024-06-21 01:22:24,819 - INFO: Epoch: 129/200, Batch: 2/29, Batch_Loss_Train: 1.247
2024-06-21 01:22:25,223 - INFO: Epoch: 129/200, Batch: 3/29, Batch_Loss_Train: 1.743
2024-06-21 01:22:25,533 - INFO: Epoch: 129/200, Batch: 4/29, Batch_Loss_Train: 1.577
2024-06-21 01:22:25,981 - INFO: Epoch: 129/200, Batch: 5/29, Batch_Loss_Train: 1.661
2024-06-21 01:22:26,287 - INFO: Epoch: 129/200, Batch: 6/29, Batch_Loss_Train: 1.397
2024-06-21 01:22:26,668 - INFO: Epoch: 129/200, Batch: 7/29, Batch_Loss_Train: 2.026
2024-06-21 01:22:26,974 - INFO: Epoch: 129/200, Batch: 8/29, Batch_Loss_Train: 1.932
2024-06-21 01:22:27,422 - INFO: Epoch: 129/200, Batch: 9/29, Batch_Loss_Train: 1.659
2024-06-21 01:22:27,724 - INFO: Epoch: 129/200, Batch: 10/29, Batch_Loss_Train: 1.589
2024-06-21 01:22:28,106 - INFO: Epoch: 129/200, Batch: 11/29, Batch_Loss_Train: 1.605
2024-06-21 01:22:28,416 - INFO: Epoch: 129/200, Batch: 12/29, Batch_Loss_Train: 1.764
2024-06-21 01:22:28,873 - INFO: Epoch: 129/200, Batch: 13/29, Batch_Loss_Train: 1.532
2024-06-21 01:22:29,182 - INFO: Epoch: 129/200, Batch: 14/29, Batch_Loss_Train: 1.426
2024-06-21 01:22:29,576 - INFO: Epoch: 129/200, Batch: 15/29, Batch_Loss_Train: 1.381
2024-06-21 01:22:29,881 - INFO: Epoch: 129/200, Batch: 16/29, Batch_Loss_Train: 1.640
2024-06-21 01:22:30,327 - INFO: Epoch: 129/200, Batch: 17/29, Batch_Loss_Train: 1.120
2024-06-21 01:22:30,633 - INFO: Epoch: 129/200, Batch: 18/29, Batch_Loss_Train: 1.521
2024-06-21 01:22:31,012 - INFO: Epoch: 129/200, Batch: 19/29, Batch_Loss_Train: 1.447
2024-06-21 01:22:31,314 - INFO: Epoch: 129/200, Batch: 20/29, Batch_Loss_Train: 1.203
2024-06-21 01:22:31,755 - INFO: Epoch: 129/200, Batch: 21/29, Batch_Loss_Train: 1.262
2024-06-21 01:22:32,064 - INFO: Epoch: 129/200, Batch: 22/29, Batch_Loss_Train: 1.850
2024-06-21 01:22:32,460 - INFO: Epoch: 129/200, Batch: 23/29, Batch_Loss_Train: 1.630
2024-06-21 01:22:32,768 - INFO: Epoch: 129/200, Batch: 24/29, Batch_Loss_Train: 1.965
2024-06-21 01:22:33,211 - INFO: Epoch: 129/200, Batch: 25/29, Batch_Loss_Train: 1.283
2024-06-21 01:22:33,514 - INFO: Epoch: 129/200, Batch: 26/29, Batch_Loss_Train: 1.894
2024-06-21 01:22:33,905 - INFO: Epoch: 129/200, Batch: 27/29, Batch_Loss_Train: 1.495
2024-06-21 01:22:34,209 - INFO: Epoch: 129/200, Batch: 28/29, Batch_Loss_Train: 1.522
2024-06-21 01:22:34,430 - INFO: Epoch: 129/200, Batch: 29/29, Batch_Loss_Train: 1.657
2024-06-21 01:22:45,451 - INFO: 129/200 final results:
2024-06-21 01:22:45,451 - INFO: Training loss: 1.565.
2024-06-21 01:22:45,451 - INFO: Training MAE: 0.989.
2024-06-21 01:22:45,451 - INFO: Training MSE: 1.564.
2024-06-21 01:23:06,310 - INFO: Epoch: 129/200, Loss_train: 1.565310576866413, Loss_val: 1.646234943948943
2024-06-21 01:23:06,357 - INFO: Saved new best metric model for epoch 129.
2024-06-21 01:23:06,357 - INFO: Best internal validation val_loss: 1.646 at epoch: 129.
2024-06-21 01:23:06,357 - INFO: Epoch 130/200...
2024-06-21 01:23:06,357 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:23:06,357 - INFO: Batch size: 32.
2024-06-21 01:23:06,362 - INFO: Dataset:
2024-06-21 01:23:06,362 - INFO: Batch size:
2024-06-21 01:23:06,362 - INFO: Number of workers:
2024-06-21 01:23:07,641 - INFO: Epoch: 130/200, Batch: 1/29, Batch_Loss_Train: 1.694
2024-06-21 01:23:07,966 - INFO: Epoch: 130/200, Batch: 2/29, Batch_Loss_Train: 2.029
2024-06-21 01:23:08,386 - INFO: Epoch: 130/200, Batch: 3/29, Batch_Loss_Train: 2.066
2024-06-21 01:23:08,709 - INFO: Epoch: 130/200, Batch: 4/29, Batch_Loss_Train: 1.230
2024-06-21 01:23:09,122 - INFO: Epoch: 130/200, Batch: 5/29, Batch_Loss_Train: 1.440
2024-06-21 01:23:09,426 - INFO: Epoch: 130/200, Batch: 6/29, Batch_Loss_Train: 1.380
2024-06-21 01:23:09,830 - INFO: Epoch: 130/200, Batch: 7/29, Batch_Loss_Train: 1.592
2024-06-21 01:23:10,148 - INFO: Epoch: 130/200, Batch: 8/29, Batch_Loss_Train: 1.483
2024-06-21 01:23:10,550 - INFO: Epoch: 130/200, Batch: 9/29, Batch_Loss_Train: 1.870
2024-06-21 01:23:10,849 - INFO: Epoch: 130/200, Batch: 10/29, Batch_Loss_Train: 1.780
2024-06-21 01:23:11,252 - INFO: Epoch: 130/200, Batch: 11/29, Batch_Loss_Train: 1.867
2024-06-21 01:23:11,571 - INFO: Epoch: 130/200, Batch: 12/29, Batch_Loss_Train: 1.033
2024-06-21 01:23:12,000 - INFO: Epoch: 130/200, Batch: 13/29, Batch_Loss_Train: 1.844
2024-06-21 01:23:12,307 - INFO: Epoch: 130/200, Batch: 14/29, Batch_Loss_Train: 1.855
2024-06-21 01:23:12,721 - INFO: Epoch: 130/200, Batch: 15/29, Batch_Loss_Train: 1.441
2024-06-21 01:23:13,034 - INFO: Epoch: 130/200, Batch: 16/29, Batch_Loss_Train: 1.612
2024-06-21 01:23:13,451 - INFO: Epoch: 130/200, Batch: 17/29, Batch_Loss_Train: 1.588
2024-06-21 01:23:13,752 - INFO: Epoch: 130/200, Batch: 18/29, Batch_Loss_Train: 1.261
2024-06-21 01:23:14,157 - INFO: Epoch: 130/200, Batch: 19/29, Batch_Loss_Train: 1.636
2024-06-21 01:23:14,467 - INFO: Epoch: 130/200, Batch: 20/29, Batch_Loss_Train: 1.202
2024-06-21 01:23:14,878 - INFO: Epoch: 130/200, Batch: 21/29, Batch_Loss_Train: 1.230
2024-06-21 01:23:15,183 - INFO: Epoch: 130/200, Batch: 22/29, Batch_Loss_Train: 1.406
2024-06-21 01:23:15,592 - INFO: Epoch: 130/200, Batch: 23/29, Batch_Loss_Train: 1.618
2024-06-21 01:23:15,910 - INFO: Epoch: 130/200, Batch: 24/29, Batch_Loss_Train: 1.456
2024-06-21 01:23:16,323 - INFO: Epoch: 130/200, Batch: 25/29, Batch_Loss_Train: 1.647
2024-06-21 01:23:16,624 - INFO: Epoch: 130/200, Batch: 26/29, Batch_Loss_Train: 1.238
2024-06-21 01:23:17,022 - INFO: Epoch: 130/200, Batch: 27/29, Batch_Loss_Train: 1.483
2024-06-21 01:23:17,336 - INFO: Epoch: 130/200, Batch: 28/29, Batch_Loss_Train: 1.755
2024-06-21 01:23:17,553 - INFO: Epoch: 130/200, Batch: 29/29, Batch_Loss_Train: 1.570
2024-06-21 01:23:28,569 - INFO: 130/200 final results:
2024-06-21 01:23:28,569 - INFO: Training loss: 1.562.
2024-06-21 01:23:28,569 - INFO: Training MAE: 0.972.
2024-06-21 01:23:28,569 - INFO: Training MSE: 1.562.
2024-06-21 01:23:48,867 - INFO: Epoch: 130/200, Loss_train: 1.5623096679819042, Loss_val: 1.6357629073077236
2024-06-21 01:23:48,913 - INFO: Saved new best metric model for epoch 130.
2024-06-21 01:23:48,913 - INFO: Best internal validation val_loss: 1.636 at epoch: 130.
2024-06-21 01:23:48,913 - INFO: Epoch 131/200...
2024-06-21 01:23:48,913 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:23:48,913 - INFO: Batch size: 32.
2024-06-21 01:23:48,918 - INFO: Dataset:
2024-06-21 01:23:48,918 - INFO: Batch size:
2024-06-21 01:23:48,918 - INFO: Number of workers:
2024-06-21 01:23:50,165 - INFO: Epoch: 131/200, Batch: 1/29, Batch_Loss_Train: 1.901
2024-06-21 01:23:50,491 - INFO: Epoch: 131/200, Batch: 2/29, Batch_Loss_Train: 1.293
2024-06-21 01:23:50,909 - INFO: Epoch: 131/200, Batch: 3/29, Batch_Loss_Train: 1.270
2024-06-21 01:23:51,235 - INFO: Epoch: 131/200, Batch: 4/29, Batch_Loss_Train: 1.558
2024-06-21 01:23:51,659 - INFO: Epoch: 131/200, Batch: 5/29, Batch_Loss_Train: 1.198
2024-06-21 01:23:51,966 - INFO: Epoch: 131/200, Batch: 6/29, Batch_Loss_Train: 1.635
2024-06-21 01:23:52,365 - INFO: Epoch: 131/200, Batch: 7/29, Batch_Loss_Train: 1.195
2024-06-21 01:23:52,685 - INFO: Epoch: 131/200, Batch: 8/29, Batch_Loss_Train: 1.368
2024-06-21 01:23:53,100 - INFO: Epoch: 131/200, Batch: 9/29, Batch_Loss_Train: 1.917
2024-06-21 01:23:53,401 - INFO: Epoch: 131/200, Batch: 10/29, Batch_Loss_Train: 1.599
2024-06-21 01:23:53,807 - INFO: Epoch: 131/200, Batch: 11/29, Batch_Loss_Train: 1.379
2024-06-21 01:23:54,130 - INFO: Epoch: 131/200, Batch: 12/29, Batch_Loss_Train: 2.371
2024-06-21 01:23:54,564 - INFO: Epoch: 131/200, Batch: 13/29, Batch_Loss_Train: 1.939
2024-06-21 01:23:54,875 - INFO: Epoch: 131/200, Batch: 14/29, Batch_Loss_Train: 1.161
2024-06-21 01:23:55,285 - INFO: Epoch: 131/200, Batch: 15/29, Batch_Loss_Train: 1.544
2024-06-21 01:23:55,604 - INFO: Epoch: 131/200, Batch: 16/29, Batch_Loss_Train: 1.573
2024-06-21 01:23:56,029 - INFO: Epoch: 131/200, Batch: 17/29, Batch_Loss_Train: 1.583
2024-06-21 01:23:56,334 - INFO: Epoch: 131/200, Batch: 18/29, Batch_Loss_Train: 1.363
2024-06-21 01:23:56,736 - INFO: Epoch: 131/200, Batch: 19/29, Batch_Loss_Train: 1.468
2024-06-21 01:23:57,050 - INFO: Epoch: 131/200, Batch: 20/29, Batch_Loss_Train: 0.954
2024-06-21 01:23:57,471 - INFO: Epoch: 131/200, Batch: 21/29, Batch_Loss_Train: 2.015
2024-06-21 01:23:57,779 - INFO: Epoch: 131/200, Batch: 22/29, Batch_Loss_Train: 1.361
2024-06-21 01:23:58,183 - INFO: Epoch: 131/200, Batch: 23/29, Batch_Loss_Train: 1.241
2024-06-21 01:23:58,504 - INFO: Epoch: 131/200, Batch: 24/29, Batch_Loss_Train: 1.227
2024-06-21 01:23:58,918 - INFO: Epoch: 131/200, Batch: 25/29, Batch_Loss_Train: 1.830
2024-06-21 01:23:59,218 - INFO: Epoch: 131/200, Batch: 26/29, Batch_Loss_Train: 1.938
2024-06-21 01:23:59,616 - INFO: Epoch: 131/200, Batch: 27/29, Batch_Loss_Train: 1.830
2024-06-21 01:23:59,928 - INFO: Epoch: 131/200, Batch: 28/29, Batch_Loss_Train: 1.445
2024-06-21 01:24:00,141 - INFO: Epoch: 131/200, Batch: 29/29, Batch_Loss_Train: 1.572
2024-06-21 01:24:11,087 - INFO: 131/200 final results:
2024-06-21 01:24:11,087 - INFO: Training loss: 1.542.
2024-06-21 01:24:11,087 - INFO: Training MAE: 0.962.
2024-06-21 01:24:11,087 - INFO: Training MSE: 1.542.
2024-06-21 01:24:31,325 - INFO: Epoch: 131/200, Loss_train: 1.5422560613730858, Loss_val: 1.6383123027867283
2024-06-21 01:24:31,325 - INFO: Best internal validation val_loss: 1.636 at epoch: 130.
2024-06-21 01:24:31,325 - INFO: Epoch 132/200...
2024-06-21 01:24:31,325 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:24:31,325 - INFO: Batch size: 32.
2024-06-21 01:24:31,329 - INFO: Dataset:
2024-06-21 01:24:31,329 - INFO: Batch size:
2024-06-21 01:24:31,329 - INFO: Number of workers:
2024-06-21 01:24:32,589 - INFO: Epoch: 132/200, Batch: 1/29, Batch_Loss_Train: 1.412
2024-06-21 01:24:32,902 - INFO: Epoch: 132/200, Batch: 2/29, Batch_Loss_Train: 1.257
2024-06-21 01:24:33,306 - INFO: Epoch: 132/200, Batch: 3/29, Batch_Loss_Train: 1.683
2024-06-21 01:24:33,633 - INFO: Epoch: 132/200, Batch: 4/29, Batch_Loss_Train: 1.428
2024-06-21 01:24:34,053 - INFO: Epoch: 132/200, Batch: 5/29, Batch_Loss_Train: 1.499
2024-06-21 01:24:34,360 - INFO: Epoch: 132/200, Batch: 6/29, Batch_Loss_Train: 1.762
2024-06-21 01:24:34,752 - INFO: Epoch: 132/200, Batch: 7/29, Batch_Loss_Train: 1.720
2024-06-21 01:24:35,071 - INFO: Epoch: 132/200, Batch: 8/29, Batch_Loss_Train: 1.658
2024-06-21 01:24:35,482 - INFO: Epoch: 132/200, Batch: 9/29, Batch_Loss_Train: 1.452
2024-06-21 01:24:35,783 - INFO: Epoch: 132/200, Batch: 10/29, Batch_Loss_Train: 1.507
2024-06-21 01:24:36,175 - INFO: Epoch: 132/200, Batch: 11/29, Batch_Loss_Train: 1.293
2024-06-21 01:24:36,497 - INFO: Epoch: 132/200, Batch: 12/29, Batch_Loss_Train: 1.585
2024-06-21 01:24:36,927 - INFO: Epoch: 132/200, Batch: 13/29, Batch_Loss_Train: 1.475
2024-06-21 01:24:37,236 - INFO: Epoch: 132/200, Batch: 14/29, Batch_Loss_Train: 1.719
2024-06-21 01:24:37,640 - INFO: Epoch: 132/200, Batch: 15/29, Batch_Loss_Train: 1.163
2024-06-21 01:24:37,958 - INFO: Epoch: 132/200, Batch: 16/29, Batch_Loss_Train: 1.358
2024-06-21 01:24:38,375 - INFO: Epoch: 132/200, Batch: 17/29, Batch_Loss_Train: 1.221
2024-06-21 01:24:38,680 - INFO: Epoch: 132/200, Batch: 18/29, Batch_Loss_Train: 1.180
2024-06-21 01:24:39,072 - INFO: Epoch: 132/200, Batch: 19/29, Batch_Loss_Train: 1.481
2024-06-21 01:24:39,387 - INFO: Epoch: 132/200, Batch: 20/29, Batch_Loss_Train: 1.604
2024-06-21 01:24:39,803 - INFO: Epoch: 132/200, Batch: 21/29, Batch_Loss_Train: 1.546
2024-06-21 01:24:40,111 - INFO: Epoch: 132/200, Batch: 22/29, Batch_Loss_Train: 1.819
2024-06-21 01:24:40,511 - INFO: Epoch: 132/200, Batch: 23/29, Batch_Loss_Train: 1.587
2024-06-21 01:24:40,831 - INFO: Epoch: 132/200, Batch: 24/29, Batch_Loss_Train: 1.565
2024-06-21 01:24:41,237 - INFO: Epoch: 132/200, Batch: 25/29, Batch_Loss_Train: 1.514
2024-06-21 01:24:41,539 - INFO: Epoch: 132/200, Batch: 26/29, Batch_Loss_Train: 1.148
2024-06-21 01:24:41,924 - INFO: Epoch: 132/200, Batch: 27/29, Batch_Loss_Train: 1.509
2024-06-21 01:24:42,240 - INFO: Epoch: 132/200, Batch: 28/29, Batch_Loss_Train: 1.785
2024-06-21 01:24:42,451 - INFO: Epoch: 132/200, Batch: 29/29, Batch_Loss_Train: 0.863
2024-06-21 01:24:53,493 - INFO: 132/200 final results:
2024-06-21 01:24:53,493 - INFO: Training loss: 1.476.
2024-06-21 01:24:53,493 - INFO: Training MAE: 0.956.
2024-06-21 01:24:53,493 - INFO: Training MSE: 1.488.
2024-06-21 01:25:13,781 - INFO: Epoch: 132/200, Loss_train: 1.4756671313581795, Loss_val: 1.6451608661947579
2024-06-21 01:25:13,782 - INFO: Best internal validation val_loss: 1.636 at epoch: 130.
2024-06-21 01:25:13,782 - INFO: Epoch 133/200...
2024-06-21 01:25:13,782 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:25:13,782 - INFO: Batch size: 32.
2024-06-21 01:25:13,786 - INFO: Dataset:
2024-06-21 01:25:13,786 - INFO: Batch size:
2024-06-21 01:25:13,786 - INFO: Number of workers:
2024-06-21 01:25:15,027 - INFO: Epoch: 133/200, Batch: 1/29, Batch_Loss_Train: 1.119
2024-06-21 01:25:15,356 - INFO: Epoch: 133/200, Batch: 2/29, Batch_Loss_Train: 1.122
2024-06-21 01:25:15,776 - INFO: Epoch: 133/200, Batch: 3/29, Batch_Loss_Train: 0.933
2024-06-21 01:25:16,103 - INFO: Epoch: 133/200, Batch: 4/29, Batch_Loss_Train: 1.473
2024-06-21 01:25:16,502 - INFO: Epoch: 133/200, Batch: 5/29, Batch_Loss_Train: 1.342
2024-06-21 01:25:16,822 - INFO: Epoch: 133/200, Batch: 6/29, Batch_Loss_Train: 1.555
2024-06-21 01:25:17,225 - INFO: Epoch: 133/200, Batch: 7/29, Batch_Loss_Train: 1.467
2024-06-21 01:25:17,545 - INFO: Epoch: 133/200, Batch: 8/29, Batch_Loss_Train: 1.454
2024-06-21 01:25:17,949 - INFO: Epoch: 133/200, Batch: 9/29, Batch_Loss_Train: 0.990
2024-06-21 01:25:18,263 - INFO: Epoch: 133/200, Batch: 10/29, Batch_Loss_Train: 1.774
2024-06-21 01:25:18,667 - INFO: Epoch: 133/200, Batch: 11/29, Batch_Loss_Train: 1.032
2024-06-21 01:25:18,991 - INFO: Epoch: 133/200, Batch: 12/29, Batch_Loss_Train: 2.262
2024-06-21 01:25:19,414 - INFO: Epoch: 133/200, Batch: 13/29, Batch_Loss_Train: 1.271
2024-06-21 01:25:19,737 - INFO: Epoch: 133/200, Batch: 14/29, Batch_Loss_Train: 1.836
2024-06-21 01:25:20,153 - INFO: Epoch: 133/200, Batch: 15/29, Batch_Loss_Train: 1.874
2024-06-21 01:25:20,471 - INFO: Epoch: 133/200, Batch: 16/29, Batch_Loss_Train: 1.447
2024-06-21 01:25:20,865 - INFO: Epoch: 133/200, Batch: 17/29, Batch_Loss_Train: 1.792
2024-06-21 01:25:21,183 - INFO: Epoch: 133/200, Batch: 18/29, Batch_Loss_Train: 1.964
2024-06-21 01:25:21,583 - INFO: Epoch: 133/200, Batch: 19/29, Batch_Loss_Train: 2.356
2024-06-21 01:25:21,896 - INFO: Epoch: 133/200, Batch: 20/29, Batch_Loss_Train: 1.697
2024-06-21 01:25:22,309 - INFO: Epoch: 133/200, Batch: 21/29, Batch_Loss_Train: 1.695
2024-06-21 01:25:22,630 - INFO: Epoch: 133/200, Batch: 22/29, Batch_Loss_Train: 1.789
2024-06-21 01:25:23,042 - INFO: Epoch: 133/200, Batch: 23/29, Batch_Loss_Train: 1.560
2024-06-21 01:25:23,363 - INFO: Epoch: 133/200, Batch: 24/29, Batch_Loss_Train: 1.286
2024-06-21 01:25:23,765 - INFO: Epoch: 133/200, Batch: 25/29, Batch_Loss_Train: 1.553
2024-06-21 01:25:24,081 - INFO: Epoch: 133/200, Batch: 26/29, Batch_Loss_Train: 1.357
2024-06-21 01:25:24,471 - INFO: Epoch: 133/200, Batch: 27/29, Batch_Loss_Train: 1.680
2024-06-21 01:25:24,787 - INFO: Epoch: 133/200, Batch: 28/29, Batch_Loss_Train: 1.654
2024-06-21 01:25:25,012 - INFO: Epoch: 133/200, Batch: 29/29, Batch_Loss_Train: 1.494
2024-06-21 01:25:35,963 - INFO: 133/200 final results:
2024-06-21 01:25:35,963 - INFO: Training loss: 1.546.
2024-06-21 01:25:35,964 - INFO: Training MAE: 0.949.
2024-06-21 01:25:35,964 - INFO: Training MSE: 1.547.
2024-06-21 01:25:56,611 - INFO: Epoch: 133/200, Loss_train: 1.5458187296472747, Loss_val: 1.8291338127234886
2024-06-21 01:25:56,611 - INFO: Best internal validation val_loss: 1.636 at epoch: 130.
2024-06-21 01:25:56,611 - INFO: Epoch 134/200...
2024-06-21 01:25:56,611 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:25:56,612 - INFO: Batch size: 32.
2024-06-21 01:25:56,616 - INFO: Dataset:
2024-06-21 01:25:56,616 - INFO: Batch size:
2024-06-21 01:25:56,616 - INFO: Number of workers:
2024-06-21 01:25:57,887 - INFO: Epoch: 134/200, Batch: 1/29, Batch_Loss_Train: 1.354
2024-06-21 01:25:58,203 - INFO: Epoch: 134/200, Batch: 2/29, Batch_Loss_Train: 1.460
2024-06-21 01:25:58,616 - INFO: Epoch: 134/200, Batch: 3/29, Batch_Loss_Train: 1.573
2024-06-21 01:25:58,941 - INFO: Epoch: 134/200, Batch: 4/29, Batch_Loss_Train: 1.447
2024-06-21 01:25:59,363 - INFO: Epoch: 134/200, Batch: 5/29, Batch_Loss_Train: 1.261
2024-06-21 01:25:59,671 - INFO: Epoch: 134/200, Batch: 6/29, Batch_Loss_Train: 1.446
2024-06-21 01:26:00,064 - INFO: Epoch: 134/200, Batch: 7/29, Batch_Loss_Train: 1.664
2024-06-21 01:26:00,383 - INFO: Epoch: 134/200, Batch: 8/29, Batch_Loss_Train: 1.390
2024-06-21 01:26:00,798 - INFO: Epoch: 134/200, Batch: 9/29, Batch_Loss_Train: 1.253
2024-06-21 01:26:01,100 - INFO: Epoch: 134/200, Batch: 10/29, Batch_Loss_Train: 1.985
2024-06-21 01:26:01,501 - INFO: Epoch: 134/200, Batch: 11/29, Batch_Loss_Train: 1.701
2024-06-21 01:26:01,824 - INFO: Epoch: 134/200, Batch: 12/29, Batch_Loss_Train: 1.359
2024-06-21 01:26:02,253 - INFO: Epoch: 134/200, Batch: 13/29, Batch_Loss_Train: 1.783
2024-06-21 01:26:02,560 - INFO: Epoch: 134/200, Batch: 14/29, Batch_Loss_Train: 1.301
2024-06-21 01:26:02,971 - INFO: Epoch: 134/200, Batch: 15/29, Batch_Loss_Train: 1.219
2024-06-21 01:26:03,286 - INFO: Epoch: 134/200, Batch: 16/29, Batch_Loss_Train: 1.484
2024-06-21 01:26:03,707 - INFO: Epoch: 134/200, Batch: 17/29, Batch_Loss_Train: 1.531
2024-06-21 01:26:04,010 - INFO: Epoch: 134/200, Batch: 18/29, Batch_Loss_Train: 1.250
2024-06-21 01:26:04,407 - INFO: Epoch: 134/200, Batch: 19/29, Batch_Loss_Train: 1.379
2024-06-21 01:26:04,723 - INFO: Epoch: 134/200, Batch: 20/29, Batch_Loss_Train: 1.850
2024-06-21 01:26:05,141 - INFO: Epoch: 134/200, Batch: 21/29, Batch_Loss_Train: 1.528
2024-06-21 01:26:05,450 - INFO: Epoch: 134/200, Batch: 22/29, Batch_Loss_Train: 1.184
2024-06-21 01:26:05,853 - INFO: Epoch: 134/200, Batch: 23/29, Batch_Loss_Train: 1.473
2024-06-21 01:26:06,174 - INFO: Epoch: 134/200, Batch: 24/29, Batch_Loss_Train: 1.323
2024-06-21 01:26:06,585 - INFO: Epoch: 134/200, Batch: 25/29, Batch_Loss_Train: 1.183
2024-06-21 01:26:06,895 - INFO: Epoch: 134/200, Batch: 26/29, Batch_Loss_Train: 1.119
2024-06-21 01:26:07,316 - INFO: Epoch: 134/200, Batch: 27/29, Batch_Loss_Train: 1.471
2024-06-21 01:26:07,638 - INFO: Epoch: 134/200, Batch: 28/29, Batch_Loss_Train: 1.639
2024-06-21 01:26:07,871 - INFO: Epoch: 134/200, Batch: 29/29, Batch_Loss_Train: 0.871
2024-06-21 01:26:18,890 - INFO: 134/200 final results:
2024-06-21 01:26:18,890 - INFO: Training loss: 1.430.
2024-06-21 01:26:18,890 - INFO: Training MAE: 0.934.
2024-06-21 01:26:18,890 - INFO: Training MSE: 1.441.
2024-06-21 01:26:39,537 - INFO: Epoch: 134/200, Loss_train: 1.4303789467647159, Loss_val: 1.641873988611945
2024-06-21 01:26:39,537 - INFO: Best internal validation val_loss: 1.636 at epoch: 130.
2024-06-21 01:26:39,537 - INFO: Epoch 135/200...
2024-06-21 01:26:39,537 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:26:39,537 - INFO: Batch size: 32.
2024-06-21 01:26:39,541 - INFO: Dataset:
2024-06-21 01:26:39,541 - INFO: Batch size:
2024-06-21 01:26:39,542 - INFO: Number of workers:
2024-06-21 01:26:40,802 - INFO: Epoch: 135/200, Batch: 1/29, Batch_Loss_Train: 2.013
2024-06-21 01:26:41,115 - INFO: Epoch: 135/200, Batch: 2/29, Batch_Loss_Train: 1.754
2024-06-21 01:26:41,529 - INFO: Epoch: 135/200, Batch: 3/29, Batch_Loss_Train: 1.128
2024-06-21 01:26:41,853 - INFO: Epoch: 135/200, Batch: 4/29, Batch_Loss_Train: 1.476
2024-06-21 01:26:42,272 - INFO: Epoch: 135/200, Batch: 5/29, Batch_Loss_Train: 1.561
2024-06-21 01:26:42,577 - INFO: Epoch: 135/200, Batch: 6/29, Batch_Loss_Train: 1.411
2024-06-21 01:26:42,973 - INFO: Epoch: 135/200, Batch: 7/29, Batch_Loss_Train: 1.231
2024-06-21 01:26:43,290 - INFO: Epoch: 135/200, Batch: 8/29, Batch_Loss_Train: 1.431
2024-06-21 01:26:43,704 - INFO: Epoch: 135/200, Batch: 9/29, Batch_Loss_Train: 1.344
2024-06-21 01:26:44,003 - INFO: Epoch: 135/200, Batch: 10/29, Batch_Loss_Train: 2.327
2024-06-21 01:26:44,397 - INFO: Epoch: 135/200, Batch: 11/29, Batch_Loss_Train: 1.075
2024-06-21 01:26:44,717 - INFO: Epoch: 135/200, Batch: 12/29, Batch_Loss_Train: 1.637
2024-06-21 01:26:45,149 - INFO: Epoch: 135/200, Batch: 13/29, Batch_Loss_Train: 1.448
2024-06-21 01:26:45,460 - INFO: Epoch: 135/200, Batch: 14/29, Batch_Loss_Train: 1.346
2024-06-21 01:26:45,870 - INFO: Epoch: 135/200, Batch: 15/29, Batch_Loss_Train: 1.924
2024-06-21 01:26:46,187 - INFO: Epoch: 135/200, Batch: 16/29, Batch_Loss_Train: 1.309
2024-06-21 01:26:46,609 - INFO: Epoch: 135/200, Batch: 17/29, Batch_Loss_Train: 1.874
2024-06-21 01:26:46,915 - INFO: Epoch: 135/200, Batch: 18/29, Batch_Loss_Train: 1.220
2024-06-21 01:26:47,310 - INFO: Epoch: 135/200, Batch: 19/29, Batch_Loss_Train: 1.320
2024-06-21 01:26:47,623 - INFO: Epoch: 135/200, Batch: 20/29, Batch_Loss_Train: 1.995
2024-06-21 01:26:48,039 - INFO: Epoch: 135/200, Batch: 21/29, Batch_Loss_Train: 1.550
2024-06-21 01:26:48,347 - INFO: Epoch: 135/200, Batch: 22/29, Batch_Loss_Train: 1.655
2024-06-21 01:26:48,760 - INFO: Epoch: 135/200, Batch: 23/29, Batch_Loss_Train: 1.800
2024-06-21 01:26:49,081 - INFO: Epoch: 135/200, Batch: 24/29, Batch_Loss_Train: 1.441
2024-06-21 01:26:49,492 - INFO: Epoch: 135/200, Batch: 25/29, Batch_Loss_Train: 1.364
2024-06-21 01:26:49,795 - INFO: Epoch: 135/200, Batch: 26/29, Batch_Loss_Train: 1.429
2024-06-21 01:26:50,197 - INFO: Epoch: 135/200, Batch: 27/29, Batch_Loss_Train: 1.287
2024-06-21 01:26:50,513 - INFO: Epoch: 135/200, Batch: 28/29, Batch_Loss_Train: 1.661
2024-06-21 01:26:50,737 - INFO: Epoch: 135/200, Batch: 29/29, Batch_Loss_Train: 1.633
2024-06-21 01:27:01,523 - INFO: 135/200 final results:
2024-06-21 01:27:01,524 - INFO: Training loss: 1.539.
2024-06-21 01:27:01,524 - INFO: Training MAE: 0.957.
2024-06-21 01:27:01,524 - INFO: Training MSE: 1.538.
2024-06-21 01:27:22,077 - INFO: Epoch: 135/200, Loss_train: 1.5393876988312294, Loss_val: 1.6308602731803368
2024-06-21 01:27:22,127 - INFO: Saved new best metric model for epoch 135.
2024-06-21 01:27:22,127 - INFO: Best internal validation val_loss: 1.631 at epoch: 135.
2024-06-21 01:27:22,127 - INFO: Epoch 136/200...
2024-06-21 01:27:22,127 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:27:22,127 - INFO: Batch size: 32.
2024-06-21 01:27:22,130 - INFO: Dataset:
2024-06-21 01:27:22,131 - INFO: Batch size:
2024-06-21 01:27:22,131 - INFO: Number of workers:
2024-06-21 01:27:23,395 - INFO: Epoch: 136/200, Batch: 1/29, Batch_Loss_Train: 1.484
2024-06-21 01:27:23,708 - INFO: Epoch: 136/200, Batch: 2/29, Batch_Loss_Train: 1.389
2024-06-21 01:27:24,117 - INFO: Epoch: 136/200, Batch: 3/29, Batch_Loss_Train: 1.004
2024-06-21 01:27:24,444 - INFO: Epoch: 136/200, Batch: 4/29, Batch_Loss_Train: 1.988
2024-06-21 01:27:24,882 - INFO: Epoch: 136/200, Batch: 5/29, Batch_Loss_Train: 1.395
2024-06-21 01:27:25,190 - INFO: Epoch: 136/200, Batch: 6/29, Batch_Loss_Train: 1.492
2024-06-21 01:27:25,586 - INFO: Epoch: 136/200, Batch: 7/29, Batch_Loss_Train: 1.581
2024-06-21 01:27:25,906 - INFO: Epoch: 136/200, Batch: 8/29, Batch_Loss_Train: 1.450
2024-06-21 01:27:26,337 - INFO: Epoch: 136/200, Batch: 9/29, Batch_Loss_Train: 2.361
2024-06-21 01:27:26,638 - INFO: Epoch: 136/200, Batch: 10/29, Batch_Loss_Train: 1.317
2024-06-21 01:27:27,033 - INFO: Epoch: 136/200, Batch: 11/29, Batch_Loss_Train: 1.981
2024-06-21 01:27:27,356 - INFO: Epoch: 136/200, Batch: 12/29, Batch_Loss_Train: 2.003
2024-06-21 01:27:27,804 - INFO: Epoch: 136/200, Batch: 13/29, Batch_Loss_Train: 1.390
2024-06-21 01:27:28,115 - INFO: Epoch: 136/200, Batch: 14/29, Batch_Loss_Train: 1.592
2024-06-21 01:27:28,522 - INFO: Epoch: 136/200, Batch: 15/29, Batch_Loss_Train: 1.534
2024-06-21 01:27:28,842 - INFO: Epoch: 136/200, Batch: 16/29, Batch_Loss_Train: 1.565
2024-06-21 01:27:29,274 - INFO: Epoch: 136/200, Batch: 17/29, Batch_Loss_Train: 1.159
2024-06-21 01:27:29,579 - INFO: Epoch: 136/200, Batch: 18/29, Batch_Loss_Train: 1.216
2024-06-21 01:27:29,972 - INFO: Epoch: 136/200, Batch: 19/29, Batch_Loss_Train: 2.242
2024-06-21 01:27:30,285 - INFO: Epoch: 136/200, Batch: 20/29, Batch_Loss_Train: 1.752
2024-06-21 01:27:30,715 - INFO: Epoch: 136/200, Batch: 21/29, Batch_Loss_Train: 1.662
2024-06-21 01:27:31,022 - INFO: Epoch: 136/200, Batch: 22/29, Batch_Loss_Train: 1.228
2024-06-21 01:27:31,416 - INFO: Epoch: 136/200, Batch: 23/29, Batch_Loss_Train: 1.120
2024-06-21 01:27:31,736 - INFO: Epoch: 136/200, Batch: 24/29, Batch_Loss_Train: 1.715
2024-06-21 01:27:32,160 - INFO: Epoch: 136/200, Batch: 25/29, Batch_Loss_Train: 0.973
2024-06-21 01:27:32,464 - INFO: Epoch: 136/200, Batch: 26/29, Batch_Loss_Train: 1.680
2024-06-21 01:27:32,848 - INFO: Epoch: 136/200, Batch: 27/29, Batch_Loss_Train: 1.154
2024-06-21 01:27:33,164 - INFO: Epoch: 136/200, Batch: 28/29, Batch_Loss_Train: 1.692
2024-06-21 01:27:33,381 - INFO: Epoch: 136/200, Batch: 29/29, Batch_Loss_Train: 1.461
2024-06-21 01:27:44,557 - INFO: 136/200 final results:
2024-06-21 01:27:44,557 - INFO: Training loss: 1.537.
2024-06-21 01:27:44,557 - INFO: Training MAE: 0.969.
2024-06-21 01:27:44,557 - INFO: Training MSE: 1.539.
2024-06-21 01:28:05,097 - INFO: Epoch: 136/200, Loss_train: 1.5372806027017791, Loss_val: 1.602151870727539
2024-06-21 01:28:05,143 - INFO: Saved new best metric model for epoch 136.
2024-06-21 01:28:05,143 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:28:05,143 - INFO: Epoch 137/200...
2024-06-21 01:28:05,143 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:28:05,143 - INFO: Batch size: 32.
2024-06-21 01:28:05,147 - INFO: Dataset:
2024-06-21 01:28:05,147 - INFO: Batch size:
2024-06-21 01:28:05,147 - INFO: Number of workers:
2024-06-21 01:28:06,388 - INFO: Epoch: 137/200, Batch: 1/29, Batch_Loss_Train: 1.669
2024-06-21 01:28:06,713 - INFO: Epoch: 137/200, Batch: 2/29, Batch_Loss_Train: 1.532
2024-06-21 01:28:07,134 - INFO: Epoch: 137/200, Batch: 3/29, Batch_Loss_Train: 1.198
2024-06-21 01:28:07,459 - INFO: Epoch: 137/200, Batch: 4/29, Batch_Loss_Train: 1.807
2024-06-21 01:28:07,868 - INFO: Epoch: 137/200, Batch: 5/29, Batch_Loss_Train: 1.135
2024-06-21 01:28:08,184 - INFO: Epoch: 137/200, Batch: 6/29, Batch_Loss_Train: 1.862
2024-06-21 01:28:08,589 - INFO: Epoch: 137/200, Batch: 7/29, Batch_Loss_Train: 1.543
2024-06-21 01:28:08,907 - INFO: Epoch: 137/200, Batch: 8/29, Batch_Loss_Train: 1.547
2024-06-21 01:28:09,304 - INFO: Epoch: 137/200, Batch: 9/29, Batch_Loss_Train: 1.869
2024-06-21 01:28:09,615 - INFO: Epoch: 137/200, Batch: 10/29, Batch_Loss_Train: 1.574
2024-06-21 01:28:10,018 - INFO: Epoch: 137/200, Batch: 11/29, Batch_Loss_Train: 1.942
2024-06-21 01:28:10,338 - INFO: Epoch: 137/200, Batch: 12/29, Batch_Loss_Train: 1.961
2024-06-21 01:28:10,763 - INFO: Epoch: 137/200, Batch: 13/29, Batch_Loss_Train: 1.142
2024-06-21 01:28:11,086 - INFO: Epoch: 137/200, Batch: 14/29, Batch_Loss_Train: 1.722
2024-06-21 01:28:11,505 - INFO: Epoch: 137/200, Batch: 15/29, Batch_Loss_Train: 1.565
2024-06-21 01:28:11,823 - INFO: Epoch: 137/200, Batch: 16/29, Batch_Loss_Train: 1.428
2024-06-21 01:28:12,236 - INFO: Epoch: 137/200, Batch: 17/29, Batch_Loss_Train: 1.380
2024-06-21 01:28:12,554 - INFO: Epoch: 137/200, Batch: 18/29, Batch_Loss_Train: 1.532
2024-06-21 01:28:12,960 - INFO: Epoch: 137/200, Batch: 19/29, Batch_Loss_Train: 1.275
2024-06-21 01:28:13,275 - INFO: Epoch: 137/200, Batch: 20/29, Batch_Loss_Train: 1.326
2024-06-21 01:28:13,682 - INFO: Epoch: 137/200, Batch: 21/29, Batch_Loss_Train: 1.695
2024-06-21 01:28:14,004 - INFO: Epoch: 137/200, Batch: 22/29, Batch_Loss_Train: 1.787
2024-06-21 01:28:14,409 - INFO: Epoch: 137/200, Batch: 23/29, Batch_Loss_Train: 1.064
2024-06-21 01:28:14,730 - INFO: Epoch: 137/200, Batch: 24/29, Batch_Loss_Train: 1.591
2024-06-21 01:28:15,135 - INFO: Epoch: 137/200, Batch: 25/29, Batch_Loss_Train: 1.160
2024-06-21 01:28:15,451 - INFO: Epoch: 137/200, Batch: 26/29, Batch_Loss_Train: 1.409
2024-06-21 01:28:15,840 - INFO: Epoch: 137/200, Batch: 27/29, Batch_Loss_Train: 1.306
2024-06-21 01:28:16,156 - INFO: Epoch: 137/200, Batch: 28/29, Batch_Loss_Train: 1.962
2024-06-21 01:28:16,373 - INFO: Epoch: 137/200, Batch: 29/29, Batch_Loss_Train: 2.271
2024-06-21 01:28:27,307 - INFO: 137/200 final results:
2024-06-21 01:28:27,307 - INFO: Training loss: 1.560.
2024-06-21 01:28:27,307 - INFO: Training MAE: 0.965.
2024-06-21 01:28:27,307 - INFO: Training MSE: 1.546.
2024-06-21 01:28:47,703 - INFO: Epoch: 137/200, Loss_train: 1.5604054763399322, Loss_val: 1.639281992254586
2024-06-21 01:28:47,704 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:28:47,704 - INFO: Epoch 138/200...
2024-06-21 01:28:47,704 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:28:47,704 - INFO: Batch size: 32.
2024-06-21 01:28:47,708 - INFO: Dataset:
2024-06-21 01:28:47,708 - INFO: Batch size:
2024-06-21 01:28:47,708 - INFO: Number of workers:
2024-06-21 01:28:48,977 - INFO: Epoch: 138/200, Batch: 1/29, Batch_Loss_Train: 0.989
2024-06-21 01:28:49,305 - INFO: Epoch: 138/200, Batch: 2/29, Batch_Loss_Train: 1.451
2024-06-21 01:28:49,700 - INFO: Epoch: 138/200, Batch: 3/29, Batch_Loss_Train: 1.355
2024-06-21 01:28:50,025 - INFO: Epoch: 138/200, Batch: 4/29, Batch_Loss_Train: 1.285
2024-06-21 01:28:50,451 - INFO: Epoch: 138/200, Batch: 5/29, Batch_Loss_Train: 1.979
2024-06-21 01:28:50,770 - INFO: Epoch: 138/200, Batch: 6/29, Batch_Loss_Train: 2.154
2024-06-21 01:28:51,167 - INFO: Epoch: 138/200, Batch: 7/29, Batch_Loss_Train: 1.433
2024-06-21 01:28:51,487 - INFO: Epoch: 138/200, Batch: 8/29, Batch_Loss_Train: 1.647
2024-06-21 01:28:51,904 - INFO: Epoch: 138/200, Batch: 9/29, Batch_Loss_Train: 2.252
2024-06-21 01:28:52,219 - INFO: Epoch: 138/200, Batch: 10/29, Batch_Loss_Train: 1.317
2024-06-21 01:28:52,615 - INFO: Epoch: 138/200, Batch: 11/29, Batch_Loss_Train: 1.826
2024-06-21 01:28:52,937 - INFO: Epoch: 138/200, Batch: 12/29, Batch_Loss_Train: 1.581
2024-06-21 01:28:53,383 - INFO: Epoch: 138/200, Batch: 13/29, Batch_Loss_Train: 1.620
2024-06-21 01:28:53,694 - INFO: Epoch: 138/200, Batch: 14/29, Batch_Loss_Train: 0.948
2024-06-21 01:28:54,103 - INFO: Epoch: 138/200, Batch: 15/29, Batch_Loss_Train: 1.321
2024-06-21 01:28:54,421 - INFO: Epoch: 138/200, Batch: 16/29, Batch_Loss_Train: 1.676
2024-06-21 01:28:54,849 - INFO: Epoch: 138/200, Batch: 17/29, Batch_Loss_Train: 1.536
2024-06-21 01:28:55,154 - INFO: Epoch: 138/200, Batch: 18/29, Batch_Loss_Train: 1.280
2024-06-21 01:28:55,546 - INFO: Epoch: 138/200, Batch: 19/29, Batch_Loss_Train: 1.387
2024-06-21 01:28:55,859 - INFO: Epoch: 138/200, Batch: 20/29, Batch_Loss_Train: 1.070
2024-06-21 01:28:56,291 - INFO: Epoch: 138/200, Batch: 21/29, Batch_Loss_Train: 1.052
2024-06-21 01:28:56,600 - INFO: Epoch: 138/200, Batch: 22/29, Batch_Loss_Train: 1.633
2024-06-21 01:28:56,988 - INFO: Epoch: 138/200, Batch: 23/29, Batch_Loss_Train: 2.174
2024-06-21 01:28:57,309 - INFO: Epoch: 138/200, Batch: 24/29, Batch_Loss_Train: 1.487
2024-06-21 01:28:57,733 - INFO: Epoch: 138/200, Batch: 25/29, Batch_Loss_Train: 1.936
2024-06-21 01:28:58,035 - INFO: Epoch: 138/200, Batch: 26/29, Batch_Loss_Train: 1.203
2024-06-21 01:28:58,410 - INFO: Epoch: 138/200, Batch: 27/29, Batch_Loss_Train: 1.397
2024-06-21 01:28:58,725 - INFO: Epoch: 138/200, Batch: 28/29, Batch_Loss_Train: 1.246
2024-06-21 01:28:58,941 - INFO: Epoch: 138/200, Batch: 29/29, Batch_Loss_Train: 1.971
2024-06-21 01:29:09,976 - INFO: 138/200 final results:
2024-06-21 01:29:09,976 - INFO: Training loss: 1.524.
2024-06-21 01:29:09,976 - INFO: Training MAE: 0.945.
2024-06-21 01:29:09,976 - INFO: Training MSE: 1.515.
2024-06-21 01:29:30,421 - INFO: Epoch: 138/200, Loss_train: 1.5243026984149013, Loss_val: 1.7044279123174733
2024-06-21 01:29:30,421 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:29:30,421 - INFO: Epoch 139/200...
2024-06-21 01:29:30,421 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:29:30,421 - INFO: Batch size: 32.
2024-06-21 01:29:30,425 - INFO: Dataset:
2024-06-21 01:29:30,425 - INFO: Batch size:
2024-06-21 01:29:30,425 - INFO: Number of workers:
2024-06-21 01:29:31,674 - INFO: Epoch: 139/200, Batch: 1/29, Batch_Loss_Train: 1.505
2024-06-21 01:29:31,986 - INFO: Epoch: 139/200, Batch: 2/29, Batch_Loss_Train: 1.574
2024-06-21 01:29:32,390 - INFO: Epoch: 139/200, Batch: 3/29, Batch_Loss_Train: 1.379
2024-06-21 01:29:32,718 - INFO: Epoch: 139/200, Batch: 4/29, Batch_Loss_Train: 1.481
2024-06-21 01:29:33,168 - INFO: Epoch: 139/200, Batch: 5/29, Batch_Loss_Train: 1.639
2024-06-21 01:29:33,475 - INFO: Epoch: 139/200, Batch: 6/29, Batch_Loss_Train: 1.171
2024-06-21 01:29:33,870 - INFO: Epoch: 139/200, Batch: 7/29, Batch_Loss_Train: 1.189
2024-06-21 01:29:34,177 - INFO: Epoch: 139/200, Batch: 8/29, Batch_Loss_Train: 1.499
2024-06-21 01:29:34,613 - INFO: Epoch: 139/200, Batch: 9/29, Batch_Loss_Train: 1.316
2024-06-21 01:29:34,915 - INFO: Epoch: 139/200, Batch: 10/29, Batch_Loss_Train: 1.213
2024-06-21 01:29:35,294 - INFO: Epoch: 139/200, Batch: 11/29, Batch_Loss_Train: 1.471
2024-06-21 01:29:35,605 - INFO: Epoch: 139/200, Batch: 12/29, Batch_Loss_Train: 1.421
2024-06-21 01:29:36,047 - INFO: Epoch: 139/200, Batch: 13/29, Batch_Loss_Train: 1.635
2024-06-21 01:29:36,357 - INFO: Epoch: 139/200, Batch: 14/29, Batch_Loss_Train: 1.530
2024-06-21 01:29:36,760 - INFO: Epoch: 139/200, Batch: 15/29, Batch_Loss_Train: 1.746
2024-06-21 01:29:37,062 - INFO: Epoch: 139/200, Batch: 16/29, Batch_Loss_Train: 1.741
2024-06-21 01:29:37,499 - INFO: Epoch: 139/200, Batch: 17/29, Batch_Loss_Train: 1.228
2024-06-21 01:29:37,805 - INFO: Epoch: 139/200, Batch: 18/29, Batch_Loss_Train: 1.958
2024-06-21 01:29:38,185 - INFO: Epoch: 139/200, Batch: 19/29, Batch_Loss_Train: 1.424
2024-06-21 01:29:38,486 - INFO: Epoch: 139/200, Batch: 20/29, Batch_Loss_Train: 1.817
2024-06-21 01:29:38,922 - INFO: Epoch: 139/200, Batch: 21/29, Batch_Loss_Train: 1.618
2024-06-21 01:29:39,230 - INFO: Epoch: 139/200, Batch: 22/29, Batch_Loss_Train: 1.124
2024-06-21 01:29:39,623 - INFO: Epoch: 139/200, Batch: 23/29, Batch_Loss_Train: 1.352
2024-06-21 01:29:39,931 - INFO: Epoch: 139/200, Batch: 24/29, Batch_Loss_Train: 1.801
2024-06-21 01:29:40,364 - INFO: Epoch: 139/200, Batch: 25/29, Batch_Loss_Train: 1.134
2024-06-21 01:29:40,665 - INFO: Epoch: 139/200, Batch: 26/29, Batch_Loss_Train: 1.318
2024-06-21 01:29:41,038 - INFO: Epoch: 139/200, Batch: 27/29, Batch_Loss_Train: 2.003
2024-06-21 01:29:41,339 - INFO: Epoch: 139/200, Batch: 28/29, Batch_Loss_Train: 1.464
2024-06-21 01:29:41,550 - INFO: Epoch: 139/200, Batch: 29/29, Batch_Loss_Train: 1.095
2024-06-21 01:29:52,541 - INFO: 139/200 final results:
2024-06-21 01:29:52,541 - INFO: Training loss: 1.477.
2024-06-21 01:29:52,541 - INFO: Training MAE: 0.941.
2024-06-21 01:29:52,541 - INFO: Training MSE: 1.485.
2024-06-21 01:30:13,018 - INFO: Epoch: 139/200, Loss_train: 1.477409814966136, Loss_val: 1.6083615442802166
2024-06-21 01:30:13,018 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:30:13,018 - INFO: Epoch 140/200...
2024-06-21 01:30:13,018 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:30:13,018 - INFO: Batch size: 32.
2024-06-21 01:30:13,022 - INFO: Dataset:
2024-06-21 01:30:13,022 - INFO: Batch size:
2024-06-21 01:30:13,022 - INFO: Number of workers:
2024-06-21 01:30:14,291 - INFO: Epoch: 140/200, Batch: 1/29, Batch_Loss_Train: 1.319
2024-06-21 01:30:14,605 - INFO: Epoch: 140/200, Batch: 2/29, Batch_Loss_Train: 1.828
2024-06-21 01:30:15,025 - INFO: Epoch: 140/200, Batch: 3/29, Batch_Loss_Train: 1.225
2024-06-21 01:30:15,350 - INFO: Epoch: 140/200, Batch: 4/29, Batch_Loss_Train: 0.994
2024-06-21 01:30:15,783 - INFO: Epoch: 140/200, Batch: 5/29, Batch_Loss_Train: 1.333
2024-06-21 01:30:16,087 - INFO: Epoch: 140/200, Batch: 6/29, Batch_Loss_Train: 1.543
2024-06-21 01:30:16,478 - INFO: Epoch: 140/200, Batch: 7/29, Batch_Loss_Train: 1.309
2024-06-21 01:30:16,797 - INFO: Epoch: 140/200, Batch: 8/29, Batch_Loss_Train: 1.788
2024-06-21 01:30:17,227 - INFO: Epoch: 140/200, Batch: 9/29, Batch_Loss_Train: 1.214
2024-06-21 01:30:17,527 - INFO: Epoch: 140/200, Batch: 10/29, Batch_Loss_Train: 1.430
2024-06-21 01:30:17,918 - INFO: Epoch: 140/200, Batch: 11/29, Batch_Loss_Train: 1.466
2024-06-21 01:30:18,240 - INFO: Epoch: 140/200, Batch: 12/29, Batch_Loss_Train: 1.295
2024-06-21 01:30:18,683 - INFO: Epoch: 140/200, Batch: 13/29, Batch_Loss_Train: 1.700
2024-06-21 01:30:18,991 - INFO: Epoch: 140/200, Batch: 14/29, Batch_Loss_Train: 1.729
2024-06-21 01:30:19,396 - INFO: Epoch: 140/200, Batch: 15/29, Batch_Loss_Train: 1.744
2024-06-21 01:30:19,713 - INFO: Epoch: 140/200, Batch: 16/29, Batch_Loss_Train: 1.869
2024-06-21 01:30:20,143 - INFO: Epoch: 140/200, Batch: 17/29, Batch_Loss_Train: 1.248
2024-06-21 01:30:20,446 - INFO: Epoch: 140/200, Batch: 18/29, Batch_Loss_Train: 1.470
2024-06-21 01:30:20,836 - INFO: Epoch: 140/200, Batch: 19/29, Batch_Loss_Train: 1.462
2024-06-21 01:30:21,149 - INFO: Epoch: 140/200, Batch: 20/29, Batch_Loss_Train: 1.722
2024-06-21 01:30:21,578 - INFO: Epoch: 140/200, Batch: 21/29, Batch_Loss_Train: 1.450
2024-06-21 01:30:21,885 - INFO: Epoch: 140/200, Batch: 22/29, Batch_Loss_Train: 1.666
2024-06-21 01:30:22,278 - INFO: Epoch: 140/200, Batch: 23/29, Batch_Loss_Train: 1.255
2024-06-21 01:30:22,598 - INFO: Epoch: 140/200, Batch: 24/29, Batch_Loss_Train: 1.624
2024-06-21 01:30:23,014 - INFO: Epoch: 140/200, Batch: 25/29, Batch_Loss_Train: 1.635
2024-06-21 01:30:23,315 - INFO: Epoch: 140/200, Batch: 26/29, Batch_Loss_Train: 1.702
2024-06-21 01:30:23,686 - INFO: Epoch: 140/200, Batch: 27/29, Batch_Loss_Train: 1.895
2024-06-21 01:30:23,999 - INFO: Epoch: 140/200, Batch: 28/29, Batch_Loss_Train: 1.811
2024-06-21 01:30:24,210 - INFO: Epoch: 140/200, Batch: 29/29, Batch_Loss_Train: 1.141
2024-06-21 01:30:35,243 - INFO: 140/200 final results:
2024-06-21 01:30:35,243 - INFO: Training loss: 1.513.
2024-06-21 01:30:35,243 - INFO: Training MAE: 0.954.
2024-06-21 01:30:35,243 - INFO: Training MSE: 1.520.
2024-06-21 01:30:55,888 - INFO: Epoch: 140/200, Loss_train: 1.51264881676641, Loss_val: 1.645044139746962
2024-06-21 01:30:55,888 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:30:55,888 - INFO: Epoch 141/200...
2024-06-21 01:30:55,888 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:30:55,888 - INFO: Batch size: 32.
2024-06-21 01:30:55,892 - INFO: Dataset:
2024-06-21 01:30:55,892 - INFO: Batch size:
2024-06-21 01:30:55,892 - INFO: Number of workers:
2024-06-21 01:30:57,173 - INFO: Epoch: 141/200, Batch: 1/29, Batch_Loss_Train: 1.961
2024-06-21 01:30:57,501 - INFO: Epoch: 141/200, Batch: 2/29, Batch_Loss_Train: 1.520
2024-06-21 01:30:57,910 - INFO: Epoch: 141/200, Batch: 3/29, Batch_Loss_Train: 2.179
2024-06-21 01:30:58,237 - INFO: Epoch: 141/200, Batch: 4/29, Batch_Loss_Train: 1.719
2024-06-21 01:30:58,661 - INFO: Epoch: 141/200, Batch: 5/29, Batch_Loss_Train: 0.843
2024-06-21 01:30:58,981 - INFO: Epoch: 141/200, Batch: 6/29, Batch_Loss_Train: 1.245
2024-06-21 01:30:59,374 - INFO: Epoch: 141/200, Batch: 7/29, Batch_Loss_Train: 1.340
2024-06-21 01:30:59,696 - INFO: Epoch: 141/200, Batch: 8/29, Batch_Loss_Train: 1.234
2024-06-21 01:31:00,115 - INFO: Epoch: 141/200, Batch: 9/29, Batch_Loss_Train: 1.490
2024-06-21 01:31:00,426 - INFO: Epoch: 141/200, Batch: 10/29, Batch_Loss_Train: 1.203
2024-06-21 01:31:00,815 - INFO: Epoch: 141/200, Batch: 11/29, Batch_Loss_Train: 1.474
2024-06-21 01:31:01,134 - INFO: Epoch: 141/200, Batch: 12/29, Batch_Loss_Train: 1.578
2024-06-21 01:31:01,575 - INFO: Epoch: 141/200, Batch: 13/29, Batch_Loss_Train: 1.838
2024-06-21 01:31:01,884 - INFO: Epoch: 141/200, Batch: 14/29, Batch_Loss_Train: 1.581
2024-06-21 01:31:02,286 - INFO: Epoch: 141/200, Batch: 15/29, Batch_Loss_Train: 1.390
2024-06-21 01:31:02,605 - INFO: Epoch: 141/200, Batch: 16/29, Batch_Loss_Train: 1.296
2024-06-21 01:31:03,034 - INFO: Epoch: 141/200, Batch: 17/29, Batch_Loss_Train: 1.445
2024-06-21 01:31:03,337 - INFO: Epoch: 141/200, Batch: 18/29, Batch_Loss_Train: 1.371
2024-06-21 01:31:03,724 - INFO: Epoch: 141/200, Batch: 19/29, Batch_Loss_Train: 1.336
2024-06-21 01:31:04,035 - INFO: Epoch: 141/200, Batch: 20/29, Batch_Loss_Train: 2.108
2024-06-21 01:31:04,465 - INFO: Epoch: 141/200, Batch: 21/29, Batch_Loss_Train: 1.010
2024-06-21 01:31:04,771 - INFO: Epoch: 141/200, Batch: 22/29, Batch_Loss_Train: 1.513
2024-06-21 01:31:05,156 - INFO: Epoch: 141/200, Batch: 23/29, Batch_Loss_Train: 1.054
2024-06-21 01:31:05,475 - INFO: Epoch: 141/200, Batch: 24/29, Batch_Loss_Train: 1.287
2024-06-21 01:31:05,891 - INFO: Epoch: 141/200, Batch: 25/29, Batch_Loss_Train: 1.168
2024-06-21 01:31:06,191 - INFO: Epoch: 141/200, Batch: 26/29, Batch_Loss_Train: 1.603
2024-06-21 01:31:06,561 - INFO: Epoch: 141/200, Batch: 27/29, Batch_Loss_Train: 1.371
2024-06-21 01:31:06,875 - INFO: Epoch: 141/200, Batch: 28/29, Batch_Loss_Train: 1.452
2024-06-21 01:31:07,086 - INFO: Epoch: 141/200, Batch: 29/29, Batch_Loss_Train: 1.534
2024-06-21 01:31:18,038 - INFO: 141/200 final results:
2024-06-21 01:31:18,038 - INFO: Training loss: 1.453.
2024-06-21 01:31:18,038 - INFO: Training MAE: 0.937.
2024-06-21 01:31:18,038 - INFO: Training MSE: 1.452.
2024-06-21 01:31:38,800 - INFO: Epoch: 141/200, Loss_train: 1.4531844278861736, Loss_val: 1.8016483002695545
2024-06-21 01:31:38,800 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:31:38,800 - INFO: Epoch 142/200...
2024-06-21 01:31:38,800 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:31:38,800 - INFO: Batch size: 32.
2024-06-21 01:31:38,804 - INFO: Dataset:
2024-06-21 01:31:38,804 - INFO: Batch size:
2024-06-21 01:31:38,804 - INFO: Number of workers:
2024-06-21 01:31:40,064 - INFO: Epoch: 142/200, Batch: 1/29, Batch_Loss_Train: 1.314
2024-06-21 01:31:40,376 - INFO: Epoch: 142/200, Batch: 2/29, Batch_Loss_Train: 1.656
2024-06-21 01:31:40,798 - INFO: Epoch: 142/200, Batch: 3/29, Batch_Loss_Train: 1.420
2024-06-21 01:31:41,123 - INFO: Epoch: 142/200, Batch: 4/29, Batch_Loss_Train: 0.854
2024-06-21 01:31:41,547 - INFO: Epoch: 142/200, Batch: 5/29, Batch_Loss_Train: 1.802
2024-06-21 01:31:41,853 - INFO: Epoch: 142/200, Batch: 6/29, Batch_Loss_Train: 1.406
2024-06-21 01:31:42,257 - INFO: Epoch: 142/200, Batch: 7/29, Batch_Loss_Train: 1.679
2024-06-21 01:31:42,575 - INFO: Epoch: 142/200, Batch: 8/29, Batch_Loss_Train: 1.052
2024-06-21 01:31:42,992 - INFO: Epoch: 142/200, Batch: 9/29, Batch_Loss_Train: 1.612
2024-06-21 01:31:43,291 - INFO: Epoch: 142/200, Batch: 10/29, Batch_Loss_Train: 1.640
2024-06-21 01:31:43,703 - INFO: Epoch: 142/200, Batch: 11/29, Batch_Loss_Train: 0.982
2024-06-21 01:31:44,024 - INFO: Epoch: 142/200, Batch: 12/29, Batch_Loss_Train: 1.354
2024-06-21 01:31:44,456 - INFO: Epoch: 142/200, Batch: 13/29, Batch_Loss_Train: 1.591
2024-06-21 01:31:44,764 - INFO: Epoch: 142/200, Batch: 14/29, Batch_Loss_Train: 1.717
2024-06-21 01:31:45,181 - INFO: Epoch: 142/200, Batch: 15/29, Batch_Loss_Train: 1.550
2024-06-21 01:31:45,496 - INFO: Epoch: 142/200, Batch: 16/29, Batch_Loss_Train: 1.126
2024-06-21 01:31:45,915 - INFO: Epoch: 142/200, Batch: 17/29, Batch_Loss_Train: 1.468
2024-06-21 01:31:46,218 - INFO: Epoch: 142/200, Batch: 18/29, Batch_Loss_Train: 1.481
2024-06-21 01:31:46,619 - INFO: Epoch: 142/200, Batch: 19/29, Batch_Loss_Train: 1.743
2024-06-21 01:31:46,931 - INFO: Epoch: 142/200, Batch: 20/29, Batch_Loss_Train: 2.040
2024-06-21 01:31:47,352 - INFO: Epoch: 142/200, Batch: 21/29, Batch_Loss_Train: 1.702
2024-06-21 01:31:47,659 - INFO: Epoch: 142/200, Batch: 22/29, Batch_Loss_Train: 1.294
2024-06-21 01:31:48,065 - INFO: Epoch: 142/200, Batch: 23/29, Batch_Loss_Train: 1.748
2024-06-21 01:31:48,384 - INFO: Epoch: 142/200, Batch: 24/29, Batch_Loss_Train: 1.904
2024-06-21 01:31:48,796 - INFO: Epoch: 142/200, Batch: 25/29, Batch_Loss_Train: 1.745
2024-06-21 01:31:49,098 - INFO: Epoch: 142/200, Batch: 26/29, Batch_Loss_Train: 1.448
2024-06-21 01:31:49,497 - INFO: Epoch: 142/200, Batch: 27/29, Batch_Loss_Train: 1.276
2024-06-21 01:31:49,812 - INFO: Epoch: 142/200, Batch: 28/29, Batch_Loss_Train: 1.160
2024-06-21 01:31:50,025 - INFO: Epoch: 142/200, Batch: 29/29, Batch_Loss_Train: 0.945
2024-06-21 01:32:00,972 - INFO: 142/200 final results:
2024-06-21 01:32:00,972 - INFO: Training loss: 1.473.
2024-06-21 01:32:00,972 - INFO: Training MAE: 0.943.
2024-06-21 01:32:00,972 - INFO: Training MSE: 1.483.
2024-06-21 01:32:21,391 - INFO: Epoch: 142/200, Loss_train: 1.4727639638144394, Loss_val: 1.605187909356479
2024-06-21 01:32:21,391 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:32:21,391 - INFO: Epoch 143/200...
2024-06-21 01:32:21,391 - INFO: Learning rate: 1.3774372125657317e-05.
2024-06-21 01:32:21,391 - INFO: Batch size: 32.
2024-06-21 01:32:21,395 - INFO: Dataset:
2024-06-21 01:32:21,395 - INFO: Batch size:
2024-06-21 01:32:21,395 - INFO: Number of workers:
2024-06-21 01:32:22,646 - INFO: Epoch: 143/200, Batch: 1/29, Batch_Loss_Train: 1.286
2024-06-21 01:32:22,960 - INFO: Epoch: 143/200, Batch: 2/29, Batch_Loss_Train: 1.344
2024-06-21 01:32:23,380 - INFO: Epoch: 143/200, Batch: 3/29, Batch_Loss_Train: 1.286
2024-06-21 01:32:23,705 - INFO: Epoch: 143/200, Batch: 4/29, Batch_Loss_Train: 1.513
2024-06-21 01:32:24,118 - INFO: Epoch: 143/200, Batch: 5/29, Batch_Loss_Train: 1.365
2024-06-21 01:32:24,423 - INFO: Epoch: 143/200, Batch: 6/29, Batch_Loss_Train: 1.517
2024-06-21 01:32:24,826 - INFO: Epoch: 143/200, Batch: 7/29, Batch_Loss_Train: 1.567
2024-06-21 01:32:25,148 - INFO: Epoch: 143/200, Batch: 8/29, Batch_Loss_Train: 1.535
2024-06-21 01:32:25,553 - INFO: Epoch: 143/200, Batch: 9/29, Batch_Loss_Train: 1.569
2024-06-21 01:32:25,855 - INFO: Epoch: 143/200, Batch: 10/29, Batch_Loss_Train: 1.342
2024-06-21 01:32:26,257 - INFO: Epoch: 143/200, Batch: 11/29, Batch_Loss_Train: 1.244
2024-06-21 01:32:26,580 - INFO: Epoch: 143/200, Batch: 12/29, Batch_Loss_Train: 1.681
2024-06-21 01:32:27,015 - INFO: Epoch: 143/200, Batch: 13/29, Batch_Loss_Train: 1.863
2024-06-21 01:32:27,325 - INFO: Epoch: 143/200, Batch: 14/29, Batch_Loss_Train: 1.267
2024-06-21 01:32:27,730 - INFO: Epoch: 143/200, Batch: 15/29, Batch_Loss_Train: 1.165
2024-06-21 01:32:28,045 - INFO: Epoch: 143/200, Batch: 16/29, Batch_Loss_Train: 1.441
2024-06-21 01:32:28,460 - INFO: Epoch: 143/200, Batch: 17/29, Batch_Loss_Train: 1.013
2024-06-21 01:32:28,765 - INFO: Epoch: 143/200, Batch: 18/29, Batch_Loss_Train: 2.191
2024-06-21 01:32:29,182 - INFO: Epoch: 143/200, Batch: 19/29, Batch_Loss_Train: 1.231
2024-06-21 01:32:29,498 - INFO: Epoch: 143/200, Batch: 20/29, Batch_Loss_Train: 1.875
2024-06-21 01:32:29,911 - INFO: Epoch: 143/200, Batch: 21/29, Batch_Loss_Train: 1.563
2024-06-21 01:32:30,220 - INFO: Epoch: 143/200, Batch: 22/29, Batch_Loss_Train: 1.501
2024-06-21 01:32:30,624 - INFO: Epoch: 143/200, Batch: 23/29, Batch_Loss_Train: 1.586
2024-06-21 01:32:30,943 - INFO: Epoch: 143/200, Batch: 24/29, Batch_Loss_Train: 1.545
2024-06-21 01:32:31,346 - INFO: Epoch: 143/200, Batch: 25/29, Batch_Loss_Train: 1.653
2024-06-21 01:32:31,647 - INFO: Epoch: 143/200, Batch: 26/29, Batch_Loss_Train: 1.666
2024-06-21 01:32:32,034 - INFO: Epoch: 143/200, Batch: 27/29, Batch_Loss_Train: 1.543
2024-06-21 01:32:32,347 - INFO: Epoch: 143/200, Batch: 28/29, Batch_Loss_Train: 1.534
2024-06-21 01:32:32,558 - INFO: Epoch: 143/200, Batch: 29/29, Batch_Loss_Train: 1.332
2024-06-21 01:32:43,553 - INFO: 143/200 final results:
2024-06-21 01:32:43,553 - INFO: Training loss: 1.490.
2024-06-21 01:32:43,553 - INFO: Training MAE: 0.955.
2024-06-21 01:32:43,553 - INFO: Training MSE: 1.493.
2024-06-21 01:33:04,487 - INFO: Epoch: 143/200, Loss_train: 1.4903172213455727, Loss_val: 1.6385258908929496
2024-06-21 01:33:04,487 - INFO: Best internal validation val_loss: 1.602 at epoch: 136.
2024-06-21 01:33:04,487 - INFO: Epoch 144/200...
2024-06-21 01:33:04,487 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:33:04,487 - INFO: Batch size: 32.
2024-06-21 01:33:04,490 - INFO: Dataset:
2024-06-21 01:33:04,491 - INFO: Batch size:
2024-06-21 01:33:04,491 - INFO: Number of workers:
2024-06-21 01:33:05,771 - INFO: Epoch: 144/200, Batch: 1/29, Batch_Loss_Train: 1.470
2024-06-21 01:33:06,082 - INFO: Epoch: 144/200, Batch: 2/29, Batch_Loss_Train: 1.431
2024-06-21 01:33:06,486 - INFO: Epoch: 144/200, Batch: 3/29, Batch_Loss_Train: 1.144
2024-06-21 01:33:06,809 - INFO: Epoch: 144/200, Batch: 4/29, Batch_Loss_Train: 1.261
2024-06-21 01:33:07,239 - INFO: Epoch: 144/200, Batch: 5/29, Batch_Loss_Train: 1.360
2024-06-21 01:33:07,542 - INFO: Epoch: 144/200, Batch: 6/29, Batch_Loss_Train: 1.435
2024-06-21 01:33:07,935 - INFO: Epoch: 144/200, Batch: 7/29, Batch_Loss_Train: 1.615
2024-06-21 01:33:08,252 - INFO: Epoch: 144/200, Batch: 8/29, Batch_Loss_Train: 1.358
2024-06-21 01:33:08,683 - INFO: Epoch: 144/200, Batch: 9/29, Batch_Loss_Train: 1.343
2024-06-21 01:33:08,981 - INFO: Epoch: 144/200, Batch: 10/29, Batch_Loss_Train: 1.272
2024-06-21 01:33:09,371 - INFO: Epoch: 144/200, Batch: 11/29, Batch_Loss_Train: 1.567
2024-06-21 01:33:09,690 - INFO: Epoch: 144/200, Batch: 12/29, Batch_Loss_Train: 1.767
2024-06-21 01:33:10,136 - INFO: Epoch: 144/200, Batch: 13/29, Batch_Loss_Train: 1.281
2024-06-21 01:33:10,447 - INFO: Epoch: 144/200, Batch: 14/29, Batch_Loss_Train: 1.439
2024-06-21 01:33:10,851 - INFO: Epoch: 144/200, Batch: 15/29, Batch_Loss_Train: 1.351
2024-06-21 01:33:11,169 - INFO: Epoch: 144/200, Batch: 16/29, Batch_Loss_Train: 1.364
2024-06-21 01:33:11,603 - INFO: Epoch: 144/200, Batch: 17/29, Batch_Loss_Train: 1.602
2024-06-21 01:33:11,908 - INFO: Epoch: 144/200, Batch: 18/29, Batch_Loss_Train: 1.211
2024-06-21 01:33:12,300 - INFO: Epoch: 144/200, Batch: 19/29, Batch_Loss_Train: 1.166
2024-06-21 01:33:12,616 - INFO: Epoch: 144/200, Batch: 20/29, Batch_Loss_Train: 1.666
2024-06-21 01:33:13,046 - INFO: Epoch: 144/200, Batch: 21/29, Batch_Loss_Train: 1.382
2024-06-21 01:33:13,354 - INFO: Epoch: 144/200, Batch: 22/29, Batch_Loss_Train: 1.420
2024-06-21 01:33:13,743 - INFO: Epoch: 144/200, Batch: 23/29, Batch_Loss_Train: 1.570
2024-06-21 01:33:14,065 - INFO: Epoch: 144/200, Batch: 24/29, Batch_Loss_Train: 1.286
2024-06-21 01:33:14,483 - INFO: Epoch: 144/200, Batch: 25/29, Batch_Loss_Train: 1.260
2024-06-21 01:33:14,786 - INFO: Epoch: 144/200, Batch: 26/29, Batch_Loss_Train: 2.240
2024-06-21 01:33:15,160 - INFO: Epoch: 144/200, Batch: 27/29, Batch_Loss_Train: 1.207
2024-06-21 01:33:15,476 - INFO: Epoch: 144/200, Batch: 28/29, Batch_Loss_Train: 1.576
2024-06-21 01:33:15,690 - INFO: Epoch: 144/200, Batch: 29/29, Batch_Loss_Train: 0.796
2024-06-21 01:33:26,742 - INFO: 144/200 final results:
2024-06-21 01:33:26,742 - INFO: Training loss: 1.408.
2024-06-21 01:33:26,742 - INFO: Training MAE: 0.928.
2024-06-21 01:33:26,742 - INFO: Training MSE: 1.420.
2024-06-21 01:33:47,180 - INFO: Epoch: 144/200, Loss_train: 1.4082315564155579, Loss_val: 1.5870428928013505
2024-06-21 01:33:47,227 - INFO: Saved new best metric model for epoch 144.
2024-06-21 01:33:47,227 - INFO: Best internal validation val_loss: 1.587 at epoch: 144.
2024-06-21 01:33:47,227 - INFO: Epoch 145/200...
2024-06-21 01:33:47,227 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:33:47,227 - INFO: Batch size: 32.
2024-06-21 01:33:47,231 - INFO: Dataset:
2024-06-21 01:33:47,231 - INFO: Batch size:
2024-06-21 01:33:47,231 - INFO: Number of workers:
2024-06-21 01:33:48,487 - INFO: Epoch: 145/200, Batch: 1/29, Batch_Loss_Train: 1.153
2024-06-21 01:33:48,799 - INFO: Epoch: 145/200, Batch: 2/29, Batch_Loss_Train: 1.285
2024-06-21 01:33:49,221 - INFO: Epoch: 145/200, Batch: 3/29, Batch_Loss_Train: 1.550
2024-06-21 01:33:49,548 - INFO: Epoch: 145/200, Batch: 4/29, Batch_Loss_Train: 1.706
2024-06-21 01:33:49,983 - INFO: Epoch: 145/200, Batch: 5/29, Batch_Loss_Train: 1.423
2024-06-21 01:33:50,287 - INFO: Epoch: 145/200, Batch: 6/29, Batch_Loss_Train: 1.208
2024-06-21 01:33:50,677 - INFO: Epoch: 145/200, Batch: 7/29, Batch_Loss_Train: 1.258
2024-06-21 01:33:50,995 - INFO: Epoch: 145/200, Batch: 8/29, Batch_Loss_Train: 1.381
2024-06-21 01:33:51,428 - INFO: Epoch: 145/200, Batch: 9/29, Batch_Loss_Train: 1.593
2024-06-21 01:33:51,726 - INFO: Epoch: 145/200, Batch: 10/29, Batch_Loss_Train: 1.803
2024-06-21 01:33:52,117 - INFO: Epoch: 145/200, Batch: 11/29, Batch_Loss_Train: 1.567
2024-06-21 01:33:52,438 - INFO: Epoch: 145/200, Batch: 12/29, Batch_Loss_Train: 1.486
2024-06-21 01:33:52,877 - INFO: Epoch: 145/200, Batch: 13/29, Batch_Loss_Train: 1.425
2024-06-21 01:33:53,184 - INFO: Epoch: 145/200, Batch: 14/29, Batch_Loss_Train: 1.470
2024-06-21 01:33:53,584 - INFO: Epoch: 145/200, Batch: 15/29, Batch_Loss_Train: 1.130
2024-06-21 01:33:53,899 - INFO: Epoch: 145/200, Batch: 16/29, Batch_Loss_Train: 1.963
2024-06-21 01:33:54,335 - INFO: Epoch: 145/200, Batch: 17/29, Batch_Loss_Train: 1.180
2024-06-21 01:33:54,640 - INFO: Epoch: 145/200, Batch: 18/29, Batch_Loss_Train: 1.407
2024-06-21 01:33:55,028 - INFO: Epoch: 145/200, Batch: 19/29, Batch_Loss_Train: 1.767
2024-06-21 01:33:55,343 - INFO: Epoch: 145/200, Batch: 20/29, Batch_Loss_Train: 1.706
2024-06-21 01:33:55,776 - INFO: Epoch: 145/200, Batch: 21/29, Batch_Loss_Train: 1.887
2024-06-21 01:33:56,085 - INFO: Epoch: 145/200, Batch: 22/29, Batch_Loss_Train: 1.038
2024-06-21 01:33:56,473 - INFO: Epoch: 145/200, Batch: 23/29, Batch_Loss_Train: 1.386
2024-06-21 01:33:56,794 - INFO: Epoch: 145/200, Batch: 24/29, Batch_Loss_Train: 1.124
2024-06-21 01:33:57,224 - INFO: Epoch: 145/200, Batch: 25/29, Batch_Loss_Train: 1.630
2024-06-21 01:33:57,527 - INFO: Epoch: 145/200, Batch: 26/29, Batch_Loss_Train: 1.686
2024-06-21 01:33:57,911 - INFO: Epoch: 145/200, Batch: 27/29, Batch_Loss_Train: 2.017
2024-06-21 01:33:58,227 - INFO: Epoch: 145/200, Batch: 28/29, Batch_Loss_Train: 1.418
2024-06-21 01:33:58,451 - INFO: Epoch: 145/200, Batch: 29/29, Batch_Loss_Train: 1.372
2024-06-21 01:34:09,650 - INFO: 145/200 final results:
2024-06-21 01:34:09,650 - INFO: Training loss: 1.483.
2024-06-21 01:34:09,650 - INFO: Training MAE: 0.948.
2024-06-21 01:34:09,650 - INFO: Training MSE: 1.486.
2024-06-21 01:34:30,005 - INFO: Epoch: 145/200, Loss_train: 1.4834210749330192, Loss_val: 1.6118649141541843
2024-06-21 01:34:30,006 - INFO: Best internal validation val_loss: 1.587 at epoch: 144.
2024-06-21 01:34:30,006 - INFO: Epoch 146/200...
2024-06-21 01:34:30,006 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:34:30,006 - INFO: Batch size: 32.
2024-06-21 01:34:30,009 - INFO: Dataset:
2024-06-21 01:34:30,010 - INFO: Batch size:
2024-06-21 01:34:30,010 - INFO: Number of workers:
2024-06-21 01:34:31,249 - INFO: Epoch: 146/200, Batch: 1/29, Batch_Loss_Train: 1.538
2024-06-21 01:34:31,574 - INFO: Epoch: 146/200, Batch: 2/29, Batch_Loss_Train: 1.890
2024-06-21 01:34:31,991 - INFO: Epoch: 146/200, Batch: 3/29, Batch_Loss_Train: 1.517
2024-06-21 01:34:32,313 - INFO: Epoch: 146/200, Batch: 4/29, Batch_Loss_Train: 1.467
2024-06-21 01:34:32,731 - INFO: Epoch: 146/200, Batch: 5/29, Batch_Loss_Train: 1.479
2024-06-21 01:34:33,034 - INFO: Epoch: 146/200, Batch: 6/29, Batch_Loss_Train: 1.143
2024-06-21 01:34:33,437 - INFO: Epoch: 146/200, Batch: 7/29, Batch_Loss_Train: 1.435
2024-06-21 01:34:33,754 - INFO: Epoch: 146/200, Batch: 8/29, Batch_Loss_Train: 1.212
2024-06-21 01:34:34,161 - INFO: Epoch: 146/200, Batch: 9/29, Batch_Loss_Train: 1.426
2024-06-21 01:34:34,460 - INFO: Epoch: 146/200, Batch: 10/29, Batch_Loss_Train: 1.582
2024-06-21 01:34:34,855 - INFO: Epoch: 146/200, Batch: 11/29, Batch_Loss_Train: 1.382
2024-06-21 01:34:35,178 - INFO: Epoch: 146/200, Batch: 12/29, Batch_Loss_Train: 1.824
2024-06-21 01:34:35,612 - INFO: Epoch: 146/200, Batch: 13/29, Batch_Loss_Train: 1.233
2024-06-21 01:34:35,922 - INFO: Epoch: 146/200, Batch: 14/29, Batch_Loss_Train: 1.248
2024-06-21 01:34:36,329 - INFO: Epoch: 146/200, Batch: 15/29, Batch_Loss_Train: 1.633
2024-06-21 01:34:36,645 - INFO: Epoch: 146/200, Batch: 16/29, Batch_Loss_Train: 1.463
2024-06-21 01:34:37,066 - INFO: Epoch: 146/200, Batch: 17/29, Batch_Loss_Train: 1.283
2024-06-21 01:34:37,369 - INFO: Epoch: 146/200, Batch: 18/29, Batch_Loss_Train: 1.033
2024-06-21 01:34:37,774 - INFO: Epoch: 146/200, Batch: 19/29, Batch_Loss_Train: 1.666
2024-06-21 01:34:38,086 - INFO: Epoch: 146/200, Batch: 20/29, Batch_Loss_Train: 1.361
2024-06-21 01:34:38,516 - INFO: Epoch: 146/200, Batch: 21/29, Batch_Loss_Train: 1.305
2024-06-21 01:34:38,825 - INFO: Epoch: 146/200, Batch: 22/29, Batch_Loss_Train: 1.248
2024-06-21 01:34:39,240 - INFO: Epoch: 146/200, Batch: 23/29, Batch_Loss_Train: 2.031
2024-06-21 01:34:39,561 - INFO: Epoch: 146/200, Batch: 24/29, Batch_Loss_Train: 1.453
2024-06-21 01:34:39,980 - INFO: Epoch: 146/200, Batch: 25/29, Batch_Loss_Train: 1.673
2024-06-21 01:34:40,284 - INFO: Epoch: 146/200, Batch: 26/29, Batch_Loss_Train: 1.345
2024-06-21 01:34:40,685 - INFO: Epoch: 146/200, Batch: 27/29, Batch_Loss_Train: 1.050
2024-06-21 01:34:40,999 - INFO: Epoch: 146/200, Batch: 28/29, Batch_Loss_Train: 1.199
2024-06-21 01:34:41,223 - INFO: Epoch: 146/200, Batch: 29/29, Batch_Loss_Train: 1.581
2024-06-21 01:34:52,245 - INFO: 146/200 final results:
2024-06-21 01:34:52,245 - INFO: Training loss: 1.438.
2024-06-21 01:34:52,245 - INFO: Training MAE: 0.946.
2024-06-21 01:34:52,245 - INFO: Training MSE: 1.435.
2024-06-21 01:35:12,934 - INFO: Epoch: 146/200, Loss_train: 1.4378593296840274, Loss_val: 1.6806999691601456
2024-06-21 01:35:12,934 - INFO: Best internal validation val_loss: 1.587 at epoch: 144.
2024-06-21 01:35:12,934 - INFO: Epoch 147/200...
2024-06-21 01:35:12,934 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:35:12,934 - INFO: Batch size: 32.
2024-06-21 01:35:12,938 - INFO: Dataset:
2024-06-21 01:35:12,938 - INFO: Batch size:
2024-06-21 01:35:12,938 - INFO: Number of workers:
2024-06-21 01:35:14,189 - INFO: Epoch: 147/200, Batch: 1/29, Batch_Loss_Train: 1.719
2024-06-21 01:35:14,516 - INFO: Epoch: 147/200, Batch: 2/29, Batch_Loss_Train: 1.358
2024-06-21 01:35:14,936 - INFO: Epoch: 147/200, Batch: 3/29, Batch_Loss_Train: 1.225
2024-06-21 01:35:15,261 - INFO: Epoch: 147/200, Batch: 4/29, Batch_Loss_Train: 1.152
2024-06-21 01:35:15,671 - INFO: Epoch: 147/200, Batch: 5/29, Batch_Loss_Train: 1.182
2024-06-21 01:35:15,988 - INFO: Epoch: 147/200, Batch: 6/29, Batch_Loss_Train: 1.488
2024-06-21 01:35:16,395 - INFO: Epoch: 147/200, Batch: 7/29, Batch_Loss_Train: 1.240
2024-06-21 01:35:16,716 - INFO: Epoch: 147/200, Batch: 8/29, Batch_Loss_Train: 1.151
2024-06-21 01:35:17,114 - INFO: Epoch: 147/200, Batch: 9/29, Batch_Loss_Train: 1.268
2024-06-21 01:35:17,429 - INFO: Epoch: 147/200, Batch: 10/29, Batch_Loss_Train: 1.161
2024-06-21 01:35:17,834 - INFO: Epoch: 147/200, Batch: 11/29, Batch_Loss_Train: 1.799
2024-06-21 01:35:18,157 - INFO: Epoch: 147/200, Batch: 12/29, Batch_Loss_Train: 1.596
2024-06-21 01:35:18,578 - INFO: Epoch: 147/200, Batch: 13/29, Batch_Loss_Train: 1.670
2024-06-21 01:35:18,901 - INFO: Epoch: 147/200, Batch: 14/29, Batch_Loss_Train: 1.655
2024-06-21 01:35:19,317 - INFO: Epoch: 147/200, Batch: 15/29, Batch_Loss_Train: 1.389
2024-06-21 01:35:19,634 - INFO: Epoch: 147/200, Batch: 16/29, Batch_Loss_Train: 1.369
2024-06-21 01:35:20,043 - INFO: Epoch: 147/200, Batch: 17/29, Batch_Loss_Train: 1.342
2024-06-21 01:35:20,361 - INFO: Epoch: 147/200, Batch: 18/29, Batch_Loss_Train: 1.295
2024-06-21 01:35:20,763 - INFO: Epoch: 147/200, Batch: 19/29, Batch_Loss_Train: 1.406
2024-06-21 01:35:21,075 - INFO: Epoch: 147/200, Batch: 20/29, Batch_Loss_Train: 1.195
2024-06-21 01:35:21,479 - INFO: Epoch: 147/200, Batch: 21/29, Batch_Loss_Train: 1.837
2024-06-21 01:35:21,799 - INFO: Epoch: 147/200, Batch: 22/29, Batch_Loss_Train: 0.841
2024-06-21 01:35:22,207 - INFO: Epoch: 147/200, Batch: 23/29, Batch_Loss_Train: 1.571
2024-06-21 01:35:22,527 - INFO: Epoch: 147/200, Batch: 24/29, Batch_Loss_Train: 1.405
2024-06-21 01:35:22,932 - INFO: Epoch: 147/200, Batch: 25/29, Batch_Loss_Train: 1.582
2024-06-21 01:35:23,246 - INFO: Epoch: 147/200, Batch: 26/29, Batch_Loss_Train: 1.523
2024-06-21 01:35:23,647 - INFO: Epoch: 147/200, Batch: 27/29, Batch_Loss_Train: 1.223
2024-06-21 01:35:23,960 - INFO: Epoch: 147/200, Batch: 28/29, Batch_Loss_Train: 1.861
2024-06-21 01:35:24,182 - INFO: Epoch: 147/200, Batch: 29/29, Batch_Loss_Train: 1.551
2024-06-21 01:35:35,237 - INFO: 147/200 final results:
2024-06-21 01:35:35,237 - INFO: Training loss: 1.416.
2024-06-21 01:35:35,237 - INFO: Training MAE: 0.920.
2024-06-21 01:35:35,237 - INFO: Training MSE: 1.413.
2024-06-21 01:35:55,496 - INFO: Epoch: 147/200, Loss_train: 1.4156424670383847, Loss_val: 1.5838090386883965
2024-06-21 01:35:55,545 - INFO: Saved new best metric model for epoch 147.
2024-06-21 01:35:55,545 - INFO: Best internal validation val_loss: 1.584 at epoch: 147.
2024-06-21 01:35:55,545 - INFO: Epoch 148/200...
2024-06-21 01:35:55,545 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:35:55,545 - INFO: Batch size: 32.
2024-06-21 01:35:55,550 - INFO: Dataset:
2024-06-21 01:35:55,550 - INFO: Batch size:
2024-06-21 01:35:55,550 - INFO: Number of workers:
2024-06-21 01:35:56,804 - INFO: Epoch: 148/200, Batch: 1/29, Batch_Loss_Train: 1.663
2024-06-21 01:35:57,130 - INFO: Epoch: 148/200, Batch: 2/29, Batch_Loss_Train: 1.850
2024-06-21 01:35:57,543 - INFO: Epoch: 148/200, Batch: 3/29, Batch_Loss_Train: 1.061
2024-06-21 01:35:57,869 - INFO: Epoch: 148/200, Batch: 4/29, Batch_Loss_Train: 1.249
2024-06-21 01:35:58,276 - INFO: Epoch: 148/200, Batch: 5/29, Batch_Loss_Train: 1.821
2024-06-21 01:35:58,594 - INFO: Epoch: 148/200, Batch: 6/29, Batch_Loss_Train: 1.810
2024-06-21 01:35:58,988 - INFO: Epoch: 148/200, Batch: 7/29, Batch_Loss_Train: 1.955
2024-06-21 01:35:59,307 - INFO: Epoch: 148/200, Batch: 8/29, Batch_Loss_Train: 1.910
2024-06-21 01:35:59,703 - INFO: Epoch: 148/200, Batch: 9/29, Batch_Loss_Train: 1.591
2024-06-21 01:36:00,014 - INFO: Epoch: 148/200, Batch: 10/29, Batch_Loss_Train: 1.403
2024-06-21 01:36:00,416 - INFO: Epoch: 148/200, Batch: 11/29, Batch_Loss_Train: 1.599
2024-06-21 01:36:00,738 - INFO: Epoch: 148/200, Batch: 12/29, Batch_Loss_Train: 1.706
2024-06-21 01:36:01,148 - INFO: Epoch: 148/200, Batch: 13/29, Batch_Loss_Train: 1.399
2024-06-21 01:36:01,468 - INFO: Epoch: 148/200, Batch: 14/29, Batch_Loss_Train: 1.340
2024-06-21 01:36:01,873 - INFO: Epoch: 148/200, Batch: 15/29, Batch_Loss_Train: 1.448
2024-06-21 01:36:02,190 - INFO: Epoch: 148/200, Batch: 16/29, Batch_Loss_Train: 1.573
2024-06-21 01:36:02,899 - INFO: Epoch: 148/200, Batch: 17/29, Batch_Loss_Train: 1.412
2024-06-21 01:36:03,215 - INFO: Epoch: 148/200, Batch: 18/29, Batch_Loss_Train: 1.805
2024-06-21 01:36:03,613 - INFO: Epoch: 148/200, Batch: 19/29, Batch_Loss_Train: 1.231
2024-06-21 01:36:03,925 - INFO: Epoch: 148/200, Batch: 20/29, Batch_Loss_Train: 1.316
2024-06-21 01:36:04,330 - INFO: Epoch: 148/200, Batch: 21/29, Batch_Loss_Train: 1.893
2024-06-21 01:36:04,650 - INFO: Epoch: 148/200, Batch: 22/29, Batch_Loss_Train: 1.645
2024-06-21 01:36:05,062 - INFO: Epoch: 148/200, Batch: 23/29, Batch_Loss_Train: 1.418
2024-06-21 01:36:05,383 - INFO: Epoch: 148/200, Batch: 24/29, Batch_Loss_Train: 1.658
2024-06-21 01:36:05,787 - INFO: Epoch: 148/200, Batch: 25/29, Batch_Loss_Train: 2.081
2024-06-21 01:36:06,101 - INFO: Epoch: 148/200, Batch: 26/29, Batch_Loss_Train: 1.130
2024-06-21 01:36:06,502 - INFO: Epoch: 148/200, Batch: 27/29, Batch_Loss_Train: 2.105
2024-06-21 01:36:06,816 - INFO: Epoch: 148/200, Batch: 28/29, Batch_Loss_Train: 1.721
2024-06-21 01:36:07,039 - INFO: Epoch: 148/200, Batch: 29/29, Batch_Loss_Train: 1.125
2024-06-21 01:36:17,759 - INFO: 148/200 final results:
2024-06-21 01:36:17,759 - INFO: Training loss: 1.583.
2024-06-21 01:36:17,759 - INFO: Training MAE: 0.972.
2024-06-21 01:36:17,759 - INFO: Training MSE: 1.592.
2024-06-21 01:36:38,325 - INFO: Epoch: 148/200, Loss_train: 1.5834034763533493, Loss_val: 1.5790287317900822
2024-06-21 01:36:38,371 - INFO: Saved new best metric model for epoch 148.
2024-06-21 01:36:38,371 - INFO: Best internal validation val_loss: 1.579 at epoch: 148.
2024-06-21 01:36:38,371 - INFO: Epoch 149/200...
2024-06-21 01:36:38,371 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:36:38,371 - INFO: Batch size: 32.
2024-06-21 01:36:38,375 - INFO: Dataset:
2024-06-21 01:36:38,376 - INFO: Batch size:
2024-06-21 01:36:38,376 - INFO: Number of workers:
2024-06-21 01:36:39,645 - INFO: Epoch: 149/200, Batch: 1/29, Batch_Loss_Train: 1.568
2024-06-21 01:36:39,959 - INFO: Epoch: 149/200, Batch: 2/29, Batch_Loss_Train: 1.825
2024-06-21 01:36:40,383 - INFO: Epoch: 149/200, Batch: 3/29, Batch_Loss_Train: 1.200
2024-06-21 01:36:40,709 - INFO: Epoch: 149/200, Batch: 4/29, Batch_Loss_Train: 1.452
2024-06-21 01:36:41,134 - INFO: Epoch: 149/200, Batch: 5/29, Batch_Loss_Train: 1.714
2024-06-21 01:36:41,441 - INFO: Epoch: 149/200, Batch: 6/29, Batch_Loss_Train: 1.503
2024-06-21 01:36:41,849 - INFO: Epoch: 149/200, Batch: 7/29, Batch_Loss_Train: 1.313
2024-06-21 01:36:42,169 - INFO: Epoch: 149/200, Batch: 8/29, Batch_Loss_Train: 1.588
2024-06-21 01:36:42,589 - INFO: Epoch: 149/200, Batch: 9/29, Batch_Loss_Train: 1.500
2024-06-21 01:36:42,891 - INFO: Epoch: 149/200, Batch: 10/29, Batch_Loss_Train: 1.143
2024-06-21 01:36:43,302 - INFO: Epoch: 149/200, Batch: 11/29, Batch_Loss_Train: 1.603
2024-06-21 01:36:43,624 - INFO: Epoch: 149/200, Batch: 12/29, Batch_Loss_Train: 1.341
2024-06-21 01:36:44,055 - INFO: Epoch: 149/200, Batch: 13/29, Batch_Loss_Train: 1.440
2024-06-21 01:36:44,363 - INFO: Epoch: 149/200, Batch: 14/29, Batch_Loss_Train: 1.600
2024-06-21 01:36:44,779 - INFO: Epoch: 149/200, Batch: 15/29, Batch_Loss_Train: 1.051
2024-06-21 01:36:45,096 - INFO: Epoch: 149/200, Batch: 16/29, Batch_Loss_Train: 1.591
2024-06-21 01:36:45,518 - INFO: Epoch: 149/200, Batch: 17/29, Batch_Loss_Train: 1.332
2024-06-21 01:36:45,821 - INFO: Epoch: 149/200, Batch: 18/29, Batch_Loss_Train: 1.661
2024-06-21 01:36:46,227 - INFO: Epoch: 149/200, Batch: 19/29, Batch_Loss_Train: 1.410
2024-06-21 01:36:46,539 - INFO: Epoch: 149/200, Batch: 20/29, Batch_Loss_Train: 2.077
2024-06-21 01:36:46,959 - INFO: Epoch: 149/200, Batch: 21/29, Batch_Loss_Train: 1.681
2024-06-21 01:36:47,265 - INFO: Epoch: 149/200, Batch: 22/29, Batch_Loss_Train: 1.402
2024-06-21 01:36:47,676 - INFO: Epoch: 149/200, Batch: 23/29, Batch_Loss_Train: 1.290
2024-06-21 01:36:47,994 - INFO: Epoch: 149/200, Batch: 24/29, Batch_Loss_Train: 1.902
2024-06-21 01:36:48,409 - INFO: Epoch: 149/200, Batch: 25/29, Batch_Loss_Train: 2.089
2024-06-21 01:36:48,711 - INFO: Epoch: 149/200, Batch: 26/29, Batch_Loss_Train: 1.206
2024-06-21 01:36:49,111 - INFO: Epoch: 149/200, Batch: 27/29, Batch_Loss_Train: 1.435
2024-06-21 01:36:49,425 - INFO: Epoch: 149/200, Batch: 28/29, Batch_Loss_Train: 1.692
2024-06-21 01:36:49,647 - INFO: Epoch: 149/200, Batch: 29/29, Batch_Loss_Train: 1.406
2024-06-21 01:37:00,704 - INFO: 149/200 final results:
2024-06-21 01:37:00,704 - INFO: Training loss: 1.518.
2024-06-21 01:37:00,704 - INFO: Training MAE: 0.954.
2024-06-21 01:37:00,704 - INFO: Training MSE: 1.520.
2024-06-21 01:37:21,051 - INFO: Epoch: 149/200, Loss_train: 1.5177803573937252, Loss_val: 1.5713674631612053
2024-06-21 01:37:21,099 - INFO: Saved new best metric model for epoch 149.
2024-06-21 01:37:21,099 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:37:21,099 - INFO: Epoch 150/200...
2024-06-21 01:37:21,099 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:37:21,099 - INFO: Batch size: 32.
2024-06-21 01:37:21,104 - INFO: Dataset:
2024-06-21 01:37:21,104 - INFO: Batch size:
2024-06-21 01:37:21,104 - INFO: Number of workers:
2024-06-21 01:37:22,336 - INFO: Epoch: 150/200, Batch: 1/29, Batch_Loss_Train: 1.696
2024-06-21 01:37:22,647 - INFO: Epoch: 150/200, Batch: 2/29, Batch_Loss_Train: 1.874
2024-06-21 01:37:23,065 - INFO: Epoch: 150/200, Batch: 3/29, Batch_Loss_Train: 1.559
2024-06-21 01:37:23,389 - INFO: Epoch: 150/200, Batch: 4/29, Batch_Loss_Train: 1.602
2024-06-21 01:37:23,825 - INFO: Epoch: 150/200, Batch: 5/29, Batch_Loss_Train: 1.240
2024-06-21 01:37:24,133 - INFO: Epoch: 150/200, Batch: 6/29, Batch_Loss_Train: 1.819
2024-06-21 01:37:24,530 - INFO: Epoch: 150/200, Batch: 7/29, Batch_Loss_Train: 1.429
2024-06-21 01:37:24,850 - INFO: Epoch: 150/200, Batch: 8/29, Batch_Loss_Train: 1.307
2024-06-21 01:37:25,282 - INFO: Epoch: 150/200, Batch: 9/29, Batch_Loss_Train: 1.209
2024-06-21 01:37:25,584 - INFO: Epoch: 150/200, Batch: 10/29, Batch_Loss_Train: 1.441
2024-06-21 01:37:25,976 - INFO: Epoch: 150/200, Batch: 11/29, Batch_Loss_Train: 1.649
2024-06-21 01:37:26,299 - INFO: Epoch: 150/200, Batch: 12/29, Batch_Loss_Train: 1.957
2024-06-21 01:37:26,743 - INFO: Epoch: 150/200, Batch: 13/29, Batch_Loss_Train: 1.052
2024-06-21 01:37:27,052 - INFO: Epoch: 150/200, Batch: 14/29, Batch_Loss_Train: 1.471
2024-06-21 01:37:27,458 - INFO: Epoch: 150/200, Batch: 15/29, Batch_Loss_Train: 1.400
2024-06-21 01:37:27,776 - INFO: Epoch: 150/200, Batch: 16/29, Batch_Loss_Train: 1.681
2024-06-21 01:37:28,208 - INFO: Epoch: 150/200, Batch: 17/29, Batch_Loss_Train: 1.572
2024-06-21 01:37:28,513 - INFO: Epoch: 150/200, Batch: 18/29, Batch_Loss_Train: 1.506
2024-06-21 01:37:28,909 - INFO: Epoch: 150/200, Batch: 19/29, Batch_Loss_Train: 1.305
2024-06-21 01:37:29,224 - INFO: Epoch: 150/200, Batch: 20/29, Batch_Loss_Train: 1.540
2024-06-21 01:37:29,655 - INFO: Epoch: 150/200, Batch: 21/29, Batch_Loss_Train: 1.378
2024-06-21 01:37:29,964 - INFO: Epoch: 150/200, Batch: 22/29, Batch_Loss_Train: 1.811
2024-06-21 01:37:30,352 - INFO: Epoch: 150/200, Batch: 23/29, Batch_Loss_Train: 1.253
2024-06-21 01:37:30,673 - INFO: Epoch: 150/200, Batch: 24/29, Batch_Loss_Train: 1.416
2024-06-21 01:37:31,096 - INFO: Epoch: 150/200, Batch: 25/29, Batch_Loss_Train: 1.444
2024-06-21 01:37:31,399 - INFO: Epoch: 150/200, Batch: 26/29, Batch_Loss_Train: 1.293
2024-06-21 01:37:31,772 - INFO: Epoch: 150/200, Batch: 27/29, Batch_Loss_Train: 1.646
2024-06-21 01:37:32,088 - INFO: Epoch: 150/200, Batch: 28/29, Batch_Loss_Train: 1.413
2024-06-21 01:37:32,299 - INFO: Epoch: 150/200, Batch: 29/29, Batch_Loss_Train: 1.798
2024-06-21 01:37:43,298 - INFO: 150/200 final results:
2024-06-21 01:37:43,298 - INFO: Training loss: 1.509.
2024-06-21 01:37:43,298 - INFO: Training MAE: 0.949.
2024-06-21 01:37:43,298 - INFO: Training MSE: 1.503.
2024-06-21 01:38:03,946 - INFO: Epoch: 150/200, Loss_train: 1.5090478000969723, Loss_val: 1.6559528564584667
2024-06-21 01:38:03,946 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:38:03,946 - INFO: Epoch 151/200...
2024-06-21 01:38:03,946 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:38:03,946 - INFO: Batch size: 32.
2024-06-21 01:38:03,950 - INFO: Dataset:
2024-06-21 01:38:03,950 - INFO: Batch size:
2024-06-21 01:38:03,950 - INFO: Number of workers:
2024-06-21 01:38:05,218 - INFO: Epoch: 151/200, Batch: 1/29, Batch_Loss_Train: 1.487
2024-06-21 01:38:05,530 - INFO: Epoch: 151/200, Batch: 2/29, Batch_Loss_Train: 1.213
2024-06-21 01:38:05,950 - INFO: Epoch: 151/200, Batch: 3/29, Batch_Loss_Train: 1.242
2024-06-21 01:38:06,274 - INFO: Epoch: 151/200, Batch: 4/29, Batch_Loss_Train: 1.238
2024-06-21 01:38:06,697 - INFO: Epoch: 151/200, Batch: 5/29, Batch_Loss_Train: 1.341
2024-06-21 01:38:07,002 - INFO: Epoch: 151/200, Batch: 6/29, Batch_Loss_Train: 1.549
2024-06-21 01:38:07,409 - INFO: Epoch: 151/200, Batch: 7/29, Batch_Loss_Train: 1.509
2024-06-21 01:38:07,727 - INFO: Epoch: 151/200, Batch: 8/29, Batch_Loss_Train: 1.269
2024-06-21 01:38:08,144 - INFO: Epoch: 151/200, Batch: 9/29, Batch_Loss_Train: 1.762
2024-06-21 01:38:08,443 - INFO: Epoch: 151/200, Batch: 10/29, Batch_Loss_Train: 1.692
2024-06-21 01:38:08,850 - INFO: Epoch: 151/200, Batch: 11/29, Batch_Loss_Train: 1.602
2024-06-21 01:38:09,170 - INFO: Epoch: 151/200, Batch: 12/29, Batch_Loss_Train: 0.941
2024-06-21 01:38:09,613 - INFO: Epoch: 151/200, Batch: 13/29, Batch_Loss_Train: 1.647
2024-06-21 01:38:09,921 - INFO: Epoch: 151/200, Batch: 14/29, Batch_Loss_Train: 1.412
2024-06-21 01:38:10,324 - INFO: Epoch: 151/200, Batch: 15/29, Batch_Loss_Train: 1.349
2024-06-21 01:38:10,641 - INFO: Epoch: 151/200, Batch: 16/29, Batch_Loss_Train: 1.110
2024-06-21 01:38:11,074 - INFO: Epoch: 151/200, Batch: 17/29, Batch_Loss_Train: 1.830
2024-06-21 01:38:11,377 - INFO: Epoch: 151/200, Batch: 18/29, Batch_Loss_Train: 1.268
2024-06-21 01:38:11,771 - INFO: Epoch: 151/200, Batch: 19/29, Batch_Loss_Train: 1.480
2024-06-21 01:38:12,083 - INFO: Epoch: 151/200, Batch: 20/29, Batch_Loss_Train: 1.437
2024-06-21 01:38:12,515 - INFO: Epoch: 151/200, Batch: 21/29, Batch_Loss_Train: 1.522
2024-06-21 01:38:12,821 - INFO: Epoch: 151/200, Batch: 22/29, Batch_Loss_Train: 1.619
2024-06-21 01:38:13,219 - INFO: Epoch: 151/200, Batch: 23/29, Batch_Loss_Train: 1.527
2024-06-21 01:38:13,539 - INFO: Epoch: 151/200, Batch: 24/29, Batch_Loss_Train: 1.298
2024-06-21 01:38:13,961 - INFO: Epoch: 151/200, Batch: 25/29, Batch_Loss_Train: 1.779
2024-06-21 01:38:14,261 - INFO: Epoch: 151/200, Batch: 26/29, Batch_Loss_Train: 1.815
2024-06-21 01:38:14,639 - INFO: Epoch: 151/200, Batch: 27/29, Batch_Loss_Train: 1.875
2024-06-21 01:38:14,952 - INFO: Epoch: 151/200, Batch: 28/29, Batch_Loss_Train: 1.540
2024-06-21 01:38:15,171 - INFO: Epoch: 151/200, Batch: 29/29, Batch_Loss_Train: 0.871
2024-06-21 01:38:26,146 - INFO: 151/200 final results:
2024-06-21 01:38:26,147 - INFO: Training loss: 1.456.
2024-06-21 01:38:26,147 - INFO: Training MAE: 0.929.
2024-06-21 01:38:26,147 - INFO: Training MSE: 1.468.
2024-06-21 01:38:46,537 - INFO: Epoch: 151/200, Loss_train: 1.455930087073096, Loss_val: 1.6199607828567768
2024-06-21 01:38:46,537 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:38:46,537 - INFO: Epoch 152/200...
2024-06-21 01:38:46,537 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:38:46,537 - INFO: Batch size: 32.
2024-06-21 01:38:46,541 - INFO: Dataset:
2024-06-21 01:38:46,541 - INFO: Batch size:
2024-06-21 01:38:46,542 - INFO: Number of workers:
2024-06-21 01:38:47,776 - INFO: Epoch: 152/200, Batch: 1/29, Batch_Loss_Train: 1.198
2024-06-21 01:38:48,102 - INFO: Epoch: 152/200, Batch: 2/29, Batch_Loss_Train: 1.311
2024-06-21 01:38:48,518 - INFO: Epoch: 152/200, Batch: 3/29, Batch_Loss_Train: 2.070
2024-06-21 01:38:48,840 - INFO: Epoch: 152/200, Batch: 4/29, Batch_Loss_Train: 1.556
2024-06-21 01:38:49,265 - INFO: Epoch: 152/200, Batch: 5/29, Batch_Loss_Train: 1.324
2024-06-21 01:38:49,570 - INFO: Epoch: 152/200, Batch: 6/29, Batch_Loss_Train: 1.005
2024-06-21 01:38:49,964 - INFO: Epoch: 152/200, Batch: 7/29, Batch_Loss_Train: 1.831
2024-06-21 01:38:50,282 - INFO: Epoch: 152/200, Batch: 8/29, Batch_Loss_Train: 1.329
2024-06-21 01:38:50,689 - INFO: Epoch: 152/200, Batch: 9/29, Batch_Loss_Train: 2.097
2024-06-21 01:38:50,989 - INFO: Epoch: 152/200, Batch: 10/29, Batch_Loss_Train: 1.374
2024-06-21 01:38:51,388 - INFO: Epoch: 152/200, Batch: 11/29, Batch_Loss_Train: 1.303
2024-06-21 01:38:51,711 - INFO: Epoch: 152/200, Batch: 12/29, Batch_Loss_Train: 1.036
2024-06-21 01:38:52,149 - INFO: Epoch: 152/200, Batch: 13/29, Batch_Loss_Train: 1.716
2024-06-21 01:38:52,459 - INFO: Epoch: 152/200, Batch: 14/29, Batch_Loss_Train: 1.709
2024-06-21 01:38:52,904 - INFO: Epoch: 152/200, Batch: 15/29, Batch_Loss_Train: 1.709
2024-06-21 01:38:53,234 - INFO: Epoch: 152/200, Batch: 16/29, Batch_Loss_Train: 1.630
2024-06-21 01:38:53,659 - INFO: Epoch: 152/200, Batch: 17/29, Batch_Loss_Train: 2.426
2024-06-21 01:38:53,964 - INFO: Epoch: 152/200, Batch: 18/29, Batch_Loss_Train: 1.280
2024-06-21 01:38:54,370 - INFO: Epoch: 152/200, Batch: 19/29, Batch_Loss_Train: 1.658
2024-06-21 01:38:54,684 - INFO: Epoch: 152/200, Batch: 20/29, Batch_Loss_Train: 1.451
2024-06-21 01:38:55,107 - INFO: Epoch: 152/200, Batch: 21/29, Batch_Loss_Train: 1.319
2024-06-21 01:38:55,414 - INFO: Epoch: 152/200, Batch: 22/29, Batch_Loss_Train: 1.619
2024-06-21 01:38:55,818 - INFO: Epoch: 152/200, Batch: 23/29, Batch_Loss_Train: 1.567
2024-06-21 01:38:56,139 - INFO: Epoch: 152/200, Batch: 24/29, Batch_Loss_Train: 1.428
2024-06-21 01:38:56,555 - INFO: Epoch: 152/200, Batch: 25/29, Batch_Loss_Train: 1.350
2024-06-21 01:38:56,858 - INFO: Epoch: 152/200, Batch: 26/29, Batch_Loss_Train: 1.134
2024-06-21 01:38:57,245 - INFO: Epoch: 152/200, Batch: 27/29, Batch_Loss_Train: 1.532
2024-06-21 01:38:57,561 - INFO: Epoch: 152/200, Batch: 28/29, Batch_Loss_Train: 1.476
2024-06-21 01:38:57,776 - INFO: Epoch: 152/200, Batch: 29/29, Batch_Loss_Train: 1.833
2024-06-21 01:39:08,808 - INFO: 152/200 final results:
2024-06-21 01:39:08,808 - INFO: Training loss: 1.527.
2024-06-21 01:39:08,808 - INFO: Training MAE: 0.955.
2024-06-21 01:39:08,808 - INFO: Training MSE: 1.521.
2024-06-21 01:39:29,507 - INFO: Epoch: 152/200, Loss_train: 1.5265861996288956, Loss_val: 1.5939923524856567
2024-06-21 01:39:29,507 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:39:29,507 - INFO: Epoch 153/200...
2024-06-21 01:39:29,507 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:39:29,507 - INFO: Batch size: 32.
2024-06-21 01:39:29,511 - INFO: Dataset:
2024-06-21 01:39:29,511 - INFO: Batch size:
2024-06-21 01:39:29,511 - INFO: Number of workers:
2024-06-21 01:39:30,786 - INFO: Epoch: 153/200, Batch: 1/29, Batch_Loss_Train: 1.281
2024-06-21 01:39:31,098 - INFO: Epoch: 153/200, Batch: 2/29, Batch_Loss_Train: 1.516
2024-06-21 01:39:31,493 - INFO: Epoch: 153/200, Batch: 3/29, Batch_Loss_Train: 1.137
2024-06-21 01:39:31,814 - INFO: Epoch: 153/200, Batch: 4/29, Batch_Loss_Train: 1.695
2024-06-21 01:39:32,246 - INFO: Epoch: 153/200, Batch: 5/29, Batch_Loss_Train: 1.668
2024-06-21 01:39:32,550 - INFO: Epoch: 153/200, Batch: 6/29, Batch_Loss_Train: 1.944
2024-06-21 01:39:32,925 - INFO: Epoch: 153/200, Batch: 7/29, Batch_Loss_Train: 1.277
2024-06-21 01:39:33,241 - INFO: Epoch: 153/200, Batch: 8/29, Batch_Loss_Train: 1.223
2024-06-21 01:39:33,669 - INFO: Epoch: 153/200, Batch: 9/29, Batch_Loss_Train: 1.467
2024-06-21 01:39:33,968 - INFO: Epoch: 153/200, Batch: 10/29, Batch_Loss_Train: 1.357
2024-06-21 01:39:34,350 - INFO: Epoch: 153/200, Batch: 11/29, Batch_Loss_Train: 1.330
2024-06-21 01:39:34,670 - INFO: Epoch: 153/200, Batch: 12/29, Batch_Loss_Train: 1.430
2024-06-21 01:39:35,112 - INFO: Epoch: 153/200, Batch: 13/29, Batch_Loss_Train: 1.653
2024-06-21 01:39:35,420 - INFO: Epoch: 153/200, Batch: 14/29, Batch_Loss_Train: 1.409
2024-06-21 01:39:35,809 - INFO: Epoch: 153/200, Batch: 15/29, Batch_Loss_Train: 2.036
2024-06-21 01:39:36,125 - INFO: Epoch: 153/200, Batch: 16/29, Batch_Loss_Train: 2.025
2024-06-21 01:39:36,556 - INFO: Epoch: 153/200, Batch: 17/29, Batch_Loss_Train: 1.230
2024-06-21 01:39:36,859 - INFO: Epoch: 153/200, Batch: 18/29, Batch_Loss_Train: 1.345
2024-06-21 01:39:37,239 - INFO: Epoch: 153/200, Batch: 19/29, Batch_Loss_Train: 1.978
2024-06-21 01:39:37,550 - INFO: Epoch: 153/200, Batch: 20/29, Batch_Loss_Train: 1.402
2024-06-21 01:39:37,981 - INFO: Epoch: 153/200, Batch: 21/29, Batch_Loss_Train: 1.606
2024-06-21 01:39:38,287 - INFO: Epoch: 153/200, Batch: 22/29, Batch_Loss_Train: 1.266
2024-06-21 01:39:38,671 - INFO: Epoch: 153/200, Batch: 23/29, Batch_Loss_Train: 1.427
2024-06-21 01:39:38,989 - INFO: Epoch: 153/200, Batch: 24/29, Batch_Loss_Train: 1.714
2024-06-21 01:39:39,406 - INFO: Epoch: 153/200, Batch: 25/29, Batch_Loss_Train: 1.162
2024-06-21 01:39:39,708 - INFO: Epoch: 153/200, Batch: 26/29, Batch_Loss_Train: 1.454
2024-06-21 01:39:40,082 - INFO: Epoch: 153/200, Batch: 27/29, Batch_Loss_Train: 1.987
2024-06-21 01:39:40,395 - INFO: Epoch: 153/200, Batch: 28/29, Batch_Loss_Train: 1.128
2024-06-21 01:39:40,611 - INFO: Epoch: 153/200, Batch: 29/29, Batch_Loss_Train: 1.917
2024-06-21 01:39:51,395 - INFO: 153/200 final results:
2024-06-21 01:39:51,395 - INFO: Training loss: 1.519.
2024-06-21 01:39:51,395 - INFO: Training MAE: 0.949.
2024-06-21 01:39:51,395 - INFO: Training MSE: 1.512.
2024-06-21 01:40:11,903 - INFO: Epoch: 153/200, Loss_train: 1.5194344191715634, Loss_val: 1.5914461838787999
2024-06-21 01:40:11,904 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:40:11,904 - INFO: Epoch 154/200...
2024-06-21 01:40:11,904 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:40:11,904 - INFO: Batch size: 32.
2024-06-21 01:40:11,908 - INFO: Dataset:
2024-06-21 01:40:11,908 - INFO: Batch size:
2024-06-21 01:40:11,908 - INFO: Number of workers:
2024-06-21 01:40:13,151 - INFO: Epoch: 154/200, Batch: 1/29, Batch_Loss_Train: 1.460
2024-06-21 01:40:13,463 - INFO: Epoch: 154/200, Batch: 2/29, Batch_Loss_Train: 1.822
2024-06-21 01:40:13,881 - INFO: Epoch: 154/200, Batch: 3/29, Batch_Loss_Train: 1.532
2024-06-21 01:40:14,205 - INFO: Epoch: 154/200, Batch: 4/29, Batch_Loss_Train: 1.209
2024-06-21 01:40:14,613 - INFO: Epoch: 154/200, Batch: 5/29, Batch_Loss_Train: 1.649
2024-06-21 01:40:14,917 - INFO: Epoch: 154/200, Batch: 6/29, Batch_Loss_Train: 1.539
2024-06-21 01:40:15,320 - INFO: Epoch: 154/200, Batch: 7/29, Batch_Loss_Train: 1.516
2024-06-21 01:40:15,638 - INFO: Epoch: 154/200, Batch: 8/29, Batch_Loss_Train: 1.278
2024-06-21 01:40:16,037 - INFO: Epoch: 154/200, Batch: 9/29, Batch_Loss_Train: 1.528
2024-06-21 01:40:16,337 - INFO: Epoch: 154/200, Batch: 10/29, Batch_Loss_Train: 1.391
2024-06-21 01:40:16,738 - INFO: Epoch: 154/200, Batch: 11/29, Batch_Loss_Train: 1.304
2024-06-21 01:40:17,057 - INFO: Epoch: 154/200, Batch: 12/29, Batch_Loss_Train: 1.725
2024-06-21 01:40:17,478 - INFO: Epoch: 154/200, Batch: 13/29, Batch_Loss_Train: 1.380
2024-06-21 01:40:17,785 - INFO: Epoch: 154/200, Batch: 14/29, Batch_Loss_Train: 1.222
2024-06-21 01:40:18,197 - INFO: Epoch: 154/200, Batch: 15/29, Batch_Loss_Train: 1.274
2024-06-21 01:40:18,512 - INFO: Epoch: 154/200, Batch: 16/29, Batch_Loss_Train: 1.089
2024-06-21 01:40:18,915 - INFO: Epoch: 154/200, Batch: 17/29, Batch_Loss_Train: 1.486
2024-06-21 01:40:19,218 - INFO: Epoch: 154/200, Batch: 18/29, Batch_Loss_Train: 1.554
2024-06-21 01:40:19,618 - INFO: Epoch: 154/200, Batch: 19/29, Batch_Loss_Train: 1.636
2024-06-21 01:40:19,930 - INFO: Epoch: 154/200, Batch: 20/29, Batch_Loss_Train: 1.318
2024-06-21 01:40:20,337 - INFO: Epoch: 154/200, Batch: 21/29, Batch_Loss_Train: 1.508
2024-06-21 01:40:20,643 - INFO: Epoch: 154/200, Batch: 22/29, Batch_Loss_Train: 1.326
2024-06-21 01:40:21,045 - INFO: Epoch: 154/200, Batch: 23/29, Batch_Loss_Train: 1.896
2024-06-21 01:40:21,367 - INFO: Epoch: 154/200, Batch: 24/29, Batch_Loss_Train: 1.254
2024-06-21 01:40:21,785 - INFO: Epoch: 154/200, Batch: 25/29, Batch_Loss_Train: 1.607
2024-06-21 01:40:22,088 - INFO: Epoch: 154/200, Batch: 26/29, Batch_Loss_Train: 1.430
2024-06-21 01:40:22,490 - INFO: Epoch: 154/200, Batch: 27/29, Batch_Loss_Train: 1.329
2024-06-21 01:40:22,807 - INFO: Epoch: 154/200, Batch: 28/29, Batch_Loss_Train: 1.571
2024-06-21 01:40:23,032 - INFO: Epoch: 154/200, Batch: 29/29, Batch_Loss_Train: 1.857
2024-06-21 01:40:34,211 - INFO: 154/200 final results:
2024-06-21 01:40:34,211 - INFO: Training loss: 1.472.
2024-06-21 01:40:34,211 - INFO: Training MAE: 0.943.
2024-06-21 01:40:34,211 - INFO: Training MSE: 1.465.
2024-06-21 01:40:54,644 - INFO: Epoch: 154/200, Loss_train: 1.4721198123076866, Loss_val: 1.578586732519084
2024-06-21 01:40:54,644 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:40:54,644 - INFO: Epoch 155/200...
2024-06-21 01:40:54,644 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:40:54,644 - INFO: Batch size: 32.
2024-06-21 01:40:54,648 - INFO: Dataset:
2024-06-21 01:40:54,648 - INFO: Batch size:
2024-06-21 01:40:54,648 - INFO: Number of workers:
2024-06-21 01:40:55,888 - INFO: Epoch: 155/200, Batch: 1/29, Batch_Loss_Train: 1.221
2024-06-21 01:40:56,216 - INFO: Epoch: 155/200, Batch: 2/29, Batch_Loss_Train: 1.087
2024-06-21 01:40:56,635 - INFO: Epoch: 155/200, Batch: 3/29, Batch_Loss_Train: 1.047
2024-06-21 01:40:56,959 - INFO: Epoch: 155/200, Batch: 4/29, Batch_Loss_Train: 1.798
2024-06-21 01:40:57,369 - INFO: Epoch: 155/200, Batch: 5/29, Batch_Loss_Train: 1.152
2024-06-21 01:40:57,674 - INFO: Epoch: 155/200, Batch: 6/29, Batch_Loss_Train: 1.041
2024-06-21 01:40:58,080 - INFO: Epoch: 155/200, Batch: 7/29, Batch_Loss_Train: 1.737
2024-06-21 01:40:58,398 - INFO: Epoch: 155/200, Batch: 8/29, Batch_Loss_Train: 1.268
2024-06-21 01:40:58,810 - INFO: Epoch: 155/200, Batch: 9/29, Batch_Loss_Train: 1.135
2024-06-21 01:40:59,111 - INFO: Epoch: 155/200, Batch: 10/29, Batch_Loss_Train: 1.178
2024-06-21 01:40:59,516 - INFO: Epoch: 155/200, Batch: 11/29, Batch_Loss_Train: 1.901
2024-06-21 01:40:59,837 - INFO: Epoch: 155/200, Batch: 12/29, Batch_Loss_Train: 1.691
2024-06-21 01:41:00,257 - INFO: Epoch: 155/200, Batch: 13/29, Batch_Loss_Train: 1.627
2024-06-21 01:41:00,566 - INFO: Epoch: 155/200, Batch: 14/29, Batch_Loss_Train: 1.154
2024-06-21 01:41:00,983 - INFO: Epoch: 155/200, Batch: 15/29, Batch_Loss_Train: 1.522
2024-06-21 01:41:01,300 - INFO: Epoch: 155/200, Batch: 16/29, Batch_Loss_Train: 1.674
2024-06-21 01:41:01,712 - INFO: Epoch: 155/200, Batch: 17/29, Batch_Loss_Train: 1.044
2024-06-21 01:41:02,015 - INFO: Epoch: 155/200, Batch: 18/29, Batch_Loss_Train: 1.424
2024-06-21 01:41:02,416 - INFO: Epoch: 155/200, Batch: 19/29, Batch_Loss_Train: 1.578
2024-06-21 01:41:02,727 - INFO: Epoch: 155/200, Batch: 20/29, Batch_Loss_Train: 1.600
2024-06-21 01:41:03,130 - INFO: Epoch: 155/200, Batch: 21/29, Batch_Loss_Train: 1.736
2024-06-21 01:41:03,436 - INFO: Epoch: 155/200, Batch: 22/29, Batch_Loss_Train: 1.304
2024-06-21 01:41:03,836 - INFO: Epoch: 155/200, Batch: 23/29, Batch_Loss_Train: 1.397
2024-06-21 01:41:04,154 - INFO: Epoch: 155/200, Batch: 24/29, Batch_Loss_Train: 1.529
2024-06-21 01:41:04,564 - INFO: Epoch: 155/200, Batch: 25/29, Batch_Loss_Train: 1.765
2024-06-21 01:41:04,866 - INFO: Epoch: 155/200, Batch: 26/29, Batch_Loss_Train: 1.248
2024-06-21 01:41:05,262 - INFO: Epoch: 155/200, Batch: 27/29, Batch_Loss_Train: 1.028
2024-06-21 01:41:05,576 - INFO: Epoch: 155/200, Batch: 28/29, Batch_Loss_Train: 1.717
2024-06-21 01:41:05,791 - INFO: Epoch: 155/200, Batch: 29/29, Batch_Loss_Train: 1.099
2024-06-21 01:41:16,792 - INFO: 155/200 final results:
2024-06-21 01:41:16,793 - INFO: Training loss: 1.403.
2024-06-21 01:41:16,793 - INFO: Training MAE: 0.925.
2024-06-21 01:41:16,793 - INFO: Training MSE: 1.409.
2024-06-21 01:41:37,660 - INFO: Epoch: 155/200, Loss_train: 1.4034710547019695, Loss_val: 1.5989746924104362
2024-06-21 01:41:37,661 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:41:37,661 - INFO: Epoch 156/200...
2024-06-21 01:41:37,661 - INFO: Learning rate: 6.887186062828659e-06.
2024-06-21 01:41:37,661 - INFO: Batch size: 32.
2024-06-21 01:41:37,665 - INFO: Dataset:
2024-06-21 01:41:37,665 - INFO: Batch size:
2024-06-21 01:41:37,665 - INFO: Number of workers:
2024-06-21 01:41:38,891 - INFO: Epoch: 156/200, Batch: 1/29, Batch_Loss_Train: 1.403
2024-06-21 01:41:39,218 - INFO: Epoch: 156/200, Batch: 2/29, Batch_Loss_Train: 1.849
2024-06-21 01:41:39,636 - INFO: Epoch: 156/200, Batch: 3/29, Batch_Loss_Train: 1.513
2024-06-21 01:41:39,961 - INFO: Epoch: 156/200, Batch: 4/29, Batch_Loss_Train: 1.422
2024-06-21 01:41:40,381 - INFO: Epoch: 156/200, Batch: 5/29, Batch_Loss_Train: 1.779
2024-06-21 01:41:40,689 - INFO: Epoch: 156/200, Batch: 6/29, Batch_Loss_Train: 1.288
2024-06-21 01:41:41,096 - INFO: Epoch: 156/200, Batch: 7/29, Batch_Loss_Train: 1.263
2024-06-21 01:41:41,415 - INFO: Epoch: 156/200, Batch: 8/29, Batch_Loss_Train: 1.134
2024-06-21 01:41:41,836 - INFO: Epoch: 156/200, Batch: 9/29, Batch_Loss_Train: 1.512
2024-06-21 01:41:42,138 - INFO: Epoch: 156/200, Batch: 10/29, Batch_Loss_Train: 1.126
2024-06-21 01:41:42,541 - INFO: Epoch: 156/200, Batch: 11/29, Batch_Loss_Train: 1.376
2024-06-21 01:41:42,863 - INFO: Epoch: 156/200, Batch: 12/29, Batch_Loss_Train: 2.167
2024-06-21 01:41:43,292 - INFO: Epoch: 156/200, Batch: 13/29, Batch_Loss_Train: 1.186
2024-06-21 01:41:43,602 - INFO: Epoch: 156/200, Batch: 14/29, Batch_Loss_Train: 1.082
2024-06-21 01:41:44,022 - INFO: Epoch: 156/200, Batch: 15/29, Batch_Loss_Train: 1.405
2024-06-21 01:41:44,340 - INFO: Epoch: 156/200, Batch: 16/29, Batch_Loss_Train: 1.635
2024-06-21 01:41:44,746 - INFO: Epoch: 156/200, Batch: 17/29, Batch_Loss_Train: 1.220
2024-06-21 01:41:45,048 - INFO: Epoch: 156/200, Batch: 18/29, Batch_Loss_Train: 1.644
2024-06-21 01:41:45,457 - INFO: Epoch: 156/200, Batch: 19/29, Batch_Loss_Train: 1.633
2024-06-21 01:41:45,771 - INFO: Epoch: 156/200, Batch: 20/29, Batch_Loss_Train: 1.437
2024-06-21 01:41:46,190 - INFO: Epoch: 156/200, Batch: 21/29, Batch_Loss_Train: 1.406
2024-06-21 01:41:46,498 - INFO: Epoch: 156/200, Batch: 22/29, Batch_Loss_Train: 1.696
2024-06-21 01:41:46,910 - INFO: Epoch: 156/200, Batch: 23/29, Batch_Loss_Train: 1.467
2024-06-21 01:41:47,230 - INFO: Epoch: 156/200, Batch: 24/29, Batch_Loss_Train: 1.313
2024-06-21 01:41:47,648 - INFO: Epoch: 156/200, Batch: 25/29, Batch_Loss_Train: 1.800
2024-06-21 01:41:47,951 - INFO: Epoch: 156/200, Batch: 26/29, Batch_Loss_Train: 1.259
2024-06-21 01:41:48,350 - INFO: Epoch: 156/200, Batch: 27/29, Batch_Loss_Train: 1.654
2024-06-21 01:41:48,666 - INFO: Epoch: 156/200, Batch: 28/29, Batch_Loss_Train: 1.705
2024-06-21 01:41:48,885 - INFO: Epoch: 156/200, Batch: 29/29, Batch_Loss_Train: 1.993
2024-06-21 01:41:59,932 - INFO: 156/200 final results:
2024-06-21 01:41:59,932 - INFO: Training loss: 1.495.
2024-06-21 01:41:59,932 - INFO: Training MAE: 0.944.
2024-06-21 01:41:59,932 - INFO: Training MSE: 1.486.
2024-06-21 01:42:20,330 - INFO: Epoch: 156/200, Loss_train: 1.4954768583692353, Loss_val: 1.5900229133408645
2024-06-21 01:42:20,331 - INFO: Best internal validation val_loss: 1.571 at epoch: 149.
2024-06-21 01:42:20,331 - INFO: Epoch 157/200...
2024-06-21 01:42:20,331 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:42:20,331 - INFO: Batch size: 32.
2024-06-21 01:42:20,335 - INFO: Dataset:
2024-06-21 01:42:20,335 - INFO: Batch size:
2024-06-21 01:42:20,335 - INFO: Number of workers:
2024-06-21 01:42:21,618 - INFO: Epoch: 157/200, Batch: 1/29, Batch_Loss_Train: 1.186
2024-06-21 01:42:21,928 - INFO: Epoch: 157/200, Batch: 2/29, Batch_Loss_Train: 1.625
2024-06-21 01:42:22,331 - INFO: Epoch: 157/200, Batch: 3/29, Batch_Loss_Train: 1.880
2024-06-21 01:42:22,655 - INFO: Epoch: 157/200, Batch: 4/29, Batch_Loss_Train: 2.027
2024-06-21 01:42:23,073 - INFO: Epoch: 157/200, Batch: 5/29, Batch_Loss_Train: 1.502
2024-06-21 01:42:23,375 - INFO: Epoch: 157/200, Batch: 6/29, Batch_Loss_Train: 1.405
2024-06-21 01:42:23,779 - INFO: Epoch: 157/200, Batch: 7/29, Batch_Loss_Train: 1.305
2024-06-21 01:42:24,096 - INFO: Epoch: 157/200, Batch: 8/29, Batch_Loss_Train: 1.189
2024-06-21 01:42:24,506 - INFO: Epoch: 157/200, Batch: 9/29, Batch_Loss_Train: 1.755
2024-06-21 01:42:24,804 - INFO: Epoch: 157/200, Batch: 10/29, Batch_Loss_Train: 1.514
2024-06-21 01:42:25,214 - INFO: Epoch: 157/200, Batch: 11/29, Batch_Loss_Train: 1.264
2024-06-21 01:42:25,533 - INFO: Epoch: 157/200, Batch: 12/29, Batch_Loss_Train: 1.358
2024-06-21 01:42:25,957 - INFO: Epoch: 157/200, Batch: 13/29, Batch_Loss_Train: 2.136
2024-06-21 01:42:26,263 - INFO: Epoch: 157/200, Batch: 14/29, Batch_Loss_Train: 1.862
2024-06-21 01:42:26,679 - INFO: Epoch: 157/200, Batch: 15/29, Batch_Loss_Train: 1.173
2024-06-21 01:42:26,994 - INFO: Epoch: 157/200, Batch: 16/29, Batch_Loss_Train: 1.570
2024-06-21 01:42:27,404 - INFO: Epoch: 157/200, Batch: 17/29, Batch_Loss_Train: 1.369
2024-06-21 01:42:27,705 - INFO: Epoch: 157/200, Batch: 18/29, Batch_Loss_Train: 1.420
2024-06-21 01:42:28,109 - INFO: Epoch: 157/200, Batch: 19/29, Batch_Loss_Train: 1.473
2024-06-21 01:42:28,420 - INFO: Epoch: 157/200, Batch: 20/29, Batch_Loss_Train: 1.541
2024-06-21 01:42:28,830 - INFO: Epoch: 157/200, Batch: 21/29, Batch_Loss_Train: 1.934
2024-06-21 01:42:29,136 - INFO: Epoch: 157/200, Batch: 22/29, Batch_Loss_Train: 1.323
2024-06-21 01:42:29,544 - INFO: Epoch: 157/200, Batch: 23/29, Batch_Loss_Train: 1.128
2024-06-21 01:42:29,862 - INFO: Epoch: 157/200, Batch: 24/29, Batch_Loss_Train: 1.509
2024-06-21 01:42:30,276 - INFO: Epoch: 157/200, Batch: 25/29, Batch_Loss_Train: 1.280
2024-06-21 01:42:30,576 - INFO: Epoch: 157/200, Batch: 26/29, Batch_Loss_Train: 1.339
2024-06-21 01:42:30,975 - INFO: Epoch: 157/200, Batch: 27/29, Batch_Loss_Train: 1.515
2024-06-21 01:42:31,288 - INFO: Epoch: 157/200, Batch: 28/29, Batch_Loss_Train: 1.333
2024-06-21 01:42:31,510 - INFO: Epoch: 157/200, Batch: 29/29, Batch_Loss_Train: 1.506
2024-06-21 01:42:42,485 - INFO: 157/200 final results:
2024-06-21 01:42:42,485 - INFO: Training loss: 1.497.
2024-06-21 01:42:42,485 - INFO: Training MAE: 0.946.
2024-06-21 01:42:42,485 - INFO: Training MSE: 1.497.
2024-06-21 01:43:02,828 - INFO: Epoch: 157/200, Loss_train: 1.497300020579634, Loss_val: 1.566176492592384
2024-06-21 01:43:02,876 - INFO: Saved new best metric model for epoch 157.
2024-06-21 01:43:02,876 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:43:02,876 - INFO: Epoch 158/200...
2024-06-21 01:43:02,876 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:43:02,876 - INFO: Batch size: 32.
2024-06-21 01:43:02,880 - INFO: Dataset:
2024-06-21 01:43:02,880 - INFO: Batch size:
2024-06-21 01:43:02,880 - INFO: Number of workers:
2024-06-21 01:43:04,124 - INFO: Epoch: 158/200, Batch: 1/29, Batch_Loss_Train: 1.286
2024-06-21 01:43:04,449 - INFO: Epoch: 158/200, Batch: 2/29, Batch_Loss_Train: 1.156
2024-06-21 01:43:04,870 - INFO: Epoch: 158/200, Batch: 3/29, Batch_Loss_Train: 1.810
2024-06-21 01:43:05,195 - INFO: Epoch: 158/200, Batch: 4/29, Batch_Loss_Train: 1.282
2024-06-21 01:43:05,597 - INFO: Epoch: 158/200, Batch: 5/29, Batch_Loss_Train: 1.370
2024-06-21 01:43:05,914 - INFO: Epoch: 158/200, Batch: 6/29, Batch_Loss_Train: 1.370
2024-06-21 01:43:06,318 - INFO: Epoch: 158/200, Batch: 7/29, Batch_Loss_Train: 1.404
2024-06-21 01:43:06,635 - INFO: Epoch: 158/200, Batch: 8/29, Batch_Loss_Train: 1.209
2024-06-21 01:43:07,031 - INFO: Epoch: 158/200, Batch: 9/29, Batch_Loss_Train: 1.545
2024-06-21 01:43:07,342 - INFO: Epoch: 158/200, Batch: 10/29, Batch_Loss_Train: 1.247
2024-06-21 01:43:07,752 - INFO: Epoch: 158/200, Batch: 11/29, Batch_Loss_Train: 1.283
2024-06-21 01:43:08,071 - INFO: Epoch: 158/200, Batch: 12/29, Batch_Loss_Train: 1.334
2024-06-21 01:43:08,488 - INFO: Epoch: 158/200, Batch: 13/29, Batch_Loss_Train: 1.831
2024-06-21 01:43:08,808 - INFO: Epoch: 158/200, Batch: 14/29, Batch_Loss_Train: 1.269
2024-06-21 01:43:09,223 - INFO: Epoch: 158/200, Batch: 15/29, Batch_Loss_Train: 1.753
2024-06-21 01:43:09,538 - INFO: Epoch: 158/200, Batch: 16/29, Batch_Loss_Train: 1.646
2024-06-21 01:43:09,938 - INFO: Epoch: 158/200, Batch: 17/29, Batch_Loss_Train: 1.225
2024-06-21 01:43:10,253 - INFO: Epoch: 158/200, Batch: 18/29, Batch_Loss_Train: 1.422
2024-06-21 01:43:10,658 - INFO: Epoch: 158/200, Batch: 19/29, Batch_Loss_Train: 1.099
2024-06-21 01:43:10,970 - INFO: Epoch: 158/200, Batch: 20/29, Batch_Loss_Train: 1.226
2024-06-21 01:43:11,369 - INFO: Epoch: 158/200, Batch: 21/29, Batch_Loss_Train: 1.046
2024-06-21 01:43:11,687 - INFO: Epoch: 158/200, Batch: 22/29, Batch_Loss_Train: 1.242
2024-06-21 01:43:12,094 - INFO: Epoch: 158/200, Batch: 23/29, Batch_Loss_Train: 1.456
2024-06-21 01:43:12,412 - INFO: Epoch: 158/200, Batch: 24/29, Batch_Loss_Train: 1.407
2024-06-21 01:43:12,816 - INFO: Epoch: 158/200, Batch: 25/29, Batch_Loss_Train: 1.535
2024-06-21 01:43:13,129 - INFO: Epoch: 158/200, Batch: 26/29, Batch_Loss_Train: 1.334
2024-06-21 01:43:13,530 - INFO: Epoch: 158/200, Batch: 27/29, Batch_Loss_Train: 0.994
2024-06-21 01:43:13,844 - INFO: Epoch: 158/200, Batch: 28/29, Batch_Loss_Train: 1.494
2024-06-21 01:43:14,067 - INFO: Epoch: 158/200, Batch: 29/29, Batch_Loss_Train: 0.828
2024-06-21 01:43:25,047 - INFO: 158/200 final results:
2024-06-21 01:43:25,047 - INFO: Training loss: 1.348.
2024-06-21 01:43:25,047 - INFO: Training MAE: 0.911.
2024-06-21 01:43:25,047 - INFO: Training MSE: 1.359.
2024-06-21 01:43:45,380 - INFO: Epoch: 158/200, Loss_train: 1.34834152254565, Loss_val: 1.5770938047047318
2024-06-21 01:43:45,380 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:43:45,380 - INFO: Epoch 159/200...
2024-06-21 01:43:45,380 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:43:45,380 - INFO: Batch size: 32.
2024-06-21 01:43:45,385 - INFO: Dataset:
2024-06-21 01:43:45,385 - INFO: Batch size:
2024-06-21 01:43:45,385 - INFO: Number of workers:
2024-06-21 01:43:46,650 - INFO: Epoch: 159/200, Batch: 1/29, Batch_Loss_Train: 1.296
2024-06-21 01:43:46,975 - INFO: Epoch: 159/200, Batch: 2/29, Batch_Loss_Train: 1.836
2024-06-21 01:43:47,380 - INFO: Epoch: 159/200, Batch: 3/29, Batch_Loss_Train: 1.024
2024-06-21 01:43:47,704 - INFO: Epoch: 159/200, Batch: 4/29, Batch_Loss_Train: 1.580
2024-06-21 01:43:48,135 - INFO: Epoch: 159/200, Batch: 5/29, Batch_Loss_Train: 1.729
2024-06-21 01:43:48,440 - INFO: Epoch: 159/200, Batch: 6/29, Batch_Loss_Train: 1.467
2024-06-21 01:43:48,830 - INFO: Epoch: 159/200, Batch: 7/29, Batch_Loss_Train: 1.113
2024-06-21 01:43:49,149 - INFO: Epoch: 159/200, Batch: 8/29, Batch_Loss_Train: 1.327
2024-06-21 01:43:49,574 - INFO: Epoch: 159/200, Batch: 9/29, Batch_Loss_Train: 1.343
2024-06-21 01:43:49,872 - INFO: Epoch: 159/200, Batch: 10/29, Batch_Loss_Train: 1.455
2024-06-21 01:43:50,264 - INFO: Epoch: 159/200, Batch: 11/29, Batch_Loss_Train: 1.469
2024-06-21 01:43:50,584 - INFO: Epoch: 159/200, Batch: 12/29, Batch_Loss_Train: 1.468
2024-06-21 01:43:51,026 - INFO: Epoch: 159/200, Batch: 13/29, Batch_Loss_Train: 1.834
2024-06-21 01:43:51,334 - INFO: Epoch: 159/200, Batch: 14/29, Batch_Loss_Train: 1.348
2024-06-21 01:43:51,735 - INFO: Epoch: 159/200, Batch: 15/29, Batch_Loss_Train: 1.187
2024-06-21 01:43:52,052 - INFO: Epoch: 159/200, Batch: 16/29, Batch_Loss_Train: 2.168
2024-06-21 01:43:52,479 - INFO: Epoch: 159/200, Batch: 17/29, Batch_Loss_Train: 1.237
2024-06-21 01:43:52,782 - INFO: Epoch: 159/200, Batch: 18/29, Batch_Loss_Train: 2.131
2024-06-21 01:43:53,173 - INFO: Epoch: 159/200, Batch: 19/29, Batch_Loss_Train: 1.622
2024-06-21 01:43:53,485 - INFO: Epoch: 159/200, Batch: 20/29, Batch_Loss_Train: 1.506
2024-06-21 01:43:53,913 - INFO: Epoch: 159/200, Batch: 21/29, Batch_Loss_Train: 1.260
2024-06-21 01:43:54,219 - INFO: Epoch: 159/200, Batch: 22/29, Batch_Loss_Train: 1.259
2024-06-21 01:43:54,606 - INFO: Epoch: 159/200, Batch: 23/29, Batch_Loss_Train: 1.627
2024-06-21 01:43:54,925 - INFO: Epoch: 159/200, Batch: 24/29, Batch_Loss_Train: 1.297
2024-06-21 01:43:55,344 - INFO: Epoch: 159/200, Batch: 25/29, Batch_Loss_Train: 1.089
2024-06-21 01:43:55,645 - INFO: Epoch: 159/200, Batch: 26/29, Batch_Loss_Train: 1.337
2024-06-21 01:43:56,025 - INFO: Epoch: 159/200, Batch: 27/29, Batch_Loss_Train: 1.486
2024-06-21 01:43:56,338 - INFO: Epoch: 159/200, Batch: 28/29, Batch_Loss_Train: 1.536
2024-06-21 01:43:56,551 - INFO: Epoch: 159/200, Batch: 29/29, Batch_Loss_Train: 1.563
2024-06-21 01:44:07,593 - INFO: 159/200 final results:
2024-06-21 01:44:07,593 - INFO: Training loss: 1.469.
2024-06-21 01:44:07,593 - INFO: Training MAE: 0.940.
2024-06-21 01:44:07,593 - INFO: Training MSE: 1.467.
2024-06-21 01:44:27,906 - INFO: Epoch: 159/200, Loss_train: 1.4687189726993954, Loss_val: 1.5786436241248558
2024-06-21 01:44:27,906 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:44:27,906 - INFO: Epoch 160/200...
2024-06-21 01:44:27,906 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:44:27,906 - INFO: Batch size: 32.
2024-06-21 01:44:27,910 - INFO: Dataset:
2024-06-21 01:44:27,910 - INFO: Batch size:
2024-06-21 01:44:27,910 - INFO: Number of workers:
2024-06-21 01:44:29,166 - INFO: Epoch: 160/200, Batch: 1/29, Batch_Loss_Train: 1.383
2024-06-21 01:44:29,478 - INFO: Epoch: 160/200, Batch: 2/29, Batch_Loss_Train: 1.670
2024-06-21 01:44:29,895 - INFO: Epoch: 160/200, Batch: 3/29, Batch_Loss_Train: 0.863
2024-06-21 01:44:30,219 - INFO: Epoch: 160/200, Batch: 4/29, Batch_Loss_Train: 1.243
2024-06-21 01:44:30,640 - INFO: Epoch: 160/200, Batch: 5/29, Batch_Loss_Train: 0.922
2024-06-21 01:44:30,943 - INFO: Epoch: 160/200, Batch: 6/29, Batch_Loss_Train: 1.293
2024-06-21 01:44:31,345 - INFO: Epoch: 160/200, Batch: 7/29, Batch_Loss_Train: 1.579
2024-06-21 01:44:31,663 - INFO: Epoch: 160/200, Batch: 8/29, Batch_Loss_Train: 1.608
2024-06-21 01:44:32,068 - INFO: Epoch: 160/200, Batch: 9/29, Batch_Loss_Train: 1.946
2024-06-21 01:44:32,369 - INFO: Epoch: 160/200, Batch: 10/29, Batch_Loss_Train: 1.051
2024-06-21 01:44:32,776 - INFO: Epoch: 160/200, Batch: 11/29, Batch_Loss_Train: 1.517
2024-06-21 01:44:33,099 - INFO: Epoch: 160/200, Batch: 12/29, Batch_Loss_Train: 1.783
2024-06-21 01:44:33,519 - INFO: Epoch: 160/200, Batch: 13/29, Batch_Loss_Train: 1.319
2024-06-21 01:44:33,828 - INFO: Epoch: 160/200, Batch: 14/29, Batch_Loss_Train: 2.280
2024-06-21 01:44:34,243 - INFO: Epoch: 160/200, Batch: 15/29, Batch_Loss_Train: 1.332
2024-06-21 01:44:34,560 - INFO: Epoch: 160/200, Batch: 16/29, Batch_Loss_Train: 2.002
2024-06-21 01:44:34,969 - INFO: Epoch: 160/200, Batch: 17/29, Batch_Loss_Train: 1.153
2024-06-21 01:44:35,273 - INFO: Epoch: 160/200, Batch: 18/29, Batch_Loss_Train: 1.053
2024-06-21 01:44:35,677 - INFO: Epoch: 160/200, Batch: 19/29, Batch_Loss_Train: 1.091
2024-06-21 01:44:35,990 - INFO: Epoch: 160/200, Batch: 20/29, Batch_Loss_Train: 1.622
2024-06-21 01:44:36,398 - INFO: Epoch: 160/200, Batch: 21/29, Batch_Loss_Train: 1.740
2024-06-21 01:44:36,707 - INFO: Epoch: 160/200, Batch: 22/29, Batch_Loss_Train: 1.570
2024-06-21 01:44:37,111 - INFO: Epoch: 160/200, Batch: 23/29, Batch_Loss_Train: 1.660
2024-06-21 01:44:37,432 - INFO: Epoch: 160/200, Batch: 24/29, Batch_Loss_Train: 1.641
2024-06-21 01:44:37,849 - INFO: Epoch: 160/200, Batch: 25/29, Batch_Loss_Train: 1.592
2024-06-21 01:44:38,149 - INFO: Epoch: 160/200, Batch: 26/29, Batch_Loss_Train: 1.312
2024-06-21 01:44:38,550 - INFO: Epoch: 160/200, Batch: 27/29, Batch_Loss_Train: 1.327
2024-06-21 01:44:38,863 - INFO: Epoch: 160/200, Batch: 28/29, Batch_Loss_Train: 1.400
2024-06-21 01:44:39,085 - INFO: Epoch: 160/200, Batch: 29/29, Batch_Loss_Train: 1.664
2024-06-21 01:44:50,050 - INFO: 160/200 final results:
2024-06-21 01:44:50,050 - INFO: Training loss: 1.470.
2024-06-21 01:44:50,050 - INFO: Training MAE: 0.933.
2024-06-21 01:44:50,050 - INFO: Training MSE: 1.466.
2024-06-21 01:45:10,624 - INFO: Epoch: 160/200, Loss_train: 1.4695316470902542, Loss_val: 1.5730115730186989
2024-06-21 01:45:10,624 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:45:10,624 - INFO: Epoch 161/200...
2024-06-21 01:45:10,624 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:45:10,624 - INFO: Batch size: 32.
2024-06-21 01:45:10,628 - INFO: Dataset:
2024-06-21 01:45:10,628 - INFO: Batch size:
2024-06-21 01:45:10,628 - INFO: Number of workers:
2024-06-21 01:45:11,866 - INFO: Epoch: 161/200, Batch: 1/29, Batch_Loss_Train: 1.407
2024-06-21 01:45:12,190 - INFO: Epoch: 161/200, Batch: 2/29, Batch_Loss_Train: 1.356
2024-06-21 01:45:12,608 - INFO: Epoch: 161/200, Batch: 3/29, Batch_Loss_Train: 1.105
2024-06-21 01:45:12,930 - INFO: Epoch: 161/200, Batch: 4/29, Batch_Loss_Train: 1.059
2024-06-21 01:45:13,352 - INFO: Epoch: 161/200, Batch: 5/29, Batch_Loss_Train: 1.136
2024-06-21 01:45:13,655 - INFO: Epoch: 161/200, Batch: 6/29, Batch_Loss_Train: 1.423
2024-06-21 01:45:14,056 - INFO: Epoch: 161/200, Batch: 7/29, Batch_Loss_Train: 1.335
2024-06-21 01:45:14,372 - INFO: Epoch: 161/200, Batch: 8/29, Batch_Loss_Train: 1.315
2024-06-21 01:45:14,788 - INFO: Epoch: 161/200, Batch: 9/29, Batch_Loss_Train: 1.339
2024-06-21 01:45:15,086 - INFO: Epoch: 161/200, Batch: 10/29, Batch_Loss_Train: 1.242
2024-06-21 01:45:15,490 - INFO: Epoch: 161/200, Batch: 11/29, Batch_Loss_Train: 1.511
2024-06-21 01:45:15,807 - INFO: Epoch: 161/200, Batch: 12/29, Batch_Loss_Train: 1.262
2024-06-21 01:45:16,240 - INFO: Epoch: 161/200, Batch: 13/29, Batch_Loss_Train: 1.731
2024-06-21 01:45:16,548 - INFO: Epoch: 161/200, Batch: 14/29, Batch_Loss_Train: 1.496
2024-06-21 01:45:16,959 - INFO: Epoch: 161/200, Batch: 15/29, Batch_Loss_Train: 1.372
2024-06-21 01:45:17,274 - INFO: Epoch: 161/200, Batch: 16/29, Batch_Loss_Train: 1.519
2024-06-21 01:45:17,696 - INFO: Epoch: 161/200, Batch: 17/29, Batch_Loss_Train: 1.701
2024-06-21 01:45:17,999 - INFO: Epoch: 161/200, Batch: 18/29, Batch_Loss_Train: 1.380
2024-06-21 01:45:18,399 - INFO: Epoch: 161/200, Batch: 19/29, Batch_Loss_Train: 1.560
2024-06-21 01:45:18,710 - INFO: Epoch: 161/200, Batch: 20/29, Batch_Loss_Train: 0.966
2024-06-21 01:45:19,130 - INFO: Epoch: 161/200, Batch: 21/29, Batch_Loss_Train: 1.550
2024-06-21 01:45:19,435 - INFO: Epoch: 161/200, Batch: 22/29, Batch_Loss_Train: 1.744
2024-06-21 01:45:19,846 - INFO: Epoch: 161/200, Batch: 23/29, Batch_Loss_Train: 0.982
2024-06-21 01:45:20,164 - INFO: Epoch: 161/200, Batch: 24/29, Batch_Loss_Train: 1.193
2024-06-21 01:45:20,575 - INFO: Epoch: 161/200, Batch: 25/29, Batch_Loss_Train: 1.350
2024-06-21 01:45:20,876 - INFO: Epoch: 161/200, Batch: 26/29, Batch_Loss_Train: 1.396
2024-06-21 01:45:21,268 - INFO: Epoch: 161/200, Batch: 27/29, Batch_Loss_Train: 1.590
2024-06-21 01:45:21,581 - INFO: Epoch: 161/200, Batch: 28/29, Batch_Loss_Train: 1.317
2024-06-21 01:45:21,804 - INFO: Epoch: 161/200, Batch: 29/29, Batch_Loss_Train: 1.602
2024-06-21 01:45:32,778 - INFO: 161/200 final results:
2024-06-21 01:45:32,778 - INFO: Training loss: 1.377.
2024-06-21 01:45:32,778 - INFO: Training MAE: 0.909.
2024-06-21 01:45:32,778 - INFO: Training MSE: 1.373.
2024-06-21 01:45:53,080 - INFO: Epoch: 161/200, Loss_train: 1.3772262293716957, Loss_val: 1.5806196331977844
2024-06-21 01:45:53,080 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:45:53,080 - INFO: Epoch 162/200...
2024-06-21 01:45:53,080 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:45:53,080 - INFO: Batch size: 32.
2024-06-21 01:45:53,084 - INFO: Dataset:
2024-06-21 01:45:53,084 - INFO: Batch size:
2024-06-21 01:45:53,084 - INFO: Number of workers:
2024-06-21 01:45:54,336 - INFO: Epoch: 162/200, Batch: 1/29, Batch_Loss_Train: 1.286
2024-06-21 01:45:54,648 - INFO: Epoch: 162/200, Batch: 2/29, Batch_Loss_Train: 2.034
2024-06-21 01:45:55,051 - INFO: Epoch: 162/200, Batch: 3/29, Batch_Loss_Train: 1.533
2024-06-21 01:45:55,375 - INFO: Epoch: 162/200, Batch: 4/29, Batch_Loss_Train: 1.673
2024-06-21 01:45:55,804 - INFO: Epoch: 162/200, Batch: 5/29, Batch_Loss_Train: 1.365
2024-06-21 01:45:56,106 - INFO: Epoch: 162/200, Batch: 6/29, Batch_Loss_Train: 1.249
2024-06-21 01:45:56,503 - INFO: Epoch: 162/200, Batch: 7/29, Batch_Loss_Train: 1.631
2024-06-21 01:45:56,823 - INFO: Epoch: 162/200, Batch: 8/29, Batch_Loss_Train: 1.215
2024-06-21 01:45:57,258 - INFO: Epoch: 162/200, Batch: 9/29, Batch_Loss_Train: 1.546
2024-06-21 01:45:57,561 - INFO: Epoch: 162/200, Batch: 10/29, Batch_Loss_Train: 1.227
2024-06-21 01:45:57,955 - INFO: Epoch: 162/200, Batch: 11/29, Batch_Loss_Train: 1.853
2024-06-21 01:45:58,279 - INFO: Epoch: 162/200, Batch: 12/29, Batch_Loss_Train: 1.305
2024-06-21 01:45:58,737 - INFO: Epoch: 162/200, Batch: 13/29, Batch_Loss_Train: 1.747
2024-06-21 01:45:59,046 - INFO: Epoch: 162/200, Batch: 14/29, Batch_Loss_Train: 1.162
2024-06-21 01:45:59,453 - INFO: Epoch: 162/200, Batch: 15/29, Batch_Loss_Train: 1.458
2024-06-21 01:45:59,759 - INFO: Epoch: 162/200, Batch: 16/29, Batch_Loss_Train: 1.430
2024-06-21 01:46:00,204 - INFO: Epoch: 162/200, Batch: 17/29, Batch_Loss_Train: 1.274
2024-06-21 01:46:00,507 - INFO: Epoch: 162/200, Batch: 18/29, Batch_Loss_Train: 0.965
2024-06-21 01:46:00,896 - INFO: Epoch: 162/200, Batch: 19/29, Batch_Loss_Train: 1.248
2024-06-21 01:46:01,195 - INFO: Epoch: 162/200, Batch: 20/29, Batch_Loss_Train: 1.110
2024-06-21 01:46:01,641 - INFO: Epoch: 162/200, Batch: 21/29, Batch_Loss_Train: 1.106
2024-06-21 01:46:01,949 - INFO: Epoch: 162/200, Batch: 22/29, Batch_Loss_Train: 1.292
2024-06-21 01:46:02,342 - INFO: Epoch: 162/200, Batch: 23/29, Batch_Loss_Train: 1.569
2024-06-21 01:46:02,651 - INFO: Epoch: 162/200, Batch: 24/29, Batch_Loss_Train: 1.627
2024-06-21 01:46:03,087 - INFO: Epoch: 162/200, Batch: 25/29, Batch_Loss_Train: 1.298
2024-06-21 01:46:03,390 - INFO: Epoch: 162/200, Batch: 26/29, Batch_Loss_Train: 1.578
2024-06-21 01:46:03,772 - INFO: Epoch: 162/200, Batch: 27/29, Batch_Loss_Train: 1.141
2024-06-21 01:46:04,075 - INFO: Epoch: 162/200, Batch: 28/29, Batch_Loss_Train: 1.391
2024-06-21 01:46:04,290 - INFO: Epoch: 162/200, Batch: 29/29, Batch_Loss_Train: 1.145
2024-06-21 01:46:15,325 - INFO: 162/200 final results:
2024-06-21 01:46:15,325 - INFO: Training loss: 1.395.
2024-06-21 01:46:15,325 - INFO: Training MAE: 0.923.
2024-06-21 01:46:15,325 - INFO: Training MSE: 1.400.
2024-06-21 01:46:35,765 - INFO: Epoch: 162/200, Loss_train: 1.395031480953611, Loss_val: 1.567226714101331
2024-06-21 01:46:35,765 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:46:35,765 - INFO: Epoch 163/200...
2024-06-21 01:46:35,765 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:46:35,765 - INFO: Batch size: 32.
2024-06-21 01:46:35,769 - INFO: Dataset:
2024-06-21 01:46:35,769 - INFO: Batch size:
2024-06-21 01:46:35,769 - INFO: Number of workers:
2024-06-21 01:46:37,041 - INFO: Epoch: 163/200, Batch: 1/29, Batch_Loss_Train: 1.580
2024-06-21 01:46:37,353 - INFO: Epoch: 163/200, Batch: 2/29, Batch_Loss_Train: 1.527
2024-06-21 01:46:37,755 - INFO: Epoch: 163/200, Batch: 3/29, Batch_Loss_Train: 1.702
2024-06-21 01:46:38,083 - INFO: Epoch: 163/200, Batch: 4/29, Batch_Loss_Train: 1.268
2024-06-21 01:46:38,533 - INFO: Epoch: 163/200, Batch: 5/29, Batch_Loss_Train: 1.812
2024-06-21 01:46:38,841 - INFO: Epoch: 163/200, Batch: 6/29, Batch_Loss_Train: 1.230
2024-06-21 01:46:39,238 - INFO: Epoch: 163/200, Batch: 7/29, Batch_Loss_Train: 1.476
2024-06-21 01:46:39,546 - INFO: Epoch: 163/200, Batch: 8/29, Batch_Loss_Train: 1.496
2024-06-21 01:46:39,998 - INFO: Epoch: 163/200, Batch: 9/29, Batch_Loss_Train: 1.209
2024-06-21 01:46:40,300 - INFO: Epoch: 163/200, Batch: 10/29, Batch_Loss_Train: 1.464
2024-06-21 01:46:40,696 - INFO: Epoch: 163/200, Batch: 11/29, Batch_Loss_Train: 1.229
2024-06-21 01:46:41,007 - INFO: Epoch: 163/200, Batch: 12/29, Batch_Loss_Train: 1.538
2024-06-21 01:46:41,463 - INFO: Epoch: 163/200, Batch: 13/29, Batch_Loss_Train: 1.633
2024-06-21 01:46:41,771 - INFO: Epoch: 163/200, Batch: 14/29, Batch_Loss_Train: 1.370
2024-06-21 01:46:42,175 - INFO: Epoch: 163/200, Batch: 15/29, Batch_Loss_Train: 2.337
2024-06-21 01:46:42,479 - INFO: Epoch: 163/200, Batch: 16/29, Batch_Loss_Train: 1.288
2024-06-21 01:46:42,921 - INFO: Epoch: 163/200, Batch: 17/29, Batch_Loss_Train: 1.454
2024-06-21 01:46:43,224 - INFO: Epoch: 163/200, Batch: 18/29, Batch_Loss_Train: 1.029
2024-06-21 01:46:43,614 - INFO: Epoch: 163/200, Batch: 19/29, Batch_Loss_Train: 1.827
2024-06-21 01:46:43,913 - INFO: Epoch: 163/200, Batch: 20/29, Batch_Loss_Train: 1.568
2024-06-21 01:46:44,355 - INFO: Epoch: 163/200, Batch: 21/29, Batch_Loss_Train: 1.342
2024-06-21 01:46:44,662 - INFO: Epoch: 163/200, Batch: 22/29, Batch_Loss_Train: 1.148
2024-06-21 01:46:45,047 - INFO: Epoch: 163/200, Batch: 23/29, Batch_Loss_Train: 1.416
2024-06-21 01:46:45,353 - INFO: Epoch: 163/200, Batch: 24/29, Batch_Loss_Train: 1.262
2024-06-21 01:46:45,784 - INFO: Epoch: 163/200, Batch: 25/29, Batch_Loss_Train: 1.391
2024-06-21 01:46:46,088 - INFO: Epoch: 163/200, Batch: 26/29, Batch_Loss_Train: 1.470
2024-06-21 01:46:46,462 - INFO: Epoch: 163/200, Batch: 27/29, Batch_Loss_Train: 1.377
2024-06-21 01:46:46,766 - INFO: Epoch: 163/200, Batch: 28/29, Batch_Loss_Train: 1.149
2024-06-21 01:46:46,985 - INFO: Epoch: 163/200, Batch: 29/29, Batch_Loss_Train: 1.849
2024-06-21 01:46:58,005 - INFO: 163/200 final results:
2024-06-21 01:46:58,005 - INFO: Training loss: 1.464.
2024-06-21 01:46:58,005 - INFO: Training MAE: 0.934.
2024-06-21 01:46:58,005 - INFO: Training MSE: 1.456.
2024-06-21 01:47:18,465 - INFO: Epoch: 163/200, Loss_train: 1.4635103858750442, Loss_val: 1.5854270026601593
2024-06-21 01:47:18,465 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:47:18,465 - INFO: Epoch 164/200...
2024-06-21 01:47:18,465 - INFO: Learning rate: 3.4435930314143293e-06.
2024-06-21 01:47:18,465 - INFO: Batch size: 32.
2024-06-21 01:47:18,469 - INFO: Dataset:
2024-06-21 01:47:18,469 - INFO: Batch size:
2024-06-21 01:47:18,469 - INFO: Number of workers:
2024-06-21 01:47:19,722 - INFO: Epoch: 164/200, Batch: 1/29, Batch_Loss_Train: 1.194
2024-06-21 01:47:20,051 - INFO: Epoch: 164/200, Batch: 2/29, Batch_Loss_Train: 1.550
2024-06-21 01:47:20,459 - INFO: Epoch: 164/200, Batch: 3/29, Batch_Loss_Train: 1.466
2024-06-21 01:47:20,784 - INFO: Epoch: 164/200, Batch: 4/29, Batch_Loss_Train: 1.510
2024-06-21 01:47:21,193 - INFO: Epoch: 164/200, Batch: 5/29, Batch_Loss_Train: 1.360
2024-06-21 01:47:21,514 - INFO: Epoch: 164/200, Batch: 6/29, Batch_Loss_Train: 1.086
2024-06-21 01:47:21,907 - INFO: Epoch: 164/200, Batch: 7/29, Batch_Loss_Train: 1.153
2024-06-21 01:47:22,226 - INFO: Epoch: 164/200, Batch: 8/29, Batch_Loss_Train: 1.242
2024-06-21 01:47:22,624 - INFO: Epoch: 164/200, Batch: 9/29, Batch_Loss_Train: 1.862
2024-06-21 01:47:22,937 - INFO: Epoch: 164/200, Batch: 10/29, Batch_Loss_Train: 1.395
2024-06-21 01:47:23,333 - INFO: Epoch: 164/200, Batch: 11/29, Batch_Loss_Train: 1.462
2024-06-21 01:47:23,661 - INFO: Epoch: 164/200, Batch: 12/29, Batch_Loss_Train: 1.857
2024-06-21 01:47:24,089 - INFO: Epoch: 164/200, Batch: 13/29, Batch_Loss_Train: 1.286
2024-06-21 01:47:24,412 - INFO: Epoch: 164/200, Batch: 14/29, Batch_Loss_Train: 1.496
2024-06-21 01:47:24,817 - INFO: Epoch: 164/200, Batch: 15/29, Batch_Loss_Train: 2.513
2024-06-21 01:47:25,135 - INFO: Epoch: 164/200, Batch: 16/29, Batch_Loss_Train: 1.798
2024-06-21 01:47:25,540 - INFO: Epoch: 164/200, Batch: 17/29, Batch_Loss_Train: 1.296
2024-06-21 01:47:25,856 - INFO: Epoch: 164/200, Batch: 18/29, Batch_Loss_Train: 1.126
2024-06-21 01:47:26,250 - INFO: Epoch: 164/200, Batch: 19/29, Batch_Loss_Train: 1.475
2024-06-21 01:47:26,565 - INFO: Epoch: 164/200, Batch: 20/29, Batch_Loss_Train: 1.459
2024-06-21 01:47:26,972 - INFO: Epoch: 164/200, Batch: 21/29, Batch_Loss_Train: 1.405
2024-06-21 01:47:27,293 - INFO: Epoch: 164/200, Batch: 22/29, Batch_Loss_Train: 1.227
2024-06-21 01:47:27,705 - INFO: Epoch: 164/200, Batch: 23/29, Batch_Loss_Train: 1.748
2024-06-21 01:47:28,026 - INFO: Epoch: 164/200, Batch: 24/29, Batch_Loss_Train: 1.226
2024-06-21 01:47:28,431 - INFO: Epoch: 164/200, Batch: 25/29, Batch_Loss_Train: 1.306
2024-06-21 01:47:28,746 - INFO: Epoch: 164/200, Batch: 26/29, Batch_Loss_Train: 1.221
2024-06-21 01:47:29,148 - INFO: Epoch: 164/200, Batch: 27/29, Batch_Loss_Train: 2.091
2024-06-21 01:47:29,464 - INFO: Epoch: 164/200, Batch: 28/29, Batch_Loss_Train: 1.767
2024-06-21 01:47:29,688 - INFO: Epoch: 164/200, Batch: 29/29, Batch_Loss_Train: 2.203
2024-06-21 01:47:40,712 - INFO: 164/200 final results:
2024-06-21 01:47:40,712 - INFO: Training loss: 1.510.
2024-06-21 01:47:40,712 - INFO: Training MAE: 0.953.
2024-06-21 01:47:40,712 - INFO: Training MSE: 1.496.
2024-06-21 01:48:01,230 - INFO: Epoch: 164/200, Loss_train: 1.5095392260058174, Loss_val: 1.5682316504675766
2024-06-21 01:48:01,230 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:48:01,230 - INFO: Epoch 165/200...
2024-06-21 01:48:01,230 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:48:01,230 - INFO: Batch size: 32.
2024-06-21 01:48:01,234 - INFO: Dataset:
2024-06-21 01:48:01,234 - INFO: Batch size:
2024-06-21 01:48:01,234 - INFO: Number of workers:
2024-06-21 01:48:02,481 - INFO: Epoch: 165/200, Batch: 1/29, Batch_Loss_Train: 1.008
2024-06-21 01:48:02,805 - INFO: Epoch: 165/200, Batch: 2/29, Batch_Loss_Train: 1.170
2024-06-21 01:48:03,221 - INFO: Epoch: 165/200, Batch: 3/29, Batch_Loss_Train: 1.502
2024-06-21 01:48:03,548 - INFO: Epoch: 165/200, Batch: 4/29, Batch_Loss_Train: 1.703
2024-06-21 01:48:03,974 - INFO: Epoch: 165/200, Batch: 5/29, Batch_Loss_Train: 1.413
2024-06-21 01:48:04,281 - INFO: Epoch: 165/200, Batch: 6/29, Batch_Loss_Train: 1.260
2024-06-21 01:48:04,685 - INFO: Epoch: 165/200, Batch: 7/29, Batch_Loss_Train: 1.515
2024-06-21 01:48:05,006 - INFO: Epoch: 165/200, Batch: 8/29, Batch_Loss_Train: 1.688
2024-06-21 01:48:05,426 - INFO: Epoch: 165/200, Batch: 9/29, Batch_Loss_Train: 1.304
2024-06-21 01:48:05,728 - INFO: Epoch: 165/200, Batch: 10/29, Batch_Loss_Train: 1.283
2024-06-21 01:48:06,134 - INFO: Epoch: 165/200, Batch: 11/29, Batch_Loss_Train: 1.203
2024-06-21 01:48:06,457 - INFO: Epoch: 165/200, Batch: 12/29, Batch_Loss_Train: 1.173
2024-06-21 01:48:06,885 - INFO: Epoch: 165/200, Batch: 13/29, Batch_Loss_Train: 1.637
2024-06-21 01:48:07,191 - INFO: Epoch: 165/200, Batch: 14/29, Batch_Loss_Train: 0.945
2024-06-21 01:48:07,610 - INFO: Epoch: 165/200, Batch: 15/29, Batch_Loss_Train: 1.387
2024-06-21 01:48:07,928 - INFO: Epoch: 165/200, Batch: 16/29, Batch_Loss_Train: 1.002
2024-06-21 01:48:08,350 - INFO: Epoch: 165/200, Batch: 17/29, Batch_Loss_Train: 1.290
2024-06-21 01:48:08,654 - INFO: Epoch: 165/200, Batch: 18/29, Batch_Loss_Train: 1.466
2024-06-21 01:48:09,047 - INFO: Epoch: 165/200, Batch: 19/29, Batch_Loss_Train: 1.049
2024-06-21 01:48:09,362 - INFO: Epoch: 165/200, Batch: 20/29, Batch_Loss_Train: 1.555
2024-06-21 01:48:09,785 - INFO: Epoch: 165/200, Batch: 21/29, Batch_Loss_Train: 1.157
2024-06-21 01:48:10,094 - INFO: Epoch: 165/200, Batch: 22/29, Batch_Loss_Train: 1.574
2024-06-21 01:48:10,503 - INFO: Epoch: 165/200, Batch: 23/29, Batch_Loss_Train: 1.190
2024-06-21 01:48:10,824 - INFO: Epoch: 165/200, Batch: 24/29, Batch_Loss_Train: 1.062
2024-06-21 01:48:11,242 - INFO: Epoch: 165/200, Batch: 25/29, Batch_Loss_Train: 1.504
2024-06-21 01:48:11,545 - INFO: Epoch: 165/200, Batch: 26/29, Batch_Loss_Train: 1.476
2024-06-21 01:48:11,948 - INFO: Epoch: 165/200, Batch: 27/29, Batch_Loss_Train: 1.477
2024-06-21 01:48:12,264 - INFO: Epoch: 165/200, Batch: 28/29, Batch_Loss_Train: 1.556
2024-06-21 01:48:12,488 - INFO: Epoch: 165/200, Batch: 29/29, Batch_Loss_Train: 0.971
2024-06-21 01:48:23,534 - INFO: 165/200 final results:
2024-06-21 01:48:23,534 - INFO: Training loss: 1.328.
2024-06-21 01:48:23,534 - INFO: Training MAE: 0.904.
2024-06-21 01:48:23,534 - INFO: Training MSE: 1.335.
2024-06-21 01:48:44,074 - INFO: Epoch: 165/200, Loss_train: 1.3282174755787026, Loss_val: 1.5715281264535312
2024-06-21 01:48:44,074 - INFO: Best internal validation val_loss: 1.566 at epoch: 157.
2024-06-21 01:48:44,074 - INFO: Epoch 166/200...
2024-06-21 01:48:44,074 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:48:44,074 - INFO: Batch size: 32.
2024-06-21 01:48:44,077 - INFO: Dataset:
2024-06-21 01:48:44,078 - INFO: Batch size:
2024-06-21 01:48:44,078 - INFO: Number of workers:
2024-06-21 01:48:45,323 - INFO: Epoch: 166/200, Batch: 1/29, Batch_Loss_Train: 1.478
2024-06-21 01:48:45,647 - INFO: Epoch: 166/200, Batch: 2/29, Batch_Loss_Train: 1.604
2024-06-21 01:48:46,066 - INFO: Epoch: 166/200, Batch: 3/29, Batch_Loss_Train: 1.466
2024-06-21 01:48:46,388 - INFO: Epoch: 166/200, Batch: 4/29, Batch_Loss_Train: 1.179
2024-06-21 01:48:46,803 - INFO: Epoch: 166/200, Batch: 5/29, Batch_Loss_Train: 1.363
2024-06-21 01:48:47,123 - INFO: Epoch: 166/200, Batch: 6/29, Batch_Loss_Train: 1.368
2024-06-21 01:48:47,530 - INFO: Epoch: 166/200, Batch: 7/29, Batch_Loss_Train: 1.434
2024-06-21 01:48:47,851 - INFO: Epoch: 166/200, Batch: 8/29, Batch_Loss_Train: 1.527
2024-06-21 01:48:48,253 - INFO: Epoch: 166/200, Batch: 9/29, Batch_Loss_Train: 2.038
2024-06-21 01:48:48,567 - INFO: Epoch: 166/200, Batch: 10/29, Batch_Loss_Train: 1.232
2024-06-21 01:48:48,970 - INFO: Epoch: 166/200, Batch: 11/29, Batch_Loss_Train: 1.274
2024-06-21 01:48:49,292 - INFO: Epoch: 166/200, Batch: 12/29, Batch_Loss_Train: 1.680
2024-06-21 01:48:49,717 - INFO: Epoch: 166/200, Batch: 13/29, Batch_Loss_Train: 1.374
2024-06-21 01:48:50,040 - INFO: Epoch: 166/200, Batch: 14/29, Batch_Loss_Train: 1.225
2024-06-21 01:48:50,457 - INFO: Epoch: 166/200, Batch: 15/29, Batch_Loss_Train: 1.550
2024-06-21 01:48:50,776 - INFO: Epoch: 166/200, Batch: 16/29, Batch_Loss_Train: 1.659
2024-06-21 01:48:51,188 - INFO: Epoch: 166/200, Batch: 17/29, Batch_Loss_Train: 1.369
2024-06-21 01:48:51,506 - INFO: Epoch: 166/200, Batch: 18/29, Batch_Loss_Train: 1.213
2024-06-21 01:48:51,912 - INFO: Epoch: 166/200, Batch: 19/29, Batch_Loss_Train: 1.536
2024-06-21 01:48:52,227 - INFO: Epoch: 166/200, Batch: 20/29, Batch_Loss_Train: 1.897
2024-06-21 01:48:52,635 - INFO: Epoch: 166/200, Batch: 21/29, Batch_Loss_Train: 1.687
2024-06-21 01:48:52,956 - INFO: Epoch: 166/200, Batch: 22/29, Batch_Loss_Train: 1.736
2024-06-21 01:48:53,360 - INFO: Epoch: 166/200, Batch: 23/29, Batch_Loss_Train: 1.603
2024-06-21 01:48:53,681 - INFO: Epoch: 166/200, Batch: 24/29, Batch_Loss_Train: 1.353
2024-06-21 01:48:54,081 - INFO: Epoch: 166/200, Batch: 25/29, Batch_Loss_Train: 1.490
2024-06-21 01:48:54,397 - INFO: Epoch: 166/200, Batch: 26/29, Batch_Loss_Train: 1.806
2024-06-21 01:48:54,785 - INFO: Epoch: 166/200, Batch: 27/29, Batch_Loss_Train: 1.535
2024-06-21 01:48:55,100 - INFO: Epoch: 166/200, Batch: 28/29, Batch_Loss_Train: 1.363
2024-06-21 01:48:55,313 - INFO: Epoch: 166/200, Batch: 29/29, Batch_Loss_Train: 0.958
2024-06-21 01:49:06,271 - INFO: 166/200 final results:
2024-06-21 01:49:06,271 - INFO: Training loss: 1.483.
2024-06-21 01:49:06,272 - INFO: Training MAE: 0.945.
2024-06-21 01:49:06,272 - INFO: Training MSE: 1.493.
2024-06-21 01:49:26,639 - INFO: Epoch: 166/200, Loss_train: 1.4826561788032795, Loss_val: 1.5654114094273797
2024-06-21 01:49:26,687 - INFO: Saved new best metric model for epoch 166.
2024-06-21 01:49:26,687 - INFO: Best internal validation val_loss: 1.565 at epoch: 166.
2024-06-21 01:49:26,687 - INFO: Epoch 167/200...
2024-06-21 01:49:26,687 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:49:26,687 - INFO: Batch size: 32.
2024-06-21 01:49:26,691 - INFO: Dataset:
2024-06-21 01:49:26,691 - INFO: Batch size:
2024-06-21 01:49:26,691 - INFO: Number of workers:
2024-06-21 01:49:27,978 - INFO: Epoch: 167/200, Batch: 1/29, Batch_Loss_Train: 1.601
2024-06-21 01:49:28,289 - INFO: Epoch: 167/200, Batch: 2/29, Batch_Loss_Train: 1.414
2024-06-21 01:49:28,694 - INFO: Epoch: 167/200, Batch: 3/29, Batch_Loss_Train: 1.269
2024-06-21 01:49:29,016 - INFO: Epoch: 167/200, Batch: 4/29, Batch_Loss_Train: 1.389
2024-06-21 01:49:29,441 - INFO: Epoch: 167/200, Batch: 5/29, Batch_Loss_Train: 1.455
2024-06-21 01:49:29,744 - INFO: Epoch: 167/200, Batch: 6/29, Batch_Loss_Train: 1.385
2024-06-21 01:49:30,131 - INFO: Epoch: 167/200, Batch: 7/29, Batch_Loss_Train: 1.439
2024-06-21 01:49:30,449 - INFO: Epoch: 167/200, Batch: 8/29, Batch_Loss_Train: 1.406
2024-06-21 01:49:30,873 - INFO: Epoch: 167/200, Batch: 9/29, Batch_Loss_Train: 1.692
2024-06-21 01:49:31,171 - INFO: Epoch: 167/200, Batch: 10/29, Batch_Loss_Train: 1.316
2024-06-21 01:49:31,557 - INFO: Epoch: 167/200, Batch: 11/29, Batch_Loss_Train: 1.308
2024-06-21 01:49:31,875 - INFO: Epoch: 167/200, Batch: 12/29, Batch_Loss_Train: 1.178
2024-06-21 01:49:32,321 - INFO: Epoch: 167/200, Batch: 13/29, Batch_Loss_Train: 1.705
2024-06-21 01:49:32,627 - INFO: Epoch: 167/200, Batch: 14/29, Batch_Loss_Train: 1.530
2024-06-21 01:49:33,031 - INFO: Epoch: 167/200, Batch: 15/29, Batch_Loss_Train: 1.712
2024-06-21 01:49:33,333 - INFO: Epoch: 167/200, Batch: 16/29, Batch_Loss_Train: 1.594
2024-06-21 01:49:33,762 - INFO: Epoch: 167/200, Batch: 17/29, Batch_Loss_Train: 1.317
2024-06-21 01:49:34,068 - INFO: Epoch: 167/200, Batch: 18/29, Batch_Loss_Train: 1.266
2024-06-21 01:49:34,457 - INFO: Epoch: 167/200, Batch: 19/29, Batch_Loss_Train: 1.328
2024-06-21 01:49:34,759 - INFO: Epoch: 167/200, Batch: 20/29, Batch_Loss_Train: 1.626
2024-06-21 01:49:35,203 - INFO: Epoch: 167/200, Batch: 21/29, Batch_Loss_Train: 0.850
2024-06-21 01:49:35,509 - INFO: Epoch: 167/200, Batch: 22/29, Batch_Loss_Train: 1.208
2024-06-21 01:49:35,898 - INFO: Epoch: 167/200, Batch: 23/29, Batch_Loss_Train: 1.422
2024-06-21 01:49:36,203 - INFO: Epoch: 167/200, Batch: 24/29, Batch_Loss_Train: 1.089
2024-06-21 01:49:36,635 - INFO: Epoch: 167/200, Batch: 25/29, Batch_Loss_Train: 1.632
2024-06-21 01:49:36,936 - INFO: Epoch: 167/200, Batch: 26/29, Batch_Loss_Train: 1.552
2024-06-21 01:49:37,314 - INFO: Epoch: 167/200, Batch: 27/29, Batch_Loss_Train: 1.284
2024-06-21 01:49:37,615 - INFO: Epoch: 167/200, Batch: 28/29, Batch_Loss_Train: 1.254
2024-06-21 01:49:37,826 - INFO: Epoch: 167/200, Batch: 29/29, Batch_Loss_Train: 1.141
2024-06-21 01:49:48,770 - INFO: 167/200 final results:
2024-06-21 01:49:48,770 - INFO: Training loss: 1.392.
2024-06-21 01:49:48,770 - INFO: Training MAE: 0.912.
2024-06-21 01:49:48,770 - INFO: Training MSE: 1.397.
2024-06-21 01:50:09,401 - INFO: Epoch: 167/200, Loss_train: 1.3918205006369229, Loss_val: 1.5598258848848015
2024-06-21 01:50:09,447 - INFO: Saved new best metric model for epoch 167.
2024-06-21 01:50:09,448 - INFO: Best internal validation val_loss: 1.560 at epoch: 167.
2024-06-21 01:50:09,448 - INFO: Epoch 168/200...
2024-06-21 01:50:09,448 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:50:09,448 - INFO: Batch size: 32.
2024-06-21 01:50:09,452 - INFO: Dataset:
2024-06-21 01:50:09,452 - INFO: Batch size:
2024-06-21 01:50:09,452 - INFO: Number of workers:
2024-06-21 01:50:10,720 - INFO: Epoch: 168/200, Batch: 1/29, Batch_Loss_Train: 1.127
2024-06-21 01:50:11,048 - INFO: Epoch: 168/200, Batch: 2/29, Batch_Loss_Train: 1.141
2024-06-21 01:50:11,471 - INFO: Epoch: 168/200, Batch: 3/29, Batch_Loss_Train: 1.569
2024-06-21 01:50:11,798 - INFO: Epoch: 168/200, Batch: 4/29, Batch_Loss_Train: 1.234
2024-06-21 01:50:12,212 - INFO: Epoch: 168/200, Batch: 5/29, Batch_Loss_Train: 1.805
2024-06-21 01:50:12,520 - INFO: Epoch: 168/200, Batch: 6/29, Batch_Loss_Train: 1.507
2024-06-21 01:50:12,928 - INFO: Epoch: 168/200, Batch: 7/29, Batch_Loss_Train: 1.381
2024-06-21 01:50:13,249 - INFO: Epoch: 168/200, Batch: 8/29, Batch_Loss_Train: 1.580
2024-06-21 01:50:13,658 - INFO: Epoch: 168/200, Batch: 9/29, Batch_Loss_Train: 1.753
2024-06-21 01:50:13,960 - INFO: Epoch: 168/200, Batch: 10/29, Batch_Loss_Train: 1.557
2024-06-21 01:50:14,364 - INFO: Epoch: 168/200, Batch: 11/29, Batch_Loss_Train: 1.635
2024-06-21 01:50:14,686 - INFO: Epoch: 168/200, Batch: 12/29, Batch_Loss_Train: 1.068
2024-06-21 01:50:15,123 - INFO: Epoch: 168/200, Batch: 13/29, Batch_Loss_Train: 1.077
2024-06-21 01:50:15,433 - INFO: Epoch: 168/200, Batch: 14/29, Batch_Loss_Train: 1.042
2024-06-21 01:50:15,847 - INFO: Epoch: 168/200, Batch: 15/29, Batch_Loss_Train: 1.100
2024-06-21 01:50:16,165 - INFO: Epoch: 168/200, Batch: 16/29, Batch_Loss_Train: 1.583
2024-06-21 01:50:16,573 - INFO: Epoch: 168/200, Batch: 17/29, Batch_Loss_Train: 1.634
2024-06-21 01:50:16,879 - INFO: Epoch: 168/200, Batch: 18/29, Batch_Loss_Train: 1.474
2024-06-21 01:50:17,283 - INFO: Epoch: 168/200, Batch: 19/29, Batch_Loss_Train: 1.823
2024-06-21 01:50:17,597 - INFO: Epoch: 168/200, Batch: 20/29, Batch_Loss_Train: 1.069
2024-06-21 01:50:18,003 - INFO: Epoch: 168/200, Batch: 21/29, Batch_Loss_Train: 1.285
2024-06-21 01:50:18,312 - INFO: Epoch: 168/200, Batch: 22/29, Batch_Loss_Train: 1.847
2024-06-21 01:50:18,712 - INFO: Epoch: 168/200, Batch: 23/29, Batch_Loss_Train: 1.112
2024-06-21 01:50:19,033 - INFO: Epoch: 168/200, Batch: 24/29, Batch_Loss_Train: 1.412
2024-06-21 01:50:19,443 - INFO: Epoch: 168/200, Batch: 25/29, Batch_Loss_Train: 1.315
2024-06-21 01:50:19,746 - INFO: Epoch: 168/200, Batch: 26/29, Batch_Loss_Train: 1.555
2024-06-21 01:50:20,132 - INFO: Epoch: 168/200, Batch: 27/29, Batch_Loss_Train: 1.450
2024-06-21 01:50:20,448 - INFO: Epoch: 168/200, Batch: 28/29, Batch_Loss_Train: 1.444
2024-06-21 01:50:20,665 - INFO: Epoch: 168/200, Batch: 29/29, Batch_Loss_Train: 1.277
2024-06-21 01:50:31,640 - INFO: 168/200 final results:
2024-06-21 01:50:31,640 - INFO: Training loss: 1.409.
2024-06-21 01:50:31,640 - INFO: Training MAE: 0.928.
2024-06-21 01:50:31,640 - INFO: Training MSE: 1.411.
2024-06-21 01:50:52,343 - INFO: Epoch: 168/200, Loss_train: 1.4088781496574139, Loss_val: 1.5662193236679867
2024-06-21 01:50:52,344 - INFO: Best internal validation val_loss: 1.560 at epoch: 167.
2024-06-21 01:50:52,344 - INFO: Epoch 169/200...
2024-06-21 01:50:52,344 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:50:52,344 - INFO: Batch size: 32.
2024-06-21 01:50:52,348 - INFO: Dataset:
2024-06-21 01:50:52,348 - INFO: Batch size:
2024-06-21 01:50:52,348 - INFO: Number of workers:
2024-06-21 01:50:53,586 - INFO: Epoch: 169/200, Batch: 1/29, Batch_Loss_Train: 0.916
2024-06-21 01:50:53,910 - INFO: Epoch: 169/200, Batch: 2/29, Batch_Loss_Train: 1.559
2024-06-21 01:50:54,319 - INFO: Epoch: 169/200, Batch: 3/29, Batch_Loss_Train: 1.562
2024-06-21 01:50:54,641 - INFO: Epoch: 169/200, Batch: 4/29, Batch_Loss_Train: 1.474
2024-06-21 01:50:55,052 - INFO: Epoch: 169/200, Batch: 5/29, Batch_Loss_Train: 1.729
2024-06-21 01:50:55,369 - INFO: Epoch: 169/200, Batch: 6/29, Batch_Loss_Train: 1.492
2024-06-21 01:50:55,775 - INFO: Epoch: 169/200, Batch: 7/29, Batch_Loss_Train: 0.933
2024-06-21 01:50:56,093 - INFO: Epoch: 169/200, Batch: 8/29, Batch_Loss_Train: 1.208
2024-06-21 01:50:56,492 - INFO: Epoch: 169/200, Batch: 9/29, Batch_Loss_Train: 1.388
2024-06-21 01:50:56,804 - INFO: Epoch: 169/200, Batch: 10/29, Batch_Loss_Train: 1.312
2024-06-21 01:50:57,206 - INFO: Epoch: 169/200, Batch: 11/29, Batch_Loss_Train: 1.434
2024-06-21 01:50:57,526 - INFO: Epoch: 169/200, Batch: 12/29, Batch_Loss_Train: 1.526
2024-06-21 01:50:57,945 - INFO: Epoch: 169/200, Batch: 13/29, Batch_Loss_Train: 1.599
2024-06-21 01:50:58,266 - INFO: Epoch: 169/200, Batch: 14/29, Batch_Loss_Train: 1.634
2024-06-21 01:50:58,682 - INFO: Epoch: 169/200, Batch: 15/29, Batch_Loss_Train: 1.571
2024-06-21 01:50:58,998 - INFO: Epoch: 169/200, Batch: 16/29, Batch_Loss_Train: 1.454
2024-06-21 01:50:59,405 - INFO: Epoch: 169/200, Batch: 17/29, Batch_Loss_Train: 1.198
2024-06-21 01:50:59,721 - INFO: Epoch: 169/200, Batch: 18/29, Batch_Loss_Train: 1.374
2024-06-21 01:51:00,127 - INFO: Epoch: 169/200, Batch: 19/29, Batch_Loss_Train: 1.489
2024-06-21 01:51:00,439 - INFO: Epoch: 169/200, Batch: 20/29, Batch_Loss_Train: 1.515
2024-06-21 01:51:00,846 - INFO: Epoch: 169/200, Batch: 21/29, Batch_Loss_Train: 1.361
2024-06-21 01:51:01,165 - INFO: Epoch: 169/200, Batch: 22/29, Batch_Loss_Train: 1.989
2024-06-21 01:51:01,573 - INFO: Epoch: 169/200, Batch: 23/29, Batch_Loss_Train: 1.599
2024-06-21 01:51:01,892 - INFO: Epoch: 169/200, Batch: 24/29, Batch_Loss_Train: 1.232
2024-06-21 01:51:02,296 - INFO: Epoch: 169/200, Batch: 25/29, Batch_Loss_Train: 1.151
2024-06-21 01:51:02,610 - INFO: Epoch: 169/200, Batch: 26/29, Batch_Loss_Train: 1.354
2024-06-21 01:51:03,011 - INFO: Epoch: 169/200, Batch: 27/29, Batch_Loss_Train: 1.594
2024-06-21 01:51:03,325 - INFO: Epoch: 169/200, Batch: 28/29, Batch_Loss_Train: 1.265
2024-06-21 01:51:03,543 - INFO: Epoch: 169/200, Batch: 29/29, Batch_Loss_Train: 0.957
2024-06-21 01:51:14,259 - INFO: 169/200 final results:
2024-06-21 01:51:14,259 - INFO: Training loss: 1.409.
2024-06-21 01:51:14,259 - INFO: Training MAE: 0.925.
2024-06-21 01:51:14,259 - INFO: Training MSE: 1.418.
2024-06-21 01:51:34,893 - INFO: Epoch: 169/200, Loss_train: 1.4092752070262515, Loss_val: 1.578874633230012
2024-06-21 01:51:34,893 - INFO: Best internal validation val_loss: 1.560 at epoch: 167.
2024-06-21 01:51:34,893 - INFO: Epoch 170/200...
2024-06-21 01:51:34,893 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:51:34,893 - INFO: Batch size: 32.
2024-06-21 01:51:34,897 - INFO: Dataset:
2024-06-21 01:51:34,898 - INFO: Batch size:
2024-06-21 01:51:34,898 - INFO: Number of workers:
2024-06-21 01:51:36,142 - INFO: Epoch: 170/200, Batch: 1/29, Batch_Loss_Train: 1.110
2024-06-21 01:51:36,470 - INFO: Epoch: 170/200, Batch: 2/29, Batch_Loss_Train: 1.270
2024-06-21 01:51:36,893 - INFO: Epoch: 170/200, Batch: 3/29, Batch_Loss_Train: 1.879
2024-06-21 01:51:37,218 - INFO: Epoch: 170/200, Batch: 4/29, Batch_Loss_Train: 1.203
2024-06-21 01:51:37,629 - INFO: Epoch: 170/200, Batch: 5/29, Batch_Loss_Train: 1.901
2024-06-21 01:51:37,948 - INFO: Epoch: 170/200, Batch: 6/29, Batch_Loss_Train: 1.332
2024-06-21 01:51:38,355 - INFO: Epoch: 170/200, Batch: 7/29, Batch_Loss_Train: 1.470
2024-06-21 01:51:38,673 - INFO: Epoch: 170/200, Batch: 8/29, Batch_Loss_Train: 1.607
2024-06-21 01:51:39,075 - INFO: Epoch: 170/200, Batch: 9/29, Batch_Loss_Train: 1.326
2024-06-21 01:51:39,388 - INFO: Epoch: 170/200, Batch: 10/29, Batch_Loss_Train: 1.110
2024-06-21 01:51:39,790 - INFO: Epoch: 170/200, Batch: 11/29, Batch_Loss_Train: 1.595
2024-06-21 01:51:40,110 - INFO: Epoch: 170/200, Batch: 12/29, Batch_Loss_Train: 1.627
2024-06-21 01:51:40,526 - INFO: Epoch: 170/200, Batch: 13/29, Batch_Loss_Train: 1.849
2024-06-21 01:51:40,849 - INFO: Epoch: 170/200, Batch: 14/29, Batch_Loss_Train: 1.589
2024-06-21 01:51:41,267 - INFO: Epoch: 170/200, Batch: 15/29, Batch_Loss_Train: 1.277
2024-06-21 01:51:41,582 - INFO: Epoch: 170/200, Batch: 16/29, Batch_Loss_Train: 1.289
2024-06-21 01:51:41,993 - INFO: Epoch: 170/200, Batch: 17/29, Batch_Loss_Train: 1.806
2024-06-21 01:51:42,311 - INFO: Epoch: 170/200, Batch: 18/29, Batch_Loss_Train: 1.386
2024-06-21 01:51:42,717 - INFO: Epoch: 170/200, Batch: 19/29, Batch_Loss_Train: 1.512
2024-06-21 01:51:43,031 - INFO: Epoch: 170/200, Batch: 20/29, Batch_Loss_Train: 1.863
2024-06-21 01:51:43,439 - INFO: Epoch: 170/200, Batch: 21/29, Batch_Loss_Train: 1.415
2024-06-21 01:51:43,760 - INFO: Epoch: 170/200, Batch: 22/29, Batch_Loss_Train: 1.701
2024-06-21 01:51:44,169 - INFO: Epoch: 170/200, Batch: 23/29, Batch_Loss_Train: 1.277
2024-06-21 01:51:44,491 - INFO: Epoch: 170/200, Batch: 24/29, Batch_Loss_Train: 1.515
2024-06-21 01:51:44,895 - INFO: Epoch: 170/200, Batch: 25/29, Batch_Loss_Train: 1.703
2024-06-21 01:51:45,211 - INFO: Epoch: 170/200, Batch: 26/29, Batch_Loss_Train: 1.341
2024-06-21 01:51:45,614 - INFO: Epoch: 170/200, Batch: 27/29, Batch_Loss_Train: 1.685
2024-06-21 01:51:45,930 - INFO: Epoch: 170/200, Batch: 28/29, Batch_Loss_Train: 1.408
2024-06-21 01:51:46,154 - INFO: Epoch: 170/200, Batch: 29/29, Batch_Loss_Train: 2.247
2024-06-21 01:51:57,180 - INFO: 170/200 final results:
2024-06-21 01:51:57,180 - INFO: Training loss: 1.527.
2024-06-21 01:51:57,180 - INFO: Training MAE: 0.949.
2024-06-21 01:51:57,180 - INFO: Training MSE: 1.513.
2024-06-21 01:52:17,582 - INFO: Epoch: 170/200, Loss_train: 1.5274268306534866, Loss_val: 1.56743684102749
2024-06-21 01:52:17,583 - INFO: Best internal validation val_loss: 1.560 at epoch: 167.
2024-06-21 01:52:17,583 - INFO: Epoch 171/200...
2024-06-21 01:52:17,583 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:52:17,583 - INFO: Batch size: 32.
2024-06-21 01:52:17,587 - INFO: Dataset:
2024-06-21 01:52:17,587 - INFO: Batch size:
2024-06-21 01:52:17,587 - INFO: Number of workers:
2024-06-21 01:52:18,828 - INFO: Epoch: 171/200, Batch: 1/29, Batch_Loss_Train: 1.464
2024-06-21 01:52:19,153 - INFO: Epoch: 171/200, Batch: 2/29, Batch_Loss_Train: 1.589
2024-06-21 01:52:19,546 - INFO: Epoch: 171/200, Batch: 3/29, Batch_Loss_Train: 1.065
2024-06-21 01:52:19,870 - INFO: Epoch: 171/200, Batch: 4/29, Batch_Loss_Train: 1.639
2024-06-21 01:52:20,305 - INFO: Epoch: 171/200, Batch: 5/29, Batch_Loss_Train: 1.844
2024-06-21 01:52:20,610 - INFO: Epoch: 171/200, Batch: 6/29, Batch_Loss_Train: 1.018
2024-06-21 01:52:21,002 - INFO: Epoch: 171/200, Batch: 7/29, Batch_Loss_Train: 1.263
2024-06-21 01:52:21,320 - INFO: Epoch: 171/200, Batch: 8/29, Batch_Loss_Train: 0.982
2024-06-21 01:52:21,748 - INFO: Epoch: 171/200, Batch: 9/29, Batch_Loss_Train: 1.361
2024-06-21 01:52:22,047 - INFO: Epoch: 171/200, Batch: 10/29, Batch_Loss_Train: 1.921
2024-06-21 01:52:22,440 - INFO: Epoch: 171/200, Batch: 11/29, Batch_Loss_Train: 0.984
2024-06-21 01:52:22,761 - INFO: Epoch: 171/200, Batch: 12/29, Batch_Loss_Train: 1.604
2024-06-21 01:52:23,203 - INFO: Epoch: 171/200, Batch: 13/29, Batch_Loss_Train: 1.052
2024-06-21 01:52:23,510 - INFO: Epoch: 171/200, Batch: 14/29, Batch_Loss_Train: 1.352
2024-06-21 01:52:23,913 - INFO: Epoch: 171/200, Batch: 15/29, Batch_Loss_Train: 1.502
2024-06-21 01:52:24,228 - INFO: Epoch: 171/200, Batch: 16/29, Batch_Loss_Train: 1.312
2024-06-21 01:52:24,657 - INFO: Epoch: 171/200, Batch: 17/29, Batch_Loss_Train: 1.451
2024-06-21 01:52:24,959 - INFO: Epoch: 171/200, Batch: 18/29, Batch_Loss_Train: 1.356
2024-06-21 01:52:25,351 - INFO: Epoch: 171/200, Batch: 19/29, Batch_Loss_Train: 1.271
2024-06-21 01:52:25,663 - INFO: Epoch: 171/200, Batch: 20/29, Batch_Loss_Train: 1.353
2024-06-21 01:52:26,093 - INFO: Epoch: 171/200, Batch: 21/29, Batch_Loss_Train: 1.533
2024-06-21 01:52:26,399 - INFO: Epoch: 171/200, Batch: 22/29, Batch_Loss_Train: 1.634
2024-06-21 01:52:26,797 - INFO: Epoch: 171/200, Batch: 23/29, Batch_Loss_Train: 1.866
2024-06-21 01:52:27,116 - INFO: Epoch: 171/200, Batch: 24/29, Batch_Loss_Train: 1.334
2024-06-21 01:52:27,542 - INFO: Epoch: 171/200, Batch: 25/29, Batch_Loss_Train: 1.698
2024-06-21 01:52:27,843 - INFO: Epoch: 171/200, Batch: 26/29, Batch_Loss_Train: 1.647
2024-06-21 01:52:28,232 - INFO: Epoch: 171/200, Batch: 27/29, Batch_Loss_Train: 1.552
2024-06-21 01:52:28,545 - INFO: Epoch: 171/200, Batch: 28/29, Batch_Loss_Train: 1.327
2024-06-21 01:52:28,762 - INFO: Epoch: 171/200, Batch: 29/29, Batch_Loss_Train: 1.546
2024-06-21 01:52:39,736 - INFO: 171/200 final results:
2024-06-21 01:52:39,736 - INFO: Training loss: 1.432.
2024-06-21 01:52:39,736 - INFO: Training MAE: 0.927.
2024-06-21 01:52:39,736 - INFO: Training MSE: 1.429.
2024-06-21 01:53:00,368 - INFO: Epoch: 171/200, Loss_train: 1.4317233726896088, Loss_val: 1.5530904182072343
2024-06-21 01:53:00,416 - INFO: Saved new best metric model for epoch 171.
2024-06-21 01:53:00,416 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:53:00,416 - INFO: Epoch 172/200...
2024-06-21 01:53:00,416 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:53:00,416 - INFO: Batch size: 32.
2024-06-21 01:53:00,421 - INFO: Dataset:
2024-06-21 01:53:00,421 - INFO: Batch size:
2024-06-21 01:53:00,421 - INFO: Number of workers:
2024-06-21 01:53:01,678 - INFO: Epoch: 172/200, Batch: 1/29, Batch_Loss_Train: 1.335
2024-06-21 01:53:01,990 - INFO: Epoch: 172/200, Batch: 2/29, Batch_Loss_Train: 1.534
2024-06-21 01:53:02,429 - INFO: Epoch: 172/200, Batch: 3/29, Batch_Loss_Train: 1.511
2024-06-21 01:53:02,743 - INFO: Epoch: 172/200, Batch: 4/29, Batch_Loss_Train: 1.323
2024-06-21 01:53:03,167 - INFO: Epoch: 172/200, Batch: 5/29, Batch_Loss_Train: 0.976
2024-06-21 01:53:03,474 - INFO: Epoch: 172/200, Batch: 6/29, Batch_Loss_Train: 1.615
2024-06-21 01:53:03,893 - INFO: Epoch: 172/200, Batch: 7/29, Batch_Loss_Train: 1.491
2024-06-21 01:53:04,201 - INFO: Epoch: 172/200, Batch: 8/29, Batch_Loss_Train: 2.140
2024-06-21 01:53:04,621 - INFO: Epoch: 172/200, Batch: 9/29, Batch_Loss_Train: 1.784
2024-06-21 01:53:04,924 - INFO: Epoch: 172/200, Batch: 10/29, Batch_Loss_Train: 1.416
2024-06-21 01:53:05,348 - INFO: Epoch: 172/200, Batch: 11/29, Batch_Loss_Train: 1.567
2024-06-21 01:53:05,657 - INFO: Epoch: 172/200, Batch: 12/29, Batch_Loss_Train: 1.436
2024-06-21 01:53:06,088 - INFO: Epoch: 172/200, Batch: 13/29, Batch_Loss_Train: 1.332
2024-06-21 01:53:06,396 - INFO: Epoch: 172/200, Batch: 14/29, Batch_Loss_Train: 1.781
2024-06-21 01:53:06,833 - INFO: Epoch: 172/200, Batch: 15/29, Batch_Loss_Train: 0.945
2024-06-21 01:53:07,137 - INFO: Epoch: 172/200, Batch: 16/29, Batch_Loss_Train: 0.919
2024-06-21 01:53:07,546 - INFO: Epoch: 172/200, Batch: 17/29, Batch_Loss_Train: 1.899
2024-06-21 01:53:07,849 - INFO: Epoch: 172/200, Batch: 18/29, Batch_Loss_Train: 1.320
2024-06-21 01:53:08,277 - INFO: Epoch: 172/200, Batch: 19/29, Batch_Loss_Train: 1.624
2024-06-21 01:53:08,576 - INFO: Epoch: 172/200, Batch: 20/29, Batch_Loss_Train: 1.845
2024-06-21 01:53:08,986 - INFO: Epoch: 172/200, Batch: 21/29, Batch_Loss_Train: 0.873
2024-06-21 01:53:09,293 - INFO: Epoch: 172/200, Batch: 22/29, Batch_Loss_Train: 1.460
2024-06-21 01:53:09,726 - INFO: Epoch: 172/200, Batch: 23/29, Batch_Loss_Train: 1.639
2024-06-21 01:53:10,032 - INFO: Epoch: 172/200, Batch: 24/29, Batch_Loss_Train: 1.256
2024-06-21 01:53:10,429 - INFO: Epoch: 172/200, Batch: 25/29, Batch_Loss_Train: 1.398
2024-06-21 01:53:10,730 - INFO: Epoch: 172/200, Batch: 26/29, Batch_Loss_Train: 1.218
2024-06-21 01:53:11,149 - INFO: Epoch: 172/200, Batch: 27/29, Batch_Loss_Train: 1.208
2024-06-21 01:53:11,450 - INFO: Epoch: 172/200, Batch: 28/29, Batch_Loss_Train: 1.649
2024-06-21 01:53:11,654 - INFO: Epoch: 172/200, Batch: 29/29, Batch_Loss_Train: 1.048
2024-06-21 01:53:22,377 - INFO: 172/200 final results:
2024-06-21 01:53:22,377 - INFO: Training loss: 1.432.
2024-06-21 01:53:22,377 - INFO: Training MAE: 0.922.
2024-06-21 01:53:22,377 - INFO: Training MSE: 1.440.
2024-06-21 01:53:43,045 - INFO: Epoch: 172/200, Loss_train: 1.4324472649344082, Loss_val: 1.561782614938144
2024-06-21 01:53:43,045 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:53:43,046 - INFO: Epoch 173/200...
2024-06-21 01:53:43,046 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:53:43,046 - INFO: Batch size: 32.
2024-06-21 01:53:43,050 - INFO: Dataset:
2024-06-21 01:53:43,050 - INFO: Batch size:
2024-06-21 01:53:43,050 - INFO: Number of workers:
2024-06-21 01:53:44,324 - INFO: Epoch: 173/200, Batch: 1/29, Batch_Loss_Train: 1.581
2024-06-21 01:53:44,636 - INFO: Epoch: 173/200, Batch: 2/29, Batch_Loss_Train: 1.408
2024-06-21 01:53:45,037 - INFO: Epoch: 173/200, Batch: 3/29, Batch_Loss_Train: 1.447
2024-06-21 01:53:45,361 - INFO: Epoch: 173/200, Batch: 4/29, Batch_Loss_Train: 1.414
2024-06-21 01:53:45,799 - INFO: Epoch: 173/200, Batch: 5/29, Batch_Loss_Train: 1.358
2024-06-21 01:53:46,107 - INFO: Epoch: 173/200, Batch: 6/29, Batch_Loss_Train: 1.287
2024-06-21 01:53:46,504 - INFO: Epoch: 173/200, Batch: 7/29, Batch_Loss_Train: 1.175
2024-06-21 01:53:46,825 - INFO: Epoch: 173/200, Batch: 8/29, Batch_Loss_Train: 1.704
2024-06-21 01:53:47,259 - INFO: Epoch: 173/200, Batch: 9/29, Batch_Loss_Train: 1.442
2024-06-21 01:53:47,561 - INFO: Epoch: 173/200, Batch: 10/29, Batch_Loss_Train: 1.463
2024-06-21 01:53:47,963 - INFO: Epoch: 173/200, Batch: 11/29, Batch_Loss_Train: 1.109
2024-06-21 01:53:48,287 - INFO: Epoch: 173/200, Batch: 12/29, Batch_Loss_Train: 1.292
2024-06-21 01:53:48,734 - INFO: Epoch: 173/200, Batch: 13/29, Batch_Loss_Train: 1.257
2024-06-21 01:53:49,045 - INFO: Epoch: 173/200, Batch: 14/29, Batch_Loss_Train: 1.502
2024-06-21 01:53:49,455 - INFO: Epoch: 173/200, Batch: 15/29, Batch_Loss_Train: 1.406
2024-06-21 01:53:49,773 - INFO: Epoch: 173/200, Batch: 16/29, Batch_Loss_Train: 1.128
2024-06-21 01:53:50,212 - INFO: Epoch: 173/200, Batch: 17/29, Batch_Loss_Train: 1.195
2024-06-21 01:53:50,518 - INFO: Epoch: 173/200, Batch: 18/29, Batch_Loss_Train: 1.775
2024-06-21 01:53:50,912 - INFO: Epoch: 173/200, Batch: 19/29, Batch_Loss_Train: 1.527
2024-06-21 01:53:51,226 - INFO: Epoch: 173/200, Batch: 20/29, Batch_Loss_Train: 1.600
2024-06-21 01:53:51,661 - INFO: Epoch: 173/200, Batch: 21/29, Batch_Loss_Train: 1.086
2024-06-21 01:53:51,970 - INFO: Epoch: 173/200, Batch: 22/29, Batch_Loss_Train: 1.234
2024-06-21 01:53:52,368 - INFO: Epoch: 173/200, Batch: 23/29, Batch_Loss_Train: 1.949
2024-06-21 01:53:52,690 - INFO: Epoch: 173/200, Batch: 24/29, Batch_Loss_Train: 1.417
2024-06-21 01:53:53,121 - INFO: Epoch: 173/200, Batch: 25/29, Batch_Loss_Train: 1.515
2024-06-21 01:53:53,424 - INFO: Epoch: 173/200, Batch: 26/29, Batch_Loss_Train: 1.509
2024-06-21 01:53:53,809 - INFO: Epoch: 173/200, Batch: 27/29, Batch_Loss_Train: 1.426
2024-06-21 01:53:54,125 - INFO: Epoch: 173/200, Batch: 28/29, Batch_Loss_Train: 1.770
2024-06-21 01:53:54,347 - INFO: Epoch: 173/200, Batch: 29/29, Batch_Loss_Train: 0.841
2024-06-21 01:54:05,392 - INFO: 173/200 final results:
2024-06-21 01:54:05,392 - INFO: Training loss: 1.408.
2024-06-21 01:54:05,392 - INFO: Training MAE: 0.927.
2024-06-21 01:54:05,392 - INFO: Training MSE: 1.419.
2024-06-21 01:54:25,830 - INFO: Epoch: 173/200, Loss_train: 1.4075422533627213, Loss_val: 1.5539799168192108
2024-06-21 01:54:25,830 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:54:25,831 - INFO: Epoch 174/200...
2024-06-21 01:54:25,831 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:54:25,831 - INFO: Batch size: 32.
2024-06-21 01:54:25,835 - INFO: Dataset:
2024-06-21 01:54:25,835 - INFO: Batch size:
2024-06-21 01:54:25,835 - INFO: Number of workers:
2024-06-21 01:54:27,067 - INFO: Epoch: 174/200, Batch: 1/29, Batch_Loss_Train: 1.684
2024-06-21 01:54:27,396 - INFO: Epoch: 174/200, Batch: 2/29, Batch_Loss_Train: 1.028
2024-06-21 01:54:27,815 - INFO: Epoch: 174/200, Batch: 3/29, Batch_Loss_Train: 1.112
2024-06-21 01:54:28,141 - INFO: Epoch: 174/200, Batch: 4/29, Batch_Loss_Train: 1.243
2024-06-21 01:54:28,565 - INFO: Epoch: 174/200, Batch: 5/29, Batch_Loss_Train: 1.145
2024-06-21 01:54:28,872 - INFO: Epoch: 174/200, Batch: 6/29, Batch_Loss_Train: 1.173
2024-06-21 01:54:29,274 - INFO: Epoch: 174/200, Batch: 7/29, Batch_Loss_Train: 1.557
2024-06-21 01:54:29,595 - INFO: Epoch: 174/200, Batch: 8/29, Batch_Loss_Train: 1.143
2024-06-21 01:54:30,010 - INFO: Epoch: 174/200, Batch: 9/29, Batch_Loss_Train: 1.633
2024-06-21 01:54:30,310 - INFO: Epoch: 174/200, Batch: 10/29, Batch_Loss_Train: 1.955
2024-06-21 01:54:30,711 - INFO: Epoch: 174/200, Batch: 11/29, Batch_Loss_Train: 1.657
2024-06-21 01:54:31,033 - INFO: Epoch: 174/200, Batch: 12/29, Batch_Loss_Train: 1.839
2024-06-21 01:54:31,460 - INFO: Epoch: 174/200, Batch: 13/29, Batch_Loss_Train: 1.784
2024-06-21 01:54:31,769 - INFO: Epoch: 174/200, Batch: 14/29, Batch_Loss_Train: 1.904
2024-06-21 01:54:32,184 - INFO: Epoch: 174/200, Batch: 15/29, Batch_Loss_Train: 1.451
2024-06-21 01:54:32,502 - INFO: Epoch: 174/200, Batch: 16/29, Batch_Loss_Train: 1.179
2024-06-21 01:54:32,916 - INFO: Epoch: 174/200, Batch: 17/29, Batch_Loss_Train: 1.144
2024-06-21 01:54:33,221 - INFO: Epoch: 174/200, Batch: 18/29, Batch_Loss_Train: 1.327
2024-06-21 01:54:33,619 - INFO: Epoch: 174/200, Batch: 19/29, Batch_Loss_Train: 1.446
2024-06-21 01:54:33,932 - INFO: Epoch: 174/200, Batch: 20/29, Batch_Loss_Train: 1.547
2024-06-21 01:54:34,350 - INFO: Epoch: 174/200, Batch: 21/29, Batch_Loss_Train: 1.247
2024-06-21 01:54:34,659 - INFO: Epoch: 174/200, Batch: 22/29, Batch_Loss_Train: 1.625
2024-06-21 01:54:35,069 - INFO: Epoch: 174/200, Batch: 23/29, Batch_Loss_Train: 0.995
2024-06-21 01:54:35,390 - INFO: Epoch: 174/200, Batch: 24/29, Batch_Loss_Train: 1.640
2024-06-21 01:54:35,809 - INFO: Epoch: 174/200, Batch: 25/29, Batch_Loss_Train: 1.518
2024-06-21 01:54:36,112 - INFO: Epoch: 174/200, Batch: 26/29, Batch_Loss_Train: 1.856
2024-06-21 01:54:36,508 - INFO: Epoch: 174/200, Batch: 27/29, Batch_Loss_Train: 1.669
2024-06-21 01:54:36,823 - INFO: Epoch: 174/200, Batch: 28/29, Batch_Loss_Train: 1.401
2024-06-21 01:54:37,046 - INFO: Epoch: 174/200, Batch: 29/29, Batch_Loss_Train: 1.328
2024-06-21 01:54:48,068 - INFO: 174/200 final results:
2024-06-21 01:54:48,068 - INFO: Training loss: 1.456.
2024-06-21 01:54:48,068 - INFO: Training MAE: 0.941.
2024-06-21 01:54:48,068 - INFO: Training MSE: 1.459.
2024-06-21 01:55:08,796 - INFO: Epoch: 174/200, Loss_train: 1.4561650629701286, Loss_val: 1.5616514600556473
2024-06-21 01:55:08,796 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:55:08,797 - INFO: Epoch 175/200...
2024-06-21 01:55:08,797 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:55:08,797 - INFO: Batch size: 32.
2024-06-21 01:55:08,801 - INFO: Dataset:
2024-06-21 01:55:08,801 - INFO: Batch size:
2024-06-21 01:55:08,801 - INFO: Number of workers:
2024-06-21 01:55:10,046 - INFO: Epoch: 175/200, Batch: 1/29, Batch_Loss_Train: 1.674
2024-06-21 01:55:10,375 - INFO: Epoch: 175/200, Batch: 2/29, Batch_Loss_Train: 1.118
2024-06-21 01:55:10,796 - INFO: Epoch: 175/200, Batch: 3/29, Batch_Loss_Train: 1.249
2024-06-21 01:55:11,122 - INFO: Epoch: 175/200, Batch: 4/29, Batch_Loss_Train: 1.426
2024-06-21 01:55:11,538 - INFO: Epoch: 175/200, Batch: 5/29, Batch_Loss_Train: 1.790
2024-06-21 01:55:11,858 - INFO: Epoch: 175/200, Batch: 6/29, Batch_Loss_Train: 1.973
2024-06-21 01:55:12,266 - INFO: Epoch: 175/200, Batch: 7/29, Batch_Loss_Train: 1.758
2024-06-21 01:55:12,588 - INFO: Epoch: 175/200, Batch: 8/29, Batch_Loss_Train: 1.317
2024-06-21 01:55:12,987 - INFO: Epoch: 175/200, Batch: 9/29, Batch_Loss_Train: 1.439
2024-06-21 01:55:13,301 - INFO: Epoch: 175/200, Batch: 10/29, Batch_Loss_Train: 1.125
2024-06-21 01:55:13,706 - INFO: Epoch: 175/200, Batch: 11/29, Batch_Loss_Train: 1.467
2024-06-21 01:55:14,029 - INFO: Epoch: 175/200, Batch: 12/29, Batch_Loss_Train: 1.472
2024-06-21 01:55:14,451 - INFO: Epoch: 175/200, Batch: 13/29, Batch_Loss_Train: 1.697
2024-06-21 01:55:14,774 - INFO: Epoch: 175/200, Batch: 14/29, Batch_Loss_Train: 1.299
2024-06-21 01:55:15,190 - INFO: Epoch: 175/200, Batch: 15/29, Batch_Loss_Train: 1.532
2024-06-21 01:55:15,508 - INFO: Epoch: 175/200, Batch: 16/29, Batch_Loss_Train: 1.037
2024-06-21 01:55:15,920 - INFO: Epoch: 175/200, Batch: 17/29, Batch_Loss_Train: 1.266
2024-06-21 01:55:16,238 - INFO: Epoch: 175/200, Batch: 18/29, Batch_Loss_Train: 1.296
2024-06-21 01:55:16,642 - INFO: Epoch: 175/200, Batch: 19/29, Batch_Loss_Train: 1.408
2024-06-21 01:55:16,956 - INFO: Epoch: 175/200, Batch: 20/29, Batch_Loss_Train: 1.599
2024-06-21 01:55:17,359 - INFO: Epoch: 175/200, Batch: 21/29, Batch_Loss_Train: 1.669
2024-06-21 01:55:17,676 - INFO: Epoch: 175/200, Batch: 22/29, Batch_Loss_Train: 1.672
2024-06-21 01:55:18,087 - INFO: Epoch: 175/200, Batch: 23/29, Batch_Loss_Train: 1.501
2024-06-21 01:55:18,405 - INFO: Epoch: 175/200, Batch: 24/29, Batch_Loss_Train: 1.134
2024-06-21 01:55:18,807 - INFO: Epoch: 175/200, Batch: 25/29, Batch_Loss_Train: 1.845
2024-06-21 01:55:19,119 - INFO: Epoch: 175/200, Batch: 26/29, Batch_Loss_Train: 1.194
2024-06-21 01:55:19,519 - INFO: Epoch: 175/200, Batch: 27/29, Batch_Loss_Train: 1.769
2024-06-21 01:55:19,831 - INFO: Epoch: 175/200, Batch: 28/29, Batch_Loss_Train: 1.206
2024-06-21 01:55:20,052 - INFO: Epoch: 175/200, Batch: 29/29, Batch_Loss_Train: 1.510
2024-06-21 01:55:31,235 - INFO: 175/200 final results:
2024-06-21 01:55:31,235 - INFO: Training loss: 1.463.
2024-06-21 01:55:31,235 - INFO: Training MAE: 0.941.
2024-06-21 01:55:31,235 - INFO: Training MSE: 1.463.
2024-06-21 01:55:51,370 - INFO: Epoch: 175/200, Loss_train: 1.4634803821300637, Loss_val: 1.5718115979227527
2024-06-21 01:55:51,370 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:55:51,370 - INFO: Epoch 176/200...
2024-06-21 01:55:51,370 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:55:51,370 - INFO: Batch size: 32.
2024-06-21 01:55:51,374 - INFO: Dataset:
2024-06-21 01:55:51,375 - INFO: Batch size:
2024-06-21 01:55:51,375 - INFO: Number of workers:
2024-06-21 01:55:52,624 - INFO: Epoch: 176/200, Batch: 1/29, Batch_Loss_Train: 1.290
2024-06-21 01:55:52,936 - INFO: Epoch: 176/200, Batch: 2/29, Batch_Loss_Train: 1.062
2024-06-21 01:55:53,356 - INFO: Epoch: 176/200, Batch: 3/29, Batch_Loss_Train: 1.348
2024-06-21 01:55:53,681 - INFO: Epoch: 176/200, Batch: 4/29, Batch_Loss_Train: 1.118
2024-06-21 01:55:54,096 - INFO: Epoch: 176/200, Batch: 5/29, Batch_Loss_Train: 1.037
2024-06-21 01:55:54,401 - INFO: Epoch: 176/200, Batch: 6/29, Batch_Loss_Train: 1.116
2024-06-21 01:55:54,808 - INFO: Epoch: 176/200, Batch: 7/29, Batch_Loss_Train: 1.311
2024-06-21 01:55:55,126 - INFO: Epoch: 176/200, Batch: 8/29, Batch_Loss_Train: 1.171
2024-06-21 01:55:55,531 - INFO: Epoch: 176/200, Batch: 9/29, Batch_Loss_Train: 1.181
2024-06-21 01:55:55,831 - INFO: Epoch: 176/200, Batch: 10/29, Batch_Loss_Train: 1.527
2024-06-21 01:55:56,233 - INFO: Epoch: 176/200, Batch: 11/29, Batch_Loss_Train: 1.556
2024-06-21 01:55:56,554 - INFO: Epoch: 176/200, Batch: 12/29, Batch_Loss_Train: 1.932
2024-06-21 01:55:56,980 - INFO: Epoch: 176/200, Batch: 13/29, Batch_Loss_Train: 1.463
2024-06-21 01:55:57,286 - INFO: Epoch: 176/200, Batch: 14/29, Batch_Loss_Train: 1.576
2024-06-21 01:55:57,990 - INFO: Epoch: 176/200, Batch: 15/29, Batch_Loss_Train: 1.719
2024-06-21 01:55:58,306 - INFO: Epoch: 176/200, Batch: 16/29, Batch_Loss_Train: 1.126
2024-06-21 01:55:58,708 - INFO: Epoch: 176/200, Batch: 17/29, Batch_Loss_Train: 1.308
2024-06-21 01:55:59,010 - INFO: Epoch: 176/200, Batch: 18/29, Batch_Loss_Train: 1.742
2024-06-21 01:55:59,410 - INFO: Epoch: 176/200, Batch: 19/29, Batch_Loss_Train: 1.256
2024-06-21 01:55:59,721 - INFO: Epoch: 176/200, Batch: 20/29, Batch_Loss_Train: 1.335
2024-06-21 01:56:00,132 - INFO: Epoch: 176/200, Batch: 21/29, Batch_Loss_Train: 1.226
2024-06-21 01:56:00,439 - INFO: Epoch: 176/200, Batch: 22/29, Batch_Loss_Train: 1.890
2024-06-21 01:56:00,836 - INFO: Epoch: 176/200, Batch: 23/29, Batch_Loss_Train: 1.245
2024-06-21 01:56:01,154 - INFO: Epoch: 176/200, Batch: 24/29, Batch_Loss_Train: 1.958
2024-06-21 01:56:01,561 - INFO: Epoch: 176/200, Batch: 25/29, Batch_Loss_Train: 1.261
2024-06-21 01:56:01,863 - INFO: Epoch: 176/200, Batch: 26/29, Batch_Loss_Train: 1.775
2024-06-21 01:56:02,253 - INFO: Epoch: 176/200, Batch: 27/29, Batch_Loss_Train: 1.808
2024-06-21 01:56:02,565 - INFO: Epoch: 176/200, Batch: 28/29, Batch_Loss_Train: 1.479
2024-06-21 01:56:02,782 - INFO: Epoch: 176/200, Batch: 29/29, Batch_Loss_Train: 2.682
2024-06-21 01:56:13,450 - INFO: 176/200 final results:
2024-06-21 01:56:13,450 - INFO: Training loss: 1.465.
2024-06-21 01:56:13,450 - INFO: Training MAE: 0.926.
2024-06-21 01:56:13,450 - INFO: Training MSE: 1.441.
2024-06-21 01:56:33,829 - INFO: Epoch: 176/200, Loss_train: 1.4654244348920624, Loss_val: 1.5676407115212803
2024-06-21 01:56:33,830 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:56:33,830 - INFO: Epoch 177/200...
2024-06-21 01:56:33,830 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:56:33,830 - INFO: Batch size: 32.
2024-06-21 01:56:33,834 - INFO: Dataset:
2024-06-21 01:56:33,834 - INFO: Batch size:
2024-06-21 01:56:33,834 - INFO: Number of workers:
2024-06-21 01:56:35,102 - INFO: Epoch: 177/200, Batch: 1/29, Batch_Loss_Train: 1.735
2024-06-21 01:56:35,416 - INFO: Epoch: 177/200, Batch: 2/29, Batch_Loss_Train: 1.334
2024-06-21 01:56:35,825 - INFO: Epoch: 177/200, Batch: 3/29, Batch_Loss_Train: 1.421
2024-06-21 01:56:36,151 - INFO: Epoch: 177/200, Batch: 4/29, Batch_Loss_Train: 1.588
2024-06-21 01:56:36,587 - INFO: Epoch: 177/200, Batch: 5/29, Batch_Loss_Train: 1.407
2024-06-21 01:56:36,891 - INFO: Epoch: 177/200, Batch: 6/29, Batch_Loss_Train: 1.437
2024-06-21 01:56:37,284 - INFO: Epoch: 177/200, Batch: 7/29, Batch_Loss_Train: 1.473
2024-06-21 01:56:37,602 - INFO: Epoch: 177/200, Batch: 8/29, Batch_Loss_Train: 1.215
2024-06-21 01:56:38,028 - INFO: Epoch: 177/200, Batch: 9/29, Batch_Loss_Train: 1.860
2024-06-21 01:56:38,328 - INFO: Epoch: 177/200, Batch: 10/29, Batch_Loss_Train: 1.689
2024-06-21 01:56:38,719 - INFO: Epoch: 177/200, Batch: 11/29, Batch_Loss_Train: 1.660
2024-06-21 01:56:39,040 - INFO: Epoch: 177/200, Batch: 12/29, Batch_Loss_Train: 1.213
2024-06-21 01:56:39,480 - INFO: Epoch: 177/200, Batch: 13/29, Batch_Loss_Train: 1.257
2024-06-21 01:56:39,787 - INFO: Epoch: 177/200, Batch: 14/29, Batch_Loss_Train: 1.068
2024-06-21 01:56:40,187 - INFO: Epoch: 177/200, Batch: 15/29, Batch_Loss_Train: 1.437
2024-06-21 01:56:40,505 - INFO: Epoch: 177/200, Batch: 16/29, Batch_Loss_Train: 1.363
2024-06-21 01:56:40,939 - INFO: Epoch: 177/200, Batch: 17/29, Batch_Loss_Train: 1.881
2024-06-21 01:56:41,244 - INFO: Epoch: 177/200, Batch: 18/29, Batch_Loss_Train: 1.124
2024-06-21 01:56:41,636 - INFO: Epoch: 177/200, Batch: 19/29, Batch_Loss_Train: 1.168
2024-06-21 01:56:41,951 - INFO: Epoch: 177/200, Batch: 20/29, Batch_Loss_Train: 1.298
2024-06-21 01:56:42,382 - INFO: Epoch: 177/200, Batch: 21/29, Batch_Loss_Train: 1.725
2024-06-21 01:56:42,690 - INFO: Epoch: 177/200, Batch: 22/29, Batch_Loss_Train: 1.442
2024-06-21 01:56:43,079 - INFO: Epoch: 177/200, Batch: 23/29, Batch_Loss_Train: 1.231
2024-06-21 01:56:43,400 - INFO: Epoch: 177/200, Batch: 24/29, Batch_Loss_Train: 1.639
2024-06-21 01:56:43,824 - INFO: Epoch: 177/200, Batch: 25/29, Batch_Loss_Train: 1.453
2024-06-21 01:56:44,127 - INFO: Epoch: 177/200, Batch: 26/29, Batch_Loss_Train: 1.449
2024-06-21 01:56:44,502 - INFO: Epoch: 177/200, Batch: 27/29, Batch_Loss_Train: 1.253
2024-06-21 01:56:44,818 - INFO: Epoch: 177/200, Batch: 28/29, Batch_Loss_Train: 1.631
2024-06-21 01:56:45,034 - INFO: Epoch: 177/200, Batch: 29/29, Batch_Loss_Train: 1.083
2024-06-21 01:56:56,017 - INFO: 177/200 final results:
2024-06-21 01:56:56,017 - INFO: Training loss: 1.432.
2024-06-21 01:56:56,017 - INFO: Training MAE: 0.920.
2024-06-21 01:56:56,017 - INFO: Training MSE: 1.439.
2024-06-21 01:57:16,637 - INFO: Epoch: 177/200, Loss_train: 1.4321886259933998, Loss_val: 1.558876082814973
2024-06-21 01:57:16,637 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:57:16,637 - INFO: Epoch 178/200...
2024-06-21 01:57:16,637 - INFO: Learning rate: 1.7217965157071647e-06.
2024-06-21 01:57:16,637 - INFO: Batch size: 32.
2024-06-21 01:57:16,641 - INFO: Dataset:
2024-06-21 01:57:16,641 - INFO: Batch size:
2024-06-21 01:57:16,641 - INFO: Number of workers:
2024-06-21 01:57:17,893 - INFO: Epoch: 178/200, Batch: 1/29, Batch_Loss_Train: 1.272
2024-06-21 01:57:18,218 - INFO: Epoch: 178/200, Batch: 2/29, Batch_Loss_Train: 1.550
2024-06-21 01:57:18,615 - INFO: Epoch: 178/200, Batch: 3/29, Batch_Loss_Train: 2.431
2024-06-21 01:57:18,942 - INFO: Epoch: 178/200, Batch: 4/29, Batch_Loss_Train: 1.375
2024-06-21 01:57:19,367 - INFO: Epoch: 178/200, Batch: 5/29, Batch_Loss_Train: 1.464
2024-06-21 01:57:19,688 - INFO: Epoch: 178/200, Batch: 6/29, Batch_Loss_Train: 1.343
2024-06-21 01:57:20,083 - INFO: Epoch: 178/200, Batch: 7/29, Batch_Loss_Train: 1.503
2024-06-21 01:57:20,403 - INFO: Epoch: 178/200, Batch: 8/29, Batch_Loss_Train: 1.344
2024-06-21 01:57:20,817 - INFO: Epoch: 178/200, Batch: 9/29, Batch_Loss_Train: 1.611
2024-06-21 01:57:21,131 - INFO: Epoch: 178/200, Batch: 10/29, Batch_Loss_Train: 1.374
2024-06-21 01:57:21,529 - INFO: Epoch: 178/200, Batch: 11/29, Batch_Loss_Train: 1.460
2024-06-21 01:57:21,851 - INFO: Epoch: 178/200, Batch: 12/29, Batch_Loss_Train: 1.518
2024-06-21 01:57:22,288 - INFO: Epoch: 178/200, Batch: 13/29, Batch_Loss_Train: 1.723
2024-06-21 01:57:22,612 - INFO: Epoch: 178/200, Batch: 14/29, Batch_Loss_Train: 1.957
2024-06-21 01:57:23,022 - INFO: Epoch: 178/200, Batch: 15/29, Batch_Loss_Train: 1.515
2024-06-21 01:57:23,340 - INFO: Epoch: 178/200, Batch: 16/29, Batch_Loss_Train: 1.603
2024-06-21 01:57:23,765 - INFO: Epoch: 178/200, Batch: 17/29, Batch_Loss_Train: 1.178
2024-06-21 01:57:24,083 - INFO: Epoch: 178/200, Batch: 18/29, Batch_Loss_Train: 1.869
2024-06-21 01:57:24,477 - INFO: Epoch: 178/200, Batch: 19/29, Batch_Loss_Train: 1.232
2024-06-21 01:57:24,792 - INFO: Epoch: 178/200, Batch: 20/29, Batch_Loss_Train: 1.888
2024-06-21 01:57:25,213 - INFO: Epoch: 178/200, Batch: 21/29, Batch_Loss_Train: 1.147
2024-06-21 01:57:25,534 - INFO: Epoch: 178/200, Batch: 22/29, Batch_Loss_Train: 1.446
2024-06-21 01:57:25,927 - INFO: Epoch: 178/200, Batch: 23/29, Batch_Loss_Train: 1.646
2024-06-21 01:57:26,248 - INFO: Epoch: 178/200, Batch: 24/29, Batch_Loss_Train: 1.460
2024-06-21 01:57:26,657 - INFO: Epoch: 178/200, Batch: 25/29, Batch_Loss_Train: 1.413
2024-06-21 01:57:26,974 - INFO: Epoch: 178/200, Batch: 26/29, Batch_Loss_Train: 1.471
2024-06-21 01:57:27,349 - INFO: Epoch: 178/200, Batch: 27/29, Batch_Loss_Train: 1.208
2024-06-21 01:57:27,665 - INFO: Epoch: 178/200, Batch: 28/29, Batch_Loss_Train: 1.294
2024-06-21 01:57:27,888 - INFO: Epoch: 178/200, Batch: 29/29, Batch_Loss_Train: 1.311
2024-06-21 01:57:38,957 - INFO: 178/200 final results:
2024-06-21 01:57:38,957 - INFO: Training loss: 1.504.
2024-06-21 01:57:38,957 - INFO: Training MAE: 0.956.
2024-06-21 01:57:38,957 - INFO: Training MSE: 1.507.
2024-06-21 01:57:59,543 - INFO: Epoch: 178/200, Loss_train: 1.5036921460053017, Loss_val: 1.55329391051983
2024-06-21 01:57:59,543 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:57:59,543 - INFO: Epoch 179/200...
2024-06-21 01:57:59,543 - INFO: Learning rate: 8.608982578535823e-07.
2024-06-21 01:57:59,543 - INFO: Batch size: 32.
2024-06-21 01:57:59,547 - INFO: Dataset:
2024-06-21 01:57:59,548 - INFO: Batch size:
2024-06-21 01:57:59,548 - INFO: Number of workers:
2024-06-21 01:58:00,793 - INFO: Epoch: 179/200, Batch: 1/29, Batch_Loss_Train: 0.950
2024-06-21 01:58:01,107 - INFO: Epoch: 179/200, Batch: 2/29, Batch_Loss_Train: 1.613
2024-06-21 01:58:01,530 - INFO: Epoch: 179/200, Batch: 3/29, Batch_Loss_Train: 1.492
2024-06-21 01:58:01,856 - INFO: Epoch: 179/200, Batch: 4/29, Batch_Loss_Train: 1.644
2024-06-21 01:58:02,282 - INFO: Epoch: 179/200, Batch: 5/29, Batch_Loss_Train: 1.994
2024-06-21 01:58:02,588 - INFO: Epoch: 179/200, Batch: 6/29, Batch_Loss_Train: 0.878
2024-06-21 01:58:02,996 - INFO: Epoch: 179/200, Batch: 7/29, Batch_Loss_Train: 1.504
2024-06-21 01:58:03,316 - INFO: Epoch: 179/200, Batch: 8/29, Batch_Loss_Train: 1.807
2024-06-21 01:58:03,731 - INFO: Epoch: 179/200, Batch: 9/29, Batch_Loss_Train: 1.335
2024-06-21 01:58:04,032 - INFO: Epoch: 179/200, Batch: 10/29, Batch_Loss_Train: 1.249
2024-06-21 01:58:04,440 - INFO: Epoch: 179/200, Batch: 11/29, Batch_Loss_Train: 1.663
2024-06-21 01:58:04,762 - INFO: Epoch: 179/200, Batch: 12/29, Batch_Loss_Train: 1.393
2024-06-21 01:58:05,196 - INFO: Epoch: 179/200, Batch: 13/29, Batch_Loss_Train: 1.615
2024-06-21 01:58:05,505 - INFO: Epoch: 179/200, Batch: 14/29, Batch_Loss_Train: 1.538
2024-06-21 01:58:05,925 - INFO: Epoch: 179/200, Batch: 15/29, Batch_Loss_Train: 1.195
2024-06-21 01:58:06,242 - INFO: Epoch: 179/200, Batch: 16/29, Batch_Loss_Train: 1.151
2024-06-21 01:58:06,660 - INFO: Epoch: 179/200, Batch: 17/29, Batch_Loss_Train: 1.618
2024-06-21 01:58:06,964 - INFO: Epoch: 179/200, Batch: 18/29, Batch_Loss_Train: 1.450
2024-06-21 01:58:07,370 - INFO: Epoch: 179/200, Batch: 19/29, Batch_Loss_Train: 1.317
2024-06-21 01:58:07,683 - INFO: Epoch: 179/200, Batch: 20/29, Batch_Loss_Train: 1.629
2024-06-21 01:58:08,103 - INFO: Epoch: 179/200, Batch: 21/29, Batch_Loss_Train: 1.273
2024-06-21 01:58:08,411 - INFO: Epoch: 179/200, Batch: 22/29, Batch_Loss_Train: 1.398
2024-06-21 01:58:08,815 - INFO: Epoch: 179/200, Batch: 23/29, Batch_Loss_Train: 1.076
2024-06-21 01:58:09,135 - INFO: Epoch: 179/200, Batch: 24/29, Batch_Loss_Train: 1.299
2024-06-21 01:58:09,543 - INFO: Epoch: 179/200, Batch: 25/29, Batch_Loss_Train: 1.358
2024-06-21 01:58:09,846 - INFO: Epoch: 179/200, Batch: 26/29, Batch_Loss_Train: 1.902
2024-06-21 01:58:10,240 - INFO: Epoch: 179/200, Batch: 27/29, Batch_Loss_Train: 1.383
2024-06-21 01:58:10,554 - INFO: Epoch: 179/200, Batch: 28/29, Batch_Loss_Train: 1.548
2024-06-21 01:58:10,770 - INFO: Epoch: 179/200, Batch: 29/29, Batch_Loss_Train: 1.231
2024-06-21 01:58:21,516 - INFO: 179/200 final results:
2024-06-21 01:58:21,516 - INFO: Training loss: 1.431.
2024-06-21 01:58:21,516 - INFO: Training MAE: 0.924.
2024-06-21 01:58:21,516 - INFO: Training MSE: 1.435.
2024-06-21 01:58:41,882 - INFO: Epoch: 179/200, Loss_train: 1.4311525986112397, Loss_val: 1.5578624112852688
2024-06-21 01:58:41,882 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:58:41,882 - INFO: Epoch 180/200...
2024-06-21 01:58:41,882 - INFO: Learning rate: 8.608982578535823e-07.
2024-06-21 01:58:41,882 - INFO: Batch size: 32.
2024-06-21 01:58:41,886 - INFO: Dataset:
2024-06-21 01:58:41,887 - INFO: Batch size:
2024-06-21 01:58:41,887 - INFO: Number of workers:
2024-06-21 01:58:43,111 - INFO: Epoch: 180/200, Batch: 1/29, Batch_Loss_Train: 1.561
2024-06-21 01:58:43,441 - INFO: Epoch: 180/200, Batch: 2/29, Batch_Loss_Train: 1.451
2024-06-21 01:58:43,863 - INFO: Epoch: 180/200, Batch: 3/29, Batch_Loss_Train: 1.506
2024-06-21 01:58:44,190 - INFO: Epoch: 180/200, Batch: 4/29, Batch_Loss_Train: 1.501
2024-06-21 01:58:44,597 - INFO: Epoch: 180/200, Batch: 5/29, Batch_Loss_Train: 1.150
2024-06-21 01:58:44,917 - INFO: Epoch: 180/200, Batch: 6/29, Batch_Loss_Train: 1.147
2024-06-21 01:58:45,324 - INFO: Epoch: 180/200, Batch: 7/29, Batch_Loss_Train: 1.839
2024-06-21 01:58:45,645 - INFO: Epoch: 180/200, Batch: 8/29, Batch_Loss_Train: 1.597
2024-06-21 01:58:46,042 - INFO: Epoch: 180/200, Batch: 9/29, Batch_Loss_Train: 1.452
2024-06-21 01:58:46,354 - INFO: Epoch: 180/200, Batch: 10/29, Batch_Loss_Train: 1.828
2024-06-21 01:58:46,754 - INFO: Epoch: 180/200, Batch: 11/29, Batch_Loss_Train: 1.427
2024-06-21 01:58:47,074 - INFO: Epoch: 180/200, Batch: 12/29, Batch_Loss_Train: 1.350
2024-06-21 01:58:47,493 - INFO: Epoch: 180/200, Batch: 13/29, Batch_Loss_Train: 1.494
2024-06-21 01:58:47,812 - INFO: Epoch: 180/200, Batch: 14/29, Batch_Loss_Train: 1.407
2024-06-21 01:58:48,225 - INFO: Epoch: 180/200, Batch: 15/29, Batch_Loss_Train: 1.414
2024-06-21 01:58:48,541 - INFO: Epoch: 180/200, Batch: 16/29, Batch_Loss_Train: 1.203
2024-06-21 01:58:48,946 - INFO: Epoch: 180/200, Batch: 17/29, Batch_Loss_Train: 1.071
2024-06-21 01:58:49,261 - INFO: Epoch: 180/200, Batch: 18/29, Batch_Loss_Train: 1.065
2024-06-21 01:58:49,661 - INFO: Epoch: 180/200, Batch: 19/29, Batch_Loss_Train: 1.216
2024-06-21 01:58:49,973 - INFO: Epoch: 180/200, Batch: 20/29, Batch_Loss_Train: 1.236
2024-06-21 01:58:50,375 - INFO: Epoch: 180/200, Batch: 21/29, Batch_Loss_Train: 1.416
2024-06-21 01:58:50,694 - INFO: Epoch: 180/200, Batch: 22/29, Batch_Loss_Train: 1.365
2024-06-21 01:58:51,094 - INFO: Epoch: 180/200, Batch: 23/29, Batch_Loss_Train: 1.511
2024-06-21 01:58:51,412 - INFO: Epoch: 180/200, Batch: 24/29, Batch_Loss_Train: 1.562
2024-06-21 01:58:51,803 - INFO: Epoch: 180/200, Batch: 25/29, Batch_Loss_Train: 1.447
2024-06-21 01:58:52,116 - INFO: Epoch: 180/200, Batch: 26/29, Batch_Loss_Train: 1.282
2024-06-21 01:58:52,499 - INFO: Epoch: 180/200, Batch: 27/29, Batch_Loss_Train: 1.670
2024-06-21 01:58:52,813 - INFO: Epoch: 180/200, Batch: 28/29, Batch_Loss_Train: 1.508
2024-06-21 01:58:53,023 - INFO: Epoch: 180/200, Batch: 29/29, Batch_Loss_Train: 0.937
2024-06-21 01:59:04,082 - INFO: 180/200 final results:
2024-06-21 01:59:04,082 - INFO: Training loss: 1.400.
2024-06-21 01:59:04,082 - INFO: Training MAE: 0.929.
2024-06-21 01:59:04,082 - INFO: Training MSE: 1.410.
2024-06-21 01:59:24,802 - INFO: Epoch: 180/200, Loss_train: 1.4004293885724297, Loss_val: 1.5679717043350483
2024-06-21 01:59:24,802 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 01:59:24,802 - INFO: Epoch 181/200...
2024-06-21 01:59:24,802 - INFO: Learning rate: 8.608982578535823e-07.
2024-06-21 01:59:24,802 - INFO: Batch size: 32.
2024-06-21 01:59:24,806 - INFO: Dataset:
2024-06-21 01:59:24,806 - INFO: Batch size:
2024-06-21 01:59:24,806 - INFO: Number of workers:
2024-06-21 01:59:26,075 - INFO: Epoch: 181/200, Batch: 1/29, Batch_Loss_Train: 1.401
2024-06-21 01:59:26,388 - INFO: Epoch: 181/200, Batch: 2/29, Batch_Loss_Train: 1.868
2024-06-21 01:59:26,794 - INFO: Epoch: 181/200, Batch: 3/29, Batch_Loss_Train: 1.591
2024-06-21 01:59:27,118 - INFO: Epoch: 181/200, Batch: 4/29, Batch_Loss_Train: 1.223
2024-06-21 01:59:27,554 - INFO: Epoch: 181/200, Batch: 5/29, Batch_Loss_Train: 1.280
2024-06-21 01:59:27,860 - INFO: Epoch: 181/200, Batch: 6/29, Batch_Loss_Train: 1.645
2024-06-21 01:59:28,254 - INFO: Epoch: 181/200, Batch: 7/29, Batch_Loss_Train: 1.338
2024-06-21 01:59:28,575 - INFO: Epoch: 181/200, Batch: 8/29, Batch_Loss_Train: 1.373
2024-06-21 01:59:29,009 - INFO: Epoch: 181/200, Batch: 9/29, Batch_Loss_Train: 1.417
2024-06-21 01:59:29,310 - INFO: Epoch: 181/200, Batch: 10/29, Batch_Loss_Train: 0.989
2024-06-21 01:59:29,697 - INFO: Epoch: 181/200, Batch: 11/29, Batch_Loss_Train: 1.599
2024-06-21 01:59:30,021 - INFO: Epoch: 181/200, Batch: 12/29, Batch_Loss_Train: 1.421
2024-06-21 01:59:30,467 - INFO: Epoch: 181/200, Batch: 13/29, Batch_Loss_Train: 1.507
2024-06-21 01:59:30,777 - INFO: Epoch: 181/200, Batch: 14/29, Batch_Loss_Train: 1.914
2024-06-21 01:59:31,181 - INFO: Epoch: 181/200, Batch: 15/29, Batch_Loss_Train: 1.494
2024-06-21 01:59:31,500 - INFO: Epoch: 181/200, Batch: 16/29, Batch_Loss_Train: 1.139
2024-06-21 01:59:31,933 - INFO: Epoch: 181/200, Batch: 17/29, Batch_Loss_Train: 2.012
2024-06-21 01:59:32,236 - INFO: Epoch: 181/200, Batch: 18/29, Batch_Loss_Train: 1.510
2024-06-21 01:59:32,630 - INFO: Epoch: 181/200, Batch: 19/29, Batch_Loss_Train: 1.410
2024-06-21 01:59:32,945 - INFO: Epoch: 181/200, Batch: 20/29, Batch_Loss_Train: 1.492
2024-06-21 01:59:33,376 - INFO: Epoch: 181/200, Batch: 21/29, Batch_Loss_Train: 1.405
2024-06-21 01:59:33,685 - INFO: Epoch: 181/200, Batch: 22/29, Batch_Loss_Train: 1.657
2024-06-21 01:59:34,074 - INFO: Epoch: 181/200, Batch: 23/29, Batch_Loss_Train: 1.390
2024-06-21 01:59:34,396 - INFO: Epoch: 181/200, Batch: 24/29, Batch_Loss_Train: 1.486
2024-06-21 01:59:34,816 - INFO: Epoch: 181/200, Batch: 25/29, Batch_Loss_Train: 1.203
2024-06-21 01:59:35,119 - INFO: Epoch: 181/200, Batch: 26/29, Batch_Loss_Train: 1.605
2024-06-21 01:59:35,494 - INFO: Epoch: 181/200, Batch: 27/29, Batch_Loss_Train: 1.760
2024-06-21 01:59:35,809 - INFO: Epoch: 181/200, Batch: 28/29, Batch_Loss_Train: 1.457
2024-06-21 01:59:36,023 - INFO: Epoch: 181/200, Batch: 29/29, Batch_Loss_Train: 0.962
2024-06-21 01:59:47,198 - INFO: 181/200 final results:
2024-06-21 01:59:47,198 - INFO: Training loss: 1.467.
2024-06-21 01:59:47,198 - INFO: Training MAE: 0.944.
2024-06-21 01:59:47,198 - INFO: Training MSE: 1.477.
2024-06-21 02:00:07,936 - INFO: Epoch: 181/200, Loss_train: 1.4671620541605457, Loss_val: 1.5557977721608918
2024-06-21 02:00:07,936 - INFO: Best internal validation val_loss: 1.553 at epoch: 171.
2024-06-21 02:00:07,936 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
