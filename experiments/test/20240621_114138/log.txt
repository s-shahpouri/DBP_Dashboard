2024-06-21 11:41:38,217 - INFO: Device: cuda.
2024-06-21 11:41:38,217 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 11:41:38,217 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 11:41:38,217 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 11:41:38,217 - INFO: Seed: 4
2024-06-21 11:41:38,218 - INFO: 42 patients have been found in the data directory.
2024-06-21 11:41:38,256 - INFO: Train set contains 32 patients.
2024-06-21 11:41:38,256 - INFO: Val set contains 5 patients.
2024-06-21 11:41:38,256 - INFO: Test set contains 5 patients.
2024-06-21 11:41:38,256 - INFO: Fold: 0
2024-06-21 11:41:38,257 - INFO: Performing 2-fold Cross Validation.
2024-06-21 11:41:38,258 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 11:41:38,258 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 11:41:38,258 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 11:41:38,388 - INFO: To_device: False.
2024-06-21 11:41:38,390 - INFO: Transformers have been made successfully.
2024-06-21 11:41:38,390 - INFO: Dataset type: cache.
2024-06-21 11:41:38,390 - INFO: Dataloader type: standard.
2024-06-21 11:43:29,618 - INFO: Train dataloader arguments.
2024-06-21 11:43:29,619 - INFO: 	Batch_size: 32.
2024-06-21 11:43:29,619 - INFO: 	Shuffle: True.
2024-06-21 11:43:29,619 - INFO: 	Sampler: None.
2024-06-21 11:43:29,619 - INFO: 	Num_workers: 4.
2024-06-21 11:43:29,619 - INFO: 	Drop_last: False.
2024-06-21 11:43:29,669 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 11:43:30,538 - INFO: Weight init name: kaiming_uniform.
2024-06-21 11:43:32,982 - INFO: Number of training iterations per epoch: 29.
2024-06-21 11:43:32,982 - INFO: Epoch 1/10...
2024-06-21 11:43:32,982 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:43:32,982 - INFO: Batch size: 32.
2024-06-21 11:43:32,982 - INFO: Dataset:
2024-06-21 11:43:32,982 - INFO: Batch size:
2024-06-21 11:43:32,982 - INFO: Number of workers:
2024-06-21 11:43:35,862 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 70.996
2024-06-21 11:43:36,201 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 83.105
2024-06-21 11:43:36,587 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 19153.533
2024-06-21 11:43:36,903 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 5204.818
2024-06-21 11:43:37,305 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 79394.266
2024-06-21 11:43:37,616 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 110632.680
2024-06-21 11:43:38,006 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 76685.234
2024-06-21 11:43:38,324 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 128506.250
2024-06-21 11:43:38,720 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 115896.461
2024-06-21 11:43:39,031 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 14401.586
2024-06-21 11:43:39,406 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 3606.475
2024-06-21 11:43:39,725 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 3007.651
2024-06-21 11:43:40,139 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 1151.817
2024-06-21 11:43:40,458 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 433.926
2024-06-21 11:43:40,849 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 271.152
2024-06-21 11:43:41,166 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 202.061
2024-06-21 11:43:41,583 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 171.597
2024-06-21 11:43:41,901 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 151.734
2024-06-21 11:43:42,290 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 203.019
2024-06-21 11:43:42,599 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 185.843
2024-06-21 11:43:42,999 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 144.080
2024-06-21 11:43:43,313 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 116.611
2024-06-21 11:43:43,699 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 150.586
2024-06-21 11:43:44,017 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 98.869
2024-06-21 11:43:44,420 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 92.806
2024-06-21 11:43:44,732 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 161.764
2024-06-21 11:43:45,120 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 130.107
2024-06-21 11:43:45,434 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 103.861
2024-06-21 11:43:46,725 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 90.211
2024-06-21 11:43:57,701 - INFO: 1/10 final results:
2024-06-21 11:43:57,701 - INFO: Training loss: 19327.693.
2024-06-21 11:43:57,701 - INFO: Training MAE: 65.303.
2024-06-21 11:43:57,701 - INFO: Training MSE: 19708.217.
2024-06-21 11:44:18,268 - INFO: Epoch: 1/10, Loss_train: 19327.693098660173, Loss_val: 1530.5950927734375
2024-06-21 11:44:18,268 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 11:44:18,268 - INFO: Epoch 2/10...
2024-06-21 11:44:18,268 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:44:18,268 - INFO: Batch size: 32.
2024-06-21 11:44:18,271 - INFO: Dataset:
2024-06-21 11:44:18,271 - INFO: Batch size:
2024-06-21 11:44:18,271 - INFO: Number of workers:
2024-06-21 11:44:19,422 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 104.739
2024-06-21 11:44:19,729 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 72.476
2024-06-21 11:44:20,123 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 98.087
2024-06-21 11:44:20,443 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 87.648
2024-06-21 11:44:20,869 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 69.490
2024-06-21 11:44:21,171 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 80.562
2024-06-21 11:44:21,560 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 88.348
2024-06-21 11:44:21,877 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 86.107
2024-06-21 11:44:22,311 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 106.974
2024-06-21 11:44:22,607 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 92.037
2024-06-21 11:44:22,981 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 89.442
2024-06-21 11:44:23,297 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 85.692
2024-06-21 11:44:23,717 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 56.235
2024-06-21 11:44:24,023 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 109.072
2024-06-21 11:44:24,414 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 102.888
2024-06-21 11:44:24,731 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 91.379
2024-06-21 11:44:25,168 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 103.724
2024-06-21 11:44:25,473 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 99.727
2024-06-21 11:44:25,861 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 106.476
2024-06-21 11:44:26,173 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 86.423
2024-06-21 11:44:26,599 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 76.446
2024-06-21 11:44:26,903 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 76.435
2024-06-21 11:44:27,293 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 82.071
2024-06-21 11:44:27,612 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 68.568
2024-06-21 11:44:28,031 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 50.086
2024-06-21 11:44:28,333 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 54.824
2024-06-21 11:44:28,715 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 76.210
2024-06-21 11:44:29,029 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 60.357
2024-06-21 11:44:29,251 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 106.001
2024-06-21 11:44:40,319 - INFO: 2/10 final results:
2024-06-21 11:44:40,319 - INFO: Training loss: 85.122.
2024-06-21 11:44:40,319 - INFO: Training MAE: 7.416.
2024-06-21 11:44:40,319 - INFO: Training MSE: 84.709.
2024-06-21 11:45:00,587 - INFO: Epoch: 2/10, Loss_train: 85.12153599180024, Loss_val: 104.79472601002661
2024-06-21 11:45:00,634 - INFO: Saved new best metric model for epoch 2.
2024-06-21 11:45:00,634 - INFO: Best internal validation val_loss: 104.795 at epoch: 2.
2024-06-21 11:45:00,635 - INFO: Epoch 3/10...
2024-06-21 11:45:00,635 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:45:00,635 - INFO: Batch size: 32.
2024-06-21 11:45:00,638 - INFO: Dataset:
2024-06-21 11:45:00,638 - INFO: Batch size:
2024-06-21 11:45:00,638 - INFO: Number of workers:
2024-06-21 11:45:01,763 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 129.174
2024-06-21 11:45:02,072 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 181.682
2024-06-21 11:45:02,476 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 143.337
2024-06-21 11:45:02,797 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 70.201
2024-06-21 11:45:03,207 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 59.535
2024-06-21 11:45:03,510 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 102.843
2024-06-21 11:45:03,907 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 141.880
2024-06-21 11:45:04,225 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 79.671
2024-06-21 11:45:04,640 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 77.911
2024-06-21 11:45:04,937 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 55.043
2024-06-21 11:45:05,330 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 64.344
2024-06-21 11:45:05,648 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 61.800
2024-06-21 11:45:06,062 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 67.076
2024-06-21 11:45:06,366 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 71.113
2024-06-21 11:45:06,769 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 119.148
2024-06-21 11:45:07,083 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 64.095
2024-06-21 11:45:07,495 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 89.763
2024-06-21 11:45:07,798 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 89.980
2024-06-21 11:45:08,194 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 59.380
2024-06-21 11:45:08,504 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 58.210
2024-06-21 11:45:08,910 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 70.279
2024-06-21 11:45:09,212 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 76.974
2024-06-21 11:45:09,598 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 92.817
2024-06-21 11:45:09,912 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 97.518
2024-06-21 11:45:10,308 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 144.413
2024-06-21 11:45:10,607 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 171.647
2024-06-21 11:45:10,988 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 272.796
2024-06-21 11:45:11,299 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 294.484
2024-06-21 11:45:11,512 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 180.618
2024-06-21 11:45:22,559 - INFO: 3/10 final results:
2024-06-21 11:45:22,559 - INFO: Training loss: 109.922.
2024-06-21 11:45:22,559 - INFO: Training MAE: 8.396.
2024-06-21 11:45:22,559 - INFO: Training MSE: 108.523.
2024-06-21 11:45:42,922 - INFO: Epoch: 3/10, Loss_train: 109.92177174009126, Loss_val: 72.44427213997676
2024-06-21 11:45:42,981 - INFO: Saved new best metric model for epoch 3.
2024-06-21 11:45:42,981 - INFO: Best internal validation val_loss: 72.444 at epoch: 3.
2024-06-21 11:45:42,981 - INFO: Epoch 4/10...
2024-06-21 11:45:42,981 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:45:42,981 - INFO: Batch size: 32.
2024-06-21 11:45:42,985 - INFO: Dataset:
2024-06-21 11:45:42,985 - INFO: Batch size:
2024-06-21 11:45:42,985 - INFO: Number of workers:
2024-06-21 11:45:44,096 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 60.638
2024-06-21 11:45:44,406 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 54.295
2024-06-21 11:45:44,813 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 58.477
2024-06-21 11:45:45,132 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 68.208
2024-06-21 11:45:45,539 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 52.925
2024-06-21 11:45:45,842 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 49.608
2024-06-21 11:45:46,241 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 76.571
2024-06-21 11:45:46,557 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 47.618
2024-06-21 11:45:46,962 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 63.678
2024-06-21 11:45:47,258 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 61.870
2024-06-21 11:45:47,648 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 50.630
2024-06-21 11:45:47,965 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 39.981
2024-06-21 11:45:48,369 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 63.994
2024-06-21 11:45:48,674 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 92.027
2024-06-21 11:45:49,078 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 65.659
2024-06-21 11:45:49,393 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 53.482
2024-06-21 11:45:49,804 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 52.633
2024-06-21 11:45:50,107 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 126.342
2024-06-21 11:45:50,506 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 334.367
2024-06-21 11:45:50,816 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 551.797
2024-06-21 11:45:51,221 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 601.964
2024-06-21 11:45:51,523 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 414.022
2024-06-21 11:45:51,919 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 202.813
2024-06-21 11:45:52,234 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 84.816
2024-06-21 11:45:52,636 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 50.049
2024-06-21 11:45:52,936 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 33.159
2024-06-21 11:45:53,323 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 39.138
2024-06-21 11:45:53,636 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 70.902
2024-06-21 11:45:53,858 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 41.733
2024-06-21 11:46:04,898 - INFO: 4/10 final results:
2024-06-21 11:46:04,899 - INFO: Training loss: 122.876.
2024-06-21 11:46:04,899 - INFO: Training MAE: 8.355.
2024-06-21 11:46:04,899 - INFO: Training MSE: 124.481.
2024-06-21 11:46:25,531 - INFO: Epoch: 4/10, Loss_train: 122.87574754912278, Loss_val: 67.34722045372273
2024-06-21 11:46:25,591 - INFO: Saved new best metric model for epoch 4.
2024-06-21 11:46:25,591 - INFO: Best internal validation val_loss: 67.347 at epoch: 4.
2024-06-21 11:46:25,591 - INFO: Epoch 5/10...
2024-06-21 11:46:25,591 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:46:25,591 - INFO: Batch size: 32.
2024-06-21 11:46:25,594 - INFO: Dataset:
2024-06-21 11:46:25,595 - INFO: Batch size:
2024-06-21 11:46:25,595 - INFO: Number of workers:
2024-06-21 11:46:26,732 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 61.107
2024-06-21 11:46:27,041 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 33.407
2024-06-21 11:46:27,450 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 31.414
2024-06-21 11:46:27,773 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 43.575
2024-06-21 11:46:28,177 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 52.817
2024-06-21 11:46:28,495 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 40.189
2024-06-21 11:46:28,898 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 49.069
2024-06-21 11:46:29,217 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 59.459
2024-06-21 11:46:29,613 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 51.894
2024-06-21 11:46:29,925 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 45.444
2024-06-21 11:46:30,317 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 73.705
2024-06-21 11:46:30,637 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 182.858
2024-06-21 11:46:31,050 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 289.389
2024-06-21 11:46:31,369 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 319.601
2024-06-21 11:46:31,779 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 255.870
2024-06-21 11:46:32,096 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 153.017
2024-06-21 11:46:32,506 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 98.730
2024-06-21 11:46:32,824 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 73.677
2024-06-21 11:46:33,228 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 65.305
2024-06-21 11:46:33,540 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 40.092
2024-06-21 11:46:33,941 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 63.109
2024-06-21 11:46:34,259 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 134.187
2024-06-21 11:46:34,648 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 168.046
2024-06-21 11:46:34,967 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 174.097
2024-06-21 11:46:35,360 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 119.064
2024-06-21 11:46:35,674 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 76.712
2024-06-21 11:46:36,076 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 66.547
2024-06-21 11:46:36,392 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 49.489
2024-06-21 11:46:36,604 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 87.973
2024-06-21 11:46:47,138 - INFO: 5/10 final results:
2024-06-21 11:46:47,138 - INFO: Training loss: 102.064.
2024-06-21 11:46:47,139 - INFO: Training MAE: 8.087.
2024-06-21 11:46:47,139 - INFO: Training MSE: 102.342.
2024-06-21 11:47:07,743 - INFO: Epoch: 5/10, Loss_train: 102.06355956505085, Loss_val: 109.94407495959052
2024-06-21 11:47:07,744 - INFO: Best internal validation val_loss: 67.347 at epoch: 4.
2024-06-21 11:47:07,744 - INFO: Epoch 6/10...
2024-06-21 11:47:07,744 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:47:07,744 - INFO: Batch size: 32.
2024-06-21 11:47:07,747 - INFO: Dataset:
2024-06-21 11:47:07,747 - INFO: Batch size:
2024-06-21 11:47:07,747 - INFO: Number of workers:
2024-06-21 11:47:08,877 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 107.575
2024-06-21 11:47:09,197 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 126.001
2024-06-21 11:47:09,591 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 105.479
2024-06-21 11:47:09,912 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 88.236
2024-06-21 11:47:10,331 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 63.064
2024-06-21 11:47:10,633 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 45.116
2024-06-21 11:47:11,021 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 40.198
2024-06-21 11:47:11,338 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 37.090
2024-06-21 11:47:11,758 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 39.963
2024-06-21 11:47:12,054 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 46.041
2024-06-21 11:47:12,432 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 33.165
2024-06-21 11:47:12,749 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 28.111
2024-06-21 11:47:13,181 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 34.427
2024-06-21 11:47:13,485 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 45.515
2024-06-21 11:47:13,876 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 47.012
2024-06-21 11:47:14,191 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 50.528
2024-06-21 11:47:14,623 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 41.475
2024-06-21 11:47:14,926 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 49.355
2024-06-21 11:47:15,310 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 40.039
2024-06-21 11:47:15,620 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 56.436
2024-06-21 11:47:16,037 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 101.212
2024-06-21 11:47:16,337 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 105.841
2024-06-21 11:47:16,724 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 116.888
2024-06-21 11:47:17,038 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 176.543
2024-06-21 11:47:17,454 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 276.433
2024-06-21 11:47:17,753 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 420.802
2024-06-21 11:47:18,135 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 617.084
2024-06-21 11:47:18,448 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 847.865
2024-06-21 11:47:18,670 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 860.949
2024-06-21 11:47:29,747 - INFO: 6/10 final results:
2024-06-21 11:47:29,747 - INFO: Training loss: 160.291.
2024-06-21 11:47:29,747 - INFO: Training MAE: 8.943.
2024-06-21 11:47:29,747 - INFO: Training MSE: 146.432.
2024-06-21 11:47:50,124 - INFO: Epoch: 6/10, Loss_train: 160.2912173435606, Loss_val: 550.4574563914332
2024-06-21 11:47:50,125 - INFO: Best internal validation val_loss: 67.347 at epoch: 4.
2024-06-21 11:47:50,125 - INFO: Epoch 7/10...
2024-06-21 11:47:50,125 - INFO: Learning rate: 0.0017631196320841366.
2024-06-21 11:47:50,125 - INFO: Batch size: 32.
2024-06-21 11:47:50,128 - INFO: Dataset:
2024-06-21 11:47:50,128 - INFO: Batch size:
2024-06-21 11:47:50,128 - INFO: Number of workers:
2024-06-21 11:47:51,261 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 619.190
2024-06-21 11:47:51,568 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 324.704
2024-06-21 11:47:51,971 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 130.658
2024-06-21 11:47:52,288 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 57.849
2024-06-21 11:47:52,698 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 25.053
2024-06-21 11:47:52,999 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 57.833
2024-06-21 11:47:53,397 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 63.112
2024-06-21 11:47:53,713 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 35.087
2024-06-21 11:47:54,120 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 37.614
2024-06-21 11:47:54,415 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 24.342
2024-06-21 11:47:54,805 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 30.182
2024-06-21 11:47:55,119 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 43.360
2024-06-21 11:47:55,539 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 42.562
2024-06-21 11:47:55,842 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 35.310
2024-06-21 11:47:56,243 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 27.535
2024-06-21 11:47:56,556 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 34.392
2024-06-21 11:47:56,961 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 37.781
2024-06-21 11:47:57,265 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 31.218
2024-06-21 11:47:57,666 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 35.470
2024-06-21 11:47:57,978 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 37.219
2024-06-21 11:47:58,378 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 37.000
2024-06-21 11:47:58,683 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 51.449
2024-06-21 11:47:59,072 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 69.241
2024-06-21 11:47:59,389 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 49.751
2024-06-21 11:47:59,788 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 41.155
2024-06-21 11:48:00,089 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 36.280
2024-06-21 11:48:00,473 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 35.548
2024-06-21 11:48:00,787 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 46.569
2024-06-21 11:48:00,996 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 51.780
