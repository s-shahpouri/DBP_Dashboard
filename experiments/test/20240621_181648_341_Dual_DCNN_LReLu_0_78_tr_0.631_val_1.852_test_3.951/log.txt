2024-06-21 17:12:13,549 - INFO: Device: cuda.
2024-06-21 17:12:13,549 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 17:12:13,549 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 17:12:13,549 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 17:12:13,549 - INFO: Seed: 4
2024-06-21 17:12:13,549 - INFO: 42 patients have been found in the data directory.
2024-06-21 17:12:13,587 - INFO: Train set contains 32 patients.
2024-06-21 17:12:13,587 - INFO: Val set contains 5 patients.
2024-06-21 17:12:13,587 - INFO: Test set contains 5 patients.
2024-06-21 17:12:13,588 - INFO: Fold: 0
2024-06-21 17:12:13,588 - INFO: Performing 2-fold Cross Validation.
2024-06-21 17:12:13,589 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 17:12:13,589 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 17:12:13,589 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 17:12:13,720 - INFO: To_device: False.
2024-06-21 17:12:13,721 - INFO: Transformers have been made successfully.
2024-06-21 17:12:13,721 - INFO: Dataset type: cache.
2024-06-21 17:12:13,721 - INFO: Dataloader type: standard.
2024-06-21 17:14:04,751 - INFO: Train dataloader arguments.
2024-06-21 17:14:04,752 - INFO: 	Batch_size: 32.
2024-06-21 17:14:04,752 - INFO: 	Shuffle: True.
2024-06-21 17:14:04,752 - INFO: 	Sampler: None.
2024-06-21 17:14:04,752 - INFO: 	Num_workers: 4.
2024-06-21 17:14:04,752 - INFO: 	Drop_last: False.
2024-06-21 17:14:04,770 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=65536, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 17:14:05,609 - INFO: Weight init name: kaiming_uniform.
2024-06-21 17:14:08,056 - INFO: Number of training iterations per epoch: 29.
2024-06-21 17:14:08,056 - INFO: Epoch 1/200...
2024-06-21 17:14:08,056 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:14:08,056 - INFO: Batch size: 32.
2024-06-21 17:14:08,057 - INFO: Dataset:
2024-06-21 17:14:08,057 - INFO: Batch size:
2024-06-21 17:14:08,057 - INFO: Number of workers:
2024-06-21 17:14:10,919 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 7.941
2024-06-21 17:14:11,212 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 7.283
2024-06-21 17:14:11,584 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 8.065
2024-06-21 17:14:11,892 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 7.159
2024-06-21 17:14:12,284 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 11.383
2024-06-21 17:14:12,590 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 8.053
2024-06-21 17:14:12,959 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 7.071
2024-06-21 17:14:13,265 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 7.700
2024-06-21 17:14:13,651 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 8.764
2024-06-21 17:14:13,953 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 9.054
2024-06-21 17:14:14,322 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 6.702
2024-06-21 17:14:14,630 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 6.761
2024-06-21 17:14:15,046 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 7.467
2024-06-21 17:14:15,357 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 7.748
2024-06-21 17:14:15,741 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 7.260
2024-06-21 17:14:16,050 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 6.604
2024-06-21 17:14:16,464 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 6.520
2024-06-21 17:14:16,774 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 5.611
2024-06-21 17:14:17,147 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 5.903
2024-06-21 17:14:17,452 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 6.401
2024-06-21 17:14:17,839 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 5.672
2024-06-21 17:14:18,146 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 5.818
2024-06-21 17:14:18,523 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 6.402
2024-06-21 17:14:18,832 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 5.689
2024-06-21 17:14:19,217 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 5.555
2024-06-21 17:14:19,523 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 6.681
2024-06-21 17:14:19,900 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 5.704
2024-06-21 17:14:20,209 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 6.625
2024-06-21 17:14:21,502 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 7.308
2024-06-21 17:14:32,559 - INFO: 1/200 final results:
2024-06-21 17:14:32,559 - INFO: Training loss: 7.066.
2024-06-21 17:14:32,559 - INFO: Training MAE: 7.061.
2024-06-21 17:14:32,559 - INFO: Training MSE: 72.329.
2024-06-21 17:14:52,729 - INFO: Epoch: 1/200, Loss_train: 7.065634184870227, Loss_val: 7.265348023381726
2024-06-21 17:14:52,729 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 17:14:52,729 - INFO: Epoch 2/200...
2024-06-21 17:14:52,729 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:14:52,729 - INFO: Batch size: 32.
2024-06-21 17:14:52,732 - INFO: Dataset:
2024-06-21 17:14:52,733 - INFO: Batch size:
2024-06-21 17:14:52,733 - INFO: Number of workers:
2024-06-21 17:14:53,783 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 6.902
2024-06-21 17:14:54,098 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 6.028
2024-06-21 17:14:54,513 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 6.066
2024-06-21 17:14:54,813 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 6.406
2024-06-21 17:14:55,215 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 5.592
2024-06-21 17:14:55,511 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 6.020
2024-06-21 17:14:55,915 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 6.136
2024-06-21 17:14:56,212 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 5.957
2024-06-21 17:14:56,604 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 5.785
2024-06-21 17:14:56,895 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 6.025
2024-06-21 17:14:57,309 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 6.576
2024-06-21 17:14:57,607 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 6.435
2024-06-21 17:14:58,017 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 6.265
2024-06-21 17:14:58,315 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 6.160
2024-06-21 17:14:58,735 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 6.690
2024-06-21 17:14:59,033 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 5.906
2024-06-21 17:14:59,426 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 6.370
2024-06-21 17:14:59,722 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 5.944
2024-06-21 17:15:00,134 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 5.732
2024-06-21 17:15:00,426 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 6.150
2024-06-21 17:15:00,805 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 6.206
2024-06-21 17:15:01,102 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 6.039
2024-06-21 17:15:01,519 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 6.383
2024-06-21 17:15:01,815 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 5.860
2024-06-21 17:15:02,196 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 6.652
2024-06-21 17:15:02,491 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 6.145
2024-06-21 17:15:02,884 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 5.744
2024-06-21 17:15:03,178 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 5.835
2024-06-21 17:15:03,376 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 5.513
2024-06-21 17:15:14,445 - INFO: 2/200 final results:
2024-06-21 17:15:14,446 - INFO: Training loss: 6.121.
2024-06-21 17:15:14,446 - INFO: Training MAE: 6.133.
2024-06-21 17:15:14,446 - INFO: Training MSE: 55.909.
2024-06-21 17:15:34,628 - INFO: Epoch: 2/200, Loss_train: 6.121452874150769, Loss_val: 6.089674653678105
2024-06-21 17:15:34,646 - INFO: Saved new best metric model for epoch 2.
2024-06-21 17:15:34,646 - INFO: Best internal validation val_loss: 6.090 at epoch: 2.
2024-06-21 17:15:34,646 - INFO: Epoch 3/200...
2024-06-21 17:15:34,646 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:15:34,646 - INFO: Batch size: 32.
2024-06-21 17:15:34,650 - INFO: Dataset:
2024-06-21 17:15:34,650 - INFO: Batch size:
2024-06-21 17:15:34,651 - INFO: Number of workers:
2024-06-21 17:15:35,703 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 6.805
2024-06-21 17:15:36,021 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 5.567
2024-06-21 17:15:36,425 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 6.319
2024-06-21 17:15:36,742 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 5.349
2024-06-21 17:15:37,147 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 5.314
2024-06-21 17:15:37,446 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 6.009
2024-06-21 17:15:37,840 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 6.161
2024-06-21 17:15:38,152 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 5.862
2024-06-21 17:15:38,558 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 6.059
2024-06-21 17:15:38,850 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 6.900
2024-06-21 17:15:39,241 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 6.397
2024-06-21 17:15:39,554 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 6.059
2024-06-21 17:15:39,966 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 6.665
2024-06-21 17:15:40,267 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 5.706
2024-06-21 17:15:40,670 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 5.570
2024-06-21 17:15:40,980 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 8.470
2024-06-21 17:15:41,394 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 7.073
2024-06-21 17:15:41,693 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 5.537
2024-06-21 17:15:42,088 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 5.637
2024-06-21 17:15:42,394 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 6.372
2024-06-21 17:15:42,796 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 6.090
2024-06-21 17:15:43,094 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 6.094
2024-06-21 17:15:43,479 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 5.876
2024-06-21 17:15:43,790 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 6.350
2024-06-21 17:15:44,185 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 5.957
2024-06-21 17:15:44,480 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 6.086
2024-06-21 17:15:44,862 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 4.922
2024-06-21 17:15:45,169 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 5.984
2024-06-21 17:15:45,379 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 6.627
2024-06-21 17:15:56,446 - INFO: 3/200 final results:
2024-06-21 17:15:56,446 - INFO: Training loss: 6.132.
2024-06-21 17:15:56,446 - INFO: Training MAE: 6.122.
2024-06-21 17:15:56,446 - INFO: Training MSE: 55.762.
2024-06-21 17:16:16,766 - INFO: Epoch: 3/200, Loss_train: 6.131585811746532, Loss_val: 6.317991092287261
2024-06-21 17:16:16,766 - INFO: Best internal validation val_loss: 6.090 at epoch: 2.
2024-06-21 17:16:16,766 - INFO: Epoch 4/200...
2024-06-21 17:16:16,766 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:16:16,766 - INFO: Batch size: 32.
2024-06-21 17:16:16,770 - INFO: Dataset:
2024-06-21 17:16:16,770 - INFO: Batch size:
2024-06-21 17:16:16,770 - INFO: Number of workers:
2024-06-21 17:16:17,830 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 6.312
2024-06-21 17:16:18,148 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 6.231
2024-06-21 17:16:18,554 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 5.199
2024-06-21 17:16:18,870 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 5.392
2024-06-21 17:16:19,270 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 4.966
2024-06-21 17:16:19,582 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 5.560
2024-06-21 17:16:19,980 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 5.663
2024-06-21 17:16:20,292 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 5.608
2024-06-21 17:16:20,683 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 6.154
2024-06-21 17:16:20,988 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 6.057
2024-06-21 17:16:21,376 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 6.095
2024-06-21 17:16:21,690 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 6.020
2024-06-21 17:16:22,096 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 6.171
2024-06-21 17:16:22,410 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 6.101
2024-06-21 17:16:22,815 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 5.501
2024-06-21 17:16:23,126 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 4.719
2024-06-21 17:16:23,531 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 5.497
2024-06-21 17:16:23,843 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 6.083
2024-06-21 17:16:24,241 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 5.315
2024-06-21 17:16:24,547 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 5.510
2024-06-21 17:16:24,944 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 5.758
2024-06-21 17:16:25,255 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 5.881
2024-06-21 17:16:25,649 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 5.018
2024-06-21 17:16:25,963 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 5.677
2024-06-21 17:16:26,351 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 5.817
2024-06-21 17:16:26,661 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 5.832
2024-06-21 17:16:27,051 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 4.967
2024-06-21 17:16:27,359 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 6.025
2024-06-21 17:16:27,578 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 6.346
2024-06-21 17:16:38,744 - INFO: 4/200 final results:
2024-06-21 17:16:38,744 - INFO: Training loss: 5.706.
2024-06-21 17:16:38,745 - INFO: Training MAE: 5.693.
2024-06-21 17:16:38,745 - INFO: Training MSE: 50.945.
2024-06-21 17:16:59,283 - INFO: Epoch: 4/200, Loss_train: 5.705944307919206, Loss_val: 5.995601818479341
2024-06-21 17:16:59,302 - INFO: Saved new best metric model for epoch 4.
2024-06-21 17:16:59,302 - INFO: Best internal validation val_loss: 5.996 at epoch: 4.
2024-06-21 17:16:59,302 - INFO: Epoch 5/200...
2024-06-21 17:16:59,302 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:16:59,302 - INFO: Batch size: 32.
2024-06-21 17:16:59,306 - INFO: Dataset:
2024-06-21 17:16:59,306 - INFO: Batch size:
2024-06-21 17:16:59,306 - INFO: Number of workers:
2024-06-21 17:17:00,381 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 5.561
2024-06-21 17:17:00,700 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 5.653
2024-06-21 17:17:01,086 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 6.059
2024-06-21 17:17:01,401 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 5.437
2024-06-21 17:17:01,810 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 5.666
2024-06-21 17:17:02,122 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 5.107
2024-06-21 17:17:02,508 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 5.543
2024-06-21 17:17:02,822 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 5.186
2024-06-21 17:17:03,225 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 5.523
2024-06-21 17:17:03,531 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 5.589
2024-06-21 17:17:03,910 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 5.338
2024-06-21 17:17:04,226 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 5.512
2024-06-21 17:17:04,644 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 5.695
2024-06-21 17:17:04,962 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 5.150
2024-06-21 17:17:05,357 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 4.906
2024-06-21 17:17:05,671 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 5.818
2024-06-21 17:17:06,086 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 4.761
2024-06-21 17:17:06,399 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 4.471
2024-06-21 17:17:06,783 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 5.643
2024-06-21 17:17:07,091 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 5.647
2024-06-21 17:17:07,496 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 6.147
2024-06-21 17:17:07,810 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 5.369
2024-06-21 17:17:08,191 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 5.379
2024-06-21 17:17:08,505 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 5.787
2024-06-21 17:17:08,902 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 5.039
2024-06-21 17:17:09,211 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 5.300
2024-06-21 17:17:09,588 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 6.164
2024-06-21 17:17:09,898 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 5.241
2024-06-21 17:17:10,108 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 5.248
2024-06-21 17:17:21,259 - INFO: 5/200 final results:
2024-06-21 17:17:21,259 - INFO: Training loss: 5.446.
2024-06-21 17:17:21,259 - INFO: Training MAE: 5.450.
2024-06-21 17:17:21,259 - INFO: Training MSE: 47.991.
2024-06-21 17:17:41,498 - INFO: Epoch: 5/200, Loss_train: 5.446167880091174, Loss_val: 62.81978054704337
2024-06-21 17:17:41,499 - INFO: Best internal validation val_loss: 5.996 at epoch: 4.
2024-06-21 17:17:41,499 - INFO: Epoch 6/200...
2024-06-21 17:17:41,499 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:17:41,499 - INFO: Batch size: 32.
2024-06-21 17:17:41,502 - INFO: Dataset:
2024-06-21 17:17:41,502 - INFO: Batch size:
2024-06-21 17:17:41,503 - INFO: Number of workers:
2024-06-21 17:17:42,568 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 5.560
2024-06-21 17:17:42,902 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 4.567
2024-06-21 17:17:43,301 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 4.596
2024-06-21 17:17:43,620 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 5.067
2024-06-21 17:17:44,020 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 5.724
2024-06-21 17:17:44,346 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 4.981
2024-06-21 17:17:44,733 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 4.367
2024-06-21 17:17:45,048 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 5.306
2024-06-21 17:17:45,437 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 5.595
2024-06-21 17:17:45,766 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 4.929
2024-06-21 17:17:46,143 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 5.231
2024-06-21 17:17:46,459 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 5.567
2024-06-21 17:17:46,864 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 5.326
2024-06-21 17:17:47,194 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 4.778
2024-06-21 17:17:47,600 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 4.490
2024-06-21 17:17:47,920 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 4.713
2024-06-21 17:17:48,341 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 5.418
2024-06-21 17:17:48,675 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 4.999
2024-06-21 17:17:49,068 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 5.217
2024-06-21 17:17:49,376 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 4.695
2024-06-21 17:17:49,772 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 5.517
2024-06-21 17:17:50,099 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 5.226
2024-06-21 17:17:50,476 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 4.948
2024-06-21 17:17:50,791 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 4.718
2024-06-21 17:17:51,176 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 4.933
2024-06-21 17:17:51,499 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 5.355
2024-06-21 17:17:51,878 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 4.902
2024-06-21 17:17:52,189 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 4.862
2024-06-21 17:17:52,403 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 5.056
2024-06-21 17:18:03,553 - INFO: 6/200 final results:
2024-06-21 17:18:03,553 - INFO: Training loss: 5.057.
2024-06-21 17:18:03,553 - INFO: Training MAE: 5.057.
2024-06-21 17:18:03,553 - INFO: Training MSE: 42.488.
2024-06-21 17:18:24,013 - INFO: Epoch: 6/200, Loss_train: 5.056639737096326, Loss_val: 5.429097093384842
2024-06-21 17:18:24,032 - INFO: Saved new best metric model for epoch 6.
2024-06-21 17:18:24,032 - INFO: Best internal validation val_loss: 5.429 at epoch: 6.
2024-06-21 17:18:24,032 - INFO: Epoch 7/200...
2024-06-21 17:18:24,032 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:18:24,032 - INFO: Batch size: 32.
2024-06-21 17:18:24,037 - INFO: Dataset:
2024-06-21 17:18:24,037 - INFO: Batch size:
2024-06-21 17:18:24,037 - INFO: Number of workers:
2024-06-21 17:18:25,107 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 5.487
2024-06-21 17:18:25,424 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 4.856
2024-06-21 17:18:25,804 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 4.458
2024-06-21 17:18:26,119 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 5.894
2024-06-21 17:18:26,522 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 4.454
2024-06-21 17:18:26,835 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 5.163
2024-06-21 17:18:27,203 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 6.046
2024-06-21 17:18:27,514 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 4.417
2024-06-21 17:18:27,908 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 4.100
2024-06-21 17:18:28,213 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 4.801
2024-06-21 17:18:28,573 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 4.541
2024-06-21 17:18:28,887 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 3.819
2024-06-21 17:18:29,310 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 4.491
2024-06-21 17:18:29,627 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 4.921
2024-06-21 17:18:30,025 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 4.482
2024-06-21 17:18:30,339 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 3.925
2024-06-21 17:18:30,758 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 4.923
2024-06-21 17:18:31,068 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 4.587
2024-06-21 17:18:31,455 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 4.666
2024-06-21 17:18:31,761 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 4.690
2024-06-21 17:18:32,172 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 4.875
2024-06-21 17:18:32,488 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 5.633
2024-06-21 17:18:32,861 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 4.533
2024-06-21 17:18:33,176 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 4.159
2024-06-21 17:18:33,577 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 5.811
2024-06-21 17:18:33,887 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 4.505
2024-06-21 17:18:34,265 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 5.339
2024-06-21 17:18:34,576 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 5.065
2024-06-21 17:18:34,792 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 4.986
2024-06-21 17:18:45,863 - INFO: 7/200 final results:
2024-06-21 17:18:45,864 - INFO: Training loss: 4.815.
2024-06-21 17:18:45,864 - INFO: Training MAE: 4.811.
2024-06-21 17:18:45,864 - INFO: Training MSE: 39.028.
2024-06-21 17:19:05,999 - INFO: Epoch: 7/200, Loss_train: 4.81472479063889, Loss_val: 5.336269033366237
2024-06-21 17:19:06,018 - INFO: Saved new best metric model for epoch 7.
2024-06-21 17:19:06,018 - INFO: Best internal validation val_loss: 5.336 at epoch: 7.
2024-06-21 17:19:06,018 - INFO: Epoch 8/200...
2024-06-21 17:19:06,018 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:19:06,018 - INFO: Batch size: 32.
2024-06-21 17:19:06,022 - INFO: Dataset:
2024-06-21 17:19:06,022 - INFO: Batch size:
2024-06-21 17:19:06,022 - INFO: Number of workers:
2024-06-21 17:19:07,110 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 4.561
2024-06-21 17:19:07,416 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 4.363
2024-06-21 17:19:07,804 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 5.099
2024-06-21 17:19:08,120 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 4.179
2024-06-21 17:19:08,538 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 4.497
2024-06-21 17:19:08,836 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 4.300
2024-06-21 17:19:09,214 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 4.407
2024-06-21 17:19:09,524 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 4.223
2024-06-21 17:19:09,938 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 4.158
2024-06-21 17:19:10,230 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 3.779
2024-06-21 17:19:10,604 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 4.489
2024-06-21 17:19:10,919 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 4.398
2024-06-21 17:19:11,346 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 3.884
2024-06-21 17:19:11,646 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 4.780
2024-06-21 17:19:12,034 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 4.937
2024-06-21 17:19:12,345 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 4.060
2024-06-21 17:19:12,766 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 5.710
2024-06-21 17:19:13,067 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 4.747
2024-06-21 17:19:13,453 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 4.669
2024-06-21 17:19:13,761 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 4.762
2024-06-21 17:19:14,185 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 4.007
2024-06-21 17:19:14,487 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 4.513
2024-06-21 17:19:14,867 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 4.217
2024-06-21 17:19:15,183 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 5.080
2024-06-21 17:19:15,596 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 4.377
2024-06-21 17:19:15,891 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 4.630
2024-06-21 17:19:16,269 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 4.132
2024-06-21 17:19:16,577 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 4.620
2024-06-21 17:19:16,790 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 4.018
2024-06-21 17:19:27,993 - INFO: 8/200 final results:
2024-06-21 17:19:27,993 - INFO: Training loss: 4.469.
2024-06-21 17:19:27,993 - INFO: Training MAE: 4.478.
2024-06-21 17:19:27,993 - INFO: Training MSE: 34.736.
2024-06-21 17:19:48,572 - INFO: Epoch: 8/200, Loss_train: 4.4688659618640765, Loss_val: 4.9376300285602435
2024-06-21 17:19:48,590 - INFO: Saved new best metric model for epoch 8.
2024-06-21 17:19:48,591 - INFO: Best internal validation val_loss: 4.938 at epoch: 8.
2024-06-21 17:19:48,591 - INFO: Epoch 9/200...
2024-06-21 17:19:48,591 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:19:48,591 - INFO: Batch size: 32.
2024-06-21 17:19:48,595 - INFO: Dataset:
2024-06-21 17:19:48,595 - INFO: Batch size:
2024-06-21 17:19:48,595 - INFO: Number of workers:
2024-06-21 17:19:49,686 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 4.585
2024-06-21 17:19:49,992 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 4.605
2024-06-21 17:19:50,385 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 3.983
2024-06-21 17:19:50,702 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 3.840
2024-06-21 17:19:51,124 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 4.863
2024-06-21 17:19:51,427 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 4.295
2024-06-21 17:19:51,811 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 3.889
2024-06-21 17:19:52,127 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 4.696
2024-06-21 17:19:52,541 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 4.714
2024-06-21 17:19:52,834 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 4.298
2024-06-21 17:19:53,207 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 4.154
2024-06-21 17:19:53,524 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 3.945
2024-06-21 17:19:53,945 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 4.416
2024-06-21 17:19:54,249 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 4.595
2024-06-21 17:19:54,657 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 4.030
2024-06-21 17:19:54,979 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 3.828
2024-06-21 17:19:55,408 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 4.091
2024-06-21 17:19:55,708 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 4.145
2024-06-21 17:19:56,091 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 3.957
2024-06-21 17:19:56,399 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 4.682
2024-06-21 17:19:56,822 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 4.037
2024-06-21 17:19:57,124 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 4.028
2024-06-21 17:19:57,512 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 4.102
2024-06-21 17:19:57,829 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 4.771
2024-06-21 17:19:58,250 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 4.492
2024-06-21 17:19:58,549 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 4.119
2024-06-21 17:19:58,936 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 4.610
2024-06-21 17:19:59,248 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 4.172
2024-06-21 17:19:59,469 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 4.537
2024-06-21 17:20:10,622 - INFO: 9/200 final results:
2024-06-21 17:20:10,622 - INFO: Training loss: 4.292.
2024-06-21 17:20:10,622 - INFO: Training MAE: 4.288.
2024-06-21 17:20:10,622 - INFO: Training MSE: 32.897.
2024-06-21 17:20:31,160 - INFO: Epoch: 9/200, Loss_train: 4.29236967810269, Loss_val: 4.314211820733958
2024-06-21 17:20:31,179 - INFO: Saved new best metric model for epoch 9.
2024-06-21 17:20:31,179 - INFO: Best internal validation val_loss: 4.314 at epoch: 9.
2024-06-21 17:20:31,179 - INFO: Epoch 10/200...
2024-06-21 17:20:31,179 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:20:31,179 - INFO: Batch size: 32.
2024-06-21 17:20:31,183 - INFO: Dataset:
2024-06-21 17:20:31,183 - INFO: Batch size:
2024-06-21 17:20:31,183 - INFO: Number of workers:
2024-06-21 17:20:32,267 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 4.413
2024-06-21 17:20:32,574 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 3.365
2024-06-21 17:20:32,971 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 4.467
2024-06-21 17:20:33,290 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 4.729
2024-06-21 17:20:33,719 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 4.078
2024-06-21 17:20:34,021 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 3.944
2024-06-21 17:20:34,412 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 4.578
2024-06-21 17:20:34,727 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 3.532
2024-06-21 17:20:35,153 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 4.096
2024-06-21 17:20:35,447 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 3.870
2024-06-21 17:20:35,823 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 4.700
2024-06-21 17:20:36,140 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 4.636
2024-06-21 17:20:36,576 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 4.863
2024-06-21 17:20:36,880 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 4.526
2024-06-21 17:20:37,277 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 4.339
2024-06-21 17:20:37,591 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 4.437
2024-06-21 17:20:38,025 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 3.946
2024-06-21 17:20:38,325 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 4.029
2024-06-21 17:20:38,714 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 4.787
2024-06-21 17:20:39,021 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 3.957
2024-06-21 17:20:39,444 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 4.041
2024-06-21 17:20:39,747 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 3.652
2024-06-21 17:20:40,134 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 4.891
2024-06-21 17:20:40,451 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 4.419
2024-06-21 17:20:40,872 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 4.317
2024-06-21 17:20:41,170 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 4.250
2024-06-21 17:20:41,550 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 5.100
2024-06-21 17:20:41,861 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 4.498
2024-06-21 17:20:42,082 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 4.641
2024-06-21 17:20:53,299 - INFO: 10/200 final results:
2024-06-21 17:20:53,299 - INFO: Training loss: 4.314.
2024-06-21 17:20:53,299 - INFO: Training MAE: 4.307.
2024-06-21 17:20:53,299 - INFO: Training MSE: 33.306.
2024-06-21 17:21:13,806 - INFO: Epoch: 10/200, Loss_train: 4.31391953599864, Loss_val: 103.39461648875269
2024-06-21 17:21:13,807 - INFO: Best internal validation val_loss: 4.314 at epoch: 9.
2024-06-21 17:21:13,807 - INFO: Epoch 11/200...
2024-06-21 17:21:13,807 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:21:13,807 - INFO: Batch size: 32.
2024-06-21 17:21:13,810 - INFO: Dataset:
2024-06-21 17:21:13,810 - INFO: Batch size:
2024-06-21 17:21:13,810 - INFO: Number of workers:
2024-06-21 17:21:14,863 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 6.001
2024-06-21 17:21:15,180 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 4.650
2024-06-21 17:21:15,588 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 4.108
2024-06-21 17:21:15,906 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 4.375
2024-06-21 17:21:16,317 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 4.460
2024-06-21 17:21:16,616 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 5.542
2024-06-21 17:21:17,015 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 3.642
2024-06-21 17:21:17,329 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 5.522
2024-06-21 17:21:17,738 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 4.302
2024-06-21 17:21:18,029 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 5.402
2024-06-21 17:21:18,418 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 4.936
2024-06-21 17:21:18,734 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 4.751
2024-06-21 17:21:19,152 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 4.129
2024-06-21 17:21:19,455 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 4.876
2024-06-21 17:21:19,860 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 4.711
2024-06-21 17:21:20,173 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 4.848
2024-06-21 17:21:20,591 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 4.086
2024-06-21 17:21:20,890 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 4.684
2024-06-21 17:21:21,287 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 3.985
2024-06-21 17:21:21,596 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 4.046
2024-06-21 17:21:22,001 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 4.915
2024-06-21 17:21:22,302 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 4.690
2024-06-21 17:21:22,701 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 4.522
2024-06-21 17:21:23,015 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 4.523
2024-06-21 17:21:23,422 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 4.417
2024-06-21 17:21:23,719 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 4.762
2024-06-21 17:21:24,116 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 4.017
2024-06-21 17:21:24,425 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 4.296
2024-06-21 17:21:24,647 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 4.370
2024-06-21 17:21:35,666 - INFO: 11/200 final results:
2024-06-21 17:21:35,666 - INFO: Training loss: 4.606.
2024-06-21 17:21:35,666 - INFO: Training MAE: 4.610.
2024-06-21 17:21:35,666 - INFO: Training MSE: 36.381.
2024-06-21 17:21:55,642 - INFO: Epoch: 11/200, Loss_train: 4.605817293298656, Loss_val: 4.235547526129361
2024-06-21 17:21:55,900 - INFO: Saved new best metric model for epoch 11.
2024-06-21 17:21:55,900 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:21:55,900 - INFO: Epoch 12/200...
2024-06-21 17:21:55,901 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:21:55,901 - INFO: Batch size: 32.
2024-06-21 17:21:55,905 - INFO: Dataset:
2024-06-21 17:21:55,905 - INFO: Batch size:
2024-06-21 17:21:55,905 - INFO: Number of workers:
2024-06-21 17:21:56,994 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 4.668
2024-06-21 17:21:57,301 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 4.343
2024-06-21 17:21:57,711 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 4.826
2024-06-21 17:21:58,031 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 5.450
2024-06-21 17:21:58,441 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 3.668
2024-06-21 17:21:58,744 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 3.814
2024-06-21 17:21:59,144 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 3.805
2024-06-21 17:21:59,459 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 5.037
2024-06-21 17:21:59,865 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 4.583
2024-06-21 17:22:00,160 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 4.426
2024-06-21 17:22:00,548 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 4.629
2024-06-21 17:22:00,866 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 4.592
2024-06-21 17:22:01,284 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 4.424
2024-06-21 17:22:01,589 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 4.392
2024-06-21 17:22:01,997 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 4.064
2024-06-21 17:22:02,309 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 4.377
2024-06-21 17:22:02,727 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 4.190
2024-06-21 17:22:03,027 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 3.918
2024-06-21 17:22:03,423 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 3.941
2024-06-21 17:22:03,732 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 4.106
2024-06-21 17:22:04,139 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 3.744
2024-06-21 17:22:04,443 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 3.804
2024-06-21 17:22:04,837 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 5.012
2024-06-21 17:22:05,152 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 3.825
2024-06-21 17:22:05,552 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 4.117
2024-06-21 17:22:05,850 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 4.699
2024-06-21 17:22:06,247 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 3.980
2024-06-21 17:22:06,559 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 4.456
2024-06-21 17:22:06,773 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 3.859
2024-06-21 17:22:17,603 - INFO: 12/200 final results:
2024-06-21 17:22:17,604 - INFO: Training loss: 4.302.
2024-06-21 17:22:17,604 - INFO: Training MAE: 4.310.
2024-06-21 17:22:17,604 - INFO: Training MSE: 33.896.
2024-06-21 17:22:37,913 - INFO: Epoch: 12/200, Loss_train: 4.301658202861917, Loss_val: 5.254584049356395
2024-06-21 17:22:37,913 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:22:37,913 - INFO: Epoch 13/200...
2024-06-21 17:22:37,913 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:22:37,913 - INFO: Batch size: 32.
2024-06-21 17:22:37,917 - INFO: Dataset:
2024-06-21 17:22:37,917 - INFO: Batch size:
2024-06-21 17:22:37,917 - INFO: Number of workers:
2024-06-21 17:22:38,985 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 3.772
2024-06-21 17:22:39,305 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 4.258
2024-06-21 17:22:39,711 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 4.187
2024-06-21 17:22:40,028 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 4.251
2024-06-21 17:22:40,439 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 4.368
2024-06-21 17:22:40,738 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 3.869
2024-06-21 17:22:41,137 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 3.770
2024-06-21 17:22:41,450 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 4.669
2024-06-21 17:22:41,856 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 4.084
2024-06-21 17:22:42,148 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 3.987
2024-06-21 17:22:42,534 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 3.997
2024-06-21 17:22:42,848 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 3.868
2024-06-21 17:22:43,263 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 3.855
2024-06-21 17:22:43,565 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 4.055
2024-06-21 17:22:43,970 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 4.858
2024-06-21 17:22:44,280 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 4.042
2024-06-21 17:22:44,695 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 3.847
2024-06-21 17:22:44,994 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 3.461
2024-06-21 17:22:45,389 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 4.732
2024-06-21 17:22:45,694 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 3.650
2024-06-21 17:22:46,101 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 3.851
2024-06-21 17:22:46,402 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 3.743
2024-06-21 17:22:46,785 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 4.280
2024-06-21 17:22:47,097 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 4.369
2024-06-21 17:22:47,495 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 4.034
2024-06-21 17:22:47,791 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 4.598
2024-06-21 17:22:48,171 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 3.918
2024-06-21 17:22:48,479 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 4.363
2024-06-21 17:22:48,691 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 3.257
2024-06-21 17:22:59,818 - INFO: 13/200 final results:
2024-06-21 17:22:59,818 - INFO: Training loss: 4.069.
2024-06-21 17:22:59,818 - INFO: Training MAE: 4.085.
2024-06-21 17:22:59,818 - INFO: Training MSE: 30.679.
2024-06-21 17:23:20,369 - INFO: Epoch: 13/200, Loss_train: 4.068816456301459, Loss_val: 5.116970637748981
2024-06-21 17:23:20,369 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:23:20,370 - INFO: Epoch 14/200...
2024-06-21 17:23:20,370 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:23:20,370 - INFO: Batch size: 32.
2024-06-21 17:23:20,373 - INFO: Dataset:
2024-06-21 17:23:20,373 - INFO: Batch size:
2024-06-21 17:23:20,373 - INFO: Number of workers:
2024-06-21 17:23:21,458 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 4.170
2024-06-21 17:23:21,764 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 4.327
2024-06-21 17:23:22,157 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 4.174
2024-06-21 17:23:22,474 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 3.835
2024-06-21 17:23:22,899 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 3.975
2024-06-21 17:23:23,199 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 4.288
2024-06-21 17:23:23,586 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 3.546
2024-06-21 17:23:23,899 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 4.398
2024-06-21 17:23:24,320 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 3.913
2024-06-21 17:23:24,611 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 4.056
2024-06-21 17:23:24,990 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 4.310
2024-06-21 17:23:25,305 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 3.871
2024-06-21 17:23:25,735 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 3.910
2024-06-21 17:23:26,038 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 3.749
2024-06-21 17:23:26,434 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 3.934
2024-06-21 17:23:26,746 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 4.789
2024-06-21 17:23:27,174 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 6.013
2024-06-21 17:23:27,473 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 3.853
2024-06-21 17:23:27,861 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 4.811
2024-06-21 17:23:28,166 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 4.243
2024-06-21 17:23:28,589 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 4.083
2024-06-21 17:23:28,891 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 3.865
2024-06-21 17:23:29,270 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 3.814
2024-06-21 17:23:29,584 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 3.982
2024-06-21 17:23:30,002 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 3.438
2024-06-21 17:23:30,298 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 4.125
2024-06-21 17:23:30,682 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 3.489
2024-06-21 17:23:30,991 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 4.162
2024-06-21 17:23:31,211 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 4.957
2024-06-21 17:23:42,265 - INFO: 14/200 final results:
2024-06-21 17:23:42,265 - INFO: Training loss: 4.141.
2024-06-21 17:23:42,265 - INFO: Training MAE: 4.124.
2024-06-21 17:23:42,265 - INFO: Training MSE: 31.418.
2024-06-21 17:24:02,772 - INFO: Epoch: 14/200, Loss_train: 4.140634454529861, Loss_val: 4.014850082068608
2024-06-21 17:24:02,791 - INFO: Saved new best metric model for epoch 14.
2024-06-21 17:24:02,792 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:24:02,792 - INFO: Epoch 15/200...
2024-06-21 17:24:02,792 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:24:02,792 - INFO: Batch size: 32.
2024-06-21 17:24:02,796 - INFO: Dataset:
2024-06-21 17:24:02,796 - INFO: Batch size:
2024-06-21 17:24:02,796 - INFO: Number of workers:
2024-06-21 17:24:03,872 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 3.810
2024-06-21 17:24:04,177 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 4.067
2024-06-21 17:24:04,584 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 3.988
2024-06-21 17:24:04,901 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 3.901
2024-06-21 17:24:05,310 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 3.856
2024-06-21 17:24:05,610 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 4.161
2024-06-21 17:24:06,008 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 3.995
2024-06-21 17:24:06,322 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 5.219
2024-06-21 17:24:06,725 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 5.127
2024-06-21 17:24:07,017 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 4.950
2024-06-21 17:24:07,401 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 4.577
2024-06-21 17:24:07,717 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 4.765
2024-06-21 17:24:08,132 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 4.448
2024-06-21 17:24:08,434 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 3.698
2024-06-21 17:24:08,837 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 5.224
2024-06-21 17:24:09,148 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 3.990
2024-06-21 17:24:09,561 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 4.670
2024-06-21 17:24:09,860 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 4.067
2024-06-21 17:24:10,255 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 4.294
2024-06-21 17:24:10,561 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 3.639
2024-06-21 17:24:10,964 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 4.223
2024-06-21 17:24:11,265 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 3.285
2024-06-21 17:24:11,653 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 3.875
2024-06-21 17:24:11,966 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 3.455
2024-06-21 17:24:12,372 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 4.038
2024-06-21 17:24:12,668 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 3.784
2024-06-21 17:24:13,047 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 3.089
2024-06-21 17:24:13,356 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 3.308
2024-06-21 17:24:13,570 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 4.678
2024-06-21 17:24:24,333 - INFO: 15/200 final results:
2024-06-21 17:24:24,333 - INFO: Training loss: 4.144.
2024-06-21 17:24:24,333 - INFO: Training MAE: 4.134.
2024-06-21 17:24:24,333 - INFO: Training MSE: 31.081.
2024-06-21 17:24:44,765 - INFO: Epoch: 15/200, Loss_train: 4.144254034963147, Loss_val: 4.3056313004987
2024-06-21 17:24:44,765 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:24:44,765 - INFO: Epoch 16/200...
2024-06-21 17:24:44,765 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:24:44,765 - INFO: Batch size: 32.
2024-06-21 17:24:44,769 - INFO: Dataset:
2024-06-21 17:24:44,769 - INFO: Batch size:
2024-06-21 17:24:44,769 - INFO: Number of workers:
2024-06-21 17:24:45,873 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 4.079
2024-06-21 17:24:46,178 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 4.478
2024-06-21 17:24:46,572 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 4.553
2024-06-21 17:24:46,877 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 3.538
2024-06-21 17:24:47,306 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 3.580
2024-06-21 17:24:47,609 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 4.636
2024-06-21 17:24:47,998 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 3.717
2024-06-21 17:24:48,301 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 3.547
2024-06-21 17:24:48,752 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 3.606
2024-06-21 17:24:49,047 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 4.282
2024-06-21 17:24:49,421 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 3.872
2024-06-21 17:24:49,727 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 3.523
2024-06-21 17:24:50,169 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 4.183
2024-06-21 17:24:50,474 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 3.542
2024-06-21 17:24:50,865 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 3.433
2024-06-21 17:24:51,165 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 3.898
2024-06-21 17:24:51,610 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 4.593
2024-06-21 17:24:51,910 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 3.461
2024-06-21 17:24:52,291 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 4.188
2024-06-21 17:24:52,586 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 5.100
2024-06-21 17:24:53,019 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 3.818
2024-06-21 17:24:53,322 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 4.168
2024-06-21 17:24:53,707 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 3.805
2024-06-21 17:24:54,011 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 3.416
2024-06-21 17:24:54,444 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 3.303
2024-06-21 17:24:54,743 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 3.193
2024-06-21 17:24:55,129 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 3.559
2024-06-21 17:24:55,429 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 3.487
2024-06-21 17:24:55,650 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 4.369
2024-06-21 17:25:06,725 - INFO: 16/200 final results:
2024-06-21 17:25:06,725 - INFO: Training loss: 3.894.
2024-06-21 17:25:06,725 - INFO: Training MAE: 3.885.
2024-06-21 17:25:06,725 - INFO: Training MSE: 28.564.
2024-06-21 17:25:27,041 - INFO: Epoch: 16/200, Loss_train: 3.8940957661332756, Loss_val: 4.3723649978637695
2024-06-21 17:25:27,041 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:25:27,042 - INFO: Epoch 17/200...
2024-06-21 17:25:27,042 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:25:27,042 - INFO: Batch size: 32.
2024-06-21 17:25:27,045 - INFO: Dataset:
2024-06-21 17:25:27,046 - INFO: Batch size:
2024-06-21 17:25:27,046 - INFO: Number of workers:
2024-06-21 17:25:28,128 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 3.725
2024-06-21 17:25:28,464 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 3.678
2024-06-21 17:25:28,858 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 3.520
2024-06-21 17:25:29,173 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 4.265
2024-06-21 17:25:29,586 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 4.103
2024-06-21 17:25:29,910 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 3.913
2024-06-21 17:25:30,295 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 3.685
2024-06-21 17:25:30,595 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 3.364
2024-06-21 17:25:31,001 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 3.881
2024-06-21 17:25:31,327 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 3.680
2024-06-21 17:25:31,699 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 3.733
2024-06-21 17:25:32,001 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 3.474
2024-06-21 17:25:32,417 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 3.520
2024-06-21 17:25:32,744 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 3.428
2024-06-21 17:25:33,139 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 3.766
2024-06-21 17:25:33,437 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 3.000
2024-06-21 17:25:33,853 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 3.496
2024-06-21 17:25:34,176 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 3.232
2024-06-21 17:25:34,561 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 3.963
2024-06-21 17:25:34,853 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 4.040
2024-06-21 17:25:35,256 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 3.224
2024-06-21 17:25:35,580 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 3.814
2024-06-21 17:25:35,952 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 3.986
2024-06-21 17:25:36,252 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 3.371
2024-06-21 17:25:36,646 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 3.734
2024-06-21 17:25:36,966 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 4.163
2024-06-21 17:25:37,347 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 3.274
2024-06-21 17:25:37,643 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 4.155
2024-06-21 17:25:37,857 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 3.504
2024-06-21 17:25:49,028 - INFO: 17/200 final results:
2024-06-21 17:25:49,029 - INFO: Training loss: 3.679.
2024-06-21 17:25:49,029 - INFO: Training MAE: 3.682.
2024-06-21 17:25:49,029 - INFO: Training MSE: 25.438.
2024-06-21 17:26:09,351 - INFO: Epoch: 17/200, Loss_train: 3.678962025149115, Loss_val: 4.068274382887216
2024-06-21 17:26:09,351 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:26:09,351 - INFO: Epoch 18/200...
2024-06-21 17:26:09,351 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:26:09,351 - INFO: Batch size: 32.
2024-06-21 17:26:09,355 - INFO: Dataset:
2024-06-21 17:26:09,355 - INFO: Batch size:
2024-06-21 17:26:09,355 - INFO: Number of workers:
2024-06-21 17:26:10,430 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 3.684
2024-06-21 17:26:10,735 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 3.575
2024-06-21 17:26:11,129 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 3.998
2024-06-21 17:26:11,446 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 3.584
2024-06-21 17:26:11,857 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 3.523
2024-06-21 17:26:12,157 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 3.446
2024-06-21 17:26:12,550 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 3.967
2024-06-21 17:26:12,864 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 3.281
2024-06-21 17:26:13,278 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 3.678
2024-06-21 17:26:13,572 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 3.770
2024-06-21 17:26:13,964 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 3.575
2024-06-21 17:26:14,282 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 3.442
2024-06-21 17:26:14,704 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 3.169
2024-06-21 17:26:15,010 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 3.408
2024-06-21 17:26:15,416 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 4.157
2024-06-21 17:26:15,731 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 4.051
2024-06-21 17:26:16,145 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 2.807
2024-06-21 17:26:16,447 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 2.800
2024-06-21 17:26:16,848 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 3.362
2024-06-21 17:26:17,156 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 3.686
2024-06-21 17:26:17,564 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 3.796
2024-06-21 17:26:17,868 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 4.512
2024-06-21 17:26:18,260 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 3.353
2024-06-21 17:26:18,576 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 3.280
2024-06-21 17:26:18,988 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 3.748
2024-06-21 17:26:19,293 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 3.531
2024-06-21 17:26:19,707 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 3.400
2024-06-21 17:26:20,026 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 4.387
2024-06-21 17:26:20,247 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 2.869
2024-06-21 17:26:31,280 - INFO: 18/200 final results:
2024-06-21 17:26:31,280 - INFO: Training loss: 3.581.
2024-06-21 17:26:31,280 - INFO: Training MAE: 3.595.
2024-06-21 17:26:31,280 - INFO: Training MSE: 24.629.
2024-06-21 17:26:51,554 - INFO: Epoch: 18/200, Loss_train: 3.580695094733403, Loss_val: 5.027417906399431
2024-06-21 17:26:51,554 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:26:51,554 - INFO: Epoch 19/200...
2024-06-21 17:26:51,554 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:26:51,554 - INFO: Batch size: 32.
2024-06-21 17:26:51,558 - INFO: Dataset:
2024-06-21 17:26:51,559 - INFO: Batch size:
2024-06-21 17:26:51,559 - INFO: Number of workers:
2024-06-21 17:26:52,663 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 4.574
2024-06-21 17:26:52,995 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 3.737
2024-06-21 17:26:53,388 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 3.407
2024-06-21 17:26:53,706 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 3.170
2024-06-21 17:26:54,103 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 3.122
2024-06-21 17:26:54,427 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 3.653
2024-06-21 17:26:54,816 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 3.874
2024-06-21 17:26:55,128 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 3.683
2024-06-21 17:26:55,514 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 4.287
2024-06-21 17:26:55,841 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 3.114
2024-06-21 17:26:56,216 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 3.781
2024-06-21 17:26:56,531 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 3.675
2024-06-21 17:26:56,936 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 3.193
2024-06-21 17:26:57,264 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 3.605
2024-06-21 17:26:57,655 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 3.089
2024-06-21 17:26:57,966 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 3.883
2024-06-21 17:26:58,370 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 3.709
2024-06-21 17:26:58,694 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 3.743
2024-06-21 17:26:59,080 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 3.369
2024-06-21 17:26:59,386 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 3.299
2024-06-21 17:26:59,781 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 3.544
2024-06-21 17:27:00,106 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 3.127
2024-06-21 17:27:00,481 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 3.776
2024-06-21 17:27:00,794 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 3.537
2024-06-21 17:27:01,186 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 3.188
2024-06-21 17:27:01,507 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 3.284
2024-06-21 17:27:01,891 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 3.757
2024-06-21 17:27:02,200 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 3.560
2024-06-21 17:27:02,417 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 3.375
2024-06-21 17:27:13,420 - INFO: 19/200 final results:
2024-06-21 17:27:13,420 - INFO: Training loss: 3.556.
2024-06-21 17:27:13,420 - INFO: Training MAE: 3.559.
2024-06-21 17:27:13,420 - INFO: Training MSE: 23.829.
2024-06-21 17:27:33,952 - INFO: Epoch: 19/200, Loss_train: 3.555648680390983, Loss_val: 3.8135190421137315
2024-06-21 17:27:33,972 - INFO: Saved new best metric model for epoch 19.
2024-06-21 17:27:33,972 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:27:33,972 - INFO: Epoch 20/200...
2024-06-21 17:27:33,972 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:27:33,972 - INFO: Batch size: 32.
2024-06-21 17:27:33,976 - INFO: Dataset:
2024-06-21 17:27:33,976 - INFO: Batch size:
2024-06-21 17:27:33,976 - INFO: Number of workers:
2024-06-21 17:27:35,038 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 2.939
2024-06-21 17:27:35,344 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 3.330
2024-06-21 17:27:35,752 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 3.294
2024-06-21 17:27:36,070 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 3.532
2024-06-21 17:27:36,495 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 3.361
2024-06-21 17:27:36,796 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 4.361
2024-06-21 17:27:37,185 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 3.489
2024-06-21 17:27:37,500 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 3.160
2024-06-21 17:27:37,927 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 3.387
2024-06-21 17:27:38,220 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 3.155
2024-06-21 17:27:38,593 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 3.739
2024-06-21 17:27:38,910 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 2.762
2024-06-21 17:27:39,340 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 3.595
2024-06-21 17:27:39,642 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 3.104
2024-06-21 17:27:40,035 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 3.112
2024-06-21 17:27:40,347 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 2.399
2024-06-21 17:27:40,775 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 2.625
2024-06-21 17:27:41,073 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 3.874
2024-06-21 17:27:41,454 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 3.184
2024-06-21 17:27:41,760 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 3.331
2024-06-21 17:27:42,179 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 3.389
2024-06-21 17:27:42,482 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 3.253
2024-06-21 17:27:42,869 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 4.758
2024-06-21 17:27:43,184 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 3.697
2024-06-21 17:27:43,601 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 3.379
2024-06-21 17:27:43,899 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 3.534
2024-06-21 17:27:44,284 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 3.270
2024-06-21 17:27:44,596 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 3.435
2024-06-21 17:27:44,816 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 4.219
2024-06-21 17:27:55,902 - INFO: 20/200 final results:
2024-06-21 17:27:55,902 - INFO: Training loss: 3.402.
2024-06-21 17:27:55,902 - INFO: Training MAE: 3.386.
2024-06-21 17:27:55,902 - INFO: Training MSE: 21.774.
2024-06-21 17:28:16,160 - INFO: Epoch: 20/200, Loss_train: 3.4022917418644347, Loss_val: 5.3965310063855405
2024-06-21 17:28:16,160 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:28:16,160 - INFO: Epoch 21/200...
2024-06-21 17:28:16,160 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:28:16,160 - INFO: Batch size: 32.
2024-06-21 17:28:16,163 - INFO: Dataset:
2024-06-21 17:28:16,164 - INFO: Batch size:
2024-06-21 17:28:16,164 - INFO: Number of workers:
2024-06-21 17:28:17,240 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 4.733
2024-06-21 17:28:17,546 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 3.309
2024-06-21 17:28:17,948 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 3.532
2024-06-21 17:28:18,267 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 3.245
2024-06-21 17:28:18,676 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 3.049
2024-06-21 17:28:18,978 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 3.435
2024-06-21 17:28:19,378 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 3.871
2024-06-21 17:28:19,694 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 3.728
2024-06-21 17:28:20,100 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 3.404
2024-06-21 17:28:20,390 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 2.563
2024-06-21 17:28:20,777 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 3.662
2024-06-21 17:28:21,092 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 3.516
2024-06-21 17:28:21,501 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 4.294
2024-06-21 17:28:21,802 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 3.416
2024-06-21 17:28:22,205 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 3.315
2024-06-21 17:28:22,515 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 3.096
2024-06-21 17:28:22,928 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 3.293
2024-06-21 17:28:23,229 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 2.817
2024-06-21 17:28:23,622 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 2.866
2024-06-21 17:28:23,931 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 3.221
2024-06-21 17:28:24,337 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 3.814
2024-06-21 17:28:24,639 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 3.516
2024-06-21 17:28:25,040 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 3.094
2024-06-21 17:28:25,357 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 3.358
2024-06-21 17:28:26,067 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 2.702
2024-06-21 17:28:26,366 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 3.403
2024-06-21 17:28:26,765 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 3.901
2024-06-21 17:28:27,081 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 3.057
2024-06-21 17:28:27,313 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 3.424
2024-06-21 17:28:38,207 - INFO: 21/200 final results:
2024-06-21 17:28:38,207 - INFO: Training loss: 3.401.
2024-06-21 17:28:38,207 - INFO: Training MAE: 3.401.
2024-06-21 17:28:38,207 - INFO: Training MSE: 22.134.
2024-06-21 17:28:58,357 - INFO: Epoch: 21/200, Loss_train: 3.4012781011647193, Loss_val: 4.699392294061595
2024-06-21 17:28:58,357 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:28:58,357 - INFO: Epoch 22/200...
2024-06-21 17:28:58,357 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:28:58,357 - INFO: Batch size: 32.
2024-06-21 17:28:58,361 - INFO: Dataset:
2024-06-21 17:28:58,361 - INFO: Batch size:
2024-06-21 17:28:58,361 - INFO: Number of workers:
2024-06-21 17:28:59,428 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 2.973
2024-06-21 17:28:59,745 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 4.035
2024-06-21 17:29:00,138 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 2.898
2024-06-21 17:29:00,454 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 2.712
2024-06-21 17:29:00,865 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 3.009
2024-06-21 17:29:01,179 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 3.596
2024-06-21 17:29:01,567 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 3.332
2024-06-21 17:29:01,880 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 3.619
2024-06-21 17:29:02,285 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 3.446
2024-06-21 17:29:02,590 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 3.795
2024-06-21 17:29:02,970 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 3.297
2024-06-21 17:29:03,288 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 3.494
2024-06-21 17:29:03,709 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 3.087
2024-06-21 17:29:04,026 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 2.782
2024-06-21 17:29:04,425 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 3.282
2024-06-21 17:29:04,739 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 2.899
2024-06-21 17:29:05,161 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 2.641
2024-06-21 17:29:05,476 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 2.681
2024-06-21 17:29:05,866 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 2.985
2024-06-21 17:29:06,174 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 3.347
2024-06-21 17:29:06,582 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 3.222
2024-06-21 17:29:06,898 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 2.924
2024-06-21 17:29:07,280 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 3.078
2024-06-21 17:29:07,596 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 2.747
2024-06-21 17:29:07,999 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 3.358
2024-06-21 17:29:08,307 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 3.636
2024-06-21 17:29:08,683 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 3.413
2024-06-21 17:29:08,992 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 2.987
2024-06-21 17:29:09,209 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 3.098
2024-06-21 17:29:20,364 - INFO: 22/200 final results:
2024-06-21 17:29:20,364 - INFO: Training loss: 3.185.
2024-06-21 17:29:20,364 - INFO: Training MAE: 3.187.
2024-06-21 17:29:20,364 - INFO: Training MSE: 19.203.
2024-06-21 17:29:41,042 - INFO: Epoch: 22/200, Loss_train: 3.1852957709082244, Loss_val: 4.295655645173172
2024-06-21 17:29:41,042 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:29:41,042 - INFO: Epoch 23/200...
2024-06-21 17:29:41,042 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:29:41,042 - INFO: Batch size: 32.
2024-06-21 17:29:41,046 - INFO: Dataset:
2024-06-21 17:29:41,046 - INFO: Batch size:
2024-06-21 17:29:41,046 - INFO: Number of workers:
2024-06-21 17:29:42,101 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 3.348
2024-06-21 17:29:42,431 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 3.328
2024-06-21 17:29:42,812 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 3.465
2024-06-21 17:29:43,127 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 3.167
2024-06-21 17:29:43,525 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 3.249
2024-06-21 17:29:43,835 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 3.250
2024-06-21 17:29:44,205 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 3.030
2024-06-21 17:29:44,516 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 3.890
2024-06-21 17:29:44,924 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 3.273
2024-06-21 17:29:45,228 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 2.588
2024-06-21 17:29:45,593 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 3.646
2024-06-21 17:29:45,907 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 2.898
2024-06-21 17:29:46,317 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 3.530
2024-06-21 17:29:46,631 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 2.642
2024-06-21 17:29:47,021 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 2.760
2024-06-21 17:29:47,331 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 3.416
2024-06-21 17:29:47,743 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 3.721
2024-06-21 17:29:48,054 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 3.177
2024-06-21 17:29:48,435 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 2.738
2024-06-21 17:29:48,740 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 3.016
2024-06-21 17:29:49,142 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 2.867
2024-06-21 17:29:49,454 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 2.799
2024-06-21 17:29:49,838 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 3.075
2024-06-21 17:29:50,151 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 3.552
2024-06-21 17:29:50,556 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 3.114
2024-06-21 17:29:50,865 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 2.550
2024-06-21 17:29:51,248 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 3.018
2024-06-21 17:29:51,556 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 3.210
2024-06-21 17:29:51,771 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 2.850
2024-06-21 17:30:02,857 - INFO: 23/200 final results:
2024-06-21 17:30:02,857 - INFO: Training loss: 3.144.
2024-06-21 17:30:02,857 - INFO: Training MAE: 3.150.
2024-06-21 17:30:02,857 - INFO: Training MSE: 18.492.
2024-06-21 17:30:22,954 - INFO: Epoch: 23/200, Loss_train: 3.1437391659309126, Loss_val: 4.340225622571748
2024-06-21 17:30:22,954 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:30:22,954 - INFO: Epoch 24/200...
2024-06-21 17:30:22,954 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:30:22,955 - INFO: Batch size: 32.
2024-06-21 17:30:22,958 - INFO: Dataset:
2024-06-21 17:30:22,958 - INFO: Batch size:
2024-06-21 17:30:22,958 - INFO: Number of workers:
2024-06-21 17:30:24,050 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 3.582
2024-06-21 17:30:24,354 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 3.339
2024-06-21 17:30:24,747 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 3.104
2024-06-21 17:30:25,063 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 2.653
2024-06-21 17:30:25,499 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 3.087
2024-06-21 17:30:25,799 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 2.850
2024-06-21 17:30:26,187 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 2.784
2024-06-21 17:30:26,487 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 3.485
2024-06-21 17:30:26,932 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 2.813
2024-06-21 17:30:27,222 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 4.199
2024-06-21 17:30:27,596 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 3.939
2024-06-21 17:30:27,896 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 2.988
2024-06-21 17:30:28,332 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 3.302
2024-06-21 17:30:28,633 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 3.106
2024-06-21 17:30:29,024 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 3.018
2024-06-21 17:30:29,320 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 3.118
2024-06-21 17:30:29,758 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 3.050
2024-06-21 17:30:30,057 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 3.194
2024-06-21 17:30:30,439 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 3.336
2024-06-21 17:30:30,732 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 3.275
2024-06-21 17:30:31,159 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 3.029
2024-06-21 17:30:31,460 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 3.285
2024-06-21 17:30:31,831 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 2.817
2024-06-21 17:30:32,131 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 2.525
2024-06-21 17:30:32,554 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 2.520
2024-06-21 17:30:32,850 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 2.847
2024-06-21 17:30:33,217 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 2.628
2024-06-21 17:30:33,513 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 2.836
2024-06-21 17:30:33,720 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 2.755
2024-06-21 17:30:44,508 - INFO: 24/200 final results:
2024-06-21 17:30:44,508 - INFO: Training loss: 3.085.
2024-06-21 17:30:44,508 - INFO: Training MAE: 3.092.
2024-06-21 17:30:44,509 - INFO: Training MSE: 17.776.
2024-06-21 17:31:05,497 - INFO: Epoch: 24/200, Loss_train: 3.085029363632202, Loss_val: 3.903879946675794
2024-06-21 17:31:05,497 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:31:05,497 - INFO: Epoch 25/200...
2024-06-21 17:31:05,497 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:31:05,497 - INFO: Batch size: 32.
2024-06-21 17:31:05,501 - INFO: Dataset:
2024-06-21 17:31:05,501 - INFO: Batch size:
2024-06-21 17:31:05,501 - INFO: Number of workers:
2024-06-21 17:31:06,589 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 3.861
2024-06-21 17:31:06,896 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 2.810
2024-06-21 17:31:07,292 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 3.177
2024-06-21 17:31:07,611 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 2.879
2024-06-21 17:31:08,039 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 2.740
2024-06-21 17:31:08,340 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 2.439
2024-06-21 17:31:08,731 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 2.952
2024-06-21 17:31:09,033 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 3.123
2024-06-21 17:31:09,458 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 3.371
2024-06-21 17:31:09,752 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 3.226
2024-06-21 17:31:10,131 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 3.059
2024-06-21 17:31:10,436 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 3.080
2024-06-21 17:31:10,870 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 3.024
2024-06-21 17:31:11,175 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 2.992
2024-06-21 17:31:11,584 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 2.823
2024-06-21 17:31:11,885 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 3.414
2024-06-21 17:31:12,310 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 2.800
2024-06-21 17:31:12,611 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 3.248
2024-06-21 17:31:13,011 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 2.808
2024-06-21 17:31:13,306 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 2.554
2024-06-21 17:31:13,725 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 3.100
2024-06-21 17:31:14,028 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 2.490
2024-06-21 17:31:14,430 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 2.434
2024-06-21 17:31:14,734 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 2.965
2024-06-21 17:31:15,147 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 3.049
2024-06-21 17:31:15,442 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 3.768
2024-06-21 17:31:15,833 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 2.401
2024-06-21 17:31:16,129 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 2.913
2024-06-21 17:31:16,349 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 3.047
2024-06-21 17:31:27,437 - INFO: 25/200 final results:
2024-06-21 17:31:27,437 - INFO: Training loss: 2.984.
2024-06-21 17:31:27,437 - INFO: Training MAE: 2.983.
2024-06-21 17:31:27,437 - INFO: Training MSE: 16.891.
2024-06-21 17:31:47,978 - INFO: Epoch: 25/200, Loss_train: 2.984373150200679, Loss_val: 3.246350173292489
2024-06-21 17:31:47,996 - INFO: Saved new best metric model for epoch 25.
2024-06-21 17:31:47,996 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:31:47,996 - INFO: Epoch 26/200...
2024-06-21 17:31:47,996 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:31:47,996 - INFO: Batch size: 32.
2024-06-21 17:31:48,000 - INFO: Dataset:
2024-06-21 17:31:48,000 - INFO: Batch size:
2024-06-21 17:31:48,000 - INFO: Number of workers:
2024-06-21 17:31:49,068 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 3.021
2024-06-21 17:31:49,385 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 3.129
2024-06-21 17:31:49,766 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 2.803
2024-06-21 17:31:50,079 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 3.591
2024-06-21 17:31:50,490 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 3.235
2024-06-21 17:31:50,787 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 2.142
2024-06-21 17:31:51,155 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 2.734
2024-06-21 17:31:51,465 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 3.096
2024-06-21 17:31:51,878 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 2.905
2024-06-21 17:31:52,169 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 2.293
2024-06-21 17:31:52,528 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 2.403
2024-06-21 17:31:52,842 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 2.988
2024-06-21 17:31:53,276 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 3.301
2024-06-21 17:31:53,581 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 3.399
2024-06-21 17:31:53,977 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 2.776
2024-06-21 17:31:54,292 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 3.054
2024-06-21 17:31:54,720 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 2.613
2024-06-21 17:31:55,022 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 3.080
2024-06-21 17:31:55,409 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 3.262
2024-06-21 17:31:55,717 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 3.430
2024-06-21 17:31:56,135 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 2.949
2024-06-21 17:31:56,438 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 3.069
2024-06-21 17:31:56,827 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 3.334
2024-06-21 17:31:57,143 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 2.651
2024-06-21 17:31:57,560 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 3.298
2024-06-21 17:31:57,858 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 2.507
2024-06-21 17:31:58,243 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 2.596
2024-06-21 17:31:58,554 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 3.103
2024-06-21 17:31:58,767 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 3.057
2024-06-21 17:32:09,556 - INFO: 26/200 final results:
2024-06-21 17:32:09,556 - INFO: Training loss: 2.959.
2024-06-21 17:32:09,557 - INFO: Training MAE: 2.957.
2024-06-21 17:32:09,557 - INFO: Training MSE: 16.389.
2024-06-21 17:32:29,714 - INFO: Epoch: 26/200, Loss_train: 2.959226665825679, Loss_val: 3.3434333307989714
2024-06-21 17:32:29,714 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:32:29,714 - INFO: Epoch 27/200...
2024-06-21 17:32:29,714 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:32:29,714 - INFO: Batch size: 32.
2024-06-21 17:32:29,718 - INFO: Dataset:
2024-06-21 17:32:29,718 - INFO: Batch size:
2024-06-21 17:32:29,718 - INFO: Number of workers:
2024-06-21 17:32:30,782 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 2.650
2024-06-21 17:32:31,100 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 3.571
2024-06-21 17:32:31,503 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 2.789
2024-06-21 17:32:31,820 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 3.068
2024-06-21 17:32:32,218 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 2.324
2024-06-21 17:32:32,529 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 2.854
2024-06-21 17:32:32,926 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 2.723
2024-06-21 17:32:33,239 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 2.700
2024-06-21 17:32:33,627 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 2.907
2024-06-21 17:32:33,931 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 2.844
2024-06-21 17:32:34,321 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 2.972
2024-06-21 17:32:34,635 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 2.880
2024-06-21 17:32:35,037 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 2.720
2024-06-21 17:32:35,351 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 2.753
2024-06-21 17:32:35,756 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 2.766
2024-06-21 17:32:36,066 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 2.591
2024-06-21 17:32:36,467 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 3.645
2024-06-21 17:32:36,779 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 2.808
2024-06-21 17:32:37,176 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 3.186
2024-06-21 17:32:37,482 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 2.846
2024-06-21 17:32:37,877 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 2.739
2024-06-21 17:32:38,190 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 2.758
2024-06-21 17:32:38,587 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 2.707
2024-06-21 17:32:38,900 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 2.726
2024-06-21 17:32:39,292 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 3.569
2024-06-21 17:32:39,600 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 2.880
2024-06-21 17:32:39,981 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 2.552
2024-06-21 17:32:40,289 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 2.349
2024-06-21 17:32:40,506 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 2.678
2024-06-21 17:32:51,701 - INFO: 27/200 final results:
2024-06-21 17:32:51,701 - INFO: Training loss: 2.847.
2024-06-21 17:32:51,701 - INFO: Training MAE: 2.850.
2024-06-21 17:32:51,701 - INFO: Training MSE: 14.952.
2024-06-21 17:33:12,138 - INFO: Epoch: 27/200, Loss_train: 2.84676587170568, Loss_val: 3.866774090405168
2024-06-21 17:33:12,138 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:33:12,138 - INFO: Epoch 28/200...
2024-06-21 17:33:12,138 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:33:12,138 - INFO: Batch size: 32.
2024-06-21 17:33:12,141 - INFO: Dataset:
2024-06-21 17:33:12,142 - INFO: Batch size:
2024-06-21 17:33:12,142 - INFO: Number of workers:
2024-06-21 17:33:13,230 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 2.629
2024-06-21 17:33:13,537 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 2.840
2024-06-21 17:33:13,926 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 2.465
2024-06-21 17:33:14,245 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 2.659
2024-06-21 17:33:14,678 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 2.528
2024-06-21 17:33:14,980 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 2.926
2024-06-21 17:33:15,360 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 2.852
2024-06-21 17:33:15,662 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 2.587
2024-06-21 17:33:16,102 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 2.724
2024-06-21 17:33:16,396 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 2.680
2024-06-21 17:33:16,771 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 2.598
2024-06-21 17:33:17,076 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 3.462
2024-06-21 17:33:17,520 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 2.651
2024-06-21 17:33:17,825 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 3.239
2024-06-21 17:33:18,219 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 2.578
2024-06-21 17:33:18,520 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 2.638
2024-06-21 17:33:18,968 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 2.900
2024-06-21 17:33:19,269 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 3.593
2024-06-21 17:33:19,654 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 2.547
2024-06-21 17:33:19,950 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 2.905
2024-06-21 17:33:20,386 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 3.030
2024-06-21 17:33:20,690 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 2.436
2024-06-21 17:33:21,065 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 2.432
2024-06-21 17:33:21,368 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 2.897
2024-06-21 17:33:21,790 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 2.772
2024-06-21 17:33:22,086 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 2.432
2024-06-21 17:33:22,466 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 2.886
2024-06-21 17:33:22,761 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 2.416
2024-06-21 17:33:22,971 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 2.523
2024-06-21 17:33:34,049 - INFO: 28/200 final results:
2024-06-21 17:33:34,050 - INFO: Training loss: 2.753.
2024-06-21 17:33:34,050 - INFO: Training MAE: 2.757.
2024-06-21 17:33:34,050 - INFO: Training MSE: 14.201.
2024-06-21 17:33:54,125 - INFO: Epoch: 28/200, Loss_train: 2.752582212974285, Loss_val: 3.7580305707865747
2024-06-21 17:33:54,125 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:33:54,125 - INFO: Epoch 29/200...
2024-06-21 17:33:54,125 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:33:54,126 - INFO: Batch size: 32.
2024-06-21 17:33:54,129 - INFO: Dataset:
2024-06-21 17:33:54,129 - INFO: Batch size:
2024-06-21 17:33:54,129 - INFO: Number of workers:
2024-06-21 17:33:55,221 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 2.326
2024-06-21 17:33:55,541 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 2.828
2024-06-21 17:33:55,949 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 2.912
2024-06-21 17:33:56,266 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 2.381
2024-06-21 17:33:56,669 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 2.105
2024-06-21 17:33:56,982 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 3.179
2024-06-21 17:33:57,384 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 3.009
2024-06-21 17:33:57,698 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 2.778
2024-06-21 17:33:58,087 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 2.798
2024-06-21 17:33:58,393 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 2.538
2024-06-21 17:33:58,782 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 3.075
2024-06-21 17:33:59,097 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 2.531
2024-06-21 17:33:59,504 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 3.338
2024-06-21 17:33:59,819 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 3.016
2024-06-21 17:34:00,224 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 3.013
2024-06-21 17:34:00,535 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 3.124
2024-06-21 17:34:00,940 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 2.579
2024-06-21 17:34:01,254 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 2.502
2024-06-21 17:34:01,646 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 2.418
2024-06-21 17:34:01,954 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 2.955
2024-06-21 17:34:02,337 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 2.713
2024-06-21 17:34:02,653 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 2.735
2024-06-21 17:34:03,046 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 2.914
2024-06-21 17:34:03,361 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 2.617
2024-06-21 17:34:03,754 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 2.155
2024-06-21 17:34:04,065 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 2.480
2024-06-21 17:34:04,463 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 2.851
2024-06-21 17:34:04,775 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 2.792
2024-06-21 17:34:04,992 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 2.722
2024-06-21 17:34:16,055 - INFO: 29/200 final results:
2024-06-21 17:34:16,055 - INFO: Training loss: 2.737.
2024-06-21 17:34:16,055 - INFO: Training MAE: 2.738.
2024-06-21 17:34:16,055 - INFO: Training MSE: 13.776.
2024-06-21 17:34:36,485 - INFO: Epoch: 29/200, Loss_train: 2.737426149434057, Loss_val: 3.1881279205453805
2024-06-21 17:34:36,505 - INFO: Saved new best metric model for epoch 29.
2024-06-21 17:34:36,505 - INFO: Best internal validation val_loss: 3.188 at epoch: 29.
2024-06-21 17:34:36,505 - INFO: Epoch 30/200...
2024-06-21 17:34:36,505 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:34:36,505 - INFO: Batch size: 32.
2024-06-21 17:34:36,508 - INFO: Dataset:
2024-06-21 17:34:36,509 - INFO: Batch size:
2024-06-21 17:34:36,509 - INFO: Number of workers:
2024-06-21 17:34:37,589 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 2.390
2024-06-21 17:34:37,894 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 2.603
2024-06-21 17:34:38,312 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 3.119
2024-06-21 17:34:38,616 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 3.225
2024-06-21 17:34:39,016 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 2.376
2024-06-21 17:34:39,328 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 2.868
2024-06-21 17:34:39,727 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 1.854
2024-06-21 17:34:40,030 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 2.889
2024-06-21 17:34:40,422 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 2.704
2024-06-21 17:34:40,729 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 2.466
2024-06-21 17:34:41,134 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 2.632
2024-06-21 17:34:41,438 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 3.072
2024-06-21 17:34:41,834 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 2.587
2024-06-21 17:34:42,151 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 2.523
2024-06-21 17:34:42,574 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 2.650
2024-06-21 17:34:42,874 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 2.407
2024-06-21 17:34:43,255 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 3.216
2024-06-21 17:34:43,568 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 3.215
2024-06-21 17:34:43,982 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 3.006
2024-06-21 17:34:44,274 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 2.346
2024-06-21 17:34:44,639 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 2.592
2024-06-21 17:34:44,952 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 2.909
2024-06-21 17:34:45,367 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 2.501
2024-06-21 17:34:45,667 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 3.536
2024-06-21 17:34:46,037 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 2.379
2024-06-21 17:34:46,345 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 2.775
2024-06-21 17:34:46,745 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 3.326
2024-06-21 17:34:47,042 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 2.287
2024-06-21 17:34:47,250 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 2.586
2024-06-21 17:34:58,397 - INFO: 30/200 final results:
2024-06-21 17:34:58,397 - INFO: Training loss: 2.726.
2024-06-21 17:34:58,397 - INFO: Training MAE: 2.728.
2024-06-21 17:34:58,397 - INFO: Training MSE: 13.490.
2024-06-21 17:35:18,593 - INFO: Epoch: 30/200, Loss_train: 2.725536457423506, Loss_val: 2.8623216316617768
2024-06-21 17:35:18,611 - INFO: Saved new best metric model for epoch 30.
2024-06-21 17:35:18,611 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:35:18,611 - INFO: Epoch 31/200...
2024-06-21 17:35:18,611 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:35:18,612 - INFO: Batch size: 32.
2024-06-21 17:35:18,616 - INFO: Dataset:
2024-06-21 17:35:18,616 - INFO: Batch size:
2024-06-21 17:35:18,616 - INFO: Number of workers:
2024-06-21 17:35:19,692 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 2.216
2024-06-21 17:35:19,998 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 2.590
2024-06-21 17:35:20,390 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 2.580
2024-06-21 17:35:20,707 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 2.624
2024-06-21 17:35:21,128 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 2.375
2024-06-21 17:35:21,428 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 2.866
2024-06-21 17:35:21,813 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 2.569
2024-06-21 17:35:22,125 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 2.620
2024-06-21 17:35:22,548 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 2.883
2024-06-21 17:35:22,839 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 2.339
2024-06-21 17:35:23,211 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 2.697
2024-06-21 17:35:23,527 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 2.279
2024-06-21 17:35:23,955 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 2.686
2024-06-21 17:35:24,257 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 2.743
2024-06-21 17:35:24,649 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 2.443
2024-06-21 17:35:24,960 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 2.101
2024-06-21 17:35:25,388 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 2.468
2024-06-21 17:35:25,687 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 2.860
2024-06-21 17:35:26,071 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 2.304
2024-06-21 17:35:26,377 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 2.337
2024-06-21 17:35:26,795 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 2.359
2024-06-21 17:35:27,096 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 3.000
2024-06-21 17:35:27,470 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 2.248
2024-06-21 17:35:27,784 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 2.662
2024-06-21 17:35:28,196 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 2.806
2024-06-21 17:35:28,492 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 2.249
2024-06-21 17:35:28,860 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 2.011
2024-06-21 17:35:29,168 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 2.702
2024-06-21 17:35:29,379 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 2.253
2024-06-21 17:35:40,428 - INFO: 31/200 final results:
2024-06-21 17:35:40,428 - INFO: Training loss: 2.513.
2024-06-21 17:35:40,428 - INFO: Training MAE: 2.518.
2024-06-21 17:35:40,428 - INFO: Training MSE: 11.691.
2024-06-21 17:36:00,541 - INFO: Epoch: 31/200, Loss_train: 2.5126695797361176, Loss_val: 3.4359739484458136
2024-06-21 17:36:00,542 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:36:00,542 - INFO: Epoch 32/200...
2024-06-21 17:36:00,542 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:36:00,542 - INFO: Batch size: 32.
2024-06-21 17:36:00,545 - INFO: Dataset:
2024-06-21 17:36:00,545 - INFO: Batch size:
2024-06-21 17:36:00,545 - INFO: Number of workers:
2024-06-21 17:36:01,624 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 2.792
2024-06-21 17:36:01,944 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 2.251
2024-06-21 17:36:02,340 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 2.563
2024-06-21 17:36:02,657 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 3.189
2024-06-21 17:36:03,068 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 2.677
2024-06-21 17:36:03,380 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 2.196
2024-06-21 17:36:03,764 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 2.170
2024-06-21 17:36:04,077 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 2.455
2024-06-21 17:36:04,490 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 2.546
2024-06-21 17:36:04,796 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 2.636
2024-06-21 17:36:05,170 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 2.468
2024-06-21 17:36:05,488 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 2.648
2024-06-21 17:36:05,906 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 2.619
2024-06-21 17:36:06,222 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 2.932
2024-06-21 17:36:06,615 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 3.375
2024-06-21 17:36:06,928 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 2.395
2024-06-21 17:36:07,329 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 2.172
2024-06-21 17:36:07,642 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 2.322
2024-06-21 17:36:08,024 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 2.294
2024-06-21 17:36:08,332 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 2.391
2024-06-21 17:36:08,739 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 2.396
2024-06-21 17:36:09,054 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 2.495
2024-06-21 17:36:09,441 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 2.544
2024-06-21 17:36:09,757 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 3.025
2024-06-21 17:36:10,163 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 2.793
2024-06-21 17:36:10,474 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 3.473
2024-06-21 17:36:10,851 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 2.004
2024-06-21 17:36:11,163 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 2.612
2024-06-21 17:36:11,383 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 3.184
2024-06-21 17:36:22,566 - INFO: 32/200 final results:
2024-06-21 17:36:22,566 - INFO: Training loss: 2.608.
2024-06-21 17:36:22,566 - INFO: Training MAE: 2.596.
2024-06-21 17:36:22,566 - INFO: Training MSE: 12.594.
2024-06-21 17:36:42,878 - INFO: Epoch: 32/200, Loss_train: 2.607512424732077, Loss_val: 5.320437924615268
2024-06-21 17:36:42,878 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:36:42,878 - INFO: Epoch 33/200...
2024-06-21 17:36:42,878 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:36:42,878 - INFO: Batch size: 32.
2024-06-21 17:36:42,882 - INFO: Dataset:
2024-06-21 17:36:42,882 - INFO: Batch size:
2024-06-21 17:36:42,882 - INFO: Number of workers:
2024-06-21 17:36:43,934 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 3.419
2024-06-21 17:36:44,264 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 2.639
2024-06-21 17:36:44,656 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 2.592
2024-06-21 17:36:44,972 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 2.239
2024-06-21 17:36:45,366 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 2.300
2024-06-21 17:36:45,691 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 2.238
2024-06-21 17:36:46,078 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 2.058
2024-06-21 17:36:46,393 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 2.539
2024-06-21 17:36:46,781 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 3.509
2024-06-21 17:36:47,110 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 2.001
2024-06-21 17:36:47,486 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 2.717
2024-06-21 17:36:47,803 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 2.268
2024-06-21 17:36:48,215 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 2.945
2024-06-21 17:36:48,543 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 2.352
2024-06-21 17:36:48,933 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 2.723
2024-06-21 17:36:49,244 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 2.311
2024-06-21 17:36:49,647 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 2.228
2024-06-21 17:36:49,970 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 3.098
2024-06-21 17:36:50,344 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 2.941
2024-06-21 17:36:50,648 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 3.032
2024-06-21 17:36:51,044 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 2.248
2024-06-21 17:36:51,368 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 2.550
2024-06-21 17:36:51,751 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 1.998
2024-06-21 17:36:52,063 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 2.572
2024-06-21 17:36:52,457 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 2.078
2024-06-21 17:36:52,777 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 2.563
2024-06-21 17:36:53,150 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 2.265
2024-06-21 17:36:53,458 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 2.180
2024-06-21 17:36:53,671 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 2.845
2024-06-21 17:37:04,496 - INFO: 33/200 final results:
2024-06-21 17:37:04,496 - INFO: Training loss: 2.533.
2024-06-21 17:37:04,496 - INFO: Training MAE: 2.526.
2024-06-21 17:37:04,496 - INFO: Training MSE: 11.969.
2024-06-21 17:37:24,972 - INFO: Epoch: 33/200, Loss_train: 2.532671574888558, Loss_val: 3.118515771010826
2024-06-21 17:37:24,972 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:37:24,972 - INFO: Epoch 34/200...
2024-06-21 17:37:24,972 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:37:24,972 - INFO: Batch size: 32.
2024-06-21 17:37:24,976 - INFO: Dataset:
2024-06-21 17:37:24,976 - INFO: Batch size:
2024-06-21 17:37:24,976 - INFO: Number of workers:
2024-06-21 17:37:26,031 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 2.344
2024-06-21 17:37:26,362 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 2.254
2024-06-21 17:37:26,746 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 2.420
2024-06-21 17:37:27,063 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 2.342
2024-06-21 17:37:27,454 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 2.483
2024-06-21 17:37:27,777 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 2.955
2024-06-21 17:37:28,180 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 2.129
2024-06-21 17:37:28,504 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 2.509
2024-06-21 17:37:28,912 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 2.168
2024-06-21 17:37:29,248 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 2.260
2024-06-21 17:37:29,626 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 2.529
2024-06-21 17:37:29,944 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 2.047
2024-06-21 17:37:30,353 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 2.619
2024-06-21 17:37:30,683 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 2.439
2024-06-21 17:37:31,079 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 3.850
2024-06-21 17:37:31,392 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 2.781
2024-06-21 17:37:31,797 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 2.376
2024-06-21 17:37:32,121 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 2.263
2024-06-21 17:37:32,508 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 2.318
2024-06-21 17:37:32,815 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 3.183
2024-06-21 17:37:33,212 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 2.677
2024-06-21 17:37:33,539 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 1.980
2024-06-21 17:37:33,925 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 2.090
2024-06-21 17:37:34,239 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 2.718
2024-06-21 17:37:34,635 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 2.388
2024-06-21 17:37:34,956 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 2.484
2024-06-21 17:37:35,339 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 2.542
2024-06-21 17:37:35,648 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 1.893
2024-06-21 17:37:35,869 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 2.738
2024-06-21 17:37:46,954 - INFO: 34/200 final results:
2024-06-21 17:37:46,955 - INFO: Training loss: 2.475.
2024-06-21 17:37:46,955 - INFO: Training MAE: 2.470.
2024-06-21 17:37:46,955 - INFO: Training MSE: 11.235.
2024-06-21 17:38:07,570 - INFO: Epoch: 34/200, Loss_train: 2.4751795571425865, Loss_val: 3.3945121354070205
2024-06-21 17:38:07,570 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:38:07,570 - INFO: Epoch 35/200...
2024-06-21 17:38:07,570 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:38:07,570 - INFO: Batch size: 32.
2024-06-21 17:38:07,573 - INFO: Dataset:
2024-06-21 17:38:07,574 - INFO: Batch size:
2024-06-21 17:38:07,574 - INFO: Number of workers:
2024-06-21 17:38:08,670 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 2.707
2024-06-21 17:38:08,977 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 2.534
2024-06-21 17:38:09,371 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 2.694
2024-06-21 17:38:09,691 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 2.217
2024-06-21 17:38:10,121 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 2.774
2024-06-21 17:38:10,423 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 2.111
2024-06-21 17:38:10,812 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 2.646
2024-06-21 17:38:11,128 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 3.064
2024-06-21 17:38:11,556 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 2.816
2024-06-21 17:38:11,849 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 2.301
2024-06-21 17:38:12,227 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 2.420
2024-06-21 17:38:12,545 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 2.449
2024-06-21 17:38:12,980 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 1.960
2024-06-21 17:38:13,284 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 2.371
2024-06-21 17:38:13,680 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 2.218
2024-06-21 17:38:13,994 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 2.132
2024-06-21 17:38:14,426 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 2.493
2024-06-21 17:38:14,727 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 2.260
2024-06-21 17:38:15,116 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 2.235
2024-06-21 17:38:15,424 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 2.992
2024-06-21 17:38:15,846 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 2.876
2024-06-21 17:38:16,149 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 2.091
2024-06-21 17:38:16,530 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 2.389
2024-06-21 17:38:16,847 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 2.465
2024-06-21 17:38:17,264 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 2.383
2024-06-21 17:38:17,562 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 3.014
2024-06-21 17:38:17,948 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 2.332
2024-06-21 17:38:18,260 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 2.160
2024-06-21 17:38:18,475 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 2.668
2024-06-21 17:38:29,411 - INFO: 35/200 final results:
2024-06-21 17:38:29,411 - INFO: Training loss: 2.475.
2024-06-21 17:38:29,411 - INFO: Training MAE: 2.471.
2024-06-21 17:38:29,411 - INFO: Training MSE: 11.136.
2024-06-21 17:38:49,702 - INFO: Epoch: 35/200, Loss_train: 2.4749287325760414, Loss_val: 3.485599838454148
2024-06-21 17:38:49,702 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:38:49,702 - INFO: Epoch 36/200...
2024-06-21 17:38:49,702 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:38:49,702 - INFO: Batch size: 32.
2024-06-21 17:38:49,706 - INFO: Dataset:
2024-06-21 17:38:49,706 - INFO: Batch size:
2024-06-21 17:38:49,706 - INFO: Number of workers:
2024-06-21 17:38:50,796 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 2.392
2024-06-21 17:38:51,100 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 3.001
2024-06-21 17:38:51,492 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 3.081
2024-06-21 17:38:51,808 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 2.860
2024-06-21 17:38:52,215 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 2.185
2024-06-21 17:38:52,524 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 2.059
2024-06-21 17:38:52,908 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 2.229
2024-06-21 17:38:53,220 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 2.228
2024-06-21 17:38:53,627 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 2.200
2024-06-21 17:38:53,930 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 2.090
2024-06-21 17:38:54,309 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 2.452
2024-06-21 17:38:54,626 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 2.024
2024-06-21 17:38:55,045 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 2.166
2024-06-21 17:38:55,363 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 2.472
2024-06-21 17:38:55,762 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 2.162
2024-06-21 17:38:56,075 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 2.135
2024-06-21 17:38:56,494 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 2.282
2024-06-21 17:38:56,808 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 2.467
2024-06-21 17:38:57,192 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 2.518
2024-06-21 17:38:57,501 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 2.832
2024-06-21 17:38:57,910 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 1.918
2024-06-21 17:38:58,226 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 2.533
2024-06-21 17:38:58,613 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 2.288
2024-06-21 17:38:58,929 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 2.588
2024-06-21 17:38:59,334 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 2.042
2024-06-21 17:38:59,646 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 2.666
2024-06-21 17:39:00,016 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 2.154
2024-06-21 17:39:00,326 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 2.403
2024-06-21 17:39:00,540 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 2.146
2024-06-21 17:39:11,650 - INFO: 36/200 final results:
2024-06-21 17:39:11,650 - INFO: Training loss: 2.365.
2024-06-21 17:39:11,650 - INFO: Training MAE: 2.369.
2024-06-21 17:39:11,650 - INFO: Training MSE: 10.222.
2024-06-21 17:39:31,789 - INFO: Epoch: 36/200, Loss_train: 2.364553657071344, Loss_val: 3.536565090047902
2024-06-21 17:39:31,789 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:39:31,789 - INFO: Epoch 37/200...
2024-06-21 17:39:31,789 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:39:31,789 - INFO: Batch size: 32.
2024-06-21 17:39:31,793 - INFO: Dataset:
2024-06-21 17:39:31,793 - INFO: Batch size:
2024-06-21 17:39:31,793 - INFO: Number of workers:
2024-06-21 17:39:32,842 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 2.701
2024-06-21 17:39:33,160 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 2.838
2024-06-21 17:39:33,559 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 2.288
2024-06-21 17:39:33,874 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 1.980
2024-06-21 17:39:34,259 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 2.378
2024-06-21 17:39:34,571 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 2.399
2024-06-21 17:39:34,967 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 2.142
2024-06-21 17:39:35,281 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 2.033
2024-06-21 17:39:35,663 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 2.939
2024-06-21 17:39:35,968 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 2.356
2024-06-21 17:39:36,347 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 2.829
2024-06-21 17:39:36,662 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 2.344
2024-06-21 17:39:37,061 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 2.143
2024-06-21 17:39:37,374 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 2.483
2024-06-21 17:39:37,774 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 2.372
2024-06-21 17:39:38,085 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 2.071
2024-06-21 17:39:38,486 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 2.235
2024-06-21 17:39:38,796 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 2.267
2024-06-21 17:39:39,186 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 2.585
2024-06-21 17:39:39,492 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 2.069
2024-06-21 17:39:39,878 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 1.962
2024-06-21 17:39:40,191 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 2.497
2024-06-21 17:39:40,585 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 2.721
2024-06-21 17:39:40,897 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 2.747
2024-06-21 17:39:41,280 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 2.618
2024-06-21 17:39:41,588 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 2.308
2024-06-21 17:39:41,976 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 2.105
2024-06-21 17:39:42,285 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 2.784
2024-06-21 17:39:42,503 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 2.172
2024-06-21 17:39:53,545 - INFO: 37/200 final results:
2024-06-21 17:39:53,545 - INFO: Training loss: 2.392.
2024-06-21 17:39:53,545 - INFO: Training MAE: 2.396.
2024-06-21 17:39:53,545 - INFO: Training MSE: 10.612.
2024-06-21 17:40:13,834 - INFO: Epoch: 37/200, Loss_train: 2.391994361219735, Loss_val: 3.3170373933068635
2024-06-21 17:40:13,834 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:40:13,834 - INFO: Epoch 38/200...
2024-06-21 17:40:13,834 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:40:13,834 - INFO: Batch size: 32.
2024-06-21 17:40:13,838 - INFO: Dataset:
2024-06-21 17:40:13,838 - INFO: Batch size:
2024-06-21 17:40:13,838 - INFO: Number of workers:
2024-06-21 17:40:14,902 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 2.676
2024-06-21 17:40:15,207 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 2.019
2024-06-21 17:40:15,616 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 3.045
2024-06-21 17:40:15,934 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 2.163
2024-06-21 17:40:16,356 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 1.999
2024-06-21 17:40:16,656 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 2.351
2024-06-21 17:40:17,039 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 2.412
2024-06-21 17:40:17,352 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 2.287
2024-06-21 17:40:17,828 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 1.915
2024-06-21 17:40:18,121 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 1.804
2024-06-21 17:40:18,498 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 2.009
2024-06-21 17:40:18,814 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 2.024
2024-06-21 17:40:19,241 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 1.691
2024-06-21 17:40:19,543 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 1.751
2024-06-21 17:40:19,935 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 1.863
2024-06-21 17:40:20,250 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 1.756
2024-06-21 17:40:20,679 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 1.448
2024-06-21 17:40:20,979 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 1.998
2024-06-21 17:40:21,363 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 1.703
2024-06-21 17:40:21,672 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 1.924
2024-06-21 17:40:22,090 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 1.833
2024-06-21 17:40:22,395 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 1.892
2024-06-21 17:40:22,782 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 1.795
2024-06-21 17:40:23,098 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 1.439
2024-06-21 17:40:23,514 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 1.932
2024-06-21 17:40:23,810 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 1.897
2024-06-21 17:40:24,186 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 2.012
2024-06-21 17:40:24,494 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 1.664
2024-06-21 17:40:24,707 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 1.676
2024-06-21 17:40:35,816 - INFO: 38/200 final results:
2024-06-21 17:40:35,816 - INFO: Training loss: 1.965.
2024-06-21 17:40:35,816 - INFO: Training MAE: 1.970.
2024-06-21 17:40:35,816 - INFO: Training MSE: 7.507.
2024-06-21 17:40:56,455 - INFO: Epoch: 38/200, Loss_train: 1.9647427756210853, Loss_val: 2.7288301895404685
2024-06-21 17:40:56,475 - INFO: Saved new best metric model for epoch 38.
2024-06-21 17:40:56,475 - INFO: Best internal validation val_loss: 2.729 at epoch: 38.
2024-06-21 17:40:56,475 - INFO: Epoch 39/200...
2024-06-21 17:40:56,475 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:40:56,475 - INFO: Batch size: 32.
2024-06-21 17:40:56,478 - INFO: Dataset:
2024-06-21 17:40:56,479 - INFO: Batch size:
2024-06-21 17:40:56,479 - INFO: Number of workers:
2024-06-21 17:40:57,546 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 1.935
2024-06-21 17:40:57,854 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 1.829
2024-06-21 17:40:58,248 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 2.422
2024-06-21 17:40:58,568 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 1.738
2024-06-21 17:40:59,007 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 2.353
2024-06-21 17:40:59,310 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 2.224
2024-06-21 17:40:59,694 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 2.227
2024-06-21 17:40:59,998 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 1.786
2024-06-21 17:41:00,444 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 1.541
2024-06-21 17:41:00,738 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 1.730
2024-06-21 17:41:01,114 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 1.969
2024-06-21 17:41:01,419 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 1.743
2024-06-21 17:41:01,861 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 1.740
2024-06-21 17:41:02,165 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 1.633
2024-06-21 17:41:02,557 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 1.765
2024-06-21 17:41:02,857 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 1.892
2024-06-21 17:41:03,293 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 1.917
2024-06-21 17:41:03,594 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 1.983
2024-06-21 17:41:03,980 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 1.883
2024-06-21 17:41:04,274 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 1.754
2024-06-21 17:41:04,707 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 2.039
2024-06-21 17:41:05,010 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 2.048
2024-06-21 17:41:05,396 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 2.152
2024-06-21 17:41:05,699 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 1.546
2024-06-21 17:41:06,129 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 1.995
2024-06-21 17:41:06,427 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 1.631
2024-06-21 17:41:06,812 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 1.778
2024-06-21 17:41:07,110 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 2.273
2024-06-21 17:41:07,332 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 2.180
2024-06-21 17:41:18,458 - INFO: 39/200 final results:
2024-06-21 17:41:18,458 - INFO: Training loss: 1.921.
2024-06-21 17:41:18,458 - INFO: Training MAE: 1.916.
2024-06-21 17:41:18,459 - INFO: Training MSE: 6.822.
2024-06-21 17:41:38,728 - INFO: Epoch: 39/200, Loss_train: 1.92094113908965, Loss_val: 2.713525155494953
2024-06-21 17:41:38,747 - INFO: Saved new best metric model for epoch 39.
2024-06-21 17:41:38,747 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:41:38,747 - INFO: Epoch 40/200...
2024-06-21 17:41:38,747 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:41:38,747 - INFO: Batch size: 32.
2024-06-21 17:41:38,751 - INFO: Dataset:
2024-06-21 17:41:38,751 - INFO: Batch size:
2024-06-21 17:41:38,751 - INFO: Number of workers:
2024-06-21 17:41:39,825 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 1.627
2024-06-21 17:41:40,131 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 1.876
2024-06-21 17:41:40,518 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 1.690
2024-06-21 17:41:40,835 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 1.837
2024-06-21 17:41:41,250 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 1.780
2024-06-21 17:41:41,565 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 1.623
2024-06-21 17:41:41,955 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 1.525
2024-06-21 17:41:42,270 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 1.662
2024-06-21 17:41:42,692 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 1.725
2024-06-21 17:41:43,007 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 1.843
2024-06-21 17:41:43,399 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 1.775
2024-06-21 17:41:43,720 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 2.198
2024-06-21 17:41:44,130 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 1.961
2024-06-21 17:41:44,447 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 1.735
2024-06-21 17:41:44,835 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 1.646
2024-06-21 17:41:45,150 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 2.334
2024-06-21 17:41:45,563 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 1.581
2024-06-21 17:41:45,877 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 1.548
2024-06-21 17:41:46,260 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 1.736
2024-06-21 17:41:46,569 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 2.189
2024-06-21 17:41:46,976 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 1.665
2024-06-21 17:41:47,292 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 1.509
2024-06-21 17:41:47,666 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 1.810
2024-06-21 17:41:47,982 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 1.675
2024-06-21 17:41:48,380 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 1.848
2024-06-21 17:41:48,691 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 1.496
2024-06-21 17:41:49,059 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 2.324
2024-06-21 17:41:49,370 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 1.706
2024-06-21 17:41:49,580 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 1.705
2024-06-21 17:42:00,556 - INFO: 40/200 final results:
2024-06-21 17:42:00,556 - INFO: Training loss: 1.780.
2024-06-21 17:42:00,556 - INFO: Training MAE: 1.782.
2024-06-21 17:42:00,556 - INFO: Training MSE: 6.020.
2024-06-21 17:42:21,072 - INFO: Epoch: 40/200, Loss_train: 1.7803601889774716, Loss_val: 3.1620103572977
2024-06-21 17:42:21,072 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:42:21,072 - INFO: Epoch 41/200...
2024-06-21 17:42:21,073 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:42:21,073 - INFO: Batch size: 32.
2024-06-21 17:42:21,076 - INFO: Dataset:
2024-06-21 17:42:21,076 - INFO: Batch size:
2024-06-21 17:42:21,076 - INFO: Number of workers:
2024-06-21 17:42:22,121 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 2.347
2024-06-21 17:42:22,462 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 1.819
2024-06-21 17:42:22,856 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 1.991
2024-06-21 17:42:23,171 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 1.719
2024-06-21 17:42:23,571 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 1.754
2024-06-21 17:42:23,908 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 1.956
2024-06-21 17:42:24,291 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 1.415
2024-06-21 17:42:24,590 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 1.746
2024-06-21 17:42:24,980 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 1.941
2024-06-21 17:42:25,320 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 1.992
2024-06-21 17:42:25,695 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 1.889
2024-06-21 17:42:25,997 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 1.828
2024-06-21 17:42:26,400 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 1.875
2024-06-21 17:42:26,750 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 1.751
2024-06-21 17:42:27,140 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 1.879
2024-06-21 17:42:27,438 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 2.002
2024-06-21 17:42:27,821 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 2.021
2024-06-21 17:42:28,167 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 2.069
2024-06-21 17:42:28,545 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 1.918
2024-06-21 17:42:28,837 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 1.846
2024-06-21 17:42:29,216 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 2.160
2024-06-21 17:42:29,564 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 1.657
2024-06-21 17:42:29,942 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 1.624
2024-06-21 17:42:30,242 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 2.131
2024-06-21 17:42:30,613 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 1.630
2024-06-21 17:42:30,950 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 1.624
2024-06-21 17:42:31,315 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 1.603
2024-06-21 17:42:31,610 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 1.723
2024-06-21 17:42:31,810 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 2.424
2024-06-21 17:42:42,635 - INFO: 41/200 final results:
2024-06-21 17:42:42,636 - INFO: Training loss: 1.874.
2024-06-21 17:42:42,636 - INFO: Training MAE: 1.863.
2024-06-21 17:42:42,636 - INFO: Training MSE: 6.462.
2024-06-21 17:43:03,060 - INFO: Epoch: 41/200, Loss_train: 1.8735965367021232, Loss_val: 2.9025889758406014
2024-06-21 17:43:03,060 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:43:03,060 - INFO: Epoch 42/200...
2024-06-21 17:43:03,060 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:43:03,060 - INFO: Batch size: 32.
2024-06-21 17:43:03,063 - INFO: Dataset:
2024-06-21 17:43:03,064 - INFO: Batch size:
2024-06-21 17:43:03,064 - INFO: Number of workers:
2024-06-21 17:43:04,112 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 1.645
2024-06-21 17:43:04,430 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 2.213
2024-06-21 17:43:04,845 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 2.029
2024-06-21 17:43:05,148 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 1.460
2024-06-21 17:43:05,560 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 2.190
2024-06-21 17:43:05,859 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 1.740
2024-06-21 17:43:06,268 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 1.885
2024-06-21 17:43:06,568 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 1.890
2024-06-21 17:43:06,972 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 1.378
2024-06-21 17:43:07,263 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 1.588
2024-06-21 17:43:07,682 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 1.656
2024-06-21 17:43:07,984 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 1.690
2024-06-21 17:43:08,399 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 1.488
2024-06-21 17:43:08,701 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 1.462
2024-06-21 17:43:09,119 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 1.761
2024-06-21 17:43:09,417 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 1.842
2024-06-21 17:43:09,811 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 1.753
2024-06-21 17:43:10,109 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 1.587
2024-06-21 17:43:10,521 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 1.487
2024-06-21 17:43:10,813 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 1.624
2024-06-21 17:43:11,204 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 1.651
2024-06-21 17:43:11,505 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 1.757
2024-06-21 17:43:11,916 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 1.622
2024-06-21 17:43:12,215 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 1.640
2024-06-21 17:43:12,602 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 1.732
2024-06-21 17:43:12,897 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:13,302 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 1.595
2024-06-21 17:43:13,596 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 1.792
2024-06-21 17:43:13,802 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 1.766
2024-06-21 17:43:24,799 - INFO: 42/200 final results:
2024-06-21 17:43:24,800 - INFO: Training loss: 1.717.
2024-06-21 17:43:24,800 - INFO: Training MAE: 1.716.
2024-06-21 17:43:24,800 - INFO: Training MSE: 5.525.
2024-06-21 17:43:45,131 - INFO: Epoch: 42/200, Loss_train: 1.7167191094365613, Loss_val: 2.8088232319930504
2024-06-21 17:43:45,131 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:43:45,131 - INFO: Epoch 43/200...
2024-06-21 17:43:45,131 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:43:45,131 - INFO: Batch size: 32.
2024-06-21 17:43:45,135 - INFO: Dataset:
2024-06-21 17:43:45,135 - INFO: Batch size:
2024-06-21 17:43:45,135 - INFO: Number of workers:
2024-06-21 17:43:46,201 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 2.082
2024-06-21 17:43:46,508 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 2.024
2024-06-21 17:43:46,917 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:47,235 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 1.474
2024-06-21 17:43:47,649 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 1.463
2024-06-21 17:43:47,950 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 1.587
2024-06-21 17:43:48,348 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 1.341
2024-06-21 17:43:48,662 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 1.914
2024-06-21 17:43:49,075 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 1.826
2024-06-21 17:43:49,368 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 1.652
2024-06-21 17:43:49,757 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 1.990
2024-06-21 17:43:50,072 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 1.444
2024-06-21 17:43:50,490 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 1.628
2024-06-21 17:43:50,792 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 1.487
2024-06-21 17:43:51,196 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 1.713
2024-06-21 17:43:51,506 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 1.772
2024-06-21 17:43:51,922 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 1.512
2024-06-21 17:43:52,220 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 1.501
2024-06-21 17:43:52,615 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 2.011
2024-06-21 17:43:52,920 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 1.974
2024-06-21 17:43:53,326 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 1.686
2024-06-21 17:43:53,627 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 2.438
2024-06-21 17:43:54,027 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 1.648
2024-06-21 17:43:54,341 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 1.442
2024-06-21 17:43:54,747 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 1.883
2024-06-21 17:43:55,043 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:55,437 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 1.780
2024-06-21 17:43:55,745 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 1.917
2024-06-21 17:43:55,966 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 1.814
2024-06-21 17:44:06,980 - INFO: 43/200 final results:
2024-06-21 17:44:06,980 - INFO: Training loss: 1.749.
2024-06-21 17:44:06,980 - INFO: Training MAE: 1.748.
2024-06-21 17:44:06,980 - INFO: Training MSE: 5.630.
2024-06-21 17:44:27,674 - INFO: Epoch: 43/200, Loss_train: 1.749013095066465, Loss_val: 2.4302909990836836
2024-06-21 17:44:27,693 - INFO: Saved new best metric model for epoch 43.
2024-06-21 17:44:27,693 - INFO: Best internal validation val_loss: 2.430 at epoch: 43.
2024-06-21 17:44:27,693 - INFO: Epoch 44/200...
2024-06-21 17:44:27,693 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:44:27,693 - INFO: Batch size: 32.
2024-06-21 17:44:27,697 - INFO: Dataset:
2024-06-21 17:44:27,697 - INFO: Batch size:
2024-06-21 17:44:27,697 - INFO: Number of workers:
2024-06-21 17:44:28,790 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 1.673
2024-06-21 17:44:29,096 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 1.731
2024-06-21 17:44:29,488 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 1.615
2024-06-21 17:44:29,805 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 1.931
2024-06-21 17:44:30,218 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 1.775
2024-06-21 17:44:30,517 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 1.753
2024-06-21 17:44:30,903 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 1.723
2024-06-21 17:44:31,217 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 1.562
2024-06-21 17:44:31,631 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 1.874
2024-06-21 17:44:31,924 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 1.926
2024-06-21 17:44:32,299 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 1.858
2024-06-21 17:44:32,614 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 1.215
2024-06-21 17:44:33,034 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 2.145
2024-06-21 17:44:33,337 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 1.650
2024-06-21 17:44:33,729 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 1.512
2024-06-21 17:44:34,040 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 1.636
2024-06-21 17:44:34,471 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 1.704
2024-06-21 17:44:34,769 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 1.634
2024-06-21 17:44:35,150 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 1.393
2024-06-21 17:44:35,456 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 1.444
2024-06-21 17:44:35,877 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 1.769
2024-06-21 17:44:36,178 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 1.803
2024-06-21 17:44:36,560 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 1.819
2024-06-21 17:44:36,873 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 1.776
2024-06-21 17:44:37,295 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 1.739
2024-06-21 17:44:37,594 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 2.126
2024-06-21 17:44:37,981 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 1.511
2024-06-21 17:44:38,294 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 1.906
2024-06-21 17:44:38,510 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 1.677
2024-06-21 17:44:49,692 - INFO: 44/200 final results:
2024-06-21 17:44:49,692 - INFO: Training loss: 1.720.
2024-06-21 17:44:49,693 - INFO: Training MAE: 1.721.
2024-06-21 17:44:49,693 - INFO: Training MSE: 5.551.
2024-06-21 17:45:09,664 - INFO: Epoch: 44/200, Loss_train: 1.7199699796479324, Loss_val: 3.5132553906276307
2024-06-21 17:45:09,664 - INFO: Best internal validation val_loss: 2.430 at epoch: 43.
2024-06-21 17:45:09,664 - INFO: Epoch 45/200...
2024-06-21 17:45:09,664 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:45:09,664 - INFO: Batch size: 32.
2024-06-21 17:45:09,668 - INFO: Dataset:
2024-06-21 17:45:09,668 - INFO: Batch size:
2024-06-21 17:45:09,668 - INFO: Number of workers:
2024-06-21 17:45:10,724 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 2.016
2024-06-21 17:45:11,043 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 1.767
2024-06-21 17:45:11,449 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 1.624
2024-06-21 17:45:11,766 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 1.558
2024-06-21 17:45:12,163 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 1.627
2024-06-21 17:45:12,475 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 1.462
2024-06-21 17:45:12,874 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 1.615
2024-06-21 17:45:13,187 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 1.738
2024-06-21 17:45:13,573 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 1.489
2024-06-21 17:45:13,878 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 1.986
2024-06-21 17:45:14,263 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 1.959
2024-06-21 17:45:14,577 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 1.861
2024-06-21 17:45:14,979 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 1.633
2024-06-21 17:45:15,294 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 1.614
2024-06-21 17:45:15,694 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 1.577
2024-06-21 17:45:16,005 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 1.770
2024-06-21 17:45:16,407 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 1.445
2024-06-21 17:45:16,718 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 1.715
2024-06-21 17:45:17,104 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 1.584
2024-06-21 17:45:17,409 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 1.472
2024-06-21 17:45:17,795 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 1.548
2024-06-21 17:45:18,108 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 1.352
2024-06-21 17:45:18,502 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 1.643
2024-06-21 17:45:18,815 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 1.590
2024-06-21 17:45:19,207 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 1.365
2024-06-21 17:45:19,516 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 1.805
2024-06-21 17:45:19,911 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 1.834
2024-06-21 17:45:20,220 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 1.588
2024-06-21 17:45:20,433 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 1.433
2024-06-21 17:45:31,555 - INFO: 45/200 final results:
2024-06-21 17:45:31,555 - INFO: Training loss: 1.644.
2024-06-21 17:45:31,555 - INFO: Training MAE: 1.648.
2024-06-21 17:45:31,555 - INFO: Training MSE: 4.966.
2024-06-21 17:45:51,816 - INFO: Epoch: 45/200, Loss_train: 1.6437602906391537, Loss_val: 2.3645453165317405
2024-06-21 17:45:51,835 - INFO: Saved new best metric model for epoch 45.
2024-06-21 17:45:51,836 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:45:51,836 - INFO: Epoch 46/200...
2024-06-21 17:45:51,836 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:45:51,836 - INFO: Batch size: 32.
2024-06-21 17:45:51,839 - INFO: Dataset:
2024-06-21 17:45:51,840 - INFO: Batch size:
2024-06-21 17:45:51,840 - INFO: Number of workers:
2024-06-21 17:45:52,891 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 1.956
2024-06-21 17:45:53,212 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 1.990
2024-06-21 17:45:53,622 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 1.680
2024-06-21 17:45:53,941 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 1.356
2024-06-21 17:45:54,345 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 1.627
2024-06-21 17:45:54,660 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 1.317
2024-06-21 17:45:55,061 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 1.523
2024-06-21 17:45:55,378 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 1.751
2024-06-21 17:45:55,771 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 1.707
2024-06-21 17:45:56,078 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 1.743
2024-06-21 17:45:56,468 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 1.656
2024-06-21 17:45:56,787 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 1.414
2024-06-21 17:45:57,197 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 1.761
2024-06-21 17:45:57,512 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 1.186
2024-06-21 17:45:57,918 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 1.741
2024-06-21 17:45:58,229 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 1.827
2024-06-21 17:45:58,635 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 1.991
2024-06-21 17:45:58,947 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 2.018
2024-06-21 17:45:59,345 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 1.548
2024-06-21 17:45:59,652 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 1.476
2024-06-21 17:46:00,048 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 1.542
2024-06-21 17:46:00,364 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 1.434
2024-06-21 17:46:00,756 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 1.699
2024-06-21 17:46:01,071 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 1.888
2024-06-21 17:46:01,462 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 1.705
2024-06-21 17:46:01,772 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 1.495
2024-06-21 17:46:02,162 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 2.004
2024-06-21 17:46:02,473 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 1.994
2024-06-21 17:46:02,686 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 1.976
2024-06-21 17:46:13,796 - INFO: 46/200 final results:
2024-06-21 17:46:13,796 - INFO: Training loss: 1.690.
2024-06-21 17:46:13,796 - INFO: Training MAE: 1.684.
2024-06-21 17:46:13,796 - INFO: Training MSE: 5.360.
2024-06-21 17:46:34,240 - INFO: Epoch: 46/200, Loss_train: 1.6898927565278679, Loss_val: 2.428579096136422
2024-06-21 17:46:34,240 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:46:34,241 - INFO: Epoch 47/200...
2024-06-21 17:46:34,241 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:46:34,241 - INFO: Batch size: 32.
2024-06-21 17:46:34,244 - INFO: Dataset:
2024-06-21 17:46:34,245 - INFO: Batch size:
2024-06-21 17:46:34,245 - INFO: Number of workers:
2024-06-21 17:46:35,315 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 1.465
2024-06-21 17:46:35,620 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 1.464
2024-06-21 17:46:36,013 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 1.427
2024-06-21 17:46:36,331 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 1.514
2024-06-21 17:46:36,762 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 1.682
2024-06-21 17:46:37,060 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 1.663
2024-06-21 17:46:37,444 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 1.693
2024-06-21 17:46:37,746 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 1.757
2024-06-21 17:46:38,192 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 1.491
2024-06-21 17:46:38,485 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 1.653
2024-06-21 17:46:38,860 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 1.761
2024-06-21 17:46:39,165 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 1.704
2024-06-21 17:46:39,608 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 1.434
2024-06-21 17:46:39,912 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 1.810
2024-06-21 17:46:40,304 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 1.817
2024-06-21 17:46:40,605 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 1.486
2024-06-21 17:46:41,047 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 1.395
2024-06-21 17:46:41,347 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 2.306
2024-06-21 17:46:41,729 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 1.797
2024-06-21 17:46:42,025 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 1.880
2024-06-21 17:46:42,456 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 1.485
2024-06-21 17:46:42,760 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 1.317
2024-06-21 17:46:43,141 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 1.899
2024-06-21 17:46:43,444 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 1.713
2024-06-21 17:46:43,878 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 1.818
2024-06-21 17:46:44,177 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 1.912
2024-06-21 17:46:44,555 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 1.552
2024-06-21 17:46:44,853 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 1.771
2024-06-21 17:46:45,072 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 1.653
2024-06-21 17:46:56,168 - INFO: 47/200 final results:
2024-06-21 17:46:56,169 - INFO: Training loss: 1.666.
2024-06-21 17:46:56,169 - INFO: Training MAE: 1.666.
2024-06-21 17:46:56,169 - INFO: Training MSE: 5.256.
2024-06-21 17:47:16,594 - INFO: Epoch: 47/200, Loss_train: 1.6661566208148826, Loss_val: 2.6962341111281822
2024-06-21 17:47:16,594 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:47:16,594 - INFO: Epoch 48/200...
2024-06-21 17:47:16,594 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:47:16,594 - INFO: Batch size: 32.
2024-06-21 17:47:16,597 - INFO: Dataset:
2024-06-21 17:47:16,598 - INFO: Batch size:
2024-06-21 17:47:16,598 - INFO: Number of workers:
2024-06-21 17:47:17,679 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 2.447
2024-06-21 17:47:18,000 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 1.473
2024-06-21 17:47:18,402 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 1.495
2024-06-21 17:47:18,720 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 1.662
2024-06-21 17:47:19,135 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 1.598
2024-06-21 17:47:19,436 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 1.998
2024-06-21 17:47:19,821 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 1.794
2024-06-21 17:47:20,134 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 1.652
2024-06-21 17:47:20,539 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 1.614
2024-06-21 17:47:20,832 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 1.668
2024-06-21 17:47:21,208 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 1.580
2024-06-21 17:47:21,525 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 1.488
2024-06-21 17:47:21,942 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 1.414
2024-06-21 17:47:22,246 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 1.545
2024-06-21 17:47:22,640 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 1.771
2024-06-21 17:47:22,953 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 1.931
2024-06-21 17:47:23,366 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 1.872
2024-06-21 17:47:23,666 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 1.547
2024-06-21 17:47:24,052 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 1.656
2024-06-21 17:47:24,359 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 1.962
2024-06-21 17:47:24,766 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 1.636
2024-06-21 17:47:25,069 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 1.645
2024-06-21 17:47:25,467 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 1.792
2024-06-21 17:47:25,781 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 1.410
2024-06-21 17:47:26,181 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 1.671
2024-06-21 17:47:26,478 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 1.839
2024-06-21 17:47:26,861 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 1.650
2024-06-21 17:47:27,170 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 1.607
2024-06-21 17:47:27,379 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 2.085
2024-06-21 17:47:38,242 - INFO: 48/200 final results:
2024-06-21 17:47:38,242 - INFO: Training loss: 1.707.
2024-06-21 17:47:38,242 - INFO: Training MAE: 1.699.
2024-06-21 17:47:38,242 - INFO: Training MSE: 5.301.
2024-06-21 17:47:58,637 - INFO: Epoch: 48/200, Loss_train: 1.7069553753425335, Loss_val: 2.747206112434124
2024-06-21 17:47:58,637 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:47:58,637 - INFO: Epoch 49/200...
2024-06-21 17:47:58,637 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:47:58,637 - INFO: Batch size: 32.
2024-06-21 17:47:58,641 - INFO: Dataset:
2024-06-21 17:47:58,641 - INFO: Batch size:
2024-06-21 17:47:58,641 - INFO: Number of workers:
2024-06-21 17:47:59,694 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 2.081
2024-06-21 17:48:00,027 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 1.498
2024-06-21 17:48:00,425 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 1.991
2024-06-21 17:48:00,742 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 1.428
2024-06-21 17:48:01,145 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 1.699
2024-06-21 17:48:01,471 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 1.616
2024-06-21 17:48:01,859 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 1.614
2024-06-21 17:48:02,173 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 1.193
2024-06-21 17:48:02,562 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 1.474
2024-06-21 17:48:02,888 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 1.663
2024-06-21 17:48:03,266 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 1.613
2024-06-21 17:48:03,583 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 1.913
2024-06-21 17:48:03,990 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 1.870
2024-06-21 17:48:04,319 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 2.173
2024-06-21 17:48:04,713 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 1.802
2024-06-21 17:48:05,027 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 1.351
2024-06-21 17:48:05,429 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 1.861
2024-06-21 17:48:05,755 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 1.835
2024-06-21 17:48:06,140 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 1.606
2024-06-21 17:48:06,448 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 1.738
2024-06-21 17:48:06,841 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 1.597
2024-06-21 17:48:07,169 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 1.856
2024-06-21 17:48:07,543 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 1.396
2024-06-21 17:48:07,858 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 1.811
2024-06-21 17:48:08,243 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 1.537
2024-06-21 17:48:08,565 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 1.554
2024-06-21 17:48:08,932 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 1.943
2024-06-21 17:48:09,243 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 1.667
2024-06-21 17:48:09,451 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 2.190
2024-06-21 17:48:20,465 - INFO: 49/200 final results:
2024-06-21 17:48:20,465 - INFO: Training loss: 1.709.
2024-06-21 17:48:20,465 - INFO: Training MAE: 1.700.
2024-06-21 17:48:20,465 - INFO: Training MSE: 5.306.
2024-06-21 17:48:40,871 - INFO: Epoch: 49/200, Loss_train: 1.7092792330117061, Loss_val: 2.12885398289253
2024-06-21 17:48:40,890 - INFO: Saved new best metric model for epoch 49.
2024-06-21 17:48:40,890 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:48:40,890 - INFO: Epoch 50/200...
2024-06-21 17:48:40,890 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:48:40,890 - INFO: Batch size: 32.
2024-06-21 17:48:40,894 - INFO: Dataset:
2024-06-21 17:48:40,894 - INFO: Batch size:
2024-06-21 17:48:40,894 - INFO: Number of workers:
2024-06-21 17:48:41,968 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 1.440
2024-06-21 17:48:42,276 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 1.416
2024-06-21 17:48:42,691 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 1.547
2024-06-21 17:48:43,012 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 1.609
2024-06-21 17:48:43,426 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 1.333
2024-06-21 17:48:43,728 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 2.122
2024-06-21 17:48:44,132 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 1.749
2024-06-21 17:48:44,449 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 1.556
2024-06-21 17:48:44,866 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 1.706
2024-06-21 17:48:45,160 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 1.571
2024-06-21 17:48:45,554 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 1.390
2024-06-21 17:48:45,871 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 1.481
2024-06-21 17:48:46,292 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 1.544
2024-06-21 17:48:46,596 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 1.481
2024-06-21 17:48:47,005 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 1.617
2024-06-21 17:48:47,316 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 1.433
2024-06-21 17:48:47,729 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 1.277
2024-06-21 17:48:48,027 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 1.293
2024-06-21 17:48:48,425 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 1.901
2024-06-21 17:48:48,730 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 1.636
2024-06-21 17:48:49,133 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 1.606
2024-06-21 17:48:49,434 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 1.318
2024-06-21 17:48:49,829 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 1.721
2024-06-21 17:48:50,143 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 1.441
2024-06-21 17:48:50,547 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 1.478
2024-06-21 17:48:50,845 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 1.297
2024-06-21 17:48:51,244 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 1.867
2024-06-21 17:48:51,555 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 1.996
2024-06-21 17:48:51,767 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 1.722
2024-06-21 17:49:02,629 - INFO: 50/200 final results:
2024-06-21 17:49:02,629 - INFO: Training loss: 1.571.
2024-06-21 17:49:02,629 - INFO: Training MAE: 1.568.
2024-06-21 17:49:02,629 - INFO: Training MSE: 4.570.
2024-06-21 17:49:22,883 - INFO: Epoch: 50/200, Loss_train: 1.570638660726876, Loss_val: 2.287520947127507
2024-06-21 17:49:22,883 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:49:22,883 - INFO: Epoch 51/200...
2024-06-21 17:49:22,883 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:49:22,884 - INFO: Batch size: 32.
2024-06-21 17:49:22,887 - INFO: Dataset:
2024-06-21 17:49:22,888 - INFO: Batch size:
2024-06-21 17:49:22,888 - INFO: Number of workers:
2024-06-21 17:49:23,977 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 1.627
2024-06-21 17:49:24,285 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 1.755
2024-06-21 17:49:24,684 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 1.428
2024-06-21 17:49:25,004 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 1.767
2024-06-21 17:49:25,433 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 2.070
2024-06-21 17:49:25,736 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 1.728
2024-06-21 17:49:26,125 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 1.724
2024-06-21 17:49:26,440 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 2.126
2024-06-21 17:49:26,865 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 1.392
2024-06-21 17:49:27,159 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 1.659
2024-06-21 17:49:27,538 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 1.188
2024-06-21 17:49:27,856 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 1.569
2024-06-21 17:49:28,288 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 1.731
2024-06-21 17:49:28,590 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 1.512
2024-06-21 17:49:28,982 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 1.285
2024-06-21 17:49:29,292 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 1.805
2024-06-21 17:49:29,720 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 1.878
2024-06-21 17:49:30,018 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 1.507
2024-06-21 17:49:30,402 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 1.516
2024-06-21 17:49:30,707 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 1.526
2024-06-21 17:49:31,128 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 1.617
2024-06-21 17:49:31,432 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 1.907
2024-06-21 17:49:31,806 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 1.440
2024-06-21 17:49:32,122 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 1.570
2024-06-21 17:49:32,532 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 1.540
2024-06-21 17:49:32,830 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 1.381
2024-06-21 17:49:33,199 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 1.357
2024-06-21 17:49:33,510 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 2.036
2024-06-21 17:49:33,719 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 1.883
2024-06-21 17:49:44,819 - INFO: 51/200 final results:
2024-06-21 17:49:44,819 - INFO: Training loss: 1.639.
2024-06-21 17:49:44,819 - INFO: Training MAE: 1.634.
2024-06-21 17:49:44,819 - INFO: Training MSE: 4.924.
2024-06-21 17:50:04,891 - INFO: Epoch: 51/200, Loss_train: 1.6387584990468518, Loss_val: 2.5013901981814155
2024-06-21 17:50:04,891 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:50:04,891 - INFO: Epoch 52/200...
2024-06-21 17:50:04,891 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:50:04,891 - INFO: Batch size: 32.
2024-06-21 17:50:04,895 - INFO: Dataset:
2024-06-21 17:50:04,895 - INFO: Batch size:
2024-06-21 17:50:04,895 - INFO: Number of workers:
2024-06-21 17:50:05,957 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 1.990
2024-06-21 17:50:06,276 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 1.855
2024-06-21 17:50:06,664 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 1.536
2024-06-21 17:50:06,980 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 1.879
2024-06-21 17:50:07,370 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 1.568
2024-06-21 17:50:07,694 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 1.653
2024-06-21 17:50:08,069 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 1.583
2024-06-21 17:50:08,381 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 1.520
2024-06-21 17:50:08,763 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 1.219
2024-06-21 17:50:09,086 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 1.303
2024-06-21 17:50:09,465 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 1.357
2024-06-21 17:50:09,783 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 2.081
2024-06-21 17:50:10,192 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 1.821
2024-06-21 17:50:10,522 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 2.172
2024-06-21 17:50:10,919 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 1.958
2024-06-21 17:50:11,233 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 1.496
2024-06-21 17:50:11,639 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 1.983
2024-06-21 17:50:11,966 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 1.846
2024-06-21 17:50:12,352 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 1.712
2024-06-21 17:50:12,660 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 1.504
2024-06-21 17:50:13,057 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 1.420
2024-06-21 17:50:13,385 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 1.720
2024-06-21 17:50:13,766 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 1.578
2024-06-21 17:50:14,083 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 1.429
2024-06-21 17:50:14,476 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 1.584
2024-06-21 17:50:14,799 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 1.311
2024-06-21 17:50:15,172 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 1.511
2024-06-21 17:50:15,483 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 1.685
2024-06-21 17:50:15,700 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 2.117
2024-06-21 17:50:26,896 - INFO: 52/200 final results:
2024-06-21 17:50:26,896 - INFO: Training loss: 1.669.
2024-06-21 17:50:26,896 - INFO: Training MAE: 1.660.
2024-06-21 17:50:26,896 - INFO: Training MSE: 5.100.
2024-06-21 17:50:47,359 - INFO: Epoch: 52/200, Loss_train: 1.6686521480823386, Loss_val: 2.659305531403114
2024-06-21 17:50:47,359 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:50:47,359 - INFO: Epoch 53/200...
2024-06-21 17:50:47,359 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:50:47,359 - INFO: Batch size: 32.
2024-06-21 17:50:47,363 - INFO: Dataset:
2024-06-21 17:50:47,363 - INFO: Batch size:
2024-06-21 17:50:47,363 - INFO: Number of workers:
2024-06-21 17:50:48,456 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 1.357
2024-06-21 17:50:48,761 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 1.551
2024-06-21 17:50:49,156 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 1.480
2024-06-21 17:50:49,473 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 1.904
2024-06-21 17:50:49,907 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 1.419
2024-06-21 17:50:50,206 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 1.777
2024-06-21 17:50:50,591 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 1.566
2024-06-21 17:50:50,892 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 1.865
2024-06-21 17:50:51,337 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 1.843
2024-06-21 17:50:51,628 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 1.898
2024-06-21 17:50:52,001 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 1.776
2024-06-21 17:50:52,304 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 1.615
2024-06-21 17:50:52,747 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 1.393
2024-06-21 17:50:53,051 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 1.204
2024-06-21 17:50:53,446 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 1.175
2024-06-21 17:50:53,746 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 1.482
2024-06-21 17:50:54,183 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 1.488
2024-06-21 17:50:54,480 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 1.519
2024-06-21 17:50:54,861 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 1.518
2024-06-21 17:50:55,153 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 1.444
2024-06-21 17:50:55,578 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 1.624
2024-06-21 17:50:55,879 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 1.906
2024-06-21 17:50:56,250 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 1.630
2024-06-21 17:50:56,550 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 1.630
2024-06-21 17:50:56,970 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 1.262
2024-06-21 17:50:57,266 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 1.510
2024-06-21 17:50:57,632 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 1.221
2024-06-21 17:50:57,928 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 1.553
2024-06-21 17:50:58,135 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 1.992
2024-06-21 17:51:09,274 - INFO: 53/200 final results:
2024-06-21 17:51:09,275 - INFO: Training loss: 1.573.
2024-06-21 17:51:09,275 - INFO: Training MAE: 1.564.
2024-06-21 17:51:09,275 - INFO: Training MSE: 4.500.
2024-06-21 17:51:29,197 - INFO: Epoch: 53/200, Loss_train: 1.5725214686887017, Loss_val: 2.3709471801231645
2024-06-21 17:51:29,197 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:51:29,197 - INFO: Epoch 54/200...
2024-06-21 17:51:29,197 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:51:29,197 - INFO: Batch size: 32.
2024-06-21 17:51:29,201 - INFO: Dataset:
2024-06-21 17:51:29,201 - INFO: Batch size:
2024-06-21 17:51:29,201 - INFO: Number of workers:
2024-06-21 17:51:30,288 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 1.728
2024-06-21 17:51:30,593 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 2.041
2024-06-21 17:51:30,994 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 1.916
2024-06-21 17:51:31,314 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 1.751
2024-06-21 17:51:31,749 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 1.519
2024-06-21 17:51:32,049 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 1.662
2024-06-21 17:51:32,438 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 1.506
2024-06-21 17:51:32,737 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 1.664
2024-06-21 17:51:33,173 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 1.445
2024-06-21 17:51:33,466 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 1.017
2024-06-21 17:51:33,842 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 1.503
2024-06-21 17:51:34,144 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 1.272
2024-06-21 17:51:34,878 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 1.435
2024-06-21 17:51:35,183 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 1.498
2024-06-21 17:51:35,580 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 1.611
2024-06-21 17:51:35,881 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 1.427
2024-06-21 17:51:36,320 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 1.560
2024-06-21 17:51:36,622 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 1.645
2024-06-21 17:51:37,009 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 1.369
2024-06-21 17:51:37,304 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 1.678
2024-06-21 17:51:37,736 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 1.650
2024-06-21 17:51:38,040 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 1.923
2024-06-21 17:51:38,428 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 1.490
2024-06-21 17:51:38,732 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 1.606
2024-06-21 17:51:39,163 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 1.566
2024-06-21 17:51:39,462 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 2.006
2024-06-21 17:51:39,842 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 1.480
2024-06-21 17:51:40,141 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 1.413
2024-06-21 17:51:40,362 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 1.437
2024-06-21 17:51:51,270 - INFO: 54/200 final results:
2024-06-21 17:51:51,271 - INFO: Training loss: 1.580.
2024-06-21 17:51:51,271 - INFO: Training MAE: 1.583.
2024-06-21 17:51:51,271 - INFO: Training MSE: 4.610.
2024-06-21 17:52:11,590 - INFO: Epoch: 54/200, Loss_train: 1.5799566014059658, Loss_val: 2.3920297129400847
2024-06-21 17:52:11,590 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:52:11,590 - INFO: Epoch 55/200...
2024-06-21 17:52:11,590 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:52:11,590 - INFO: Batch size: 32.
2024-06-21 17:52:11,594 - INFO: Dataset:
2024-06-21 17:52:11,594 - INFO: Batch size:
2024-06-21 17:52:11,594 - INFO: Number of workers:
2024-06-21 17:52:12,707 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 1.878
2024-06-21 17:52:13,025 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 1.922
2024-06-21 17:52:13,419 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 1.523
2024-06-21 17:52:13,735 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 1.513
2024-06-21 17:52:14,156 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 1.409
2024-06-21 17:52:14,456 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 1.421
2024-06-21 17:52:14,843 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 1.623
2024-06-21 17:52:15,155 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 1.399
2024-06-21 17:52:15,587 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 1.293
2024-06-21 17:52:15,880 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 1.426
2024-06-21 17:52:16,258 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 1.591
2024-06-21 17:52:16,573 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 1.492
2024-06-21 17:52:16,999 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 1.720
2024-06-21 17:52:17,303 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 1.776
2024-06-21 17:52:17,700 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 1.687
2024-06-21 17:52:18,013 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 1.676
2024-06-21 17:52:18,445 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 1.510
2024-06-21 17:52:18,745 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 1.419
2024-06-21 17:52:19,117 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 1.308
2024-06-21 17:52:19,425 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 1.336
2024-06-21 17:52:19,838 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 1.323
2024-06-21 17:52:20,147 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 1.373
2024-06-21 17:52:20,548 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 1.671
2024-06-21 17:52:20,872 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 1.414
2024-06-21 17:52:21,291 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 1.857
2024-06-21 17:52:21,587 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 1.904
2024-06-21 17:52:21,967 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 1.686
2024-06-21 17:52:22,275 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 1.320
2024-06-21 17:52:22,493 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 2.101
2024-06-21 17:52:33,481 - INFO: 55/200 final results:
2024-06-21 17:52:33,482 - INFO: Training loss: 1.571.
2024-06-21 17:52:33,482 - INFO: Training MAE: 1.561.
2024-06-21 17:52:33,482 - INFO: Training MSE: 4.427.
2024-06-21 17:52:53,705 - INFO: Epoch: 55/200, Loss_train: 1.5713629722595215, Loss_val: 2.234514330995494
2024-06-21 17:52:53,705 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:52:53,705 - INFO: Epoch 56/200...
2024-06-21 17:52:53,705 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:52:53,705 - INFO: Batch size: 32.
2024-06-21 17:52:53,709 - INFO: Dataset:
2024-06-21 17:52:53,709 - INFO: Batch size:
2024-06-21 17:52:53,709 - INFO: Number of workers:
2024-06-21 17:52:54,764 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 1.324
2024-06-21 17:52:55,085 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 1.265
2024-06-21 17:52:55,495 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 1.267
2024-06-21 17:52:55,814 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 1.215
2024-06-21 17:52:56,218 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 1.255
2024-06-21 17:52:56,517 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 1.680
2024-06-21 17:52:56,911 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 1.583
2024-06-21 17:52:57,226 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 1.504
2024-06-21 17:52:57,632 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 2.021
2024-06-21 17:52:57,926 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 1.375
2024-06-21 17:52:58,309 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 1.222
2024-06-21 17:52:58,626 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 1.080
2024-06-21 17:52:59,043 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 1.579
2024-06-21 17:52:59,348 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 1.775
2024-06-21 17:52:59,751 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 1.629
2024-06-21 17:53:00,064 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 1.334
2024-06-21 17:53:00,480 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 1.356
2024-06-21 17:53:00,780 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 1.747
2024-06-21 17:53:01,164 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 1.389
2024-06-21 17:53:01,474 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 1.671
2024-06-21 17:53:01,879 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 1.440
2024-06-21 17:53:02,183 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 1.606
2024-06-21 17:53:02,582 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 1.337
2024-06-21 17:53:02,897 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 1.581
2024-06-21 17:53:03,306 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 1.406
2024-06-21 17:53:03,604 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 1.511
2024-06-21 17:53:04,002 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 1.832
2024-06-21 17:53:04,314 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 1.423
2024-06-21 17:53:04,534 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 1.265
2024-06-21 17:53:15,660 - INFO: 56/200 final results:
2024-06-21 17:53:15,660 - INFO: Training loss: 1.471.
2024-06-21 17:53:15,660 - INFO: Training MAE: 1.475.
2024-06-21 17:53:15,660 - INFO: Training MSE: 3.901.
2024-06-21 17:53:35,781 - INFO: Epoch: 56/200, Loss_train: 1.4713498477278084, Loss_val: 2.5679900235143203
2024-06-21 17:53:35,781 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:53:35,781 - INFO: Epoch 57/200...
2024-06-21 17:53:35,781 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:53:35,781 - INFO: Batch size: 32.
2024-06-21 17:53:35,785 - INFO: Dataset:
2024-06-21 17:53:35,785 - INFO: Batch size:
2024-06-21 17:53:35,785 - INFO: Number of workers:
2024-06-21 17:53:36,870 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 1.547
2024-06-21 17:53:37,175 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 1.396
2024-06-21 17:53:37,572 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 1.536
2024-06-21 17:53:37,890 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 1.479
2024-06-21 17:53:38,314 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 1.544
2024-06-21 17:53:38,614 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 1.169
2024-06-21 17:53:38,997 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 1.176
2024-06-21 17:53:39,312 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 1.300
2024-06-21 17:53:39,724 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 1.593
2024-06-21 17:53:40,016 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 1.240
2024-06-21 17:53:40,378 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 1.208
2024-06-21 17:53:40,692 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 1.239
2024-06-21 17:53:41,127 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 1.294
2024-06-21 17:53:41,431 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 1.251
2024-06-21 17:53:41,827 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 1.569
2024-06-21 17:53:42,141 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 1.419
2024-06-21 17:53:42,573 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 1.153
2024-06-21 17:53:42,873 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 0.996
2024-06-21 17:53:43,245 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 1.307
2024-06-21 17:53:43,553 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 1.169
2024-06-21 17:53:43,972 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 1.590
2024-06-21 17:53:44,273 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 1.335
2024-06-21 17:53:44,654 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 1.641
2024-06-21 17:53:44,967 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 1.158
2024-06-21 17:53:45,378 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 1.746
2024-06-21 17:53:45,674 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 1.230
2024-06-21 17:53:46,043 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 1.200
2024-06-21 17:53:46,352 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 1.389
2024-06-21 17:53:46,571 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 1.316
2024-06-21 17:53:57,602 - INFO: 57/200 final results:
2024-06-21 17:53:57,602 - INFO: Training loss: 1.351.
2024-06-21 17:53:57,602 - INFO: Training MAE: 1.352.
2024-06-21 17:53:57,602 - INFO: Training MSE: 3.432.
2024-06-21 17:54:18,008 - INFO: Epoch: 57/200, Loss_train: 1.3514444786926796, Loss_val: 2.253265717933918
2024-06-21 17:54:18,008 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:54:18,008 - INFO: Epoch 58/200...
2024-06-21 17:54:18,008 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:54:18,008 - INFO: Batch size: 32.
2024-06-21 17:54:18,012 - INFO: Dataset:
2024-06-21 17:54:18,012 - INFO: Batch size:
2024-06-21 17:54:18,012 - INFO: Number of workers:
2024-06-21 17:54:19,079 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 1.280
2024-06-21 17:54:19,400 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 1.056
2024-06-21 17:54:19,798 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 1.313
2024-06-21 17:54:20,116 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 1.392
2024-06-21 17:54:20,530 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 0.981
2024-06-21 17:54:20,830 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 1.035
2024-06-21 17:54:21,220 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 1.100
2024-06-21 17:54:21,533 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 1.093
2024-06-21 17:54:21,943 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 1.568
2024-06-21 17:54:22,237 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 1.515
2024-06-21 17:54:22,614 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 1.092
2024-06-21 17:54:22,931 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 1.063
2024-06-21 17:54:23,348 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 1.487
2024-06-21 17:54:23,652 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 1.229
2024-06-21 17:54:24,057 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 1.066
2024-06-21 17:54:24,371 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 1.322
2024-06-21 17:54:24,776 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 1.165
2024-06-21 17:54:25,076 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 1.075
2024-06-21 17:54:25,473 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 1.159
2024-06-21 17:54:25,781 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 1.151
2024-06-21 17:54:26,174 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 1.266
2024-06-21 17:54:26,477 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 1.207
2024-06-21 17:54:26,876 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 1.176
2024-06-21 17:54:27,198 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 1.318
2024-06-21 17:54:27,620 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 1.220
2024-06-21 17:54:27,917 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 1.558
2024-06-21 17:54:28,312 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 1.300
2024-06-21 17:54:28,622 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 1.231
2024-06-21 17:54:28,836 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 1.475
2024-06-21 17:54:39,871 - INFO: 58/200 final results:
2024-06-21 17:54:39,871 - INFO: Training loss: 1.238.
2024-06-21 17:54:39,871 - INFO: Training MAE: 1.233.
2024-06-21 17:54:39,871 - INFO: Training MSE: 2.863.
2024-06-21 17:55:00,030 - INFO: Epoch: 58/200, Loss_train: 1.2377645085597861, Loss_val: 2.097023577525698
2024-06-21 17:55:00,049 - INFO: Saved new best metric model for epoch 58.
2024-06-21 17:55:00,049 - INFO: Best internal validation val_loss: 2.097 at epoch: 58.
2024-06-21 17:55:00,049 - INFO: Epoch 59/200...
2024-06-21 17:55:00,049 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:55:00,049 - INFO: Batch size: 32.
2024-06-21 17:55:00,053 - INFO: Dataset:
2024-06-21 17:55:00,053 - INFO: Batch size:
2024-06-21 17:55:00,053 - INFO: Number of workers:
2024-06-21 17:55:01,108 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 1.119
2024-06-21 17:55:01,431 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 1.227
2024-06-21 17:55:01,842 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 1.410
2024-06-21 17:55:02,160 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 1.553
2024-06-21 17:55:02,561 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 1.279
2024-06-21 17:55:02,874 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 1.167
2024-06-21 17:55:03,256 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 1.155
2024-06-21 17:55:03,571 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 1.480
2024-06-21 17:55:03,962 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 1.078
2024-06-21 17:55:04,268 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 1.617
2024-06-21 17:55:04,659 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 1.152
2024-06-21 17:55:04,976 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 1.426
2024-06-21 17:55:05,382 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 1.277
2024-06-21 17:55:05,700 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 1.296
2024-06-21 17:55:06,095 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 1.151
2024-06-21 17:55:06,409 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 1.196
2024-06-21 17:55:06,811 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 1.205
2024-06-21 17:55:07,125 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 1.501
2024-06-21 17:55:07,508 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 1.581
2024-06-21 17:55:07,817 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 1.398
2024-06-21 17:55:08,210 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 1.167
2024-06-21 17:55:08,525 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 1.064
2024-06-21 17:55:08,911 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 1.123
2024-06-21 17:55:09,226 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 1.369
2024-06-21 17:55:09,610 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 1.225
2024-06-21 17:55:09,920 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 1.132
2024-06-21 17:55:10,299 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 1.287
2024-06-21 17:55:10,609 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 1.396
2024-06-21 17:55:10,818 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 1.138
2024-06-21 17:55:21,933 - INFO: 59/200 final results:
2024-06-21 17:55:21,933 - INFO: Training loss: 1.282.
2024-06-21 17:55:21,933 - INFO: Training MAE: 1.285.
2024-06-21 17:55:21,933 - INFO: Training MSE: 3.041.
2024-06-21 17:55:42,022 - INFO: Epoch: 59/200, Loss_train: 1.2817197462608074, Loss_val: 1.9598168710182453
2024-06-21 17:55:42,041 - INFO: Saved new best metric model for epoch 59.
2024-06-21 17:55:42,041 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:55:42,041 - INFO: Epoch 60/200...
2024-06-21 17:55:42,041 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:55:42,041 - INFO: Batch size: 32.
2024-06-21 17:55:42,045 - INFO: Dataset:
2024-06-21 17:55:42,045 - INFO: Batch size:
2024-06-21 17:55:42,045 - INFO: Number of workers:
2024-06-21 17:55:43,128 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 1.013
2024-06-21 17:55:43,432 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 1.282
2024-06-21 17:55:43,828 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 1.045
2024-06-21 17:55:44,144 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 1.071
2024-06-21 17:55:44,565 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 1.319
2024-06-21 17:55:44,863 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 1.237
2024-06-21 17:55:45,250 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 1.133
2024-06-21 17:55:45,562 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 1.092
2024-06-21 17:55:45,987 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 1.349
2024-06-21 17:55:46,279 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 1.269
2024-06-21 17:55:46,654 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 1.216
2024-06-21 17:55:46,968 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 1.279
2024-06-21 17:55:47,397 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 1.051
2024-06-21 17:55:47,699 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 1.139
2024-06-21 17:55:48,094 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 1.099
2024-06-21 17:55:48,406 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 1.324
2024-06-21 17:55:48,835 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 1.380
2024-06-21 17:55:49,133 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 1.081
2024-06-21 17:55:49,517 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 1.273
2024-06-21 17:55:49,823 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 1.294
2024-06-21 17:55:50,233 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 1.218
2024-06-21 17:55:50,534 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 1.149
2024-06-21 17:55:50,917 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 1.213
2024-06-21 17:55:51,230 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 1.214
2024-06-21 17:55:51,646 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 1.172
2024-06-21 17:55:51,942 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 1.014
2024-06-21 17:55:52,323 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 1.066
2024-06-21 17:55:52,633 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 1.240
2024-06-21 17:55:52,843 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 0.723
2024-06-21 17:56:03,862 - INFO: 60/200 final results:
2024-06-21 17:56:03,862 - INFO: Training loss: 1.171.
2024-06-21 17:56:03,862 - INFO: Training MAE: 1.180.
2024-06-21 17:56:03,862 - INFO: Training MSE: 2.637.
2024-06-21 17:56:24,248 - INFO: Epoch: 60/200, Loss_train: 1.1709334459798089, Loss_val: 2.222502724877719
2024-06-21 17:56:24,248 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:56:24,248 - INFO: Epoch 61/200...
2024-06-21 17:56:24,248 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:56:24,248 - INFO: Batch size: 32.
2024-06-21 17:56:24,252 - INFO: Dataset:
2024-06-21 17:56:24,252 - INFO: Batch size:
2024-06-21 17:56:24,252 - INFO: Number of workers:
2024-06-21 17:56:25,318 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 1.160
2024-06-21 17:56:25,659 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 1.176
2024-06-21 17:56:26,054 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 1.236
2024-06-21 17:56:26,371 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 1.370
2024-06-21 17:56:26,771 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 1.253
2024-06-21 17:56:27,108 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 1.335
2024-06-21 17:56:27,495 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 1.186
2024-06-21 17:56:27,796 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 1.056
2024-06-21 17:56:28,185 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 1.262
2024-06-21 17:56:28,524 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 0.962
2024-06-21 17:56:28,900 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 0.958
2024-06-21 17:56:29,203 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 1.384
2024-06-21 17:56:29,609 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 1.189
2024-06-21 17:56:29,949 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 1.185
2024-06-21 17:56:30,344 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 1.137
2024-06-21 17:56:30,642 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 1.090
2024-06-21 17:56:31,046 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 1.490
2024-06-21 17:56:31,383 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 0.989
2024-06-21 17:56:31,768 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 1.241
2024-06-21 17:56:32,061 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 1.090
2024-06-21 17:56:32,459 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 1.194
2024-06-21 17:56:32,796 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 1.013
2024-06-21 17:56:33,169 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 0.978
2024-06-21 17:56:33,470 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 1.314
2024-06-21 17:56:33,867 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 1.117
2024-06-21 17:56:34,200 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 1.116
2024-06-21 17:56:34,584 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 1.273
2024-06-21 17:56:34,880 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 1.047
2024-06-21 17:56:35,101 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 1.386
2024-06-21 17:56:45,883 - INFO: 61/200 final results:
2024-06-21 17:56:45,884 - INFO: Training loss: 1.179.
2024-06-21 17:56:45,884 - INFO: Training MAE: 1.175.
2024-06-21 17:56:45,884 - INFO: Training MSE: 2.617.
2024-06-21 17:57:05,980 - INFO: Epoch: 61/200, Loss_train: 1.178932331759354, Loss_val: 2.3403911426149566
2024-06-21 17:57:05,980 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:57:05,980 - INFO: Epoch 62/200...
2024-06-21 17:57:05,980 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:57:05,980 - INFO: Batch size: 32.
2024-06-21 17:57:05,983 - INFO: Dataset:
2024-06-21 17:57:05,984 - INFO: Batch size:
2024-06-21 17:57:05,984 - INFO: Number of workers:
2024-06-21 17:57:07,053 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 1.375
2024-06-21 17:57:07,372 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 1.199
2024-06-21 17:57:07,781 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 1.293
2024-06-21 17:57:08,097 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 1.402
2024-06-21 17:57:08,485 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 1.212
2024-06-21 17:57:08,797 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 1.334
2024-06-21 17:57:09,194 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 1.206
2024-06-21 17:57:09,507 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 1.225
2024-06-21 17:57:09,883 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 1.123
2024-06-21 17:57:10,188 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 0.949
2024-06-21 17:57:10,572 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 1.127
2024-06-21 17:57:10,887 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 1.188
2024-06-21 17:57:11,275 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 1.135
2024-06-21 17:57:11,590 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 1.037
2024-06-21 17:57:11,993 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 1.141
2024-06-21 17:57:12,305 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 1.259
2024-06-21 17:57:12,690 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 1.111
2024-06-21 17:57:13,001 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 0.982
2024-06-21 17:57:13,395 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 1.021
2024-06-21 17:57:13,700 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 1.083
2024-06-21 17:57:14,077 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 1.046
2024-06-21 17:57:14,391 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 1.074
2024-06-21 17:57:14,782 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 1.172
2024-06-21 17:57:15,095 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 1.313
2024-06-21 17:57:15,481 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 1.034
2024-06-21 17:57:15,789 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 1.255
2024-06-21 17:57:16,176 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 1.402
2024-06-21 17:57:16,485 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 1.152
2024-06-21 17:57:16,697 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 1.674
2024-06-21 17:57:27,755 - INFO: 62/200 final results:
2024-06-21 17:57:27,756 - INFO: Training loss: 1.190.
2024-06-21 17:57:27,756 - INFO: Training MAE: 1.181.
2024-06-21 17:57:27,756 - INFO: Training MSE: 2.599.
2024-06-21 17:57:48,191 - INFO: Epoch: 62/200, Loss_train: 1.190498514422055, Loss_val: 2.571770133643315
2024-06-21 17:57:48,191 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:57:48,191 - INFO: Epoch 63/200...
2024-06-21 17:57:48,191 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:57:48,191 - INFO: Batch size: 32.
2024-06-21 17:57:48,194 - INFO: Dataset:
2024-06-21 17:57:48,195 - INFO: Batch size:
2024-06-21 17:57:48,195 - INFO: Number of workers:
2024-06-21 17:57:49,262 - INFO: Epoch: 63/200, Batch: 1/29, Batch_Loss_Train: 1.406
2024-06-21 17:57:49,582 - INFO: Epoch: 63/200, Batch: 2/29, Batch_Loss_Train: 1.646
2024-06-21 17:57:49,985 - INFO: Epoch: 63/200, Batch: 3/29, Batch_Loss_Train: 1.391
2024-06-21 17:57:50,303 - INFO: Epoch: 63/200, Batch: 4/29, Batch_Loss_Train: 1.147
2024-06-21 17:57:50,695 - INFO: Epoch: 63/200, Batch: 5/29, Batch_Loss_Train: 0.987
2024-06-21 17:57:51,009 - INFO: Epoch: 63/200, Batch: 6/29, Batch_Loss_Train: 1.023
2024-06-21 17:57:51,409 - INFO: Epoch: 63/200, Batch: 7/29, Batch_Loss_Train: 1.281
2024-06-21 17:57:51,725 - INFO: Epoch: 63/200, Batch: 8/29, Batch_Loss_Train: 1.243
2024-06-21 17:57:52,103 - INFO: Epoch: 63/200, Batch: 9/29, Batch_Loss_Train: 1.094
2024-06-21 17:57:52,412 - INFO: Epoch: 63/200, Batch: 10/29, Batch_Loss_Train: 1.166
2024-06-21 17:57:52,795 - INFO: Epoch: 63/200, Batch: 11/29, Batch_Loss_Train: 0.968
2024-06-21 17:57:53,112 - INFO: Epoch: 63/200, Batch: 12/29, Batch_Loss_Train: 1.074
2024-06-21 17:57:53,522 - INFO: Epoch: 63/200, Batch: 13/29, Batch_Loss_Train: 1.205
2024-06-21 17:57:53,840 - INFO: Epoch: 63/200, Batch: 14/29, Batch_Loss_Train: 1.136
2024-06-21 17:57:54,240 - INFO: Epoch: 63/200, Batch: 15/29, Batch_Loss_Train: 1.177
2024-06-21 17:57:54,554 - INFO: Epoch: 63/200, Batch: 16/29, Batch_Loss_Train: 1.243
2024-06-21 17:57:54,962 - INFO: Epoch: 63/200, Batch: 17/29, Batch_Loss_Train: 1.052
2024-06-21 17:57:55,276 - INFO: Epoch: 63/200, Batch: 18/29, Batch_Loss_Train: 1.254
2024-06-21 17:57:55,669 - INFO: Epoch: 63/200, Batch: 19/29, Batch_Loss_Train: 1.027
2024-06-21 17:57:55,977 - INFO: Epoch: 63/200, Batch: 20/29, Batch_Loss_Train: 1.137
2024-06-21 17:57:56,374 - INFO: Epoch: 63/200, Batch: 21/29, Batch_Loss_Train: 0.968
2024-06-21 17:57:56,690 - INFO: Epoch: 63/200, Batch: 22/29, Batch_Loss_Train: 1.165
2024-06-21 17:57:57,090 - INFO: Epoch: 63/200, Batch: 23/29, Batch_Loss_Train: 1.126
2024-06-21 17:57:57,405 - INFO: Epoch: 63/200, Batch: 24/29, Batch_Loss_Train: 0.856
2024-06-21 17:57:57,800 - INFO: Epoch: 63/200, Batch: 25/29, Batch_Loss_Train: 0.933
2024-06-21 17:57:58,112 - INFO: Epoch: 63/200, Batch: 26/29, Batch_Loss_Train: 1.406
2024-06-21 17:57:58,505 - INFO: Epoch: 63/200, Batch: 27/29, Batch_Loss_Train: 1.221
2024-06-21 17:57:58,817 - INFO: Epoch: 63/200, Batch: 28/29, Batch_Loss_Train: 1.318
2024-06-21 17:57:59,037 - INFO: Epoch: 63/200, Batch: 29/29, Batch_Loss_Train: 1.045
2024-06-21 17:58:10,205 - INFO: 63/200 final results:
2024-06-21 17:58:10,205 - INFO: Training loss: 1.162.
2024-06-21 17:58:10,205 - INFO: Training MAE: 1.164.
2024-06-21 17:58:10,205 - INFO: Training MSE: 2.595.
2024-06-21 17:58:30,684 - INFO: Epoch: 63/200, Loss_train: 1.162010751921555, Loss_val: 2.0562038010564345
2024-06-21 17:58:30,684 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:58:30,684 - INFO: Epoch 64/200...
2024-06-21 17:58:30,684 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:58:30,685 - INFO: Batch size: 32.
2024-06-21 17:58:30,688 - INFO: Dataset:
2024-06-21 17:58:30,689 - INFO: Batch size:
2024-06-21 17:58:30,689 - INFO: Number of workers:
2024-06-21 17:58:31,759 - INFO: Epoch: 64/200, Batch: 1/29, Batch_Loss_Train: 1.228
2024-06-21 17:58:32,078 - INFO: Epoch: 64/200, Batch: 2/29, Batch_Loss_Train: 1.170
2024-06-21 17:58:32,476 - INFO: Epoch: 64/200, Batch: 3/29, Batch_Loss_Train: 0.917
2024-06-21 17:58:32,792 - INFO: Epoch: 64/200, Batch: 4/29, Batch_Loss_Train: 1.063
2024-06-21 17:58:33,185 - INFO: Epoch: 64/200, Batch: 5/29, Batch_Loss_Train: 1.183
2024-06-21 17:58:33,497 - INFO: Epoch: 64/200, Batch: 6/29, Batch_Loss_Train: 1.161
2024-06-21 17:58:33,882 - INFO: Epoch: 64/200, Batch: 7/29, Batch_Loss_Train: 0.996
2024-06-21 17:58:34,194 - INFO: Epoch: 64/200, Batch: 8/29, Batch_Loss_Train: 1.151
2024-06-21 17:58:34,578 - INFO: Epoch: 64/200, Batch: 9/29, Batch_Loss_Train: 0.917
2024-06-21 17:58:34,883 - INFO: Epoch: 64/200, Batch: 10/29, Batch_Loss_Train: 1.497
2024-06-21 17:58:35,263 - INFO: Epoch: 64/200, Batch: 11/29, Batch_Loss_Train: 1.289
2024-06-21 17:58:35,579 - INFO: Epoch: 64/200, Batch: 12/29, Batch_Loss_Train: 0.978
2024-06-21 17:58:35,982 - INFO: Epoch: 64/200, Batch: 13/29, Batch_Loss_Train: 1.192
2024-06-21 17:58:36,297 - INFO: Epoch: 64/200, Batch: 14/29, Batch_Loss_Train: 1.358
2024-06-21 17:58:36,693 - INFO: Epoch: 64/200, Batch: 15/29, Batch_Loss_Train: 1.215
2024-06-21 17:58:37,004 - INFO: Epoch: 64/200, Batch: 16/29, Batch_Loss_Train: 1.180
2024-06-21 17:58:37,407 - INFO: Epoch: 64/200, Batch: 17/29, Batch_Loss_Train: 1.108
2024-06-21 17:58:37,720 - INFO: Epoch: 64/200, Batch: 18/29, Batch_Loss_Train: 1.175
2024-06-21 17:58:38,110 - INFO: Epoch: 64/200, Batch: 19/29, Batch_Loss_Train: 1.284
2024-06-21 17:58:38,416 - INFO: Epoch: 64/200, Batch: 20/29, Batch_Loss_Train: 1.477
2024-06-21 17:58:38,804 - INFO: Epoch: 64/200, Batch: 21/29, Batch_Loss_Train: 1.147
2024-06-21 17:58:39,118 - INFO: Epoch: 64/200, Batch: 22/29, Batch_Loss_Train: 1.241
2024-06-21 17:58:39,512 - INFO: Epoch: 64/200, Batch: 23/29, Batch_Loss_Train: 1.253
2024-06-21 17:58:39,825 - INFO: Epoch: 64/200, Batch: 24/29, Batch_Loss_Train: 1.365
2024-06-21 17:58:40,213 - INFO: Epoch: 64/200, Batch: 25/29, Batch_Loss_Train: 1.236
2024-06-21 17:58:40,521 - INFO: Epoch: 64/200, Batch: 26/29, Batch_Loss_Train: 1.176
2024-06-21 17:58:40,917 - INFO: Epoch: 64/200, Batch: 27/29, Batch_Loss_Train: 1.212
2024-06-21 17:58:41,225 - INFO: Epoch: 64/200, Batch: 28/29, Batch_Loss_Train: 1.159
2024-06-21 17:58:41,443 - INFO: Epoch: 64/200, Batch: 29/29, Batch_Loss_Train: 1.362
2024-06-21 17:58:52,428 - INFO: 64/200 final results:
2024-06-21 17:58:52,428 - INFO: Training loss: 1.196.
2024-06-21 17:58:52,428 - INFO: Training MAE: 1.193.
2024-06-21 17:58:52,428 - INFO: Training MSE: 2.703.
2024-06-21 17:59:12,548 - INFO: Epoch: 64/200, Loss_train: 1.1961852558727921, Loss_val: 2.3428746174121726
2024-06-21 17:59:12,548 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:59:12,548 - INFO: Epoch 65/200...
2024-06-21 17:59:12,548 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:59:12,548 - INFO: Batch size: 32.
2024-06-21 17:59:12,552 - INFO: Dataset:
2024-06-21 17:59:12,552 - INFO: Batch size:
2024-06-21 17:59:12,552 - INFO: Number of workers:
2024-06-21 17:59:13,611 - INFO: Epoch: 65/200, Batch: 1/29, Batch_Loss_Train: 1.700
2024-06-21 17:59:13,929 - INFO: Epoch: 65/200, Batch: 2/29, Batch_Loss_Train: 1.362
2024-06-21 17:59:14,337 - INFO: Epoch: 65/200, Batch: 3/29, Batch_Loss_Train: 0.938
2024-06-21 17:59:14,654 - INFO: Epoch: 65/200, Batch: 4/29, Batch_Loss_Train: 1.188
2024-06-21 17:59:15,041 - INFO: Epoch: 65/200, Batch: 5/29, Batch_Loss_Train: 0.984
2024-06-21 17:59:15,353 - INFO: Epoch: 65/200, Batch: 6/29, Batch_Loss_Train: 0.967
2024-06-21 17:59:15,749 - INFO: Epoch: 65/200, Batch: 7/29, Batch_Loss_Train: 0.916
2024-06-21 17:59:16,062 - INFO: Epoch: 65/200, Batch: 8/29, Batch_Loss_Train: 0.977
2024-06-21 17:59:16,437 - INFO: Epoch: 65/200, Batch: 9/29, Batch_Loss_Train: 1.102
2024-06-21 17:59:16,743 - INFO: Epoch: 65/200, Batch: 10/29, Batch_Loss_Train: 1.514
2024-06-21 17:59:17,127 - INFO: Epoch: 65/200, Batch: 11/29, Batch_Loss_Train: 1.066
2024-06-21 17:59:17,441 - INFO: Epoch: 65/200, Batch: 12/29, Batch_Loss_Train: 1.414
2024-06-21 17:59:17,834 - INFO: Epoch: 65/200, Batch: 13/29, Batch_Loss_Train: 1.327
2024-06-21 17:59:18,147 - INFO: Epoch: 65/200, Batch: 14/29, Batch_Loss_Train: 1.112
2024-06-21 17:59:18,548 - INFO: Epoch: 65/200, Batch: 15/29, Batch_Loss_Train: 1.038
2024-06-21 17:59:18,860 - INFO: Epoch: 65/200, Batch: 16/29, Batch_Loss_Train: 1.175
2024-06-21 17:59:19,247 - INFO: Epoch: 65/200, Batch: 17/29, Batch_Loss_Train: 1.030
2024-06-21 17:59:19,557 - INFO: Epoch: 65/200, Batch: 18/29, Batch_Loss_Train: 1.453
2024-06-21 17:59:19,952 - INFO: Epoch: 65/200, Batch: 19/29, Batch_Loss_Train: 1.110
2024-06-21 17:59:20,257 - INFO: Epoch: 65/200, Batch: 20/29, Batch_Loss_Train: 1.274
2024-06-21 17:59:20,634 - INFO: Epoch: 65/200, Batch: 21/29, Batch_Loss_Train: 1.031
2024-06-21 17:59:20,947 - INFO: Epoch: 65/200, Batch: 22/29, Batch_Loss_Train: 1.236
2024-06-21 17:59:21,331 - INFO: Epoch: 65/200, Batch: 23/29, Batch_Loss_Train: 1.163
2024-06-21 17:59:21,643 - INFO: Epoch: 65/200, Batch: 24/29, Batch_Loss_Train: 1.035
2024-06-21 17:59:22,026 - INFO: Epoch: 65/200, Batch: 25/29, Batch_Loss_Train: 1.346
2024-06-21 17:59:22,333 - INFO: Epoch: 65/200, Batch: 26/29, Batch_Loss_Train: 1.132
2024-06-21 17:59:22,711 - INFO: Epoch: 65/200, Batch: 27/29, Batch_Loss_Train: 1.080
2024-06-21 17:59:23,019 - INFO: Epoch: 65/200, Batch: 28/29, Batch_Loss_Train: 1.237
2024-06-21 17:59:23,225 - INFO: Epoch: 65/200, Batch: 29/29, Batch_Loss_Train: 1.844
2024-06-21 17:59:33,848 - INFO: 65/200 final results:
2024-06-21 17:59:33,848 - INFO: Training loss: 1.198.
2024-06-21 17:59:33,848 - INFO: Training MAE: 1.186.
2024-06-21 17:59:33,848 - INFO: Training MSE: 2.644.
2024-06-21 17:59:54,268 - INFO: Epoch: 65/200, Loss_train: 1.198356803121238, Loss_val: 2.3052204888442467
2024-06-21 17:59:54,268 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:59:54,268 - INFO: Epoch 66/200...
2024-06-21 17:59:54,268 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:59:54,268 - INFO: Batch size: 32.
2024-06-21 17:59:54,272 - INFO: Dataset:
2024-06-21 17:59:54,272 - INFO: Batch size:
2024-06-21 17:59:54,272 - INFO: Number of workers:
2024-06-21 17:59:55,345 - INFO: Epoch: 66/200, Batch: 1/29, Batch_Loss_Train: 1.088
2024-06-21 17:59:55,663 - INFO: Epoch: 66/200, Batch: 2/29, Batch_Loss_Train: 1.218
2024-06-21 17:59:56,054 - INFO: Epoch: 66/200, Batch: 3/29, Batch_Loss_Train: 1.086
2024-06-21 17:59:56,371 - INFO: Epoch: 66/200, Batch: 4/29, Batch_Loss_Train: 1.094
2024-06-21 17:59:56,763 - INFO: Epoch: 66/200, Batch: 5/29, Batch_Loss_Train: 1.122
2024-06-21 17:59:57,090 - INFO: Epoch: 66/200, Batch: 6/29, Batch_Loss_Train: 1.178
2024-06-21 17:59:57,481 - INFO: Epoch: 66/200, Batch: 7/29, Batch_Loss_Train: 1.105
2024-06-21 17:59:57,796 - INFO: Epoch: 66/200, Batch: 8/29, Batch_Loss_Train: 1.197
2024-06-21 17:59:58,178 - INFO: Epoch: 66/200, Batch: 9/29, Batch_Loss_Train: 1.288
2024-06-21 17:59:58,511 - INFO: Epoch: 66/200, Batch: 10/29, Batch_Loss_Train: 1.184
2024-06-21 17:59:58,873 - INFO: Epoch: 66/200, Batch: 11/29, Batch_Loss_Train: 1.371
2024-06-21 17:59:59,191 - INFO: Epoch: 66/200, Batch: 12/29, Batch_Loss_Train: 0.987
2024-06-21 17:59:59,599 - INFO: Epoch: 66/200, Batch: 13/29, Batch_Loss_Train: 1.142
2024-06-21 17:59:59,928 - INFO: Epoch: 66/200, Batch: 14/29, Batch_Loss_Train: 1.095
2024-06-21 18:00:00,327 - INFO: Epoch: 66/200, Batch: 15/29, Batch_Loss_Train: 1.322
2024-06-21 18:00:00,640 - INFO: Epoch: 66/200, Batch: 16/29, Batch_Loss_Train: 1.308
2024-06-21 18:00:01,043 - INFO: Epoch: 66/200, Batch: 17/29, Batch_Loss_Train: 0.910
2024-06-21 18:00:01,369 - INFO: Epoch: 66/200, Batch: 18/29, Batch_Loss_Train: 1.101
2024-06-21 18:00:01,756 - INFO: Epoch: 66/200, Batch: 19/29, Batch_Loss_Train: 1.196
2024-06-21 18:00:02,063 - INFO: Epoch: 66/200, Batch: 20/29, Batch_Loss_Train: 1.029
2024-06-21 18:00:02,455 - INFO: Epoch: 66/200, Batch: 21/29, Batch_Loss_Train: 0.966
2024-06-21 18:00:02,782 - INFO: Epoch: 66/200, Batch: 22/29, Batch_Loss_Train: 1.043
2024-06-21 18:00:03,156 - INFO: Epoch: 66/200, Batch: 23/29, Batch_Loss_Train: 1.033
2024-06-21 18:00:03,471 - INFO: Epoch: 66/200, Batch: 24/29, Batch_Loss_Train: 1.148
2024-06-21 18:00:03,857 - INFO: Epoch: 66/200, Batch: 25/29, Batch_Loss_Train: 1.224
2024-06-21 18:00:04,179 - INFO: Epoch: 66/200, Batch: 26/29, Batch_Loss_Train: 1.101
2024-06-21 18:00:04,547 - INFO: Epoch: 66/200, Batch: 27/29, Batch_Loss_Train: 1.140
2024-06-21 18:00:04,857 - INFO: Epoch: 66/200, Batch: 28/29, Batch_Loss_Train: 1.253
2024-06-21 18:00:05,068 - INFO: Epoch: 66/200, Batch: 29/29, Batch_Loss_Train: 1.110
2024-06-21 18:00:16,218 - INFO: 66/200 final results:
2024-06-21 18:00:16,218 - INFO: Training loss: 1.139.
2024-06-21 18:00:16,218 - INFO: Training MAE: 1.140.
2024-06-21 18:00:16,218 - INFO: Training MSE: 2.429.
2024-06-21 18:00:36,806 - INFO: Epoch: 66/200, Loss_train: 1.1391984100999504, Loss_val: 2.3081327800093026
2024-06-21 18:00:36,806 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 18:00:36,806 - INFO: Epoch 67/200...
2024-06-21 18:00:36,806 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:00:36,806 - INFO: Batch size: 32.
2024-06-21 18:00:36,809 - INFO: Dataset:
2024-06-21 18:00:36,810 - INFO: Batch size:
2024-06-21 18:00:36,810 - INFO: Number of workers:
2024-06-21 18:00:37,887 - INFO: Epoch: 67/200, Batch: 1/29, Batch_Loss_Train: 1.130
2024-06-21 18:00:38,204 - INFO: Epoch: 67/200, Batch: 2/29, Batch_Loss_Train: 1.307
2024-06-21 18:00:38,597 - INFO: Epoch: 67/200, Batch: 3/29, Batch_Loss_Train: 0.952
2024-06-21 18:00:38,913 - INFO: Epoch: 67/200, Batch: 4/29, Batch_Loss_Train: 1.209
2024-06-21 18:00:39,335 - INFO: Epoch: 67/200, Batch: 5/29, Batch_Loss_Train: 1.169
2024-06-21 18:00:39,633 - INFO: Epoch: 67/200, Batch: 6/29, Batch_Loss_Train: 0.999
2024-06-21 18:00:40,017 - INFO: Epoch: 67/200, Batch: 7/29, Batch_Loss_Train: 0.968
2024-06-21 18:00:40,329 - INFO: Epoch: 67/200, Batch: 8/29, Batch_Loss_Train: 0.940
2024-06-21 18:00:40,755 - INFO: Epoch: 67/200, Batch: 9/29, Batch_Loss_Train: 1.204
2024-06-21 18:00:41,046 - INFO: Epoch: 67/200, Batch: 10/29, Batch_Loss_Train: 0.905
2024-06-21 18:00:41,416 - INFO: Epoch: 67/200, Batch: 11/29, Batch_Loss_Train: 0.884
2024-06-21 18:00:41,731 - INFO: Epoch: 67/200, Batch: 12/29, Batch_Loss_Train: 0.961
2024-06-21 18:00:42,160 - INFO: Epoch: 67/200, Batch: 13/29, Batch_Loss_Train: 1.075
2024-06-21 18:00:42,464 - INFO: Epoch: 67/200, Batch: 14/29, Batch_Loss_Train: 1.107
2024-06-21 18:00:42,858 - INFO: Epoch: 67/200, Batch: 15/29, Batch_Loss_Train: 0.926
2024-06-21 18:00:43,170 - INFO: Epoch: 67/200, Batch: 16/29, Batch_Loss_Train: 1.084
2024-06-21 18:00:43,600 - INFO: Epoch: 67/200, Batch: 17/29, Batch_Loss_Train: 1.044
2024-06-21 18:00:43,901 - INFO: Epoch: 67/200, Batch: 18/29, Batch_Loss_Train: 1.111
2024-06-21 18:00:44,277 - INFO: Epoch: 67/200, Batch: 19/29, Batch_Loss_Train: 0.924
2024-06-21 18:00:44,585 - INFO: Epoch: 67/200, Batch: 20/29, Batch_Loss_Train: 0.912
2024-06-21 18:00:44,998 - INFO: Epoch: 67/200, Batch: 21/29, Batch_Loss_Train: 0.713
2024-06-21 18:00:45,301 - INFO: Epoch: 67/200, Batch: 22/29, Batch_Loss_Train: 0.962
2024-06-21 18:00:45,688 - INFO: Epoch: 67/200, Batch: 23/29, Batch_Loss_Train: 0.975
2024-06-21 18:00:46,004 - INFO: Epoch: 67/200, Batch: 24/29, Batch_Loss_Train: 0.884
2024-06-21 18:00:46,422 - INFO: Epoch: 67/200, Batch: 25/29, Batch_Loss_Train: 1.309
2024-06-21 18:00:46,720 - INFO: Epoch: 67/200, Batch: 26/29, Batch_Loss_Train: 1.122
2024-06-21 18:00:47,107 - INFO: Epoch: 67/200, Batch: 27/29, Batch_Loss_Train: 0.924
2024-06-21 18:00:47,418 - INFO: Epoch: 67/200, Batch: 28/29, Batch_Loss_Train: 1.022
2024-06-21 18:00:47,639 - INFO: Epoch: 67/200, Batch: 29/29, Batch_Loss_Train: 1.196
2024-06-21 18:00:58,789 - INFO: 67/200 final results:
2024-06-21 18:00:58,789 - INFO: Training loss: 1.032.
2024-06-21 18:00:58,789 - INFO: Training MAE: 1.028.
2024-06-21 18:00:58,789 - INFO: Training MSE: 2.091.
2024-06-21 18:01:18,772 - INFO: Epoch: 67/200, Loss_train: 1.031591670266513, Loss_val: 1.9057752222850406
2024-06-21 18:01:18,791 - INFO: Saved new best metric model for epoch 67.
2024-06-21 18:01:18,791 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:01:18,791 - INFO: Epoch 68/200...
2024-06-21 18:01:18,791 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:01:18,791 - INFO: Batch size: 32.
2024-06-21 18:01:18,795 - INFO: Dataset:
2024-06-21 18:01:18,795 - INFO: Batch size:
2024-06-21 18:01:18,795 - INFO: Number of workers:
2024-06-21 18:01:19,858 - INFO: Epoch: 68/200, Batch: 1/29, Batch_Loss_Train: 1.083
2024-06-21 18:01:20,175 - INFO: Epoch: 68/200, Batch: 2/29, Batch_Loss_Train: 1.180
2024-06-21 18:01:20,570 - INFO: Epoch: 68/200, Batch: 3/29, Batch_Loss_Train: 0.959
2024-06-21 18:01:20,884 - INFO: Epoch: 68/200, Batch: 4/29, Batch_Loss_Train: 1.053
2024-06-21 18:01:21,278 - INFO: Epoch: 68/200, Batch: 5/29, Batch_Loss_Train: 0.939
2024-06-21 18:01:21,589 - INFO: Epoch: 68/200, Batch: 6/29, Batch_Loss_Train: 1.315
2024-06-21 18:01:21,981 - INFO: Epoch: 68/200, Batch: 7/29, Batch_Loss_Train: 0.785
2024-06-21 18:01:22,293 - INFO: Epoch: 68/200, Batch: 8/29, Batch_Loss_Train: 1.096
2024-06-21 18:01:22,675 - INFO: Epoch: 68/200, Batch: 9/29, Batch_Loss_Train: 0.929
2024-06-21 18:01:22,979 - INFO: Epoch: 68/200, Batch: 10/29, Batch_Loss_Train: 0.992
2024-06-21 18:01:23,361 - INFO: Epoch: 68/200, Batch: 11/29, Batch_Loss_Train: 1.017
2024-06-21 18:01:23,676 - INFO: Epoch: 68/200, Batch: 12/29, Batch_Loss_Train: 0.912
2024-06-21 18:01:24,074 - INFO: Epoch: 68/200, Batch: 13/29, Batch_Loss_Train: 1.173
2024-06-21 18:01:24,390 - INFO: Epoch: 68/200, Batch: 14/29, Batch_Loss_Train: 0.870
2024-06-21 18:01:24,792 - INFO: Epoch: 68/200, Batch: 15/29, Batch_Loss_Train: 1.063
2024-06-21 18:01:25,103 - INFO: Epoch: 68/200, Batch: 16/29, Batch_Loss_Train: 1.066
2024-06-21 18:01:25,490 - INFO: Epoch: 68/200, Batch: 17/29, Batch_Loss_Train: 0.941
2024-06-21 18:01:25,801 - INFO: Epoch: 68/200, Batch: 18/29, Batch_Loss_Train: 0.781
2024-06-21 18:01:26,196 - INFO: Epoch: 68/200, Batch: 19/29, Batch_Loss_Train: 0.969
2024-06-21 18:01:26,501 - INFO: Epoch: 68/200, Batch: 20/29, Batch_Loss_Train: 1.188
2024-06-21 18:01:26,891 - INFO: Epoch: 68/200, Batch: 21/29, Batch_Loss_Train: 0.809
2024-06-21 18:01:27,203 - INFO: Epoch: 68/200, Batch: 22/29, Batch_Loss_Train: 1.046
2024-06-21 18:01:27,600 - INFO: Epoch: 68/200, Batch: 23/29, Batch_Loss_Train: 0.858
2024-06-21 18:01:27,913 - INFO: Epoch: 68/200, Batch: 24/29, Batch_Loss_Train: 1.005
2024-06-21 18:01:28,303 - INFO: Epoch: 68/200, Batch: 25/29, Batch_Loss_Train: 0.927
2024-06-21 18:01:28,612 - INFO: Epoch: 68/200, Batch: 26/29, Batch_Loss_Train: 1.390
2024-06-21 18:01:28,998 - INFO: Epoch: 68/200, Batch: 27/29, Batch_Loss_Train: 0.940
2024-06-21 18:01:29,306 - INFO: Epoch: 68/200, Batch: 28/29, Batch_Loss_Train: 1.012
2024-06-21 18:01:29,523 - INFO: Epoch: 68/200, Batch: 29/29, Batch_Loss_Train: 1.299
2024-06-21 18:01:40,488 - INFO: 68/200 final results:
2024-06-21 18:01:40,488 - INFO: Training loss: 1.021.
2024-06-21 18:01:40,488 - INFO: Training MAE: 1.015.
2024-06-21 18:01:40,488 - INFO: Training MSE: 1.961.
2024-06-21 18:02:00,936 - INFO: Epoch: 68/200, Loss_train: 1.0206950389105698, Loss_val: 2.039431666505748
2024-06-21 18:02:00,936 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:02:00,936 - INFO: Epoch 69/200...
2024-06-21 18:02:00,936 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:02:00,936 - INFO: Batch size: 32.
2024-06-21 18:02:00,940 - INFO: Dataset:
2024-06-21 18:02:00,940 - INFO: Batch size:
2024-06-21 18:02:00,940 - INFO: Number of workers:
2024-06-21 18:02:02,007 - INFO: Epoch: 69/200, Batch: 1/29, Batch_Loss_Train: 1.228
2024-06-21 18:02:02,328 - INFO: Epoch: 69/200, Batch: 2/29, Batch_Loss_Train: 0.915
2024-06-21 18:02:02,725 - INFO: Epoch: 69/200, Batch: 3/29, Batch_Loss_Train: 0.942
2024-06-21 18:02:03,043 - INFO: Epoch: 69/200, Batch: 4/29, Batch_Loss_Train: 1.145
2024-06-21 18:02:03,445 - INFO: Epoch: 69/200, Batch: 5/29, Batch_Loss_Train: 1.019
2024-06-21 18:02:03,761 - INFO: Epoch: 69/200, Batch: 6/29, Batch_Loss_Train: 0.939
2024-06-21 18:02:04,147 - INFO: Epoch: 69/200, Batch: 7/29, Batch_Loss_Train: 0.992
2024-06-21 18:02:04,461 - INFO: Epoch: 69/200, Batch: 8/29, Batch_Loss_Train: 1.200
2024-06-21 18:02:04,851 - INFO: Epoch: 69/200, Batch: 9/29, Batch_Loss_Train: 0.797
2024-06-21 18:02:05,158 - INFO: Epoch: 69/200, Batch: 10/29, Batch_Loss_Train: 0.977
2024-06-21 18:02:05,534 - INFO: Epoch: 69/200, Batch: 11/29, Batch_Loss_Train: 0.946
2024-06-21 18:02:05,850 - INFO: Epoch: 69/200, Batch: 12/29, Batch_Loss_Train: 1.148
2024-06-21 18:02:06,256 - INFO: Epoch: 69/200, Batch: 13/29, Batch_Loss_Train: 0.753
2024-06-21 18:02:06,574 - INFO: Epoch: 69/200, Batch: 14/29, Batch_Loss_Train: 0.894
2024-06-21 18:02:06,978 - INFO: Epoch: 69/200, Batch: 15/29, Batch_Loss_Train: 0.875
2024-06-21 18:02:07,291 - INFO: Epoch: 69/200, Batch: 16/29, Batch_Loss_Train: 0.955
2024-06-21 18:02:07,692 - INFO: Epoch: 69/200, Batch: 17/29, Batch_Loss_Train: 0.911
2024-06-21 18:02:08,006 - INFO: Epoch: 69/200, Batch: 18/29, Batch_Loss_Train: 1.186
2024-06-21 18:02:08,396 - INFO: Epoch: 69/200, Batch: 19/29, Batch_Loss_Train: 0.954
2024-06-21 18:02:08,703 - INFO: Epoch: 69/200, Batch: 20/29, Batch_Loss_Train: 1.009
2024-06-21 18:02:09,095 - INFO: Epoch: 69/200, Batch: 21/29, Batch_Loss_Train: 0.916
2024-06-21 18:02:09,411 - INFO: Epoch: 69/200, Batch: 22/29, Batch_Loss_Train: 0.765
2024-06-21 18:02:09,807 - INFO: Epoch: 69/200, Batch: 23/29, Batch_Loss_Train: 0.977
2024-06-21 18:02:10,123 - INFO: Epoch: 69/200, Batch: 24/29, Batch_Loss_Train: 1.080
2024-06-21 18:02:10,517 - INFO: Epoch: 69/200, Batch: 25/29, Batch_Loss_Train: 0.981
2024-06-21 18:02:10,828 - INFO: Epoch: 69/200, Batch: 26/29, Batch_Loss_Train: 0.916
2024-06-21 18:02:11,223 - INFO: Epoch: 69/200, Batch: 27/29, Batch_Loss_Train: 0.994
2024-06-21 18:02:11,534 - INFO: Epoch: 69/200, Batch: 28/29, Batch_Loss_Train: 0.920
2024-06-21 18:02:11,747 - INFO: Epoch: 69/200, Batch: 29/29, Batch_Loss_Train: 0.856
2024-06-21 18:02:22,932 - INFO: 69/200 final results:
2024-06-21 18:02:22,932 - INFO: Training loss: 0.972.
2024-06-21 18:02:22,933 - INFO: Training MAE: 0.974.
2024-06-21 18:02:22,933 - INFO: Training MSE: 1.873.
2024-06-21 18:02:43,165 - INFO: Epoch: 69/200, Loss_train: 0.9720834431977108, Loss_val: 1.9229647661077565
2024-06-21 18:02:43,165 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:02:43,165 - INFO: Epoch 70/200...
2024-06-21 18:02:43,165 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:02:43,165 - INFO: Batch size: 32.
2024-06-21 18:02:43,169 - INFO: Dataset:
2024-06-21 18:02:43,169 - INFO: Batch size:
2024-06-21 18:02:43,169 - INFO: Number of workers:
2024-06-21 18:02:44,244 - INFO: Epoch: 70/200, Batch: 1/29, Batch_Loss_Train: 0.886
2024-06-21 18:02:44,549 - INFO: Epoch: 70/200, Batch: 2/29, Batch_Loss_Train: 0.805
2024-06-21 18:02:44,930 - INFO: Epoch: 70/200, Batch: 3/29, Batch_Loss_Train: 0.955
2024-06-21 18:02:45,247 - INFO: Epoch: 70/200, Batch: 4/29, Batch_Loss_Train: 0.852
2024-06-21 18:02:45,658 - INFO: Epoch: 70/200, Batch: 5/29, Batch_Loss_Train: 0.930
2024-06-21 18:02:45,957 - INFO: Epoch: 70/200, Batch: 6/29, Batch_Loss_Train: 0.899
2024-06-21 18:02:46,329 - INFO: Epoch: 70/200, Batch: 7/29, Batch_Loss_Train: 1.036
2024-06-21 18:02:46,640 - INFO: Epoch: 70/200, Batch: 8/29, Batch_Loss_Train: 1.031
2024-06-21 18:02:47,051 - INFO: Epoch: 70/200, Batch: 9/29, Batch_Loss_Train: 0.958
2024-06-21 18:02:47,343 - INFO: Epoch: 70/200, Batch: 10/29, Batch_Loss_Train: 0.965
2024-06-21 18:02:47,703 - INFO: Epoch: 70/200, Batch: 11/29, Batch_Loss_Train: 1.119
2024-06-21 18:02:48,017 - INFO: Epoch: 70/200, Batch: 12/29, Batch_Loss_Train: 1.085
2024-06-21 18:02:48,443 - INFO: Epoch: 70/200, Batch: 13/29, Batch_Loss_Train: 0.918
2024-06-21 18:02:48,746 - INFO: Epoch: 70/200, Batch: 14/29, Batch_Loss_Train: 1.014
2024-06-21 18:02:49,132 - INFO: Epoch: 70/200, Batch: 15/29, Batch_Loss_Train: 1.016
2024-06-21 18:02:49,446 - INFO: Epoch: 70/200, Batch: 16/29, Batch_Loss_Train: 0.775
2024-06-21 18:02:49,860 - INFO: Epoch: 70/200, Batch: 17/29, Batch_Loss_Train: 0.892
2024-06-21 18:02:50,160 - INFO: Epoch: 70/200, Batch: 18/29, Batch_Loss_Train: 0.800
2024-06-21 18:02:50,536 - INFO: Epoch: 70/200, Batch: 19/29, Batch_Loss_Train: 0.843
2024-06-21 18:02:50,845 - INFO: Epoch: 70/200, Batch: 20/29, Batch_Loss_Train: 0.917
2024-06-21 18:02:51,249 - INFO: Epoch: 70/200, Batch: 21/29, Batch_Loss_Train: 1.070
2024-06-21 18:02:51,551 - INFO: Epoch: 70/200, Batch: 22/29, Batch_Loss_Train: 1.097
2024-06-21 18:02:51,926 - INFO: Epoch: 70/200, Batch: 23/29, Batch_Loss_Train: 0.915
2024-06-21 18:02:52,240 - INFO: Epoch: 70/200, Batch: 24/29, Batch_Loss_Train: 0.985
2024-06-21 18:02:52,651 - INFO: Epoch: 70/200, Batch: 25/29, Batch_Loss_Train: 0.978
2024-06-21 18:02:52,949 - INFO: Epoch: 70/200, Batch: 26/29, Batch_Loss_Train: 0.895
2024-06-21 18:02:53,317 - INFO: Epoch: 70/200, Batch: 27/29, Batch_Loss_Train: 0.895
2024-06-21 18:02:53,627 - INFO: Epoch: 70/200, Batch: 28/29, Batch_Loss_Train: 1.116
2024-06-21 18:02:53,835 - INFO: Epoch: 70/200, Batch: 29/29, Batch_Loss_Train: 1.126
2024-06-21 18:03:05,030 - INFO: 70/200 final results:
2024-06-21 18:03:05,030 - INFO: Training loss: 0.958.
2024-06-21 18:03:05,030 - INFO: Training MAE: 0.954.
2024-06-21 18:03:05,030 - INFO: Training MSE: 1.826.
2024-06-21 18:03:25,501 - INFO: Epoch: 70/200, Loss_train: 0.9577394292272371, Loss_val: 1.9450677633285522
2024-06-21 18:03:25,501 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:03:25,501 - INFO: Epoch 71/200...
2024-06-21 18:03:25,501 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:03:25,501 - INFO: Batch size: 32.
2024-06-21 18:03:25,505 - INFO: Dataset:
2024-06-21 18:03:25,505 - INFO: Batch size:
2024-06-21 18:03:25,505 - INFO: Number of workers:
2024-06-21 18:03:26,568 - INFO: Epoch: 71/200, Batch: 1/29, Batch_Loss_Train: 1.190
2024-06-21 18:03:26,886 - INFO: Epoch: 71/200, Batch: 2/29, Batch_Loss_Train: 0.984
2024-06-21 18:03:27,294 - INFO: Epoch: 71/200, Batch: 3/29, Batch_Loss_Train: 1.016
2024-06-21 18:03:27,610 - INFO: Epoch: 71/200, Batch: 4/29, Batch_Loss_Train: 1.039
2024-06-21 18:03:28,002 - INFO: Epoch: 71/200, Batch: 5/29, Batch_Loss_Train: 0.921
2024-06-21 18:03:28,313 - INFO: Epoch: 71/200, Batch: 6/29, Batch_Loss_Train: 1.047
2024-06-21 18:03:28,711 - INFO: Epoch: 71/200, Batch: 7/29, Batch_Loss_Train: 1.025
2024-06-21 18:03:29,023 - INFO: Epoch: 71/200, Batch: 8/29, Batch_Loss_Train: 0.827
2024-06-21 18:03:29,405 - INFO: Epoch: 71/200, Batch: 9/29, Batch_Loss_Train: 0.956
2024-06-21 18:03:29,709 - INFO: Epoch: 71/200, Batch: 10/29, Batch_Loss_Train: 0.840
2024-06-21 18:03:30,095 - INFO: Epoch: 71/200, Batch: 11/29, Batch_Loss_Train: 0.951
2024-06-21 18:03:30,408 - INFO: Epoch: 71/200, Batch: 12/29, Batch_Loss_Train: 0.918
2024-06-21 18:03:30,806 - INFO: Epoch: 71/200, Batch: 13/29, Batch_Loss_Train: 1.008
2024-06-21 18:03:31,120 - INFO: Epoch: 71/200, Batch: 14/29, Batch_Loss_Train: 0.823
2024-06-21 18:03:31,522 - INFO: Epoch: 71/200, Batch: 15/29, Batch_Loss_Train: 0.877
2024-06-21 18:03:31,833 - INFO: Epoch: 71/200, Batch: 16/29, Batch_Loss_Train: 0.907
2024-06-21 18:03:32,232 - INFO: Epoch: 71/200, Batch: 17/29, Batch_Loss_Train: 0.842
2024-06-21 18:03:32,543 - INFO: Epoch: 71/200, Batch: 18/29, Batch_Loss_Train: 0.879
2024-06-21 18:03:32,938 - INFO: Epoch: 71/200, Batch: 19/29, Batch_Loss_Train: 0.947
2024-06-21 18:03:33,244 - INFO: Epoch: 71/200, Batch: 20/29, Batch_Loss_Train: 0.898
2024-06-21 18:03:33,630 - INFO: Epoch: 71/200, Batch: 21/29, Batch_Loss_Train: 0.907
2024-06-21 18:03:33,943 - INFO: Epoch: 71/200, Batch: 22/29, Batch_Loss_Train: 0.861
2024-06-21 18:03:34,326 - INFO: Epoch: 71/200, Batch: 23/29, Batch_Loss_Train: 0.874
2024-06-21 18:03:34,638 - INFO: Epoch: 71/200, Batch: 24/29, Batch_Loss_Train: 1.159
2024-06-21 18:03:35,028 - INFO: Epoch: 71/200, Batch: 25/29, Batch_Loss_Train: 1.015
2024-06-21 18:03:35,338 - INFO: Epoch: 71/200, Batch: 26/29, Batch_Loss_Train: 0.892
2024-06-21 18:03:35,726 - INFO: Epoch: 71/200, Batch: 27/29, Batch_Loss_Train: 0.988
2024-06-21 18:03:36,035 - INFO: Epoch: 71/200, Batch: 28/29, Batch_Loss_Train: 0.842
2024-06-21 18:03:36,250 - INFO: Epoch: 71/200, Batch: 29/29, Batch_Loss_Train: 1.284
2024-06-21 18:03:47,259 - INFO: 71/200 final results:
2024-06-21 18:03:47,259 - INFO: Training loss: 0.956.
2024-06-21 18:03:47,259 - INFO: Training MAE: 0.949.
2024-06-21 18:03:47,259 - INFO: Training MSE: 1.785.
2024-06-21 18:04:07,438 - INFO: Epoch: 71/200, Loss_train: 0.9558101049784956, Loss_val: 2.0477164778216133
2024-06-21 18:04:07,438 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:04:07,438 - INFO: Epoch 72/200...
2024-06-21 18:04:07,438 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:04:07,438 - INFO: Batch size: 32.
2024-06-21 18:04:07,442 - INFO: Dataset:
2024-06-21 18:04:07,442 - INFO: Batch size:
2024-06-21 18:04:07,442 - INFO: Number of workers:
2024-06-21 18:04:08,508 - INFO: Epoch: 72/200, Batch: 1/29, Batch_Loss_Train: 1.201
2024-06-21 18:04:08,830 - INFO: Epoch: 72/200, Batch: 2/29, Batch_Loss_Train: 0.943
2024-06-21 18:04:09,251 - INFO: Epoch: 72/200, Batch: 3/29, Batch_Loss_Train: 0.953
2024-06-21 18:04:09,556 - INFO: Epoch: 72/200, Batch: 4/29, Batch_Loss_Train: 1.017
2024-06-21 18:04:09,964 - INFO: Epoch: 72/200, Batch: 5/29, Batch_Loss_Train: 0.738
2024-06-21 18:04:10,265 - INFO: Epoch: 72/200, Batch: 6/29, Batch_Loss_Train: 0.846
2024-06-21 18:04:10,674 - INFO: Epoch: 72/200, Batch: 7/29, Batch_Loss_Train: 1.107
2024-06-21 18:04:10,976 - INFO: Epoch: 72/200, Batch: 8/29, Batch_Loss_Train: 0.759
2024-06-21 18:04:11,389 - INFO: Epoch: 72/200, Batch: 9/29, Batch_Loss_Train: 0.818
2024-06-21 18:04:11,684 - INFO: Epoch: 72/200, Batch: 10/29, Batch_Loss_Train: 0.963
2024-06-21 18:04:12,088 - INFO: Epoch: 72/200, Batch: 11/29, Batch_Loss_Train: 0.872
2024-06-21 18:04:12,391 - INFO: Epoch: 72/200, Batch: 12/29, Batch_Loss_Train: 1.009
2024-06-21 18:04:12,809 - INFO: Epoch: 72/200, Batch: 13/29, Batch_Loss_Train: 0.812
2024-06-21 18:04:13,112 - INFO: Epoch: 72/200, Batch: 14/29, Batch_Loss_Train: 0.956
2024-06-21 18:04:13,536 - INFO: Epoch: 72/200, Batch: 15/29, Batch_Loss_Train: 1.086
2024-06-21 18:04:13,836 - INFO: Epoch: 72/200, Batch: 16/29, Batch_Loss_Train: 0.874
2024-06-21 18:04:14,234 - INFO: Epoch: 72/200, Batch: 17/29, Batch_Loss_Train: 0.906
2024-06-21 18:04:14,535 - INFO: Epoch: 72/200, Batch: 18/29, Batch_Loss_Train: 0.843
2024-06-21 18:04:14,953 - INFO: Epoch: 72/200, Batch: 19/29, Batch_Loss_Train: 0.806
2024-06-21 18:04:15,248 - INFO: Epoch: 72/200, Batch: 20/29, Batch_Loss_Train: 0.793
2024-06-21 18:04:15,644 - INFO: Epoch: 72/200, Batch: 21/29, Batch_Loss_Train: 1.074
2024-06-21 18:04:15,946 - INFO: Epoch: 72/200, Batch: 22/29, Batch_Loss_Train: 1.084
2024-06-21 18:04:16,367 - INFO: Epoch: 72/200, Batch: 23/29, Batch_Loss_Train: 0.904
2024-06-21 18:04:16,670 - INFO: Epoch: 72/200, Batch: 24/29, Batch_Loss_Train: 0.911
2024-06-21 18:04:17,068 - INFO: Epoch: 72/200, Batch: 25/29, Batch_Loss_Train: 0.999
2024-06-21 18:04:17,366 - INFO: Epoch: 72/200, Batch: 26/29, Batch_Loss_Train: 0.991
2024-06-21 18:04:17,778 - INFO: Epoch: 72/200, Batch: 27/29, Batch_Loss_Train: 0.945
2024-06-21 18:04:18,077 - INFO: Epoch: 72/200, Batch: 28/29, Batch_Loss_Train: 0.907
2024-06-21 18:04:18,292 - INFO: Epoch: 72/200, Batch: 29/29, Batch_Loss_Train: 1.037
2024-06-21 18:04:29,062 - INFO: 72/200 final results:
2024-06-21 18:04:29,062 - INFO: Training loss: 0.936.
2024-06-21 18:04:29,062 - INFO: Training MAE: 0.934.
2024-06-21 18:04:29,062 - INFO: Training MSE: 1.715.
2024-06-21 18:04:49,311 - INFO: Epoch: 72/200, Loss_train: 0.9363737229643196, Loss_val: 1.9617830761547745
2024-06-21 18:04:49,311 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:04:49,311 - INFO: Epoch 73/200...
2024-06-21 18:04:49,311 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:04:49,311 - INFO: Batch size: 32.
2024-06-21 18:04:49,315 - INFO: Dataset:
2024-06-21 18:04:49,315 - INFO: Batch size:
2024-06-21 18:04:49,315 - INFO: Number of workers:
2024-06-21 18:04:50,373 - INFO: Epoch: 73/200, Batch: 1/29, Batch_Loss_Train: 0.824
2024-06-21 18:04:50,689 - INFO: Epoch: 73/200, Batch: 2/29, Batch_Loss_Train: 0.883
2024-06-21 18:04:51,099 - INFO: Epoch: 73/200, Batch: 3/29, Batch_Loss_Train: 0.844
2024-06-21 18:04:51,420 - INFO: Epoch: 73/200, Batch: 4/29, Batch_Loss_Train: 0.905
2024-06-21 18:04:51,836 - INFO: Epoch: 73/200, Batch: 5/29, Batch_Loss_Train: 0.861
2024-06-21 18:04:52,139 - INFO: Epoch: 73/200, Batch: 6/29, Batch_Loss_Train: 0.942
2024-06-21 18:04:52,540 - INFO: Epoch: 73/200, Batch: 7/29, Batch_Loss_Train: 1.098
2024-06-21 18:04:52,856 - INFO: Epoch: 73/200, Batch: 8/29, Batch_Loss_Train: 0.778
2024-06-21 18:04:53,266 - INFO: Epoch: 73/200, Batch: 9/29, Batch_Loss_Train: 0.753
2024-06-21 18:04:53,560 - INFO: Epoch: 73/200, Batch: 10/29, Batch_Loss_Train: 0.855
2024-06-21 18:04:53,954 - INFO: Epoch: 73/200, Batch: 11/29, Batch_Loss_Train: 1.079
2024-06-21 18:04:54,272 - INFO: Epoch: 73/200, Batch: 12/29, Batch_Loss_Train: 1.197
2024-06-21 18:04:54,695 - INFO: Epoch: 73/200, Batch: 13/29, Batch_Loss_Train: 0.857
2024-06-21 18:04:54,999 - INFO: Epoch: 73/200, Batch: 14/29, Batch_Loss_Train: 1.001
2024-06-21 18:04:55,398 - INFO: Epoch: 73/200, Batch: 15/29, Batch_Loss_Train: 0.819
2024-06-21 18:04:55,709 - INFO: Epoch: 73/200, Batch: 16/29, Batch_Loss_Train: 0.757
2024-06-21 18:04:56,127 - INFO: Epoch: 73/200, Batch: 17/29, Batch_Loss_Train: 1.107
2024-06-21 18:04:56,426 - INFO: Epoch: 73/200, Batch: 18/29, Batch_Loss_Train: 1.052
2024-06-21 18:04:56,819 - INFO: Epoch: 73/200, Batch: 19/29, Batch_Loss_Train: 1.083
2024-06-21 18:04:57,128 - INFO: Epoch: 73/200, Batch: 20/29, Batch_Loss_Train: 0.930
2024-06-21 18:04:57,526 - INFO: Epoch: 73/200, Batch: 21/29, Batch_Loss_Train: 1.106
2024-06-21 18:04:57,831 - INFO: Epoch: 73/200, Batch: 22/29, Batch_Loss_Train: 0.926
2024-06-21 18:04:58,224 - INFO: Epoch: 73/200, Batch: 23/29, Batch_Loss_Train: 1.102
2024-06-21 18:04:58,540 - INFO: Epoch: 73/200, Batch: 24/29, Batch_Loss_Train: 0.886
2024-06-21 18:04:58,941 - INFO: Epoch: 73/200, Batch: 25/29, Batch_Loss_Train: 1.092
2024-06-21 18:04:59,240 - INFO: Epoch: 73/200, Batch: 26/29, Batch_Loss_Train: 0.879
2024-06-21 18:04:59,639 - INFO: Epoch: 73/200, Batch: 27/29, Batch_Loss_Train: 1.101
2024-06-21 18:04:59,950 - INFO: Epoch: 73/200, Batch: 28/29, Batch_Loss_Train: 0.889
2024-06-21 18:05:00,159 - INFO: Epoch: 73/200, Batch: 29/29, Batch_Loss_Train: 0.938
2024-06-21 18:05:11,286 - INFO: 73/200 final results:
2024-06-21 18:05:11,286 - INFO: Training loss: 0.950.
2024-06-21 18:05:11,286 - INFO: Training MAE: 0.950.
2024-06-21 18:05:11,286 - INFO: Training MSE: 1.809.
2024-06-21 18:05:31,808 - INFO: Epoch: 73/200, Loss_train: 0.9497855840058163, Loss_val: 2.017741051213495
2024-06-21 18:05:31,808 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:05:31,808 - INFO: Epoch 74/200...
2024-06-21 18:05:31,808 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:05:31,808 - INFO: Batch size: 32.
2024-06-21 18:05:31,811 - INFO: Dataset:
2024-06-21 18:05:31,812 - INFO: Batch size:
2024-06-21 18:05:31,812 - INFO: Number of workers:
2024-06-21 18:05:32,867 - INFO: Epoch: 74/200, Batch: 1/29, Batch_Loss_Train: 0.911
2024-06-21 18:05:33,185 - INFO: Epoch: 74/200, Batch: 2/29, Batch_Loss_Train: 1.043
2024-06-21 18:05:33,598 - INFO: Epoch: 74/200, Batch: 3/29, Batch_Loss_Train: 0.773
2024-06-21 18:05:33,902 - INFO: Epoch: 74/200, Batch: 4/29, Batch_Loss_Train: 1.036
2024-06-21 18:05:34,301 - INFO: Epoch: 74/200, Batch: 5/29, Batch_Loss_Train: 0.753
2024-06-21 18:05:34,613 - INFO: Epoch: 74/200, Batch: 6/29, Batch_Loss_Train: 0.837
2024-06-21 18:05:35,021 - INFO: Epoch: 74/200, Batch: 7/29, Batch_Loss_Train: 0.758
2024-06-21 18:05:35,321 - INFO: Epoch: 74/200, Batch: 8/29, Batch_Loss_Train: 0.874
2024-06-21 18:05:35,711 - INFO: Epoch: 74/200, Batch: 9/29, Batch_Loss_Train: 1.017
2024-06-21 18:05:36,017 - INFO: Epoch: 74/200, Batch: 10/29, Batch_Loss_Train: 0.798
2024-06-21 18:05:36,418 - INFO: Epoch: 74/200, Batch: 11/29, Batch_Loss_Train: 1.118
2024-06-21 18:05:36,720 - INFO: Epoch: 74/200, Batch: 12/29, Batch_Loss_Train: 0.918
2024-06-21 18:05:37,126 - INFO: Epoch: 74/200, Batch: 13/29, Batch_Loss_Train: 0.980
2024-06-21 18:05:37,440 - INFO: Epoch: 74/200, Batch: 14/29, Batch_Loss_Train: 0.896
2024-06-21 18:05:37,864 - INFO: Epoch: 74/200, Batch: 15/29, Batch_Loss_Train: 0.875
2024-06-21 18:05:38,161 - INFO: Epoch: 74/200, Batch: 16/29, Batch_Loss_Train: 1.031
2024-06-21 18:05:38,552 - INFO: Epoch: 74/200, Batch: 17/29, Batch_Loss_Train: 0.908
2024-06-21 18:05:38,862 - INFO: Epoch: 74/200, Batch: 18/29, Batch_Loss_Train: 0.796
2024-06-21 18:05:39,271 - INFO: Epoch: 74/200, Batch: 19/29, Batch_Loss_Train: 0.867
2024-06-21 18:05:39,564 - INFO: Epoch: 74/200, Batch: 20/29, Batch_Loss_Train: 0.864
2024-06-21 18:05:39,946 - INFO: Epoch: 74/200, Batch: 21/29, Batch_Loss_Train: 0.957
2024-06-21 18:05:40,258 - INFO: Epoch: 74/200, Batch: 22/29, Batch_Loss_Train: 0.867
2024-06-21 18:05:40,677 - INFO: Epoch: 74/200, Batch: 23/29, Batch_Loss_Train: 0.850
2024-06-21 18:05:40,977 - INFO: Epoch: 74/200, Batch: 24/29, Batch_Loss_Train: 1.318
2024-06-21 18:05:41,349 - INFO: Epoch: 74/200, Batch: 25/29, Batch_Loss_Train: 0.868
2024-06-21 18:05:41,658 - INFO: Epoch: 74/200, Batch: 26/29, Batch_Loss_Train: 1.068
2024-06-21 18:05:42,055 - INFO: Epoch: 74/200, Batch: 27/29, Batch_Loss_Train: 1.171
2024-06-21 18:05:42,350 - INFO: Epoch: 74/200, Batch: 28/29, Batch_Loss_Train: 1.107
2024-06-21 18:05:42,560 - INFO: Epoch: 74/200, Batch: 29/29, Batch_Loss_Train: 1.051
2024-06-21 18:05:53,633 - INFO: 74/200 final results:
2024-06-21 18:05:53,634 - INFO: Training loss: 0.942.
2024-06-21 18:05:53,634 - INFO: Training MAE: 0.940.
2024-06-21 18:05:53,634 - INFO: Training MSE: 1.704.
2024-06-21 18:06:13,920 - INFO: Epoch: 74/200, Loss_train: 0.9417317427437881, Loss_val: 1.917458073846225
2024-06-21 18:06:13,920 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:06:13,920 - INFO: Epoch 75/200...
2024-06-21 18:06:13,920 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:06:13,920 - INFO: Batch size: 32.
2024-06-21 18:06:13,923 - INFO: Dataset:
2024-06-21 18:06:13,924 - INFO: Batch size:
2024-06-21 18:06:13,924 - INFO: Number of workers:
2024-06-21 18:06:15,015 - INFO: Epoch: 75/200, Batch: 1/29, Batch_Loss_Train: 0.998
2024-06-21 18:06:15,320 - INFO: Epoch: 75/200, Batch: 2/29, Batch_Loss_Train: 1.004
2024-06-21 18:06:15,717 - INFO: Epoch: 75/200, Batch: 3/29, Batch_Loss_Train: 0.921
2024-06-21 18:06:16,037 - INFO: Epoch: 75/200, Batch: 4/29, Batch_Loss_Train: 0.852
2024-06-21 18:06:16,462 - INFO: Epoch: 75/200, Batch: 5/29, Batch_Loss_Train: 0.851
2024-06-21 18:06:16,762 - INFO: Epoch: 75/200, Batch: 6/29, Batch_Loss_Train: 0.794
2024-06-21 18:06:17,148 - INFO: Epoch: 75/200, Batch: 7/29, Batch_Loss_Train: 0.749
2024-06-21 18:06:17,462 - INFO: Epoch: 75/200, Batch: 8/29, Batch_Loss_Train: 0.968
2024-06-21 18:06:17,877 - INFO: Epoch: 75/200, Batch: 9/29, Batch_Loss_Train: 1.082
2024-06-21 18:06:18,169 - INFO: Epoch: 75/200, Batch: 10/29, Batch_Loss_Train: 1.023
2024-06-21 18:06:18,542 - INFO: Epoch: 75/200, Batch: 11/29, Batch_Loss_Train: 1.031
2024-06-21 18:06:18,860 - INFO: Epoch: 75/200, Batch: 12/29, Batch_Loss_Train: 0.952
2024-06-21 18:06:19,292 - INFO: Epoch: 75/200, Batch: 13/29, Batch_Loss_Train: 0.838
2024-06-21 18:06:19,597 - INFO: Epoch: 75/200, Batch: 14/29, Batch_Loss_Train: 0.877
2024-06-21 18:06:19,987 - INFO: Epoch: 75/200, Batch: 15/29, Batch_Loss_Train: 0.904
2024-06-21 18:06:20,300 - INFO: Epoch: 75/200, Batch: 16/29, Batch_Loss_Train: 0.896
2024-06-21 18:06:20,732 - INFO: Epoch: 75/200, Batch: 17/29, Batch_Loss_Train: 0.814
2024-06-21 18:06:21,033 - INFO: Epoch: 75/200, Batch: 18/29, Batch_Loss_Train: 0.980
2024-06-21 18:06:21,411 - INFO: Epoch: 75/200, Batch: 19/29, Batch_Loss_Train: 0.861
2024-06-21 18:06:21,720 - INFO: Epoch: 75/200, Batch: 20/29, Batch_Loss_Train: 0.785
2024-06-21 18:06:22,139 - INFO: Epoch: 75/200, Batch: 21/29, Batch_Loss_Train: 0.844
2024-06-21 18:06:22,442 - INFO: Epoch: 75/200, Batch: 22/29, Batch_Loss_Train: 0.785
2024-06-21 18:06:22,816 - INFO: Epoch: 75/200, Batch: 23/29, Batch_Loss_Train: 1.012
2024-06-21 18:06:23,131 - INFO: Epoch: 75/200, Batch: 24/29, Batch_Loss_Train: 0.982
2024-06-21 18:06:23,547 - INFO: Epoch: 75/200, Batch: 25/29, Batch_Loss_Train: 0.866
2024-06-21 18:06:23,845 - INFO: Epoch: 75/200, Batch: 26/29, Batch_Loss_Train: 0.836
2024-06-21 18:06:24,228 - INFO: Epoch: 75/200, Batch: 27/29, Batch_Loss_Train: 0.940
2024-06-21 18:06:24,539 - INFO: Epoch: 75/200, Batch: 28/29, Batch_Loss_Train: 0.922
2024-06-21 18:06:24,755 - INFO: Epoch: 75/200, Batch: 29/29, Batch_Loss_Train: 0.711
2024-06-21 18:06:35,721 - INFO: 75/200 final results:
2024-06-21 18:06:35,721 - INFO: Training loss: 0.899.
2024-06-21 18:06:35,721 - INFO: Training MAE: 0.903.
2024-06-21 18:06:35,721 - INFO: Training MSE: 1.615.
2024-06-21 18:06:55,878 - INFO: Epoch: 75/200, Loss_train: 0.8992860440550179, Loss_val: 1.911831859884591
2024-06-21 18:06:55,878 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:06:55,878 - INFO: Epoch 76/200...
2024-06-21 18:06:55,878 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:06:55,878 - INFO: Batch size: 32.
2024-06-21 18:06:55,882 - INFO: Dataset:
2024-06-21 18:06:55,882 - INFO: Batch size:
2024-06-21 18:06:55,882 - INFO: Number of workers:
2024-06-21 18:06:56,983 - INFO: Epoch: 76/200, Batch: 1/29, Batch_Loss_Train: 0.767
2024-06-21 18:06:57,290 - INFO: Epoch: 76/200, Batch: 2/29, Batch_Loss_Train: 0.926
2024-06-21 18:06:57,685 - INFO: Epoch: 76/200, Batch: 3/29, Batch_Loss_Train: 0.900
2024-06-21 18:06:58,004 - INFO: Epoch: 76/200, Batch: 4/29, Batch_Loss_Train: 0.683
2024-06-21 18:06:58,434 - INFO: Epoch: 76/200, Batch: 5/29, Batch_Loss_Train: 0.927
2024-06-21 18:06:58,734 - INFO: Epoch: 76/200, Batch: 6/29, Batch_Loss_Train: 0.882
2024-06-21 18:06:59,117 - INFO: Epoch: 76/200, Batch: 7/29, Batch_Loss_Train: 0.777
2024-06-21 18:06:59,418 - INFO: Epoch: 76/200, Batch: 8/29, Batch_Loss_Train: 0.723
2024-06-21 18:06:59,865 - INFO: Epoch: 76/200, Batch: 9/29, Batch_Loss_Train: 0.780
2024-06-21 18:07:00,159 - INFO: Epoch: 76/200, Batch: 10/29, Batch_Loss_Train: 0.908
2024-06-21 18:07:00,525 - INFO: Epoch: 76/200, Batch: 11/29, Batch_Loss_Train: 0.850
2024-06-21 18:07:00,828 - INFO: Epoch: 76/200, Batch: 12/29, Batch_Loss_Train: 0.688
2024-06-21 18:07:01,278 - INFO: Epoch: 76/200, Batch: 13/29, Batch_Loss_Train: 0.727
2024-06-21 18:07:01,582 - INFO: Epoch: 76/200, Batch: 14/29, Batch_Loss_Train: 1.096
2024-06-21 18:07:01,974 - INFO: Epoch: 76/200, Batch: 15/29, Batch_Loss_Train: 0.803
2024-06-21 18:07:02,275 - INFO: Epoch: 76/200, Batch: 16/29, Batch_Loss_Train: 0.693
2024-06-21 18:07:02,707 - INFO: Epoch: 76/200, Batch: 17/29, Batch_Loss_Train: 0.937
2024-06-21 18:07:03,008 - INFO: Epoch: 76/200, Batch: 18/29, Batch_Loss_Train: 0.759
2024-06-21 18:07:03,389 - INFO: Epoch: 76/200, Batch: 19/29, Batch_Loss_Train: 0.946
2024-06-21 18:07:03,685 - INFO: Epoch: 76/200, Batch: 20/29, Batch_Loss_Train: 0.836
2024-06-21 18:07:04,102 - INFO: Epoch: 76/200, Batch: 21/29, Batch_Loss_Train: 0.797
2024-06-21 18:07:04,404 - INFO: Epoch: 76/200, Batch: 22/29, Batch_Loss_Train: 0.795
2024-06-21 18:07:04,786 - INFO: Epoch: 76/200, Batch: 23/29, Batch_Loss_Train: 0.879
2024-06-21 18:07:05,088 - INFO: Epoch: 76/200, Batch: 24/29, Batch_Loss_Train: 0.876
2024-06-21 18:07:05,511 - INFO: Epoch: 76/200, Batch: 25/29, Batch_Loss_Train: 0.750
2024-06-21 18:07:05,809 - INFO: Epoch: 76/200, Batch: 26/29, Batch_Loss_Train: 0.930
2024-06-21 18:07:06,176 - INFO: Epoch: 76/200, Batch: 27/29, Batch_Loss_Train: 0.789
2024-06-21 18:07:06,474 - INFO: Epoch: 76/200, Batch: 28/29, Batch_Loss_Train: 1.057
2024-06-21 18:07:06,688 - INFO: Epoch: 76/200, Batch: 29/29, Batch_Loss_Train: 0.879
2024-06-21 18:07:17,891 - INFO: 76/200 final results:
2024-06-21 18:07:17,892 - INFO: Training loss: 0.840.
2024-06-21 18:07:17,892 - INFO: Training MAE: 0.839.
2024-06-21 18:07:17,892 - INFO: Training MSE: 1.421.
2024-06-21 18:07:38,231 - INFO: Epoch: 76/200, Loss_train: 0.8400052436466875, Loss_val: 1.8852612602299657
2024-06-21 18:07:38,250 - INFO: Saved new best metric model for epoch 76.
2024-06-21 18:07:38,250 - INFO: Best internal validation val_loss: 1.885 at epoch: 76.
2024-06-21 18:07:38,251 - INFO: Epoch 77/200...
2024-06-21 18:07:38,251 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:07:38,251 - INFO: Batch size: 32.
2024-06-21 18:07:38,254 - INFO: Dataset:
2024-06-21 18:07:38,254 - INFO: Batch size:
2024-06-21 18:07:38,255 - INFO: Number of workers:
2024-06-21 18:07:39,352 - INFO: Epoch: 77/200, Batch: 1/29, Batch_Loss_Train: 0.756
2024-06-21 18:07:39,655 - INFO: Epoch: 77/200, Batch: 2/29, Batch_Loss_Train: 0.783
2024-06-21 18:07:40,061 - INFO: Epoch: 77/200, Batch: 3/29, Batch_Loss_Train: 0.905
2024-06-21 18:07:40,375 - INFO: Epoch: 77/200, Batch: 4/29, Batch_Loss_Train: 0.793
2024-06-21 18:07:40,779 - INFO: Epoch: 77/200, Batch: 5/29, Batch_Loss_Train: 0.804
2024-06-21 18:07:41,076 - INFO: Epoch: 77/200, Batch: 6/29, Batch_Loss_Train: 0.695
2024-06-21 18:07:41,470 - INFO: Epoch: 77/200, Batch: 7/29, Batch_Loss_Train: 0.721
2024-06-21 18:07:41,781 - INFO: Epoch: 77/200, Batch: 8/29, Batch_Loss_Train: 0.900
2024-06-21 18:07:42,190 - INFO: Epoch: 77/200, Batch: 9/29, Batch_Loss_Train: 0.886
2024-06-21 18:07:42,484 - INFO: Epoch: 77/200, Batch: 10/29, Batch_Loss_Train: 0.890
2024-06-21 18:07:42,871 - INFO: Epoch: 77/200, Batch: 11/29, Batch_Loss_Train: 0.674
2024-06-21 18:07:43,188 - INFO: Epoch: 77/200, Batch: 12/29, Batch_Loss_Train: 1.089
2024-06-21 18:07:43,609 - INFO: Epoch: 77/200, Batch: 13/29, Batch_Loss_Train: 1.019
2024-06-21 18:07:43,914 - INFO: Epoch: 77/200, Batch: 14/29, Batch_Loss_Train: 0.696
2024-06-21 18:07:44,319 - INFO: Epoch: 77/200, Batch: 15/29, Batch_Loss_Train: 0.712
2024-06-21 18:07:44,632 - INFO: Epoch: 77/200, Batch: 16/29, Batch_Loss_Train: 0.751
2024-06-21 18:07:45,052 - INFO: Epoch: 77/200, Batch: 17/29, Batch_Loss_Train: 0.793
2024-06-21 18:07:45,354 - INFO: Epoch: 77/200, Batch: 18/29, Batch_Loss_Train: 0.831
2024-06-21 18:07:45,747 - INFO: Epoch: 77/200, Batch: 19/29, Batch_Loss_Train: 0.826
2024-06-21 18:07:46,056 - INFO: Epoch: 77/200, Batch: 20/29, Batch_Loss_Train: 0.851
2024-06-21 18:07:46,468 - INFO: Epoch: 77/200, Batch: 21/29, Batch_Loss_Train: 0.801
2024-06-21 18:07:46,771 - INFO: Epoch: 77/200, Batch: 22/29, Batch_Loss_Train: 0.815
2024-06-21 18:07:47,164 - INFO: Epoch: 77/200, Batch: 23/29, Batch_Loss_Train: 0.882
2024-06-21 18:07:47,480 - INFO: Epoch: 77/200, Batch: 24/29, Batch_Loss_Train: 1.073
2024-06-21 18:07:47,883 - INFO: Epoch: 77/200, Batch: 25/29, Batch_Loss_Train: 1.025
2024-06-21 18:07:48,178 - INFO: Epoch: 77/200, Batch: 26/29, Batch_Loss_Train: 0.802
2024-06-21 18:07:48,565 - INFO: Epoch: 77/200, Batch: 27/29, Batch_Loss_Train: 0.767
2024-06-21 18:07:48,873 - INFO: Epoch: 77/200, Batch: 28/29, Batch_Loss_Train: 0.885
2024-06-21 18:07:49,085 - INFO: Epoch: 77/200, Batch: 29/29, Batch_Loss_Train: 0.764
2024-06-21 18:08:00,254 - INFO: 77/200 final results:
2024-06-21 18:08:00,255 - INFO: Training loss: 0.834.
2024-06-21 18:08:00,255 - INFO: Training MAE: 0.835.
2024-06-21 18:08:00,255 - INFO: Training MSE: 1.412.
2024-06-21 18:08:20,682 - INFO: Epoch: 77/200, Loss_train: 0.8340380294569607, Loss_val: 1.8750350434204628
2024-06-21 18:08:20,705 - INFO: Saved new best metric model for epoch 77.
2024-06-21 18:08:20,705 - INFO: Best internal validation val_loss: 1.875 at epoch: 77.
2024-06-21 18:08:20,705 - INFO: Epoch 78/200...
2024-06-21 18:08:20,706 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:08:20,706 - INFO: Batch size: 32.
2024-06-21 18:08:20,709 - INFO: Dataset:
2024-06-21 18:08:20,709 - INFO: Batch size:
2024-06-21 18:08:20,709 - INFO: Number of workers:
2024-06-21 18:08:21,769 - INFO: Epoch: 78/200, Batch: 1/29, Batch_Loss_Train: 0.886
2024-06-21 18:08:22,100 - INFO: Epoch: 78/200, Batch: 2/29, Batch_Loss_Train: 0.848
2024-06-21 18:08:22,499 - INFO: Epoch: 78/200, Batch: 3/29, Batch_Loss_Train: 0.802
2024-06-21 18:08:22,818 - INFO: Epoch: 78/200, Batch: 4/29, Batch_Loss_Train: 0.676
2024-06-21 18:08:23,221 - INFO: Epoch: 78/200, Batch: 5/29, Batch_Loss_Train: 0.975
2024-06-21 18:08:23,547 - INFO: Epoch: 78/200, Batch: 6/29, Batch_Loss_Train: 0.910
2024-06-21 18:08:23,935 - INFO: Epoch: 78/200, Batch: 7/29, Batch_Loss_Train: 0.851
2024-06-21 18:08:24,250 - INFO: Epoch: 78/200, Batch: 8/29, Batch_Loss_Train: 1.024
2024-06-21 18:08:24,642 - INFO: Epoch: 78/200, Batch: 9/29, Batch_Loss_Train: 0.795
2024-06-21 18:08:24,972 - INFO: Epoch: 78/200, Batch: 10/29, Batch_Loss_Train: 0.935
2024-06-21 18:08:25,347 - INFO: Epoch: 78/200, Batch: 11/29, Batch_Loss_Train: 0.920
2024-06-21 18:08:25,664 - INFO: Epoch: 78/200, Batch: 12/29, Batch_Loss_Train: 0.749
2024-06-21 18:08:26,071 - INFO: Epoch: 78/200, Batch: 13/29, Batch_Loss_Train: 0.682
2024-06-21 18:08:26,402 - INFO: Epoch: 78/200, Batch: 14/29, Batch_Loss_Train: 0.888
2024-06-21 18:08:26,796 - INFO: Epoch: 78/200, Batch: 15/29, Batch_Loss_Train: 0.967
2024-06-21 18:08:27,110 - INFO: Epoch: 78/200, Batch: 16/29, Batch_Loss_Train: 0.800
2024-06-21 18:08:27,512 - INFO: Epoch: 78/200, Batch: 17/29, Batch_Loss_Train: 0.759
2024-06-21 18:08:27,839 - INFO: Epoch: 78/200, Batch: 18/29, Batch_Loss_Train: 0.822
2024-06-21 18:08:28,224 - INFO: Epoch: 78/200, Batch: 19/29, Batch_Loss_Train: 0.904
2024-06-21 18:08:28,532 - INFO: Epoch: 78/200, Batch: 20/29, Batch_Loss_Train: 0.871
2024-06-21 18:08:28,925 - INFO: Epoch: 78/200, Batch: 21/29, Batch_Loss_Train: 0.894
2024-06-21 18:08:29,253 - INFO: Epoch: 78/200, Batch: 22/29, Batch_Loss_Train: 0.991
2024-06-21 18:08:29,627 - INFO: Epoch: 78/200, Batch: 23/29, Batch_Loss_Train: 0.783
2024-06-21 18:08:29,942 - INFO: Epoch: 78/200, Batch: 24/29, Batch_Loss_Train: 0.660
2024-06-21 18:08:30,328 - INFO: Epoch: 78/200, Batch: 25/29, Batch_Loss_Train: 0.715
2024-06-21 18:08:30,650 - INFO: Epoch: 78/200, Batch: 26/29, Batch_Loss_Train: 0.823
2024-06-21 18:08:31,018 - INFO: Epoch: 78/200, Batch: 27/29, Batch_Loss_Train: 0.974
2024-06-21 18:08:31,329 - INFO: Epoch: 78/200, Batch: 28/29, Batch_Loss_Train: 0.892
2024-06-21 18:08:31,551 - INFO: Epoch: 78/200, Batch: 29/29, Batch_Loss_Train: 0.889
2024-06-21 18:08:42,562 - INFO: 78/200 final results:
2024-06-21 18:08:42,562 - INFO: Training loss: 0.851.
2024-06-21 18:08:42,562 - INFO: Training MAE: 0.850.
2024-06-21 18:08:42,562 - INFO: Training MSE: 1.462.
2024-06-21 18:09:02,430 - INFO: Epoch: 78/200, Loss_train: 0.8512286256099569, Loss_val: 1.8516622535113632
2024-06-21 18:09:02,448 - INFO: Saved new best metric model for epoch 78.
2024-06-21 18:09:02,448 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:09:02,448 - INFO: Epoch 79/200...
2024-06-21 18:09:02,448 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:09:02,449 - INFO: Batch size: 32.
2024-06-21 18:09:02,453 - INFO: Dataset:
2024-06-21 18:09:02,453 - INFO: Batch size:
2024-06-21 18:09:02,453 - INFO: Number of workers:
2024-06-21 18:09:03,501 - INFO: Epoch: 79/200, Batch: 1/29, Batch_Loss_Train: 0.870
2024-06-21 18:09:03,818 - INFO: Epoch: 79/200, Batch: 2/29, Batch_Loss_Train: 0.662
2024-06-21 18:09:04,225 - INFO: Epoch: 79/200, Batch: 3/29, Batch_Loss_Train: 0.922
2024-06-21 18:09:04,542 - INFO: Epoch: 79/200, Batch: 4/29, Batch_Loss_Train: 0.786
2024-06-21 18:09:04,939 - INFO: Epoch: 79/200, Batch: 5/29, Batch_Loss_Train: 1.035
2024-06-21 18:09:05,249 - INFO: Epoch: 79/200, Batch: 6/29, Batch_Loss_Train: 1.102
2024-06-21 18:09:05,637 - INFO: Epoch: 79/200, Batch: 7/29, Batch_Loss_Train: 0.929
2024-06-21 18:09:05,949 - INFO: Epoch: 79/200, Batch: 8/29, Batch_Loss_Train: 0.804
2024-06-21 18:09:06,334 - INFO: Epoch: 79/200, Batch: 9/29, Batch_Loss_Train: 0.875
2024-06-21 18:09:06,640 - INFO: Epoch: 79/200, Batch: 10/29, Batch_Loss_Train: 0.895
2024-06-21 18:09:07,011 - INFO: Epoch: 79/200, Batch: 11/29, Batch_Loss_Train: 0.748
2024-06-21 18:09:07,329 - INFO: Epoch: 79/200, Batch: 12/29, Batch_Loss_Train: 0.942
2024-06-21 18:09:07,737 - INFO: Epoch: 79/200, Batch: 13/29, Batch_Loss_Train: 0.943
2024-06-21 18:09:08,354 - INFO: Epoch: 79/200, Batch: 14/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:08,762 - INFO: Epoch: 79/200, Batch: 15/29, Batch_Loss_Train: 0.763
2024-06-21 18:09:09,076 - INFO: Epoch: 79/200, Batch: 16/29, Batch_Loss_Train: 0.768
2024-06-21 18:09:09,481 - INFO: Epoch: 79/200, Batch: 17/29, Batch_Loss_Train: 0.958
2024-06-21 18:09:09,794 - INFO: Epoch: 79/200, Batch: 18/29, Batch_Loss_Train: 0.945
2024-06-21 18:09:10,181 - INFO: Epoch: 79/200, Batch: 19/29, Batch_Loss_Train: 0.777
2024-06-21 18:09:10,490 - INFO: Epoch: 79/200, Batch: 20/29, Batch_Loss_Train: 0.724
2024-06-21 18:09:10,885 - INFO: Epoch: 79/200, Batch: 21/29, Batch_Loss_Train: 0.763
2024-06-21 18:09:11,197 - INFO: Epoch: 79/200, Batch: 22/29, Batch_Loss_Train: 0.811
2024-06-21 18:09:11,593 - INFO: Epoch: 79/200, Batch: 23/29, Batch_Loss_Train: 0.796
2024-06-21 18:09:11,909 - INFO: Epoch: 79/200, Batch: 24/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:12,306 - INFO: Epoch: 79/200, Batch: 25/29, Batch_Loss_Train: 0.835
2024-06-21 18:09:12,617 - INFO: Epoch: 79/200, Batch: 26/29, Batch_Loss_Train: 0.806
2024-06-21 18:09:13,013 - INFO: Epoch: 79/200, Batch: 27/29, Batch_Loss_Train: 0.716
2024-06-21 18:09:13,325 - INFO: Epoch: 79/200, Batch: 28/29, Batch_Loss_Train: 0.834
2024-06-21 18:09:13,546 - INFO: Epoch: 79/200, Batch: 29/29, Batch_Loss_Train: 1.202
2024-06-21 18:09:24,480 - INFO: 79/200 final results:
2024-06-21 18:09:24,480 - INFO: Training loss: 0.851.
2024-06-21 18:09:24,480 - INFO: Training MAE: 0.844.
2024-06-21 18:09:24,480 - INFO: Training MSE: 1.466.
2024-06-21 18:09:44,704 - INFO: Epoch: 79/200, Loss_train: 0.8505148640994368, Loss_val: 1.8676618954231
2024-06-21 18:09:44,704 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:09:44,704 - INFO: Epoch 80/200...
2024-06-21 18:09:44,704 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:09:44,704 - INFO: Batch size: 32.
2024-06-21 18:09:44,708 - INFO: Dataset:
2024-06-21 18:09:44,708 - INFO: Batch size:
2024-06-21 18:09:44,708 - INFO: Number of workers:
2024-06-21 18:09:45,802 - INFO: Epoch: 80/200, Batch: 1/29, Batch_Loss_Train: 0.713
2024-06-21 18:09:46,110 - INFO: Epoch: 80/200, Batch: 2/29, Batch_Loss_Train: 0.758
2024-06-21 18:09:46,506 - INFO: Epoch: 80/200, Batch: 3/29, Batch_Loss_Train: 1.002
2024-06-21 18:09:46,826 - INFO: Epoch: 80/200, Batch: 4/29, Batch_Loss_Train: 0.761
2024-06-21 18:09:47,261 - INFO: Epoch: 80/200, Batch: 5/29, Batch_Loss_Train: 0.907
2024-06-21 18:09:47,560 - INFO: Epoch: 80/200, Batch: 6/29, Batch_Loss_Train: 0.969
2024-06-21 18:09:47,946 - INFO: Epoch: 80/200, Batch: 7/29, Batch_Loss_Train: 0.707
2024-06-21 18:09:48,248 - INFO: Epoch: 80/200, Batch: 8/29, Batch_Loss_Train: 0.970
2024-06-21 18:09:48,680 - INFO: Epoch: 80/200, Batch: 9/29, Batch_Loss_Train: 0.962
2024-06-21 18:09:48,971 - INFO: Epoch: 80/200, Batch: 10/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:49,343 - INFO: Epoch: 80/200, Batch: 11/29, Batch_Loss_Train: 0.754
2024-06-21 18:09:49,646 - INFO: Epoch: 80/200, Batch: 12/29, Batch_Loss_Train: 0.714
2024-06-21 18:09:50,094 - INFO: Epoch: 80/200, Batch: 13/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:50,406 - INFO: Epoch: 80/200, Batch: 14/29, Batch_Loss_Train: 1.010
2024-06-21 18:09:50,813 - INFO: Epoch: 80/200, Batch: 15/29, Batch_Loss_Train: 0.797
2024-06-21 18:09:51,121 - INFO: Epoch: 80/200, Batch: 16/29, Batch_Loss_Train: 0.906
2024-06-21 18:09:51,580 - INFO: Epoch: 80/200, Batch: 17/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:51,881 - INFO: Epoch: 80/200, Batch: 18/29, Batch_Loss_Train: 0.796
2024-06-21 18:09:52,265 - INFO: Epoch: 80/200, Batch: 19/29, Batch_Loss_Train: 0.891
2024-06-21 18:09:52,561 - INFO: Epoch: 80/200, Batch: 20/29, Batch_Loss_Train: 0.862
2024-06-21 18:09:52,985 - INFO: Epoch: 80/200, Batch: 21/29, Batch_Loss_Train: 0.853
2024-06-21 18:09:53,286 - INFO: Epoch: 80/200, Batch: 22/29, Batch_Loss_Train: 1.264
2024-06-21 18:09:53,668 - INFO: Epoch: 80/200, Batch: 23/29, Batch_Loss_Train: 0.786
2024-06-21 18:09:53,969 - INFO: Epoch: 80/200, Batch: 24/29, Batch_Loss_Train: 0.854
2024-06-21 18:09:54,403 - INFO: Epoch: 80/200, Batch: 25/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:54,702 - INFO: Epoch: 80/200, Batch: 26/29, Batch_Loss_Train: 1.012
2024-06-21 18:09:55,088 - INFO: Epoch: 80/200, Batch: 27/29, Batch_Loss_Train: 0.734
2024-06-21 18:09:55,387 - INFO: Epoch: 80/200, Batch: 28/29, Batch_Loss_Train: 0.728
2024-06-21 18:09:55,608 - INFO: Epoch: 80/200, Batch: 29/29, Batch_Loss_Train: 1.113
2024-06-21 18:10:06,642 - INFO: 80/200 final results:
2024-06-21 18:10:06,643 - INFO: Training loss: 0.853.
2024-06-21 18:10:06,643 - INFO: Training MAE: 0.848.
2024-06-21 18:10:06,643 - INFO: Training MSE: 1.436.
2024-06-21 18:10:27,387 - INFO: Epoch: 80/200, Loss_train: 0.852845901045306, Loss_val: 1.896312635520409
2024-06-21 18:10:27,387 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:10:27,387 - INFO: Epoch 81/200...
2024-06-21 18:10:27,387 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:10:27,387 - INFO: Batch size: 32.
2024-06-21 18:10:27,391 - INFO: Dataset:
2024-06-21 18:10:27,391 - INFO: Batch size:
2024-06-21 18:10:27,391 - INFO: Number of workers:
2024-06-21 18:10:28,476 - INFO: Epoch: 81/200, Batch: 1/29, Batch_Loss_Train: 0.796
2024-06-21 18:10:28,779 - INFO: Epoch: 81/200, Batch: 2/29, Batch_Loss_Train: 0.812
2024-06-21 18:10:29,160 - INFO: Epoch: 81/200, Batch: 3/29, Batch_Loss_Train: 1.062
2024-06-21 18:10:29,474 - INFO: Epoch: 81/200, Batch: 4/29, Batch_Loss_Train: 0.936
2024-06-21 18:10:29,888 - INFO: Epoch: 81/200, Batch: 5/29, Batch_Loss_Train: 0.751
2024-06-21 18:10:30,185 - INFO: Epoch: 81/200, Batch: 6/29, Batch_Loss_Train: 0.779
2024-06-21 18:10:30,555 - INFO: Epoch: 81/200, Batch: 7/29, Batch_Loss_Train: 0.785
2024-06-21 18:10:30,866 - INFO: Epoch: 81/200, Batch: 8/29, Batch_Loss_Train: 0.857
2024-06-21 18:10:31,276 - INFO: Epoch: 81/200, Batch: 9/29, Batch_Loss_Train: 0.771
2024-06-21 18:10:31,570 - INFO: Epoch: 81/200, Batch: 10/29, Batch_Loss_Train: 0.774
2024-06-21 18:10:31,949 - INFO: Epoch: 81/200, Batch: 11/29, Batch_Loss_Train: 0.876
2024-06-21 18:10:32,264 - INFO: Epoch: 81/200, Batch: 12/29, Batch_Loss_Train: 0.758
2024-06-21 18:10:32,694 - INFO: Epoch: 81/200, Batch: 13/29, Batch_Loss_Train: 0.819
2024-06-21 18:10:32,996 - INFO: Epoch: 81/200, Batch: 14/29, Batch_Loss_Train: 0.682
2024-06-21 18:10:33,388 - INFO: Epoch: 81/200, Batch: 15/29, Batch_Loss_Train: 0.797
2024-06-21 18:10:33,699 - INFO: Epoch: 81/200, Batch: 16/29, Batch_Loss_Train: 1.177
2024-06-21 18:10:34,135 - INFO: Epoch: 81/200, Batch: 17/29, Batch_Loss_Train: 0.718
2024-06-21 18:10:34,436 - INFO: Epoch: 81/200, Batch: 18/29, Batch_Loss_Train: 1.051
2024-06-21 18:10:34,823 - INFO: Epoch: 81/200, Batch: 19/29, Batch_Loss_Train: 0.764
2024-06-21 18:10:35,131 - INFO: Epoch: 81/200, Batch: 20/29, Batch_Loss_Train: 0.877
2024-06-21 18:10:35,556 - INFO: Epoch: 81/200, Batch: 21/29, Batch_Loss_Train: 0.899
2024-06-21 18:10:35,859 - INFO: Epoch: 81/200, Batch: 22/29, Batch_Loss_Train: 0.893
2024-06-21 18:10:36,236 - INFO: Epoch: 81/200, Batch: 23/29, Batch_Loss_Train: 0.696
2024-06-21 18:10:36,553 - INFO: Epoch: 81/200, Batch: 24/29, Batch_Loss_Train: 0.913
2024-06-21 18:10:36,968 - INFO: Epoch: 81/200, Batch: 25/29, Batch_Loss_Train: 0.747
2024-06-21 18:10:37,266 - INFO: Epoch: 81/200, Batch: 26/29, Batch_Loss_Train: 0.889
2024-06-21 18:10:37,644 - INFO: Epoch: 81/200, Batch: 27/29, Batch_Loss_Train: 0.833
2024-06-21 18:10:37,957 - INFO: Epoch: 81/200, Batch: 28/29, Batch_Loss_Train: 0.936
2024-06-21 18:10:38,176 - INFO: Epoch: 81/200, Batch: 29/29, Batch_Loss_Train: 0.975
2024-06-21 18:10:49,332 - INFO: 81/200 final results:
2024-06-21 18:10:49,333 - INFO: Training loss: 0.849.
2024-06-21 18:10:49,333 - INFO: Training MAE: 0.847.
2024-06-21 18:10:49,333 - INFO: Training MSE: 1.447.
2024-06-21 18:11:09,888 - INFO: Epoch: 81/200, Loss_train: 0.8491239588836144, Loss_val: 1.856054663658142
2024-06-21 18:11:09,889 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:11:09,889 - INFO: Epoch 82/200...
2024-06-21 18:11:09,889 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:11:09,889 - INFO: Batch size: 32.
2024-06-21 18:11:09,892 - INFO: Dataset:
2024-06-21 18:11:09,893 - INFO: Batch size:
2024-06-21 18:11:09,893 - INFO: Number of workers:
2024-06-21 18:11:10,990 - INFO: Epoch: 82/200, Batch: 1/29, Batch_Loss_Train: 0.822
2024-06-21 18:11:11,294 - INFO: Epoch: 82/200, Batch: 2/29, Batch_Loss_Train: 0.897
2024-06-21 18:11:11,689 - INFO: Epoch: 82/200, Batch: 3/29, Batch_Loss_Train: 0.840
2024-06-21 18:11:12,005 - INFO: Epoch: 82/200, Batch: 4/29, Batch_Loss_Train: 0.722
2024-06-21 18:11:12,439 - INFO: Epoch: 82/200, Batch: 5/29, Batch_Loss_Train: 0.761
2024-06-21 18:11:12,738 - INFO: Epoch: 82/200, Batch: 6/29, Batch_Loss_Train: 0.758
2024-06-21 18:11:13,112 - INFO: Epoch: 82/200, Batch: 7/29, Batch_Loss_Train: 0.928
2024-06-21 18:11:13,414 - INFO: Epoch: 82/200, Batch: 8/29, Batch_Loss_Train: 0.808
2024-06-21 18:11:13,860 - INFO: Epoch: 82/200, Batch: 9/29, Batch_Loss_Train: 0.730
2024-06-21 18:11:14,152 - INFO: Epoch: 82/200, Batch: 10/29, Batch_Loss_Train: 0.729
2024-06-21 18:11:14,528 - INFO: Epoch: 82/200, Batch: 11/29, Batch_Loss_Train: 0.806
2024-06-21 18:11:14,829 - INFO: Epoch: 82/200, Batch: 12/29, Batch_Loss_Train: 0.811
2024-06-21 18:11:15,273 - INFO: Epoch: 82/200, Batch: 13/29, Batch_Loss_Train: 0.735
2024-06-21 18:11:15,578 - INFO: Epoch: 82/200, Batch: 14/29, Batch_Loss_Train: 0.768
2024-06-21 18:11:15,962 - INFO: Epoch: 82/200, Batch: 15/29, Batch_Loss_Train: 0.771
2024-06-21 18:11:16,263 - INFO: Epoch: 82/200, Batch: 16/29, Batch_Loss_Train: 0.884
2024-06-21 18:11:16,702 - INFO: Epoch: 82/200, Batch: 17/29, Batch_Loss_Train: 0.858
2024-06-21 18:11:16,998 - INFO: Epoch: 82/200, Batch: 18/29, Batch_Loss_Train: 1.018
2024-06-21 18:11:17,382 - INFO: Epoch: 82/200, Batch: 19/29, Batch_Loss_Train: 0.676
2024-06-21 18:11:17,673 - INFO: Epoch: 82/200, Batch: 20/29, Batch_Loss_Train: 0.843
2024-06-21 18:11:18,098 - INFO: Epoch: 82/200, Batch: 21/29, Batch_Loss_Train: 0.830
2024-06-21 18:11:18,398 - INFO: Epoch: 82/200, Batch: 22/29, Batch_Loss_Train: 0.773
2024-06-21 18:11:18,781 - INFO: Epoch: 82/200, Batch: 23/29, Batch_Loss_Train: 1.000
2024-06-21 18:11:19,080 - INFO: Epoch: 82/200, Batch: 24/29, Batch_Loss_Train: 0.617
2024-06-21 18:11:19,509 - INFO: Epoch: 82/200, Batch: 25/29, Batch_Loss_Train: 0.790
2024-06-21 18:11:19,804 - INFO: Epoch: 82/200, Batch: 26/29, Batch_Loss_Train: 0.714
2024-06-21 18:11:20,184 - INFO: Epoch: 82/200, Batch: 27/29, Batch_Loss_Train: 0.677
2024-06-21 18:11:20,479 - INFO: Epoch: 82/200, Batch: 28/29, Batch_Loss_Train: 0.615
2024-06-21 18:11:20,698 - INFO: Epoch: 82/200, Batch: 29/29, Batch_Loss_Train: 0.752
2024-06-21 18:11:31,634 - INFO: 82/200 final results:
2024-06-21 18:11:31,634 - INFO: Training loss: 0.791.
2024-06-21 18:11:31,634 - INFO: Training MAE: 0.792.
2024-06-21 18:11:31,634 - INFO: Training MSE: 1.350.
2024-06-21 18:11:51,648 - INFO: Epoch: 82/200, Loss_train: 0.7908163892811743, Loss_val: 1.9593568875871856
2024-06-21 18:11:51,648 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:11:51,648 - INFO: Epoch 83/200...
2024-06-21 18:11:51,648 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:11:51,648 - INFO: Batch size: 32.
2024-06-21 18:11:51,652 - INFO: Dataset:
2024-06-21 18:11:51,652 - INFO: Batch size:
2024-06-21 18:11:51,652 - INFO: Number of workers:
2024-06-21 18:11:52,716 - INFO: Epoch: 83/200, Batch: 1/29, Batch_Loss_Train: 0.781
2024-06-21 18:11:53,020 - INFO: Epoch: 83/200, Batch: 2/29, Batch_Loss_Train: 0.744
2024-06-21 18:11:53,427 - INFO: Epoch: 83/200, Batch: 3/29, Batch_Loss_Train: 0.796
2024-06-21 18:11:53,742 - INFO: Epoch: 83/200, Batch: 4/29, Batch_Loss_Train: 0.768
2024-06-21 18:11:54,155 - INFO: Epoch: 83/200, Batch: 5/29, Batch_Loss_Train: 0.670
2024-06-21 18:11:54,453 - INFO: Epoch: 83/200, Batch: 6/29, Batch_Loss_Train: 0.925
2024-06-21 18:11:54,840 - INFO: Epoch: 83/200, Batch: 7/29, Batch_Loss_Train: 0.746
2024-06-21 18:11:55,151 - INFO: Epoch: 83/200, Batch: 8/29, Batch_Loss_Train: 1.030
2024-06-21 18:11:55,575 - INFO: Epoch: 83/200, Batch: 9/29, Batch_Loss_Train: 0.729
2024-06-21 18:11:55,866 - INFO: Epoch: 83/200, Batch: 10/29, Batch_Loss_Train: 0.794
2024-06-21 18:11:56,236 - INFO: Epoch: 83/200, Batch: 11/29, Batch_Loss_Train: 0.662
2024-06-21 18:11:56,550 - INFO: Epoch: 83/200, Batch: 12/29, Batch_Loss_Train: 0.715
2024-06-21 18:11:56,967 - INFO: Epoch: 83/200, Batch: 13/29, Batch_Loss_Train: 0.842
2024-06-21 18:11:57,267 - INFO: Epoch: 83/200, Batch: 14/29, Batch_Loss_Train: 0.730
2024-06-21 18:11:57,657 - INFO: Epoch: 83/200, Batch: 15/29, Batch_Loss_Train: 0.718
2024-06-21 18:11:57,967 - INFO: Epoch: 83/200, Batch: 16/29, Batch_Loss_Train: 0.642
2024-06-21 18:11:58,384 - INFO: Epoch: 83/200, Batch: 17/29, Batch_Loss_Train: 0.731
2024-06-21 18:11:58,681 - INFO: Epoch: 83/200, Batch: 18/29, Batch_Loss_Train: 1.013
2024-06-21 18:11:59,060 - INFO: Epoch: 83/200, Batch: 19/29, Batch_Loss_Train: 0.908
2024-06-21 18:11:59,366 - INFO: Epoch: 83/200, Batch: 20/29, Batch_Loss_Train: 0.883
2024-06-21 18:11:59,776 - INFO: Epoch: 83/200, Batch: 21/29, Batch_Loss_Train: 0.741
2024-06-21 18:12:00,076 - INFO: Epoch: 83/200, Batch: 22/29, Batch_Loss_Train: 0.819
2024-06-21 18:12:00,450 - INFO: Epoch: 83/200, Batch: 23/29, Batch_Loss_Train: 0.856
2024-06-21 18:12:00,762 - INFO: Epoch: 83/200, Batch: 24/29, Batch_Loss_Train: 0.706
2024-06-21 18:12:01,173 - INFO: Epoch: 83/200, Batch: 25/29, Batch_Loss_Train: 0.894
2024-06-21 18:12:01,468 - INFO: Epoch: 83/200, Batch: 26/29, Batch_Loss_Train: 0.865
2024-06-21 18:12:01,838 - INFO: Epoch: 83/200, Batch: 27/29, Batch_Loss_Train: 0.775
2024-06-21 18:12:02,145 - INFO: Epoch: 83/200, Batch: 28/29, Batch_Loss_Train: 0.799
2024-06-21 18:12:02,359 - INFO: Epoch: 83/200, Batch: 29/29, Batch_Loss_Train: 0.791
2024-06-21 18:12:13,468 - INFO: 83/200 final results:
2024-06-21 18:12:13,469 - INFO: Training loss: 0.796.
2024-06-21 18:12:13,469 - INFO: Training MAE: 0.796.
2024-06-21 18:12:13,469 - INFO: Training MSE: 1.300.
2024-06-21 18:12:33,589 - INFO: Epoch: 83/200, Loss_train: 0.7956108738636148, Loss_val: 1.9049080322528709
2024-06-21 18:12:33,589 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:12:33,589 - INFO: Epoch 84/200...
2024-06-21 18:12:33,589 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:12:33,589 - INFO: Batch size: 32.
2024-06-21 18:12:33,592 - INFO: Dataset:
2024-06-21 18:12:33,593 - INFO: Batch size:
2024-06-21 18:12:33,593 - INFO: Number of workers:
2024-06-21 18:12:34,651 - INFO: Epoch: 84/200, Batch: 1/29, Batch_Loss_Train: 0.669
2024-06-21 18:12:34,969 - INFO: Epoch: 84/200, Batch: 2/29, Batch_Loss_Train: 0.712
2024-06-21 18:12:35,374 - INFO: Epoch: 84/200, Batch: 3/29, Batch_Loss_Train: 0.796
2024-06-21 18:12:35,692 - INFO: Epoch: 84/200, Batch: 4/29, Batch_Loss_Train: 0.827
2024-06-21 18:12:36,105 - INFO: Epoch: 84/200, Batch: 5/29, Batch_Loss_Train: 0.629
2024-06-21 18:12:36,406 - INFO: Epoch: 84/200, Batch: 6/29, Batch_Loss_Train: 0.871
2024-06-21 18:12:36,809 - INFO: Epoch: 84/200, Batch: 7/29, Batch_Loss_Train: 0.762
2024-06-21 18:12:37,125 - INFO: Epoch: 84/200, Batch: 8/29, Batch_Loss_Train: 0.702
2024-06-21 18:12:37,538 - INFO: Epoch: 84/200, Batch: 9/29, Batch_Loss_Train: 0.899
2024-06-21 18:12:37,833 - INFO: Epoch: 84/200, Batch: 10/29, Batch_Loss_Train: 0.853
2024-06-21 18:12:38,222 - INFO: Epoch: 84/200, Batch: 11/29, Batch_Loss_Train: 0.690
2024-06-21 18:12:38,539 - INFO: Epoch: 84/200, Batch: 12/29, Batch_Loss_Train: 0.819
2024-06-21 18:12:38,959 - INFO: Epoch: 84/200, Batch: 13/29, Batch_Loss_Train: 0.639
2024-06-21 18:12:39,264 - INFO: Epoch: 84/200, Batch: 14/29, Batch_Loss_Train: 0.657
2024-06-21 18:12:39,672 - INFO: Epoch: 84/200, Batch: 15/29, Batch_Loss_Train: 0.799
2024-06-21 18:12:39,984 - INFO: Epoch: 84/200, Batch: 16/29, Batch_Loss_Train: 0.667
2024-06-21 18:12:40,401 - INFO: Epoch: 84/200, Batch: 17/29, Batch_Loss_Train: 1.009
2024-06-21 18:12:40,703 - INFO: Epoch: 84/200, Batch: 18/29, Batch_Loss_Train: 0.761
2024-06-21 18:12:41,099 - INFO: Epoch: 84/200, Batch: 19/29, Batch_Loss_Train: 0.731
2024-06-21 18:12:41,406 - INFO: Epoch: 84/200, Batch: 20/29, Batch_Loss_Train: 0.759
2024-06-21 18:12:41,810 - INFO: Epoch: 84/200, Batch: 21/29, Batch_Loss_Train: 0.712
2024-06-21 18:12:42,111 - INFO: Epoch: 84/200, Batch: 22/29, Batch_Loss_Train: 0.840
2024-06-21 18:12:42,499 - INFO: Epoch: 84/200, Batch: 23/29, Batch_Loss_Train: 0.718
2024-06-21 18:12:42,812 - INFO: Epoch: 84/200, Batch: 24/29, Batch_Loss_Train: 0.986
2024-06-21 18:12:43,206 - INFO: Epoch: 84/200, Batch: 25/29, Batch_Loss_Train: 0.671
2024-06-21 18:12:43,501 - INFO: Epoch: 84/200, Batch: 26/29, Batch_Loss_Train: 0.851
2024-06-21 18:12:43,878 - INFO: Epoch: 84/200, Batch: 27/29, Batch_Loss_Train: 0.952
2024-06-21 18:12:44,185 - INFO: Epoch: 84/200, Batch: 28/29, Batch_Loss_Train: 0.936
2024-06-21 18:12:44,401 - INFO: Epoch: 84/200, Batch: 29/29, Batch_Loss_Train: 0.811
2024-06-21 18:12:55,540 - INFO: 84/200 final results:
2024-06-21 18:12:55,540 - INFO: Training loss: 0.784.
2024-06-21 18:12:55,540 - INFO: Training MAE: 0.783.
2024-06-21 18:12:55,540 - INFO: Training MSE: 1.261.
2024-06-21 18:13:15,858 - INFO: Epoch: 84/200, Loss_train: 0.7837535204558537, Loss_val: 1.8577203134010578
2024-06-21 18:13:15,858 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:13:15,858 - INFO: Epoch 85/200...
2024-06-21 18:13:15,858 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:13:15,858 - INFO: Batch size: 32.
2024-06-21 18:13:15,861 - INFO: Dataset:
2024-06-21 18:13:15,862 - INFO: Batch size:
2024-06-21 18:13:15,862 - INFO: Number of workers:
2024-06-21 18:13:16,936 - INFO: Epoch: 85/200, Batch: 1/29, Batch_Loss_Train: 0.825
2024-06-21 18:13:17,241 - INFO: Epoch: 85/200, Batch: 2/29, Batch_Loss_Train: 0.947
2024-06-21 18:13:17,650 - INFO: Epoch: 85/200, Batch: 3/29, Batch_Loss_Train: 0.845
2024-06-21 18:13:17,967 - INFO: Epoch: 85/200, Batch: 4/29, Batch_Loss_Train: 0.784
2024-06-21 18:13:18,386 - INFO: Epoch: 85/200, Batch: 5/29, Batch_Loss_Train: 0.893
2024-06-21 18:13:18,685 - INFO: Epoch: 85/200, Batch: 6/29, Batch_Loss_Train: 0.901
2024-06-21 18:13:19,073 - INFO: Epoch: 85/200, Batch: 7/29, Batch_Loss_Train: 0.821
2024-06-21 18:13:19,386 - INFO: Epoch: 85/200, Batch: 8/29, Batch_Loss_Train: 0.758
2024-06-21 18:13:19,818 - INFO: Epoch: 85/200, Batch: 9/29, Batch_Loss_Train: 0.684
2024-06-21 18:13:20,109 - INFO: Epoch: 85/200, Batch: 10/29, Batch_Loss_Train: 0.929
2024-06-21 18:13:20,485 - INFO: Epoch: 85/200, Batch: 11/29, Batch_Loss_Train: 0.825
2024-06-21 18:13:20,800 - INFO: Epoch: 85/200, Batch: 12/29, Batch_Loss_Train: 0.750
2024-06-21 18:13:21,228 - INFO: Epoch: 85/200, Batch: 13/29, Batch_Loss_Train: 0.973
2024-06-21 18:13:21,530 - INFO: Epoch: 85/200, Batch: 14/29, Batch_Loss_Train: 0.753
2024-06-21 18:13:21,923 - INFO: Epoch: 85/200, Batch: 15/29, Batch_Loss_Train: 0.929
2024-06-21 18:13:22,233 - INFO: Epoch: 85/200, Batch: 16/29, Batch_Loss_Train: 0.539
2024-06-21 18:13:22,659 - INFO: Epoch: 85/200, Batch: 17/29, Batch_Loss_Train: 0.637
2024-06-21 18:13:22,957 - INFO: Epoch: 85/200, Batch: 18/29, Batch_Loss_Train: 0.617
2024-06-21 18:13:23,337 - INFO: Epoch: 85/200, Batch: 19/29, Batch_Loss_Train: 0.731
2024-06-21 18:13:23,643 - INFO: Epoch: 85/200, Batch: 20/29, Batch_Loss_Train: 1.090
2024-06-21 18:13:24,050 - INFO: Epoch: 85/200, Batch: 21/29, Batch_Loss_Train: 0.815
2024-06-21 18:13:24,349 - INFO: Epoch: 85/200, Batch: 22/29, Batch_Loss_Train: 0.672
2024-06-21 18:13:24,720 - INFO: Epoch: 85/200, Batch: 23/29, Batch_Loss_Train: 1.027
2024-06-21 18:13:25,032 - INFO: Epoch: 85/200, Batch: 24/29, Batch_Loss_Train: 0.851
2024-06-21 18:13:25,438 - INFO: Epoch: 85/200, Batch: 25/29, Batch_Loss_Train: 0.833
2024-06-21 18:13:25,734 - INFO: Epoch: 85/200, Batch: 26/29, Batch_Loss_Train: 0.701
2024-06-21 18:13:26,116 - INFO: Epoch: 85/200, Batch: 27/29, Batch_Loss_Train: 0.925
2024-06-21 18:13:26,424 - INFO: Epoch: 85/200, Batch: 28/29, Batch_Loss_Train: 0.869
2024-06-21 18:13:26,633 - INFO: Epoch: 85/200, Batch: 29/29, Batch_Loss_Train: 0.891
2024-06-21 18:13:37,697 - INFO: 85/200 final results:
2024-06-21 18:13:37,697 - INFO: Training loss: 0.821.
2024-06-21 18:13:37,697 - INFO: Training MAE: 0.820.
2024-06-21 18:13:37,697 - INFO: Training MSE: 1.343.
2024-06-21 18:13:58,130 - INFO: Epoch: 85/200, Loss_train: 0.8212578831047848, Loss_val: 1.9186989603371456
2024-06-21 18:13:58,130 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:13:58,130 - INFO: Epoch 86/200...
2024-06-21 18:13:58,130 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:13:58,130 - INFO: Batch size: 32.
2024-06-21 18:13:58,134 - INFO: Dataset:
2024-06-21 18:13:58,134 - INFO: Batch size:
2024-06-21 18:13:58,134 - INFO: Number of workers:
2024-06-21 18:13:59,190 - INFO: Epoch: 86/200, Batch: 1/29, Batch_Loss_Train: 1.062
2024-06-21 18:13:59,510 - INFO: Epoch: 86/200, Batch: 2/29, Batch_Loss_Train: 0.804
2024-06-21 18:13:59,920 - INFO: Epoch: 86/200, Batch: 3/29, Batch_Loss_Train: 0.816
2024-06-21 18:14:00,238 - INFO: Epoch: 86/200, Batch: 4/29, Batch_Loss_Train: 0.724
2024-06-21 18:14:00,641 - INFO: Epoch: 86/200, Batch: 5/29, Batch_Loss_Train: 0.828
2024-06-21 18:14:00,941 - INFO: Epoch: 86/200, Batch: 6/29, Batch_Loss_Train: 0.633
2024-06-21 18:14:01,339 - INFO: Epoch: 86/200, Batch: 7/29, Batch_Loss_Train: 0.724
2024-06-21 18:14:01,655 - INFO: Epoch: 86/200, Batch: 8/29, Batch_Loss_Train: 0.761
2024-06-21 18:14:02,056 - INFO: Epoch: 86/200, Batch: 9/29, Batch_Loss_Train: 0.799
2024-06-21 18:14:02,349 - INFO: Epoch: 86/200, Batch: 10/29, Batch_Loss_Train: 0.673
2024-06-21 18:14:02,736 - INFO: Epoch: 86/200, Batch: 11/29, Batch_Loss_Train: 0.738
2024-06-21 18:14:03,052 - INFO: Epoch: 86/200, Batch: 12/29, Batch_Loss_Train: 0.758
2024-06-21 18:14:03,461 - INFO: Epoch: 86/200, Batch: 13/29, Batch_Loss_Train: 0.777
2024-06-21 18:14:03,762 - INFO: Epoch: 86/200, Batch: 14/29, Batch_Loss_Train: 0.617
2024-06-21 18:14:04,166 - INFO: Epoch: 86/200, Batch: 15/29, Batch_Loss_Train: 0.782
2024-06-21 18:14:04,477 - INFO: Epoch: 86/200, Batch: 16/29, Batch_Loss_Train: 0.587
2024-06-21 18:14:04,880 - INFO: Epoch: 86/200, Batch: 17/29, Batch_Loss_Train: 0.812
2024-06-21 18:14:05,178 - INFO: Epoch: 86/200, Batch: 18/29, Batch_Loss_Train: 0.798
2024-06-21 18:14:05,571 - INFO: Epoch: 86/200, Batch: 19/29, Batch_Loss_Train: 0.779
2024-06-21 18:14:05,878 - INFO: Epoch: 86/200, Batch: 20/29, Batch_Loss_Train: 0.742
2024-06-21 18:14:06,271 - INFO: Epoch: 86/200, Batch: 21/29, Batch_Loss_Train: 0.911
2024-06-21 18:14:06,572 - INFO: Epoch: 86/200, Batch: 22/29, Batch_Loss_Train: 0.891
2024-06-21 18:14:06,956 - INFO: Epoch: 86/200, Batch: 23/29, Batch_Loss_Train: 0.856
2024-06-21 18:14:07,269 - INFO: Epoch: 86/200, Batch: 24/29, Batch_Loss_Train: 0.638
2024-06-21 18:14:07,667 - INFO: Epoch: 86/200, Batch: 25/29, Batch_Loss_Train: 0.645
2024-06-21 18:14:07,964 - INFO: Epoch: 86/200, Batch: 26/29, Batch_Loss_Train: 0.821
2024-06-21 18:14:08,344 - INFO: Epoch: 86/200, Batch: 27/29, Batch_Loss_Train: 0.832
2024-06-21 18:14:08,653 - INFO: Epoch: 86/200, Batch: 28/29, Batch_Loss_Train: 0.795
2024-06-21 18:14:08,876 - INFO: Epoch: 86/200, Batch: 29/29, Batch_Loss_Train: 0.863
2024-06-21 18:14:20,025 - INFO: 86/200 final results:
2024-06-21 18:14:20,025 - INFO: Training loss: 0.775.
2024-06-21 18:14:20,025 - INFO: Training MAE: 0.773.
2024-06-21 18:14:20,025 - INFO: Training MSE: 1.247.
2024-06-21 18:14:39,964 - INFO: Epoch: 86/200, Loss_train: 0.7746825999227064, Loss_val: 1.872085587731723
2024-06-21 18:14:39,965 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:14:39,965 - INFO: Epoch 87/200...
2024-06-21 18:14:39,965 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:14:39,965 - INFO: Batch size: 32.
2024-06-21 18:14:39,968 - INFO: Dataset:
2024-06-21 18:14:39,969 - INFO: Batch size:
2024-06-21 18:14:39,969 - INFO: Number of workers:
2024-06-21 18:14:41,018 - INFO: Epoch: 87/200, Batch: 1/29, Batch_Loss_Train: 0.884
2024-06-21 18:14:41,339 - INFO: Epoch: 87/200, Batch: 2/29, Batch_Loss_Train: 0.710
2024-06-21 18:14:41,750 - INFO: Epoch: 87/200, Batch: 3/29, Batch_Loss_Train: 0.698
2024-06-21 18:14:42,069 - INFO: Epoch: 87/200, Batch: 4/29, Batch_Loss_Train: 0.753
2024-06-21 18:14:42,480 - INFO: Epoch: 87/200, Batch: 5/29, Batch_Loss_Train: 0.811
2024-06-21 18:14:42,783 - INFO: Epoch: 87/200, Batch: 6/29, Batch_Loss_Train: 0.825
2024-06-21 18:14:43,179 - INFO: Epoch: 87/200, Batch: 7/29, Batch_Loss_Train: 0.582
2024-06-21 18:14:43,492 - INFO: Epoch: 87/200, Batch: 8/29, Batch_Loss_Train: 0.804
2024-06-21 18:14:43,908 - INFO: Epoch: 87/200, Batch: 9/29, Batch_Loss_Train: 0.866
2024-06-21 18:14:44,202 - INFO: Epoch: 87/200, Batch: 10/29, Batch_Loss_Train: 0.908
2024-06-21 18:14:44,582 - INFO: Epoch: 87/200, Batch: 11/29, Batch_Loss_Train: 0.812
2024-06-21 18:14:44,899 - INFO: Epoch: 87/200, Batch: 12/29, Batch_Loss_Train: 0.668
2024-06-21 18:14:45,318 - INFO: Epoch: 87/200, Batch: 13/29, Batch_Loss_Train: 0.831
2024-06-21 18:14:45,620 - INFO: Epoch: 87/200, Batch: 14/29, Batch_Loss_Train: 0.856
2024-06-21 18:14:46,019 - INFO: Epoch: 87/200, Batch: 15/29, Batch_Loss_Train: 0.719
2024-06-21 18:14:46,330 - INFO: Epoch: 87/200, Batch: 16/29, Batch_Loss_Train: 0.644
2024-06-21 18:14:46,744 - INFO: Epoch: 87/200, Batch: 17/29, Batch_Loss_Train: 0.816
2024-06-21 18:14:47,042 - INFO: Epoch: 87/200, Batch: 18/29, Batch_Loss_Train: 0.639
2024-06-21 18:14:47,434 - INFO: Epoch: 87/200, Batch: 19/29, Batch_Loss_Train: 0.720
2024-06-21 18:14:47,739 - INFO: Epoch: 87/200, Batch: 20/29, Batch_Loss_Train: 0.686
2024-06-21 18:14:48,143 - INFO: Epoch: 87/200, Batch: 21/29, Batch_Loss_Train: 0.924
2024-06-21 18:14:48,444 - INFO: Epoch: 87/200, Batch: 22/29, Batch_Loss_Train: 0.721
2024-06-21 18:14:49,128 - INFO: Epoch: 87/200, Batch: 23/29, Batch_Loss_Train: 0.900
2024-06-21 18:14:49,441 - INFO: Epoch: 87/200, Batch: 24/29, Batch_Loss_Train: 0.768
2024-06-21 18:14:49,841 - INFO: Epoch: 87/200, Batch: 25/29, Batch_Loss_Train: 0.560
2024-06-21 18:14:50,137 - INFO: Epoch: 87/200, Batch: 26/29, Batch_Loss_Train: 0.775
2024-06-21 18:14:50,524 - INFO: Epoch: 87/200, Batch: 27/29, Batch_Loss_Train: 0.745
2024-06-21 18:14:50,832 - INFO: Epoch: 87/200, Batch: 28/29, Batch_Loss_Train: 0.633
2024-06-21 18:14:51,044 - INFO: Epoch: 87/200, Batch: 29/29, Batch_Loss_Train: 1.197
2024-06-21 18:15:01,769 - INFO: 87/200 final results:
2024-06-21 18:15:01,769 - INFO: Training loss: 0.774.
2024-06-21 18:15:01,769 - INFO: Training MAE: 0.766.
2024-06-21 18:15:01,769 - INFO: Training MSE: 1.227.
2024-06-21 18:15:22,393 - INFO: Epoch: 87/200, Loss_train: 0.7743224069989961, Loss_val: 1.871932181818732
2024-06-21 18:15:22,393 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:15:22,393 - INFO: Epoch 88/200...
2024-06-21 18:15:22,393 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:15:22,394 - INFO: Batch size: 32.
2024-06-21 18:15:22,397 - INFO: Dataset:
2024-06-21 18:15:22,398 - INFO: Batch size:
2024-06-21 18:15:22,398 - INFO: Number of workers:
2024-06-21 18:15:23,476 - INFO: Epoch: 88/200, Batch: 1/29, Batch_Loss_Train: 0.825
2024-06-21 18:15:23,782 - INFO: Epoch: 88/200, Batch: 2/29, Batch_Loss_Train: 0.779
2024-06-21 18:15:24,184 - INFO: Epoch: 88/200, Batch: 3/29, Batch_Loss_Train: 0.684
2024-06-21 18:15:24,501 - INFO: Epoch: 88/200, Batch: 4/29, Batch_Loss_Train: 0.875
2024-06-21 18:15:24,909 - INFO: Epoch: 88/200, Batch: 5/29, Batch_Loss_Train: 0.780
2024-06-21 18:15:25,208 - INFO: Epoch: 88/200, Batch: 6/29, Batch_Loss_Train: 0.877
2024-06-21 18:15:25,603 - INFO: Epoch: 88/200, Batch: 7/29, Batch_Loss_Train: 0.763
2024-06-21 18:15:25,916 - INFO: Epoch: 88/200, Batch: 8/29, Batch_Loss_Train: 0.773
2024-06-21 18:15:26,314 - INFO: Epoch: 88/200, Batch: 9/29, Batch_Loss_Train: 0.807
2024-06-21 18:15:26,609 - INFO: Epoch: 88/200, Batch: 10/29, Batch_Loss_Train: 0.679
2024-06-21 18:15:27,004 - INFO: Epoch: 88/200, Batch: 11/29, Batch_Loss_Train: 0.736
2024-06-21 18:15:27,321 - INFO: Epoch: 88/200, Batch: 12/29, Batch_Loss_Train: 0.687
2024-06-21 18:15:27,741 - INFO: Epoch: 88/200, Batch: 13/29, Batch_Loss_Train: 0.879
2024-06-21 18:15:28,046 - INFO: Epoch: 88/200, Batch: 14/29, Batch_Loss_Train: 0.872
2024-06-21 18:15:28,457 - INFO: Epoch: 88/200, Batch: 15/29, Batch_Loss_Train: 0.679
2024-06-21 18:15:28,771 - INFO: Epoch: 88/200, Batch: 16/29, Batch_Loss_Train: 0.796
2024-06-21 18:15:29,192 - INFO: Epoch: 88/200, Batch: 17/29, Batch_Loss_Train: 0.705
2024-06-21 18:15:29,493 - INFO: Epoch: 88/200, Batch: 18/29, Batch_Loss_Train: 0.777
2024-06-21 18:15:29,891 - INFO: Epoch: 88/200, Batch: 19/29, Batch_Loss_Train: 0.745
2024-06-21 18:15:30,200 - INFO: Epoch: 88/200, Batch: 20/29, Batch_Loss_Train: 0.820
2024-06-21 18:15:30,609 - INFO: Epoch: 88/200, Batch: 21/29, Batch_Loss_Train: 0.822
2024-06-21 18:15:30,911 - INFO: Epoch: 88/200, Batch: 22/29, Batch_Loss_Train: 0.720
2024-06-21 18:15:31,299 - INFO: Epoch: 88/200, Batch: 23/29, Batch_Loss_Train: 0.817
2024-06-21 18:15:31,612 - INFO: Epoch: 88/200, Batch: 24/29, Batch_Loss_Train: 0.768
2024-06-21 18:15:32,010 - INFO: Epoch: 88/200, Batch: 25/29, Batch_Loss_Train: 0.742
2024-06-21 18:15:32,306 - INFO: Epoch: 88/200, Batch: 26/29, Batch_Loss_Train: 0.682
2024-06-21 18:15:32,688 - INFO: Epoch: 88/200, Batch: 27/29, Batch_Loss_Train: 0.662
2024-06-21 18:15:32,995 - INFO: Epoch: 88/200, Batch: 28/29, Batch_Loss_Train: 0.713
2024-06-21 18:15:33,208 - INFO: Epoch: 88/200, Batch: 29/29, Batch_Loss_Train: 0.979
2024-06-21 18:15:44,374 - INFO: 88/200 final results:
2024-06-21 18:15:44,374 - INFO: Training loss: 0.774.
2024-06-21 18:15:44,374 - INFO: Training MAE: 0.770.
2024-06-21 18:15:44,374 - INFO: Training MSE: 1.220.
2024-06-21 18:16:04,675 - INFO: Epoch: 88/200, Loss_train: 0.7739076367739973, Loss_val: 1.8797403861736428
2024-06-21 18:16:04,676 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:16:04,676 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
