2024-06-21 13:57:39,409 - INFO: Device: cuda.
2024-06-21 13:57:39,409 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 13:57:39,409 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 13:57:39,409 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 13:57:39,409 - INFO: Seed: 4
2024-06-21 13:57:39,409 - INFO: 42 patients have been found in the data directory.
2024-06-21 13:57:39,448 - INFO: Train set contains 32 patients.
2024-06-21 13:57:39,448 - INFO: Val set contains 5 patients.
2024-06-21 13:57:39,448 - INFO: Test set contains 5 patients.
2024-06-21 13:57:39,448 - INFO: Fold: 0
2024-06-21 13:57:39,449 - INFO: Performing 2-fold Cross Validation.
2024-06-21 13:57:39,449 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 13:57:39,449 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 13:57:39,449 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 13:57:39,581 - INFO: To_device: False.
2024-06-21 13:57:39,583 - INFO: Transformers have been made successfully.
2024-06-21 13:57:39,583 - INFO: Dataset type: cache.
2024-06-21 13:57:39,583 - INFO: Dataloader type: standard.
2024-06-21 13:59:30,531 - INFO: Train dataloader arguments.
2024-06-21 13:59:30,532 - INFO: 	Batch_size: 32.
2024-06-21 13:59:30,532 - INFO: 	Shuffle: True.
2024-06-21 13:59:30,532 - INFO: 	Sampler: None.
2024-06-21 13:59:30,532 - INFO: 	Num_workers: 4.
2024-06-21 13:59:30,532 - INFO: 	Drop_last: False.
2024-06-21 13:59:30,702 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 7, 7), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 7, 7), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 7, 7), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 7, 7), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=1048576, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 13:59:31,567 - INFO: Weight init name: kaiming_uniform.
2024-06-21 13:59:34,536 - INFO: Number of training iterations per epoch: 29.
2024-06-21 13:59:34,536 - INFO: Epoch 1/10...
2024-06-21 13:59:34,536 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 13:59:34,536 - INFO: Batch size: 32.
2024-06-21 13:59:34,536 - INFO: Dataset:
2024-06-21 13:59:34,537 - INFO: Batch size:
2024-06-21 13:59:34,537 - INFO: Number of workers:
2024-06-21 13:59:37,938 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:38,007 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 78.765
2024-06-21 13:59:38,249 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:38,399 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 62.098
2024-06-21 13:59:38,684 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:38,834 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 70.039
2024-06-21 13:59:39,047 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:39,196 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 74.273
2024-06-21 13:59:39,488 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:39,637 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 65.409
2024-06-21 13:59:39,843 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:39,992 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 79.374
2024-06-21 13:59:40,278 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:40,427 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 659.936
2024-06-21 13:59:40,641 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:40,790 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 24710180.000
2024-06-21 13:59:41,090 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:41,240 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 2228797304897770510985723904.000
2024-06-21 13:59:41,444 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:41,593 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 13:59:41,859 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:42,008 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 13:59:42,219 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:42,368 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 13:59:42,689 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:42,838 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 13:59:43,049 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:43,198 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 13:59:43,486 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:43,635 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 13:59:43,844 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:43,993 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 13:59:44,310 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:44,459 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 13:59:44,670 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:44,819 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 13:59:45,101 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:45,249 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 13:59:45,454 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:45,603 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 13:59:45,908 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:46,057 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 13:59:46,266 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:46,415 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 13:59:46,682 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:46,831 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 13:59:47,040 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:47,189 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 13:59:47,492 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:47,641 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 13:59:47,851 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:48,000 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 13:59:48,286 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:48,435 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 13:59:48,646 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:48,795 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 13:59:50,542 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 13:59:50,573 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:00:01,851 - INFO: 1/10 final results:
2024-06-21 14:00:01,851 - INFO: Training loss: nan.
2024-06-21 14:00:01,851 - INFO: Training MAE: nan.
2024-06-21 14:00:01,851 - INFO: Training MSE: nan.
2024-06-21 14:00:22,348 - INFO: Epoch: 1/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:00:22,349 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:00:22,349 - INFO: Epoch 2/10...
2024-06-21 14:00:22,349 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:00:22,349 - INFO: Batch size: 32.
2024-06-21 14:00:22,351 - INFO: Dataset:
2024-06-21 14:00:22,351 - INFO: Batch size:
2024-06-21 14:00:22,351 - INFO: Number of workers:
2024-06-21 14:00:23,396 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:23,545 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:00:23,748 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:23,897 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:00:24,185 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:24,334 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:00:24,549 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:24,698 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:00:25,017 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:25,166 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:00:25,365 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:25,514 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:00:25,796 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:25,945 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:00:26,157 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:26,306 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:00:26,628 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:26,777 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:00:26,967 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:27,116 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:00:27,389 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:27,538 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:00:27,750 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:27,899 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:00:28,227 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:28,376 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:00:28,575 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:28,724 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:00:29,012 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:29,161 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:00:29,370 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:29,519 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:00:29,840 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:29,989 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:00:30,186 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:30,335 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:00:30,614 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:30,763 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:00:30,967 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:31,116 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:00:31,429 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:31,578 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:00:31,775 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:31,924 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:00:32,190 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:32,339 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:00:32,548 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:32,697 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:00:33,000 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:33,150 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:00:33,345 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:33,494 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:00:33,759 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:33,908 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:00:34,116 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:34,265 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:00:34,430 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:00:34,501 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:00:45,685 - INFO: 2/10 final results:
2024-06-21 14:00:45,685 - INFO: Training loss: nan.
2024-06-21 14:00:45,685 - INFO: Training MAE: nan.
2024-06-21 14:00:45,685 - INFO: Training MSE: nan.
2024-06-21 14:01:06,400 - INFO: Epoch: 2/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:01:06,400 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:01:06,400 - INFO: Epoch 3/10...
2024-06-21 14:01:06,400 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:01:06,400 - INFO: Batch size: 32.
2024-06-21 14:01:06,403 - INFO: Dataset:
2024-06-21 14:01:06,403 - INFO: Batch size:
2024-06-21 14:01:06,403 - INFO: Number of workers:
2024-06-21 14:01:07,419 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:07,567 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:01:07,782 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:07,931 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:01:08,219 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:08,368 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:01:08,582 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:08,731 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:01:09,037 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:09,186 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:01:09,383 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:09,532 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:01:09,812 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:09,961 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:01:10,170 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:10,319 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:01:10,624 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:10,773 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:01:10,964 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:11,113 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:01:11,388 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:11,536 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:01:11,747 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:11,896 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:01:12,208 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:12,357 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:01:12,556 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:12,705 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:01:12,995 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:13,144 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:01:13,353 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:13,501 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:01:13,811 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:13,960 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:01:14,157 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:14,306 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:01:14,593 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:14,742 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:01:14,948 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:15,097 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:01:15,400 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:15,548 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:01:15,744 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:15,894 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:01:16,175 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:16,324 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:01:16,532 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:16,681 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:01:16,978 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:17,127 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:01:17,322 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:17,471 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:01:17,751 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:17,900 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:01:18,107 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:18,256 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:01:18,420 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:18,491 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:01:29,626 - INFO: 3/10 final results:
2024-06-21 14:01:29,627 - INFO: Training loss: nan.
2024-06-21 14:01:29,627 - INFO: Training MAE: nan.
2024-06-21 14:01:29,627 - INFO: Training MSE: nan.
2024-06-21 14:01:50,486 - INFO: Epoch: 3/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:01:50,486 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:01:50,486 - INFO: Epoch 4/10...
2024-06-21 14:01:50,486 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:01:50,486 - INFO: Batch size: 32.
2024-06-21 14:01:50,488 - INFO: Dataset:
2024-06-21 14:01:50,489 - INFO: Batch size:
2024-06-21 14:01:50,489 - INFO: Number of workers:
2024-06-21 14:01:51,518 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:51,666 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:01:51,882 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:52,031 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:01:52,336 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:52,485 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:01:52,700 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:52,849 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:01:53,140 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:53,289 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:01:53,500 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:53,648 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:01:53,938 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:54,087 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:01:54,299 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:54,447 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:01:54,733 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:54,882 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:01:55,086 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:55,235 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:01:55,521 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:55,670 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:01:55,883 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:56,032 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:01:56,333 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:56,481 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:01:56,694 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:56,843 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:01:57,142 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:57,291 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:01:57,502 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:57,651 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:01:57,950 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:58,099 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:01:58,309 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:58,458 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:01:58,752 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:58,901 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:01:59,106 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:59,255 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:01:59,539 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:01:59,688 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:01:59,899 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:00,048 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:02:00,341 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:00,490 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:02:00,701 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:00,850 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:02:01,138 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:01,287 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:02:01,495 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:01,644 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:02:01,938 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:02,087 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:02:02,295 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:02,444 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:02:02,615 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:02,685 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:02:13,621 - INFO: 4/10 final results:
2024-06-21 14:02:13,621 - INFO: Training loss: nan.
2024-06-21 14:02:13,621 - INFO: Training MAE: nan.
2024-06-21 14:02:13,621 - INFO: Training MSE: nan.
2024-06-21 14:02:34,284 - INFO: Epoch: 4/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:02:34,285 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:02:34,285 - INFO: Epoch 5/10...
2024-06-21 14:02:34,285 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:02:34,285 - INFO: Batch size: 32.
2024-06-21 14:02:34,287 - INFO: Dataset:
2024-06-21 14:02:34,287 - INFO: Batch size:
2024-06-21 14:02:34,287 - INFO: Number of workers:
2024-06-21 14:02:35,296 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:35,445 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:02:35,661 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:35,810 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:02:36,113 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:36,261 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:02:36,462 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:36,611 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:02:36,912 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:37,061 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:02:37,258 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:37,407 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:02:37,700 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:37,849 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:02:38,046 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:38,195 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:02:38,485 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:38,634 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:02:38,826 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:38,975 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:02:39,278 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:39,427 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:02:39,626 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:39,775 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:02:40,075 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:40,224 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:02:40,423 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:40,572 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:02:40,883 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:41,032 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:02:41,228 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:41,377 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:02:41,661 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:41,809 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:02:42,006 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:42,155 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:02:42,461 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:42,610 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:02:42,803 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:42,952 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:02:43,237 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:43,386 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:02:43,583 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:43,732 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:02:44,037 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:44,185 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:02:44,382 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:44,531 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:02:44,811 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:44,960 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:02:45,155 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:45,304 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:02:45,599 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:45,748 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:02:45,943 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:46,092 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:02:46,244 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:02:46,315 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:02:57,513 - INFO: 5/10 final results:
2024-06-21 14:02:57,513 - INFO: Training loss: nan.
2024-06-21 14:02:57,513 - INFO: Training MAE: nan.
2024-06-21 14:02:57,513 - INFO: Training MSE: nan.
2024-06-21 14:03:18,127 - INFO: Epoch: 5/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:03:18,127 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:03:18,127 - INFO: Epoch 6/10...
2024-06-21 14:03:18,127 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:03:18,127 - INFO: Batch size: 32.
2024-06-21 14:03:18,129 - INFO: Dataset:
2024-06-21 14:03:18,130 - INFO: Batch size:
2024-06-21 14:03:18,130 - INFO: Number of workers:
2024-06-21 14:03:19,182 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:19,331 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:03:19,535 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:19,684 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:03:19,972 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:20,121 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:03:20,337 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:20,486 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:03:20,811 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:20,960 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:03:21,158 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:21,307 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:03:21,591 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:21,739 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:03:21,953 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:22,101 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:03:22,418 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:22,567 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:03:22,761 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:22,910 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:03:23,182 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:23,331 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:03:23,546 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:23,695 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:03:24,023 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:24,172 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:03:24,372 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:24,521 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:03:24,807 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:24,956 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:03:25,168 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:25,317 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:03:25,645 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:25,794 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:03:25,992 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:26,141 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:03:26,418 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:26,567 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:03:26,773 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:26,922 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:03:27,243 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:27,392 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:03:27,591 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:27,739 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:03:28,020 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:28,169 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:03:28,381 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:28,530 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:03:28,847 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:28,996 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:03:29,192 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:29,341 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:03:29,622 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:29,771 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:03:29,980 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:30,129 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:03:30,300 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:03:30,371 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:03:41,515 - INFO: 6/10 final results:
2024-06-21 14:03:41,516 - INFO: Training loss: nan.
2024-06-21 14:03:41,516 - INFO: Training MAE: nan.
2024-06-21 14:03:41,516 - INFO: Training MSE: nan.
2024-06-21 14:04:01,840 - INFO: Epoch: 6/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:04:01,840 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:04:01,840 - INFO: Epoch 7/10...
2024-06-21 14:04:01,841 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 14:04:01,841 - INFO: Batch size: 32.
2024-06-21 14:04:01,843 - INFO: Dataset:
2024-06-21 14:04:01,843 - INFO: Batch size:
2024-06-21 14:04:01,843 - INFO: Number of workers:
2024-06-21 14:04:02,845 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:02,994 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:04:03,224 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:03,373 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:04:03,666 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:03,815 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:04:04,030 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:04,179 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:04:04,484 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:04,632 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:04:04,844 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:04,993 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:04:05,278 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:05,426 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:04:05,640 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:05,788 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:04:06,085 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:06,234 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:04:06,439 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:06,588 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:04:06,863 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:07,012 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:04:07,226 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:07,375 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:04:07,686 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:07,835 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:04:08,048 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:08,197 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:04:08,490 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:08,639 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:04:08,851 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:09,000 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:04:09,306 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:09,454 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:04:09,665 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:09,814 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:04:10,099 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:10,248 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:04:10,454 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:10,603 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:04:10,906 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:11,055 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:04:11,266 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:11,415 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:04:11,696 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:11,845 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:04:12,056 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:12,205 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:04:12,508 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:12,657 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:04:12,867 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:13,016 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:04:13,299 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:13,448 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:04:13,657 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:13,806 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:04:13,979 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.002542355689225227
2024-06-21 14:04:14,050 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 14:04:25,213 - INFO: 7/10 final results:
2024-06-21 14:04:25,213 - INFO: Training loss: nan.
2024-06-21 14:04:25,213 - INFO: Training MAE: nan.
2024-06-21 14:04:25,213 - INFO: Training MSE: nan.
2024-06-21 14:04:45,677 - INFO: Epoch: 7/10, Loss_train: nan, Loss_val: nan
2024-06-21 14:04:45,677 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 14:04:45,677 - INFO: Epoch 8/10...
2024-06-21 14:04:45,677 - INFO: Learning rate: 0.0012711778446126134.
2024-06-21 14:04:45,677 - INFO: Batch size: 32.
2024-06-21 14:04:45,679 - INFO: Dataset:
2024-06-21 14:04:45,680 - INFO: Batch size:
2024-06-21 14:04:45,680 - INFO: Number of workers:
2024-06-21 14:04:46,691 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:46,840 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 14:04:47,069 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:47,218 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 14:04:47,509 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:47,658 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 14:04:47,874 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:48,023 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 14:04:48,327 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:48,476 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 14:04:48,689 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:48,838 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 14:04:49,120 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:49,269 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 14:04:49,481 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:49,630 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 14:04:49,931 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:50,080 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 14:04:50,288 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:50,437 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 14:04:50,717 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:50,866 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 14:04:51,079 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:51,228 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 14:04:51,550 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:51,698 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 14:04:51,914 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:52,063 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 14:04:52,354 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:52,503 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 14:04:52,716 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:52,865 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 14:04:53,182 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:53,331 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 14:04:53,544 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:53,693 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 14:04:53,980 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:54,128 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 14:04:54,337 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:54,486 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 14:04:54,795 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:54,944 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 14:04:55,158 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:55,307 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 14:04:55,597 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:55,746 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 14:04:55,960 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:56,109 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 14:04:56,411 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:56,560 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 14:04:56,769 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:56,918 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 14:04:57,202 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:57,351 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 14:04:57,560 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:57,708 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 14:04:57,883 - INFO: HIIIIIIIIIIIIIIIIIIIIIIIIIII Learning rate: 0.0012711778446126134
2024-06-21 14:04:57,954 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: nan
