2024-06-21 17:12:13,549 - INFO: Device: cuda.
2024-06-21 17:12:13,549 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 17:12:13,549 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 17:12:13,549 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 17:12:13,549 - INFO: Seed: 4
2024-06-21 17:12:13,549 - INFO: 42 patients have been found in the data directory.
2024-06-21 17:12:13,587 - INFO: Train set contains 32 patients.
2024-06-21 17:12:13,587 - INFO: Val set contains 5 patients.
2024-06-21 17:12:13,587 - INFO: Test set contains 5 patients.
2024-06-21 17:12:13,588 - INFO: Fold: 0
2024-06-21 17:12:13,588 - INFO: Performing 2-fold Cross Validation.
2024-06-21 17:12:13,589 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 17:12:13,589 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 17:12:13,589 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 17:12:13,720 - INFO: To_device: False.
2024-06-21 17:12:13,721 - INFO: Transformers have been made successfully.
2024-06-21 17:12:13,721 - INFO: Dataset type: cache.
2024-06-21 17:12:13,721 - INFO: Dataloader type: standard.
2024-06-21 17:14:04,751 - INFO: Train dataloader arguments.
2024-06-21 17:14:04,752 - INFO: 	Batch_size: 32.
2024-06-21 17:14:04,752 - INFO: 	Shuffle: True.
2024-06-21 17:14:04,752 - INFO: 	Sampler: None.
2024-06-21 17:14:04,752 - INFO: 	Num_workers: 4.
2024-06-21 17:14:04,752 - INFO: 	Drop_last: False.
2024-06-21 17:14:04,770 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=65536, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 17:14:05,609 - INFO: Weight init name: kaiming_uniform.
2024-06-21 17:14:08,056 - INFO: Number of training iterations per epoch: 29.
2024-06-21 17:14:08,056 - INFO: Epoch 1/200...
2024-06-21 17:14:08,056 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:14:08,056 - INFO: Batch size: 32.
2024-06-21 17:14:08,057 - INFO: Dataset:
2024-06-21 17:14:08,057 - INFO: Batch size:
2024-06-21 17:14:08,057 - INFO: Number of workers:
2024-06-21 17:14:10,919 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 7.941
2024-06-21 17:14:11,212 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 7.283
2024-06-21 17:14:11,584 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 8.065
2024-06-21 17:14:11,892 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 7.159
2024-06-21 17:14:12,284 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 11.383
2024-06-21 17:14:12,590 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 8.053
2024-06-21 17:14:12,959 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 7.071
2024-06-21 17:14:13,265 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 7.700
2024-06-21 17:14:13,651 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 8.764
2024-06-21 17:14:13,953 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 9.054
2024-06-21 17:14:14,322 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 6.702
2024-06-21 17:14:14,630 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 6.761
2024-06-21 17:14:15,046 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 7.467
2024-06-21 17:14:15,357 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 7.748
2024-06-21 17:14:15,741 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 7.260
2024-06-21 17:14:16,050 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 6.604
2024-06-21 17:14:16,464 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 6.520
2024-06-21 17:14:16,774 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 5.611
2024-06-21 17:14:17,147 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 5.903
2024-06-21 17:14:17,452 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 6.401
2024-06-21 17:14:17,839 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 5.672
2024-06-21 17:14:18,146 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 5.818
2024-06-21 17:14:18,523 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 6.402
2024-06-21 17:14:18,832 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 5.689
2024-06-21 17:14:19,217 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 5.555
2024-06-21 17:14:19,523 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 6.681
2024-06-21 17:14:19,900 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 5.704
2024-06-21 17:14:20,209 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 6.625
2024-06-21 17:14:21,502 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 7.308
2024-06-21 17:14:32,559 - INFO: 1/200 final results:
2024-06-21 17:14:32,559 - INFO: Training loss: 7.066.
2024-06-21 17:14:32,559 - INFO: Training MAE: 7.061.
2024-06-21 17:14:32,559 - INFO: Training MSE: 72.329.
2024-06-21 17:14:52,729 - INFO: Epoch: 1/200, Loss_train: 7.065634184870227, Loss_val: 7.265348023381726
2024-06-21 17:14:52,729 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 17:14:52,729 - INFO: Epoch 2/200...
2024-06-21 17:14:52,729 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:14:52,729 - INFO: Batch size: 32.
2024-06-21 17:14:52,732 - INFO: Dataset:
2024-06-21 17:14:52,733 - INFO: Batch size:
2024-06-21 17:14:52,733 - INFO: Number of workers:
2024-06-21 17:14:53,783 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 6.902
2024-06-21 17:14:54,098 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 6.028
2024-06-21 17:14:54,513 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 6.066
2024-06-21 17:14:54,813 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 6.406
2024-06-21 17:14:55,215 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 5.592
2024-06-21 17:14:55,511 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 6.020
2024-06-21 17:14:55,915 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 6.136
2024-06-21 17:14:56,212 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 5.957
2024-06-21 17:14:56,604 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 5.785
2024-06-21 17:14:56,895 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 6.025
2024-06-21 17:14:57,309 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 6.576
2024-06-21 17:14:57,607 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 6.435
2024-06-21 17:14:58,017 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 6.265
2024-06-21 17:14:58,315 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 6.160
2024-06-21 17:14:58,735 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 6.690
2024-06-21 17:14:59,033 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 5.906
2024-06-21 17:14:59,426 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 6.370
2024-06-21 17:14:59,722 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 5.944
2024-06-21 17:15:00,134 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 5.732
2024-06-21 17:15:00,426 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 6.150
2024-06-21 17:15:00,805 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 6.206
2024-06-21 17:15:01,102 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 6.039
2024-06-21 17:15:01,519 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 6.383
2024-06-21 17:15:01,815 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 5.860
2024-06-21 17:15:02,196 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 6.652
2024-06-21 17:15:02,491 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 6.145
2024-06-21 17:15:02,884 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 5.744
2024-06-21 17:15:03,178 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 5.835
2024-06-21 17:15:03,376 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 5.513
2024-06-21 17:15:14,445 - INFO: 2/200 final results:
2024-06-21 17:15:14,446 - INFO: Training loss: 6.121.
2024-06-21 17:15:14,446 - INFO: Training MAE: 6.133.
2024-06-21 17:15:14,446 - INFO: Training MSE: 55.909.
2024-06-21 17:15:34,628 - INFO: Epoch: 2/200, Loss_train: 6.121452874150769, Loss_val: 6.089674653678105
2024-06-21 17:15:34,646 - INFO: Saved new best metric model for epoch 2.
2024-06-21 17:15:34,646 - INFO: Best internal validation val_loss: 6.090 at epoch: 2.
2024-06-21 17:15:34,646 - INFO: Epoch 3/200...
2024-06-21 17:15:34,646 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:15:34,646 - INFO: Batch size: 32.
2024-06-21 17:15:34,650 - INFO: Dataset:
2024-06-21 17:15:34,650 - INFO: Batch size:
2024-06-21 17:15:34,651 - INFO: Number of workers:
2024-06-21 17:15:35,703 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 6.805
2024-06-21 17:15:36,021 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 5.567
2024-06-21 17:15:36,425 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 6.319
2024-06-21 17:15:36,742 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 5.349
2024-06-21 17:15:37,147 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 5.314
2024-06-21 17:15:37,446 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 6.009
2024-06-21 17:15:37,840 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 6.161
2024-06-21 17:15:38,152 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 5.862
2024-06-21 17:15:38,558 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 6.059
2024-06-21 17:15:38,850 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 6.900
2024-06-21 17:15:39,241 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 6.397
2024-06-21 17:15:39,554 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 6.059
2024-06-21 17:15:39,966 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 6.665
2024-06-21 17:15:40,267 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 5.706
2024-06-21 17:15:40,670 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 5.570
2024-06-21 17:15:40,980 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 8.470
2024-06-21 17:15:41,394 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 7.073
2024-06-21 17:15:41,693 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 5.537
2024-06-21 17:15:42,088 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 5.637
2024-06-21 17:15:42,394 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 6.372
2024-06-21 17:15:42,796 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 6.090
2024-06-21 17:15:43,094 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 6.094
2024-06-21 17:15:43,479 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 5.876
2024-06-21 17:15:43,790 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 6.350
2024-06-21 17:15:44,185 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 5.957
2024-06-21 17:15:44,480 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 6.086
2024-06-21 17:15:44,862 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 4.922
2024-06-21 17:15:45,169 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 5.984
2024-06-21 17:15:45,379 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 6.627
2024-06-21 17:15:56,446 - INFO: 3/200 final results:
2024-06-21 17:15:56,446 - INFO: Training loss: 6.132.
2024-06-21 17:15:56,446 - INFO: Training MAE: 6.122.
2024-06-21 17:15:56,446 - INFO: Training MSE: 55.762.
2024-06-21 17:16:16,766 - INFO: Epoch: 3/200, Loss_train: 6.131585811746532, Loss_val: 6.317991092287261
2024-06-21 17:16:16,766 - INFO: Best internal validation val_loss: 6.090 at epoch: 2.
2024-06-21 17:16:16,766 - INFO: Epoch 4/200...
2024-06-21 17:16:16,766 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:16:16,766 - INFO: Batch size: 32.
2024-06-21 17:16:16,770 - INFO: Dataset:
2024-06-21 17:16:16,770 - INFO: Batch size:
2024-06-21 17:16:16,770 - INFO: Number of workers:
2024-06-21 17:16:17,830 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 6.312
2024-06-21 17:16:18,148 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 6.231
2024-06-21 17:16:18,554 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 5.199
2024-06-21 17:16:18,870 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 5.392
2024-06-21 17:16:19,270 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 4.966
2024-06-21 17:16:19,582 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 5.560
2024-06-21 17:16:19,980 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 5.663
2024-06-21 17:16:20,292 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 5.608
2024-06-21 17:16:20,683 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 6.154
2024-06-21 17:16:20,988 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 6.057
2024-06-21 17:16:21,376 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 6.095
2024-06-21 17:16:21,690 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 6.020
2024-06-21 17:16:22,096 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 6.171
2024-06-21 17:16:22,410 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 6.101
2024-06-21 17:16:22,815 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 5.501
2024-06-21 17:16:23,126 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 4.719
2024-06-21 17:16:23,531 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 5.497
2024-06-21 17:16:23,843 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 6.083
2024-06-21 17:16:24,241 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 5.315
2024-06-21 17:16:24,547 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 5.510
2024-06-21 17:16:24,944 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 5.758
2024-06-21 17:16:25,255 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 5.881
2024-06-21 17:16:25,649 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 5.018
2024-06-21 17:16:25,963 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 5.677
2024-06-21 17:16:26,351 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 5.817
2024-06-21 17:16:26,661 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 5.832
2024-06-21 17:16:27,051 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 4.967
2024-06-21 17:16:27,359 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 6.025
2024-06-21 17:16:27,578 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 6.346
2024-06-21 17:16:38,744 - INFO: 4/200 final results:
2024-06-21 17:16:38,744 - INFO: Training loss: 5.706.
2024-06-21 17:16:38,745 - INFO: Training MAE: 5.693.
2024-06-21 17:16:38,745 - INFO: Training MSE: 50.945.
2024-06-21 17:16:59,283 - INFO: Epoch: 4/200, Loss_train: 5.705944307919206, Loss_val: 5.995601818479341
2024-06-21 17:16:59,302 - INFO: Saved new best metric model for epoch 4.
2024-06-21 17:16:59,302 - INFO: Best internal validation val_loss: 5.996 at epoch: 4.
2024-06-21 17:16:59,302 - INFO: Epoch 5/200...
2024-06-21 17:16:59,302 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:16:59,302 - INFO: Batch size: 32.
2024-06-21 17:16:59,306 - INFO: Dataset:
2024-06-21 17:16:59,306 - INFO: Batch size:
2024-06-21 17:16:59,306 - INFO: Number of workers:
2024-06-21 17:17:00,381 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 5.561
2024-06-21 17:17:00,700 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 5.653
2024-06-21 17:17:01,086 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 6.059
2024-06-21 17:17:01,401 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 5.437
2024-06-21 17:17:01,810 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 5.666
2024-06-21 17:17:02,122 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 5.107
2024-06-21 17:17:02,508 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 5.543
2024-06-21 17:17:02,822 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 5.186
2024-06-21 17:17:03,225 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 5.523
2024-06-21 17:17:03,531 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 5.589
2024-06-21 17:17:03,910 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 5.338
2024-06-21 17:17:04,226 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 5.512
2024-06-21 17:17:04,644 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 5.695
2024-06-21 17:17:04,962 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 5.150
2024-06-21 17:17:05,357 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 4.906
2024-06-21 17:17:05,671 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 5.818
2024-06-21 17:17:06,086 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 4.761
2024-06-21 17:17:06,399 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 4.471
2024-06-21 17:17:06,783 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 5.643
2024-06-21 17:17:07,091 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 5.647
2024-06-21 17:17:07,496 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 6.147
2024-06-21 17:17:07,810 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 5.369
2024-06-21 17:17:08,191 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 5.379
2024-06-21 17:17:08,505 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 5.787
2024-06-21 17:17:08,902 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 5.039
2024-06-21 17:17:09,211 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 5.300
2024-06-21 17:17:09,588 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 6.164
2024-06-21 17:17:09,898 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 5.241
2024-06-21 17:17:10,108 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 5.248
2024-06-21 17:17:21,259 - INFO: 5/200 final results:
2024-06-21 17:17:21,259 - INFO: Training loss: 5.446.
2024-06-21 17:17:21,259 - INFO: Training MAE: 5.450.
2024-06-21 17:17:21,259 - INFO: Training MSE: 47.991.
2024-06-21 17:17:41,498 - INFO: Epoch: 5/200, Loss_train: 5.446167880091174, Loss_val: 62.81978054704337
2024-06-21 17:17:41,499 - INFO: Best internal validation val_loss: 5.996 at epoch: 4.
2024-06-21 17:17:41,499 - INFO: Epoch 6/200...
2024-06-21 17:17:41,499 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:17:41,499 - INFO: Batch size: 32.
2024-06-21 17:17:41,502 - INFO: Dataset:
2024-06-21 17:17:41,502 - INFO: Batch size:
2024-06-21 17:17:41,503 - INFO: Number of workers:
2024-06-21 17:17:42,568 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 5.560
2024-06-21 17:17:42,902 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 4.567
2024-06-21 17:17:43,301 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 4.596
2024-06-21 17:17:43,620 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 5.067
2024-06-21 17:17:44,020 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 5.724
2024-06-21 17:17:44,346 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 4.981
2024-06-21 17:17:44,733 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 4.367
2024-06-21 17:17:45,048 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 5.306
2024-06-21 17:17:45,437 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 5.595
2024-06-21 17:17:45,766 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 4.929
2024-06-21 17:17:46,143 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 5.231
2024-06-21 17:17:46,459 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 5.567
2024-06-21 17:17:46,864 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 5.326
2024-06-21 17:17:47,194 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 4.778
2024-06-21 17:17:47,600 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 4.490
2024-06-21 17:17:47,920 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 4.713
2024-06-21 17:17:48,341 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 5.418
2024-06-21 17:17:48,675 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 4.999
2024-06-21 17:17:49,068 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 5.217
2024-06-21 17:17:49,376 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 4.695
2024-06-21 17:17:49,772 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 5.517
2024-06-21 17:17:50,099 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 5.226
2024-06-21 17:17:50,476 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 4.948
2024-06-21 17:17:50,791 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 4.718
2024-06-21 17:17:51,176 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 4.933
2024-06-21 17:17:51,499 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 5.355
2024-06-21 17:17:51,878 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 4.902
2024-06-21 17:17:52,189 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 4.862
2024-06-21 17:17:52,403 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 5.056
2024-06-21 17:18:03,553 - INFO: 6/200 final results:
2024-06-21 17:18:03,553 - INFO: Training loss: 5.057.
2024-06-21 17:18:03,553 - INFO: Training MAE: 5.057.
2024-06-21 17:18:03,553 - INFO: Training MSE: 42.488.
2024-06-21 17:18:24,013 - INFO: Epoch: 6/200, Loss_train: 5.056639737096326, Loss_val: 5.429097093384842
2024-06-21 17:18:24,032 - INFO: Saved new best metric model for epoch 6.
2024-06-21 17:18:24,032 - INFO: Best internal validation val_loss: 5.429 at epoch: 6.
2024-06-21 17:18:24,032 - INFO: Epoch 7/200...
2024-06-21 17:18:24,032 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:18:24,032 - INFO: Batch size: 32.
2024-06-21 17:18:24,037 - INFO: Dataset:
2024-06-21 17:18:24,037 - INFO: Batch size:
2024-06-21 17:18:24,037 - INFO: Number of workers:
2024-06-21 17:18:25,107 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 5.487
2024-06-21 17:18:25,424 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 4.856
2024-06-21 17:18:25,804 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 4.458
2024-06-21 17:18:26,119 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 5.894
2024-06-21 17:18:26,522 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 4.454
2024-06-21 17:18:26,835 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 5.163
2024-06-21 17:18:27,203 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 6.046
2024-06-21 17:18:27,514 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 4.417
2024-06-21 17:18:27,908 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 4.100
2024-06-21 17:18:28,213 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 4.801
2024-06-21 17:18:28,573 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 4.541
2024-06-21 17:18:28,887 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 3.819
2024-06-21 17:18:29,310 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 4.491
2024-06-21 17:18:29,627 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 4.921
2024-06-21 17:18:30,025 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 4.482
2024-06-21 17:18:30,339 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 3.925
2024-06-21 17:18:30,758 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 4.923
2024-06-21 17:18:31,068 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 4.587
2024-06-21 17:18:31,455 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 4.666
2024-06-21 17:18:31,761 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 4.690
2024-06-21 17:18:32,172 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 4.875
2024-06-21 17:18:32,488 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 5.633
2024-06-21 17:18:32,861 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 4.533
2024-06-21 17:18:33,176 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 4.159
2024-06-21 17:18:33,577 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 5.811
2024-06-21 17:18:33,887 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 4.505
2024-06-21 17:18:34,265 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 5.339
2024-06-21 17:18:34,576 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 5.065
2024-06-21 17:18:34,792 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 4.986
2024-06-21 17:18:45,863 - INFO: 7/200 final results:
2024-06-21 17:18:45,864 - INFO: Training loss: 4.815.
2024-06-21 17:18:45,864 - INFO: Training MAE: 4.811.
2024-06-21 17:18:45,864 - INFO: Training MSE: 39.028.
2024-06-21 17:19:05,999 - INFO: Epoch: 7/200, Loss_train: 4.81472479063889, Loss_val: 5.336269033366237
2024-06-21 17:19:06,018 - INFO: Saved new best metric model for epoch 7.
2024-06-21 17:19:06,018 - INFO: Best internal validation val_loss: 5.336 at epoch: 7.
2024-06-21 17:19:06,018 - INFO: Epoch 8/200...
2024-06-21 17:19:06,018 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:19:06,018 - INFO: Batch size: 32.
2024-06-21 17:19:06,022 - INFO: Dataset:
2024-06-21 17:19:06,022 - INFO: Batch size:
2024-06-21 17:19:06,022 - INFO: Number of workers:
2024-06-21 17:19:07,110 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 4.561
2024-06-21 17:19:07,416 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 4.363
2024-06-21 17:19:07,804 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 5.099
2024-06-21 17:19:08,120 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 4.179
2024-06-21 17:19:08,538 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 4.497
2024-06-21 17:19:08,836 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 4.300
2024-06-21 17:19:09,214 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 4.407
2024-06-21 17:19:09,524 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 4.223
2024-06-21 17:19:09,938 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 4.158
2024-06-21 17:19:10,230 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 3.779
2024-06-21 17:19:10,604 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 4.489
2024-06-21 17:19:10,919 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 4.398
2024-06-21 17:19:11,346 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 3.884
2024-06-21 17:19:11,646 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 4.780
2024-06-21 17:19:12,034 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 4.937
2024-06-21 17:19:12,345 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 4.060
2024-06-21 17:19:12,766 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 5.710
2024-06-21 17:19:13,067 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 4.747
2024-06-21 17:19:13,453 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 4.669
2024-06-21 17:19:13,761 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 4.762
2024-06-21 17:19:14,185 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 4.007
2024-06-21 17:19:14,487 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 4.513
2024-06-21 17:19:14,867 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 4.217
2024-06-21 17:19:15,183 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 5.080
2024-06-21 17:19:15,596 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 4.377
2024-06-21 17:19:15,891 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 4.630
2024-06-21 17:19:16,269 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 4.132
2024-06-21 17:19:16,577 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 4.620
2024-06-21 17:19:16,790 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 4.018
2024-06-21 17:19:27,993 - INFO: 8/200 final results:
2024-06-21 17:19:27,993 - INFO: Training loss: 4.469.
2024-06-21 17:19:27,993 - INFO: Training MAE: 4.478.
2024-06-21 17:19:27,993 - INFO: Training MSE: 34.736.
2024-06-21 17:19:48,572 - INFO: Epoch: 8/200, Loss_train: 4.4688659618640765, Loss_val: 4.9376300285602435
2024-06-21 17:19:48,590 - INFO: Saved new best metric model for epoch 8.
2024-06-21 17:19:48,591 - INFO: Best internal validation val_loss: 4.938 at epoch: 8.
2024-06-21 17:19:48,591 - INFO: Epoch 9/200...
2024-06-21 17:19:48,591 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:19:48,591 - INFO: Batch size: 32.
2024-06-21 17:19:48,595 - INFO: Dataset:
2024-06-21 17:19:48,595 - INFO: Batch size:
2024-06-21 17:19:48,595 - INFO: Number of workers:
2024-06-21 17:19:49,686 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 4.585
2024-06-21 17:19:49,992 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 4.605
2024-06-21 17:19:50,385 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 3.983
2024-06-21 17:19:50,702 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 3.840
2024-06-21 17:19:51,124 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 4.863
2024-06-21 17:19:51,427 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 4.295
2024-06-21 17:19:51,811 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 3.889
2024-06-21 17:19:52,127 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 4.696
2024-06-21 17:19:52,541 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 4.714
2024-06-21 17:19:52,834 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 4.298
2024-06-21 17:19:53,207 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 4.154
2024-06-21 17:19:53,524 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 3.945
2024-06-21 17:19:53,945 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 4.416
2024-06-21 17:19:54,249 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 4.595
2024-06-21 17:19:54,657 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 4.030
2024-06-21 17:19:54,979 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 3.828
2024-06-21 17:19:55,408 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 4.091
2024-06-21 17:19:55,708 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 4.145
2024-06-21 17:19:56,091 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 3.957
2024-06-21 17:19:56,399 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 4.682
2024-06-21 17:19:56,822 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 4.037
2024-06-21 17:19:57,124 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 4.028
2024-06-21 17:19:57,512 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 4.102
2024-06-21 17:19:57,829 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 4.771
2024-06-21 17:19:58,250 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 4.492
2024-06-21 17:19:58,549 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 4.119
2024-06-21 17:19:58,936 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 4.610
2024-06-21 17:19:59,248 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 4.172
2024-06-21 17:19:59,469 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 4.537
2024-06-21 17:20:10,622 - INFO: 9/200 final results:
2024-06-21 17:20:10,622 - INFO: Training loss: 4.292.
2024-06-21 17:20:10,622 - INFO: Training MAE: 4.288.
2024-06-21 17:20:10,622 - INFO: Training MSE: 32.897.
2024-06-21 17:20:31,160 - INFO: Epoch: 9/200, Loss_train: 4.29236967810269, Loss_val: 4.314211820733958
2024-06-21 17:20:31,179 - INFO: Saved new best metric model for epoch 9.
2024-06-21 17:20:31,179 - INFO: Best internal validation val_loss: 4.314 at epoch: 9.
2024-06-21 17:20:31,179 - INFO: Epoch 10/200...
2024-06-21 17:20:31,179 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:20:31,179 - INFO: Batch size: 32.
2024-06-21 17:20:31,183 - INFO: Dataset:
2024-06-21 17:20:31,183 - INFO: Batch size:
2024-06-21 17:20:31,183 - INFO: Number of workers:
2024-06-21 17:20:32,267 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 4.413
2024-06-21 17:20:32,574 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 3.365
2024-06-21 17:20:32,971 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 4.467
2024-06-21 17:20:33,290 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 4.729
2024-06-21 17:20:33,719 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 4.078
2024-06-21 17:20:34,021 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 3.944
2024-06-21 17:20:34,412 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 4.578
2024-06-21 17:20:34,727 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 3.532
2024-06-21 17:20:35,153 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 4.096
2024-06-21 17:20:35,447 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 3.870
2024-06-21 17:20:35,823 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 4.700
2024-06-21 17:20:36,140 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 4.636
2024-06-21 17:20:36,576 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 4.863
2024-06-21 17:20:36,880 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 4.526
2024-06-21 17:20:37,277 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 4.339
2024-06-21 17:20:37,591 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 4.437
2024-06-21 17:20:38,025 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 3.946
2024-06-21 17:20:38,325 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 4.029
2024-06-21 17:20:38,714 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 4.787
2024-06-21 17:20:39,021 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 3.957
2024-06-21 17:20:39,444 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 4.041
2024-06-21 17:20:39,747 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 3.652
2024-06-21 17:20:40,134 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 4.891
2024-06-21 17:20:40,451 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 4.419
2024-06-21 17:20:40,872 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 4.317
2024-06-21 17:20:41,170 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 4.250
2024-06-21 17:20:41,550 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 5.100
2024-06-21 17:20:41,861 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 4.498
2024-06-21 17:20:42,082 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 4.641
2024-06-21 17:20:53,299 - INFO: 10/200 final results:
2024-06-21 17:20:53,299 - INFO: Training loss: 4.314.
2024-06-21 17:20:53,299 - INFO: Training MAE: 4.307.
2024-06-21 17:20:53,299 - INFO: Training MSE: 33.306.
2024-06-21 17:21:13,806 - INFO: Epoch: 10/200, Loss_train: 4.31391953599864, Loss_val: 103.39461648875269
2024-06-21 17:21:13,807 - INFO: Best internal validation val_loss: 4.314 at epoch: 9.
2024-06-21 17:21:13,807 - INFO: Epoch 11/200...
2024-06-21 17:21:13,807 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:21:13,807 - INFO: Batch size: 32.
2024-06-21 17:21:13,810 - INFO: Dataset:
2024-06-21 17:21:13,810 - INFO: Batch size:
2024-06-21 17:21:13,810 - INFO: Number of workers:
2024-06-21 17:21:14,863 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 6.001
2024-06-21 17:21:15,180 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 4.650
2024-06-21 17:21:15,588 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 4.108
2024-06-21 17:21:15,906 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 4.375
2024-06-21 17:21:16,317 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 4.460
2024-06-21 17:21:16,616 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 5.542
2024-06-21 17:21:17,015 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 3.642
2024-06-21 17:21:17,329 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 5.522
2024-06-21 17:21:17,738 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 4.302
2024-06-21 17:21:18,029 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 5.402
2024-06-21 17:21:18,418 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 4.936
2024-06-21 17:21:18,734 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 4.751
2024-06-21 17:21:19,152 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 4.129
2024-06-21 17:21:19,455 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 4.876
2024-06-21 17:21:19,860 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 4.711
2024-06-21 17:21:20,173 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 4.848
2024-06-21 17:21:20,591 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 4.086
2024-06-21 17:21:20,890 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 4.684
2024-06-21 17:21:21,287 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 3.985
2024-06-21 17:21:21,596 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 4.046
2024-06-21 17:21:22,001 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 4.915
2024-06-21 17:21:22,302 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 4.690
2024-06-21 17:21:22,701 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 4.522
2024-06-21 17:21:23,015 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 4.523
2024-06-21 17:21:23,422 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 4.417
2024-06-21 17:21:23,719 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 4.762
2024-06-21 17:21:24,116 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 4.017
2024-06-21 17:21:24,425 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 4.296
2024-06-21 17:21:24,647 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 4.370
2024-06-21 17:21:35,666 - INFO: 11/200 final results:
2024-06-21 17:21:35,666 - INFO: Training loss: 4.606.
2024-06-21 17:21:35,666 - INFO: Training MAE: 4.610.
2024-06-21 17:21:35,666 - INFO: Training MSE: 36.381.
2024-06-21 17:21:55,642 - INFO: Epoch: 11/200, Loss_train: 4.605817293298656, Loss_val: 4.235547526129361
2024-06-21 17:21:55,900 - INFO: Saved new best metric model for epoch 11.
2024-06-21 17:21:55,900 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:21:55,900 - INFO: Epoch 12/200...
2024-06-21 17:21:55,901 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:21:55,901 - INFO: Batch size: 32.
2024-06-21 17:21:55,905 - INFO: Dataset:
2024-06-21 17:21:55,905 - INFO: Batch size:
2024-06-21 17:21:55,905 - INFO: Number of workers:
2024-06-21 17:21:56,994 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 4.668
2024-06-21 17:21:57,301 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 4.343
2024-06-21 17:21:57,711 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 4.826
2024-06-21 17:21:58,031 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 5.450
2024-06-21 17:21:58,441 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 3.668
2024-06-21 17:21:58,744 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 3.814
2024-06-21 17:21:59,144 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 3.805
2024-06-21 17:21:59,459 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 5.037
2024-06-21 17:21:59,865 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 4.583
2024-06-21 17:22:00,160 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 4.426
2024-06-21 17:22:00,548 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 4.629
2024-06-21 17:22:00,866 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 4.592
2024-06-21 17:22:01,284 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 4.424
2024-06-21 17:22:01,589 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 4.392
2024-06-21 17:22:01,997 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 4.064
2024-06-21 17:22:02,309 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 4.377
2024-06-21 17:22:02,727 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 4.190
2024-06-21 17:22:03,027 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 3.918
2024-06-21 17:22:03,423 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 3.941
2024-06-21 17:22:03,732 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 4.106
2024-06-21 17:22:04,139 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 3.744
2024-06-21 17:22:04,443 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 3.804
2024-06-21 17:22:04,837 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 5.012
2024-06-21 17:22:05,152 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 3.825
2024-06-21 17:22:05,552 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 4.117
2024-06-21 17:22:05,850 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 4.699
2024-06-21 17:22:06,247 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 3.980
2024-06-21 17:22:06,559 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 4.456
2024-06-21 17:22:06,773 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 3.859
2024-06-21 17:22:17,603 - INFO: 12/200 final results:
2024-06-21 17:22:17,604 - INFO: Training loss: 4.302.
2024-06-21 17:22:17,604 - INFO: Training MAE: 4.310.
2024-06-21 17:22:17,604 - INFO: Training MSE: 33.896.
2024-06-21 17:22:37,913 - INFO: Epoch: 12/200, Loss_train: 4.301658202861917, Loss_val: 5.254584049356395
2024-06-21 17:22:37,913 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:22:37,913 - INFO: Epoch 13/200...
2024-06-21 17:22:37,913 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:22:37,913 - INFO: Batch size: 32.
2024-06-21 17:22:37,917 - INFO: Dataset:
2024-06-21 17:22:37,917 - INFO: Batch size:
2024-06-21 17:22:37,917 - INFO: Number of workers:
2024-06-21 17:22:38,985 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 3.772
2024-06-21 17:22:39,305 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 4.258
2024-06-21 17:22:39,711 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 4.187
2024-06-21 17:22:40,028 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 4.251
2024-06-21 17:22:40,439 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 4.368
2024-06-21 17:22:40,738 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 3.869
2024-06-21 17:22:41,137 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 3.770
2024-06-21 17:22:41,450 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 4.669
2024-06-21 17:22:41,856 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 4.084
2024-06-21 17:22:42,148 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 3.987
2024-06-21 17:22:42,534 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 3.997
2024-06-21 17:22:42,848 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 3.868
2024-06-21 17:22:43,263 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 3.855
2024-06-21 17:22:43,565 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 4.055
2024-06-21 17:22:43,970 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 4.858
2024-06-21 17:22:44,280 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 4.042
2024-06-21 17:22:44,695 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 3.847
2024-06-21 17:22:44,994 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 3.461
2024-06-21 17:22:45,389 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 4.732
2024-06-21 17:22:45,694 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 3.650
2024-06-21 17:22:46,101 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 3.851
2024-06-21 17:22:46,402 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 3.743
2024-06-21 17:22:46,785 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 4.280
2024-06-21 17:22:47,097 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 4.369
2024-06-21 17:22:47,495 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 4.034
2024-06-21 17:22:47,791 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 4.598
2024-06-21 17:22:48,171 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 3.918
2024-06-21 17:22:48,479 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 4.363
2024-06-21 17:22:48,691 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 3.257
2024-06-21 17:22:59,818 - INFO: 13/200 final results:
2024-06-21 17:22:59,818 - INFO: Training loss: 4.069.
2024-06-21 17:22:59,818 - INFO: Training MAE: 4.085.
2024-06-21 17:22:59,818 - INFO: Training MSE: 30.679.
2024-06-21 17:23:20,369 - INFO: Epoch: 13/200, Loss_train: 4.068816456301459, Loss_val: 5.116970637748981
2024-06-21 17:23:20,369 - INFO: Best internal validation val_loss: 4.236 at epoch: 11.
2024-06-21 17:23:20,370 - INFO: Epoch 14/200...
2024-06-21 17:23:20,370 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:23:20,370 - INFO: Batch size: 32.
2024-06-21 17:23:20,373 - INFO: Dataset:
2024-06-21 17:23:20,373 - INFO: Batch size:
2024-06-21 17:23:20,373 - INFO: Number of workers:
2024-06-21 17:23:21,458 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 4.170
2024-06-21 17:23:21,764 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 4.327
2024-06-21 17:23:22,157 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 4.174
2024-06-21 17:23:22,474 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 3.835
2024-06-21 17:23:22,899 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 3.975
2024-06-21 17:23:23,199 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 4.288
2024-06-21 17:23:23,586 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 3.546
2024-06-21 17:23:23,899 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 4.398
2024-06-21 17:23:24,320 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 3.913
2024-06-21 17:23:24,611 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 4.056
2024-06-21 17:23:24,990 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 4.310
2024-06-21 17:23:25,305 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 3.871
2024-06-21 17:23:25,735 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 3.910
2024-06-21 17:23:26,038 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 3.749
2024-06-21 17:23:26,434 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 3.934
2024-06-21 17:23:26,746 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 4.789
2024-06-21 17:23:27,174 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 6.013
2024-06-21 17:23:27,473 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 3.853
2024-06-21 17:23:27,861 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 4.811
2024-06-21 17:23:28,166 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 4.243
2024-06-21 17:23:28,589 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 4.083
2024-06-21 17:23:28,891 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 3.865
2024-06-21 17:23:29,270 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 3.814
2024-06-21 17:23:29,584 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 3.982
2024-06-21 17:23:30,002 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 3.438
2024-06-21 17:23:30,298 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 4.125
2024-06-21 17:23:30,682 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 3.489
2024-06-21 17:23:30,991 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 4.162
2024-06-21 17:23:31,211 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 4.957
2024-06-21 17:23:42,265 - INFO: 14/200 final results:
2024-06-21 17:23:42,265 - INFO: Training loss: 4.141.
2024-06-21 17:23:42,265 - INFO: Training MAE: 4.124.
2024-06-21 17:23:42,265 - INFO: Training MSE: 31.418.
2024-06-21 17:24:02,772 - INFO: Epoch: 14/200, Loss_train: 4.140634454529861, Loss_val: 4.014850082068608
2024-06-21 17:24:02,791 - INFO: Saved new best metric model for epoch 14.
2024-06-21 17:24:02,792 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:24:02,792 - INFO: Epoch 15/200...
2024-06-21 17:24:02,792 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:24:02,792 - INFO: Batch size: 32.
2024-06-21 17:24:02,796 - INFO: Dataset:
2024-06-21 17:24:02,796 - INFO: Batch size:
2024-06-21 17:24:02,796 - INFO: Number of workers:
2024-06-21 17:24:03,872 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 3.810
2024-06-21 17:24:04,177 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 4.067
2024-06-21 17:24:04,584 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 3.988
2024-06-21 17:24:04,901 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 3.901
2024-06-21 17:24:05,310 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 3.856
2024-06-21 17:24:05,610 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 4.161
2024-06-21 17:24:06,008 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 3.995
2024-06-21 17:24:06,322 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 5.219
2024-06-21 17:24:06,725 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 5.127
2024-06-21 17:24:07,017 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 4.950
2024-06-21 17:24:07,401 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 4.577
2024-06-21 17:24:07,717 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 4.765
2024-06-21 17:24:08,132 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 4.448
2024-06-21 17:24:08,434 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 3.698
2024-06-21 17:24:08,837 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 5.224
2024-06-21 17:24:09,148 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 3.990
2024-06-21 17:24:09,561 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 4.670
2024-06-21 17:24:09,860 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 4.067
2024-06-21 17:24:10,255 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 4.294
2024-06-21 17:24:10,561 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 3.639
2024-06-21 17:24:10,964 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 4.223
2024-06-21 17:24:11,265 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 3.285
2024-06-21 17:24:11,653 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 3.875
2024-06-21 17:24:11,966 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 3.455
2024-06-21 17:24:12,372 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 4.038
2024-06-21 17:24:12,668 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 3.784
2024-06-21 17:24:13,047 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 3.089
2024-06-21 17:24:13,356 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 3.308
2024-06-21 17:24:13,570 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 4.678
2024-06-21 17:24:24,333 - INFO: 15/200 final results:
2024-06-21 17:24:24,333 - INFO: Training loss: 4.144.
2024-06-21 17:24:24,333 - INFO: Training MAE: 4.134.
2024-06-21 17:24:24,333 - INFO: Training MSE: 31.081.
2024-06-21 17:24:44,765 - INFO: Epoch: 15/200, Loss_train: 4.144254034963147, Loss_val: 4.3056313004987
2024-06-21 17:24:44,765 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:24:44,765 - INFO: Epoch 16/200...
2024-06-21 17:24:44,765 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:24:44,765 - INFO: Batch size: 32.
2024-06-21 17:24:44,769 - INFO: Dataset:
2024-06-21 17:24:44,769 - INFO: Batch size:
2024-06-21 17:24:44,769 - INFO: Number of workers:
2024-06-21 17:24:45,873 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 4.079
2024-06-21 17:24:46,178 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 4.478
2024-06-21 17:24:46,572 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 4.553
2024-06-21 17:24:46,877 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 3.538
2024-06-21 17:24:47,306 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 3.580
2024-06-21 17:24:47,609 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 4.636
2024-06-21 17:24:47,998 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 3.717
2024-06-21 17:24:48,301 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 3.547
2024-06-21 17:24:48,752 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 3.606
2024-06-21 17:24:49,047 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 4.282
2024-06-21 17:24:49,421 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 3.872
2024-06-21 17:24:49,727 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 3.523
2024-06-21 17:24:50,169 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 4.183
2024-06-21 17:24:50,474 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 3.542
2024-06-21 17:24:50,865 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 3.433
2024-06-21 17:24:51,165 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 3.898
2024-06-21 17:24:51,610 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 4.593
2024-06-21 17:24:51,910 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 3.461
2024-06-21 17:24:52,291 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 4.188
2024-06-21 17:24:52,586 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 5.100
2024-06-21 17:24:53,019 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 3.818
2024-06-21 17:24:53,322 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 4.168
2024-06-21 17:24:53,707 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 3.805
2024-06-21 17:24:54,011 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 3.416
2024-06-21 17:24:54,444 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 3.303
2024-06-21 17:24:54,743 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 3.193
2024-06-21 17:24:55,129 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 3.559
2024-06-21 17:24:55,429 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 3.487
2024-06-21 17:24:55,650 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 4.369
2024-06-21 17:25:06,725 - INFO: 16/200 final results:
2024-06-21 17:25:06,725 - INFO: Training loss: 3.894.
2024-06-21 17:25:06,725 - INFO: Training MAE: 3.885.
2024-06-21 17:25:06,725 - INFO: Training MSE: 28.564.
2024-06-21 17:25:27,041 - INFO: Epoch: 16/200, Loss_train: 3.8940957661332756, Loss_val: 4.3723649978637695
2024-06-21 17:25:27,041 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:25:27,042 - INFO: Epoch 17/200...
2024-06-21 17:25:27,042 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:25:27,042 - INFO: Batch size: 32.
2024-06-21 17:25:27,045 - INFO: Dataset:
2024-06-21 17:25:27,046 - INFO: Batch size:
2024-06-21 17:25:27,046 - INFO: Number of workers:
2024-06-21 17:25:28,128 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 3.725
2024-06-21 17:25:28,464 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 3.678
2024-06-21 17:25:28,858 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 3.520
2024-06-21 17:25:29,173 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 4.265
2024-06-21 17:25:29,586 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 4.103
2024-06-21 17:25:29,910 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 3.913
2024-06-21 17:25:30,295 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 3.685
2024-06-21 17:25:30,595 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 3.364
2024-06-21 17:25:31,001 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 3.881
2024-06-21 17:25:31,327 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 3.680
2024-06-21 17:25:31,699 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 3.733
2024-06-21 17:25:32,001 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 3.474
2024-06-21 17:25:32,417 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 3.520
2024-06-21 17:25:32,744 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 3.428
2024-06-21 17:25:33,139 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 3.766
2024-06-21 17:25:33,437 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 3.000
2024-06-21 17:25:33,853 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 3.496
2024-06-21 17:25:34,176 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 3.232
2024-06-21 17:25:34,561 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 3.963
2024-06-21 17:25:34,853 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 4.040
2024-06-21 17:25:35,256 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 3.224
2024-06-21 17:25:35,580 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 3.814
2024-06-21 17:25:35,952 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 3.986
2024-06-21 17:25:36,252 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 3.371
2024-06-21 17:25:36,646 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 3.734
2024-06-21 17:25:36,966 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 4.163
2024-06-21 17:25:37,347 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 3.274
2024-06-21 17:25:37,643 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 4.155
2024-06-21 17:25:37,857 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 3.504
2024-06-21 17:25:49,028 - INFO: 17/200 final results:
2024-06-21 17:25:49,029 - INFO: Training loss: 3.679.
2024-06-21 17:25:49,029 - INFO: Training MAE: 3.682.
2024-06-21 17:25:49,029 - INFO: Training MSE: 25.438.
2024-06-21 17:26:09,351 - INFO: Epoch: 17/200, Loss_train: 3.678962025149115, Loss_val: 4.068274382887216
2024-06-21 17:26:09,351 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:26:09,351 - INFO: Epoch 18/200...
2024-06-21 17:26:09,351 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:26:09,351 - INFO: Batch size: 32.
2024-06-21 17:26:09,355 - INFO: Dataset:
2024-06-21 17:26:09,355 - INFO: Batch size:
2024-06-21 17:26:09,355 - INFO: Number of workers:
2024-06-21 17:26:10,430 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 3.684
2024-06-21 17:26:10,735 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 3.575
2024-06-21 17:26:11,129 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 3.998
2024-06-21 17:26:11,446 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 3.584
2024-06-21 17:26:11,857 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 3.523
2024-06-21 17:26:12,157 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 3.446
2024-06-21 17:26:12,550 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 3.967
2024-06-21 17:26:12,864 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 3.281
2024-06-21 17:26:13,278 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 3.678
2024-06-21 17:26:13,572 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 3.770
2024-06-21 17:26:13,964 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 3.575
2024-06-21 17:26:14,282 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 3.442
2024-06-21 17:26:14,704 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 3.169
2024-06-21 17:26:15,010 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 3.408
2024-06-21 17:26:15,416 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 4.157
2024-06-21 17:26:15,731 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 4.051
2024-06-21 17:26:16,145 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 2.807
2024-06-21 17:26:16,447 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 2.800
2024-06-21 17:26:16,848 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 3.362
2024-06-21 17:26:17,156 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 3.686
2024-06-21 17:26:17,564 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 3.796
2024-06-21 17:26:17,868 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 4.512
2024-06-21 17:26:18,260 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 3.353
2024-06-21 17:26:18,576 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 3.280
2024-06-21 17:26:18,988 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 3.748
2024-06-21 17:26:19,293 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 3.531
2024-06-21 17:26:19,707 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 3.400
2024-06-21 17:26:20,026 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 4.387
2024-06-21 17:26:20,247 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 2.869
2024-06-21 17:26:31,280 - INFO: 18/200 final results:
2024-06-21 17:26:31,280 - INFO: Training loss: 3.581.
2024-06-21 17:26:31,280 - INFO: Training MAE: 3.595.
2024-06-21 17:26:31,280 - INFO: Training MSE: 24.629.
2024-06-21 17:26:51,554 - INFO: Epoch: 18/200, Loss_train: 3.580695094733403, Loss_val: 5.027417906399431
2024-06-21 17:26:51,554 - INFO: Best internal validation val_loss: 4.015 at epoch: 14.
2024-06-21 17:26:51,554 - INFO: Epoch 19/200...
2024-06-21 17:26:51,554 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:26:51,554 - INFO: Batch size: 32.
2024-06-21 17:26:51,558 - INFO: Dataset:
2024-06-21 17:26:51,559 - INFO: Batch size:
2024-06-21 17:26:51,559 - INFO: Number of workers:
2024-06-21 17:26:52,663 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 4.574
2024-06-21 17:26:52,995 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 3.737
2024-06-21 17:26:53,388 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 3.407
2024-06-21 17:26:53,706 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 3.170
2024-06-21 17:26:54,103 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 3.122
2024-06-21 17:26:54,427 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 3.653
2024-06-21 17:26:54,816 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 3.874
2024-06-21 17:26:55,128 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 3.683
2024-06-21 17:26:55,514 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 4.287
2024-06-21 17:26:55,841 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 3.114
2024-06-21 17:26:56,216 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 3.781
2024-06-21 17:26:56,531 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 3.675
2024-06-21 17:26:56,936 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 3.193
2024-06-21 17:26:57,264 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 3.605
2024-06-21 17:26:57,655 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 3.089
2024-06-21 17:26:57,966 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 3.883
2024-06-21 17:26:58,370 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 3.709
2024-06-21 17:26:58,694 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 3.743
2024-06-21 17:26:59,080 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 3.369
2024-06-21 17:26:59,386 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 3.299
2024-06-21 17:26:59,781 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 3.544
2024-06-21 17:27:00,106 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 3.127
2024-06-21 17:27:00,481 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 3.776
2024-06-21 17:27:00,794 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 3.537
2024-06-21 17:27:01,186 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 3.188
2024-06-21 17:27:01,507 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 3.284
2024-06-21 17:27:01,891 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 3.757
2024-06-21 17:27:02,200 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 3.560
2024-06-21 17:27:02,417 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 3.375
2024-06-21 17:27:13,420 - INFO: 19/200 final results:
2024-06-21 17:27:13,420 - INFO: Training loss: 3.556.
2024-06-21 17:27:13,420 - INFO: Training MAE: 3.559.
2024-06-21 17:27:13,420 - INFO: Training MSE: 23.829.
2024-06-21 17:27:33,952 - INFO: Epoch: 19/200, Loss_train: 3.555648680390983, Loss_val: 3.8135190421137315
2024-06-21 17:27:33,972 - INFO: Saved new best metric model for epoch 19.
2024-06-21 17:27:33,972 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:27:33,972 - INFO: Epoch 20/200...
2024-06-21 17:27:33,972 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:27:33,972 - INFO: Batch size: 32.
2024-06-21 17:27:33,976 - INFO: Dataset:
2024-06-21 17:27:33,976 - INFO: Batch size:
2024-06-21 17:27:33,976 - INFO: Number of workers:
2024-06-21 17:27:35,038 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 2.939
2024-06-21 17:27:35,344 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 3.330
2024-06-21 17:27:35,752 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 3.294
2024-06-21 17:27:36,070 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 3.532
2024-06-21 17:27:36,495 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 3.361
2024-06-21 17:27:36,796 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 4.361
2024-06-21 17:27:37,185 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 3.489
2024-06-21 17:27:37,500 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 3.160
2024-06-21 17:27:37,927 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 3.387
2024-06-21 17:27:38,220 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 3.155
2024-06-21 17:27:38,593 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 3.739
2024-06-21 17:27:38,910 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 2.762
2024-06-21 17:27:39,340 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 3.595
2024-06-21 17:27:39,642 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 3.104
2024-06-21 17:27:40,035 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 3.112
2024-06-21 17:27:40,347 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 2.399
2024-06-21 17:27:40,775 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 2.625
2024-06-21 17:27:41,073 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 3.874
2024-06-21 17:27:41,454 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 3.184
2024-06-21 17:27:41,760 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 3.331
2024-06-21 17:27:42,179 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 3.389
2024-06-21 17:27:42,482 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 3.253
2024-06-21 17:27:42,869 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 4.758
2024-06-21 17:27:43,184 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 3.697
2024-06-21 17:27:43,601 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 3.379
2024-06-21 17:27:43,899 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 3.534
2024-06-21 17:27:44,284 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 3.270
2024-06-21 17:27:44,596 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 3.435
2024-06-21 17:27:44,816 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 4.219
2024-06-21 17:27:55,902 - INFO: 20/200 final results:
2024-06-21 17:27:55,902 - INFO: Training loss: 3.402.
2024-06-21 17:27:55,902 - INFO: Training MAE: 3.386.
2024-06-21 17:27:55,902 - INFO: Training MSE: 21.774.
2024-06-21 17:28:16,160 - INFO: Epoch: 20/200, Loss_train: 3.4022917418644347, Loss_val: 5.3965310063855405
2024-06-21 17:28:16,160 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:28:16,160 - INFO: Epoch 21/200...
2024-06-21 17:28:16,160 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:28:16,160 - INFO: Batch size: 32.
2024-06-21 17:28:16,163 - INFO: Dataset:
2024-06-21 17:28:16,164 - INFO: Batch size:
2024-06-21 17:28:16,164 - INFO: Number of workers:
2024-06-21 17:28:17,240 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 4.733
2024-06-21 17:28:17,546 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 3.309
2024-06-21 17:28:17,948 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 3.532
2024-06-21 17:28:18,267 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 3.245
2024-06-21 17:28:18,676 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 3.049
2024-06-21 17:28:18,978 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 3.435
2024-06-21 17:28:19,378 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 3.871
2024-06-21 17:28:19,694 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 3.728
2024-06-21 17:28:20,100 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 3.404
2024-06-21 17:28:20,390 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 2.563
2024-06-21 17:28:20,777 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 3.662
2024-06-21 17:28:21,092 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 3.516
2024-06-21 17:28:21,501 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 4.294
2024-06-21 17:28:21,802 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 3.416
2024-06-21 17:28:22,205 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 3.315
2024-06-21 17:28:22,515 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 3.096
2024-06-21 17:28:22,928 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 3.293
2024-06-21 17:28:23,229 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 2.817
2024-06-21 17:28:23,622 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 2.866
2024-06-21 17:28:23,931 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 3.221
2024-06-21 17:28:24,337 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 3.814
2024-06-21 17:28:24,639 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 3.516
2024-06-21 17:28:25,040 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 3.094
2024-06-21 17:28:25,357 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 3.358
2024-06-21 17:28:26,067 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 2.702
2024-06-21 17:28:26,366 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 3.403
2024-06-21 17:28:26,765 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 3.901
2024-06-21 17:28:27,081 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 3.057
2024-06-21 17:28:27,313 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 3.424
2024-06-21 17:28:38,207 - INFO: 21/200 final results:
2024-06-21 17:28:38,207 - INFO: Training loss: 3.401.
2024-06-21 17:28:38,207 - INFO: Training MAE: 3.401.
2024-06-21 17:28:38,207 - INFO: Training MSE: 22.134.
2024-06-21 17:28:58,357 - INFO: Epoch: 21/200, Loss_train: 3.4012781011647193, Loss_val: 4.699392294061595
2024-06-21 17:28:58,357 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:28:58,357 - INFO: Epoch 22/200...
2024-06-21 17:28:58,357 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:28:58,357 - INFO: Batch size: 32.
2024-06-21 17:28:58,361 - INFO: Dataset:
2024-06-21 17:28:58,361 - INFO: Batch size:
2024-06-21 17:28:58,361 - INFO: Number of workers:
2024-06-21 17:28:59,428 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 2.973
2024-06-21 17:28:59,745 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 4.035
2024-06-21 17:29:00,138 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 2.898
2024-06-21 17:29:00,454 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 2.712
2024-06-21 17:29:00,865 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 3.009
2024-06-21 17:29:01,179 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 3.596
2024-06-21 17:29:01,567 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 3.332
2024-06-21 17:29:01,880 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 3.619
2024-06-21 17:29:02,285 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 3.446
2024-06-21 17:29:02,590 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 3.795
2024-06-21 17:29:02,970 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 3.297
2024-06-21 17:29:03,288 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 3.494
2024-06-21 17:29:03,709 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 3.087
2024-06-21 17:29:04,026 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 2.782
2024-06-21 17:29:04,425 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 3.282
2024-06-21 17:29:04,739 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 2.899
2024-06-21 17:29:05,161 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 2.641
2024-06-21 17:29:05,476 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 2.681
2024-06-21 17:29:05,866 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 2.985
2024-06-21 17:29:06,174 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 3.347
2024-06-21 17:29:06,582 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 3.222
2024-06-21 17:29:06,898 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 2.924
2024-06-21 17:29:07,280 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 3.078
2024-06-21 17:29:07,596 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 2.747
2024-06-21 17:29:07,999 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 3.358
2024-06-21 17:29:08,307 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 3.636
2024-06-21 17:29:08,683 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 3.413
2024-06-21 17:29:08,992 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 2.987
2024-06-21 17:29:09,209 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 3.098
2024-06-21 17:29:20,364 - INFO: 22/200 final results:
2024-06-21 17:29:20,364 - INFO: Training loss: 3.185.
2024-06-21 17:29:20,364 - INFO: Training MAE: 3.187.
2024-06-21 17:29:20,364 - INFO: Training MSE: 19.203.
2024-06-21 17:29:41,042 - INFO: Epoch: 22/200, Loss_train: 3.1852957709082244, Loss_val: 4.295655645173172
2024-06-21 17:29:41,042 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:29:41,042 - INFO: Epoch 23/200...
2024-06-21 17:29:41,042 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:29:41,042 - INFO: Batch size: 32.
2024-06-21 17:29:41,046 - INFO: Dataset:
2024-06-21 17:29:41,046 - INFO: Batch size:
2024-06-21 17:29:41,046 - INFO: Number of workers:
2024-06-21 17:29:42,101 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 3.348
2024-06-21 17:29:42,431 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 3.328
2024-06-21 17:29:42,812 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 3.465
2024-06-21 17:29:43,127 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 3.167
2024-06-21 17:29:43,525 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 3.249
2024-06-21 17:29:43,835 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 3.250
2024-06-21 17:29:44,205 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 3.030
2024-06-21 17:29:44,516 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 3.890
2024-06-21 17:29:44,924 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 3.273
2024-06-21 17:29:45,228 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 2.588
2024-06-21 17:29:45,593 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 3.646
2024-06-21 17:29:45,907 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 2.898
2024-06-21 17:29:46,317 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 3.530
2024-06-21 17:29:46,631 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 2.642
2024-06-21 17:29:47,021 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 2.760
2024-06-21 17:29:47,331 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 3.416
2024-06-21 17:29:47,743 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 3.721
2024-06-21 17:29:48,054 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 3.177
2024-06-21 17:29:48,435 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 2.738
2024-06-21 17:29:48,740 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 3.016
2024-06-21 17:29:49,142 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 2.867
2024-06-21 17:29:49,454 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 2.799
2024-06-21 17:29:49,838 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 3.075
2024-06-21 17:29:50,151 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 3.552
2024-06-21 17:29:50,556 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 3.114
2024-06-21 17:29:50,865 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 2.550
2024-06-21 17:29:51,248 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 3.018
2024-06-21 17:29:51,556 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 3.210
2024-06-21 17:29:51,771 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 2.850
2024-06-21 17:30:02,857 - INFO: 23/200 final results:
2024-06-21 17:30:02,857 - INFO: Training loss: 3.144.
2024-06-21 17:30:02,857 - INFO: Training MAE: 3.150.
2024-06-21 17:30:02,857 - INFO: Training MSE: 18.492.
2024-06-21 17:30:22,954 - INFO: Epoch: 23/200, Loss_train: 3.1437391659309126, Loss_val: 4.340225622571748
2024-06-21 17:30:22,954 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:30:22,954 - INFO: Epoch 24/200...
2024-06-21 17:30:22,954 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:30:22,955 - INFO: Batch size: 32.
2024-06-21 17:30:22,958 - INFO: Dataset:
2024-06-21 17:30:22,958 - INFO: Batch size:
2024-06-21 17:30:22,958 - INFO: Number of workers:
2024-06-21 17:30:24,050 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 3.582
2024-06-21 17:30:24,354 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 3.339
2024-06-21 17:30:24,747 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 3.104
2024-06-21 17:30:25,063 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 2.653
2024-06-21 17:30:25,499 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 3.087
2024-06-21 17:30:25,799 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 2.850
2024-06-21 17:30:26,187 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 2.784
2024-06-21 17:30:26,487 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 3.485
2024-06-21 17:30:26,932 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 2.813
2024-06-21 17:30:27,222 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 4.199
2024-06-21 17:30:27,596 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 3.939
2024-06-21 17:30:27,896 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 2.988
2024-06-21 17:30:28,332 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 3.302
2024-06-21 17:30:28,633 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 3.106
2024-06-21 17:30:29,024 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 3.018
2024-06-21 17:30:29,320 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 3.118
2024-06-21 17:30:29,758 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 3.050
2024-06-21 17:30:30,057 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 3.194
2024-06-21 17:30:30,439 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 3.336
2024-06-21 17:30:30,732 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 3.275
2024-06-21 17:30:31,159 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 3.029
2024-06-21 17:30:31,460 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 3.285
2024-06-21 17:30:31,831 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 2.817
2024-06-21 17:30:32,131 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 2.525
2024-06-21 17:30:32,554 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 2.520
2024-06-21 17:30:32,850 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 2.847
2024-06-21 17:30:33,217 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 2.628
2024-06-21 17:30:33,513 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 2.836
2024-06-21 17:30:33,720 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 2.755
2024-06-21 17:30:44,508 - INFO: 24/200 final results:
2024-06-21 17:30:44,508 - INFO: Training loss: 3.085.
2024-06-21 17:30:44,508 - INFO: Training MAE: 3.092.
2024-06-21 17:30:44,509 - INFO: Training MSE: 17.776.
2024-06-21 17:31:05,497 - INFO: Epoch: 24/200, Loss_train: 3.085029363632202, Loss_val: 3.903879946675794
2024-06-21 17:31:05,497 - INFO: Best internal validation val_loss: 3.814 at epoch: 19.
2024-06-21 17:31:05,497 - INFO: Epoch 25/200...
2024-06-21 17:31:05,497 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:31:05,497 - INFO: Batch size: 32.
2024-06-21 17:31:05,501 - INFO: Dataset:
2024-06-21 17:31:05,501 - INFO: Batch size:
2024-06-21 17:31:05,501 - INFO: Number of workers:
2024-06-21 17:31:06,589 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 3.861
2024-06-21 17:31:06,896 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 2.810
2024-06-21 17:31:07,292 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 3.177
2024-06-21 17:31:07,611 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 2.879
2024-06-21 17:31:08,039 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 2.740
2024-06-21 17:31:08,340 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 2.439
2024-06-21 17:31:08,731 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 2.952
2024-06-21 17:31:09,033 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 3.123
2024-06-21 17:31:09,458 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 3.371
2024-06-21 17:31:09,752 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 3.226
2024-06-21 17:31:10,131 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 3.059
2024-06-21 17:31:10,436 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 3.080
2024-06-21 17:31:10,870 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 3.024
2024-06-21 17:31:11,175 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 2.992
2024-06-21 17:31:11,584 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 2.823
2024-06-21 17:31:11,885 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 3.414
2024-06-21 17:31:12,310 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 2.800
2024-06-21 17:31:12,611 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 3.248
2024-06-21 17:31:13,011 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 2.808
2024-06-21 17:31:13,306 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 2.554
2024-06-21 17:31:13,725 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 3.100
2024-06-21 17:31:14,028 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 2.490
2024-06-21 17:31:14,430 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 2.434
2024-06-21 17:31:14,734 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 2.965
2024-06-21 17:31:15,147 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 3.049
2024-06-21 17:31:15,442 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 3.768
2024-06-21 17:31:15,833 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 2.401
2024-06-21 17:31:16,129 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 2.913
2024-06-21 17:31:16,349 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 3.047
2024-06-21 17:31:27,437 - INFO: 25/200 final results:
2024-06-21 17:31:27,437 - INFO: Training loss: 2.984.
2024-06-21 17:31:27,437 - INFO: Training MAE: 2.983.
2024-06-21 17:31:27,437 - INFO: Training MSE: 16.891.
2024-06-21 17:31:47,978 - INFO: Epoch: 25/200, Loss_train: 2.984373150200679, Loss_val: 3.246350173292489
2024-06-21 17:31:47,996 - INFO: Saved new best metric model for epoch 25.
2024-06-21 17:31:47,996 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:31:47,996 - INFO: Epoch 26/200...
2024-06-21 17:31:47,996 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:31:47,996 - INFO: Batch size: 32.
2024-06-21 17:31:48,000 - INFO: Dataset:
2024-06-21 17:31:48,000 - INFO: Batch size:
2024-06-21 17:31:48,000 - INFO: Number of workers:
2024-06-21 17:31:49,068 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 3.021
2024-06-21 17:31:49,385 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 3.129
2024-06-21 17:31:49,766 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 2.803
2024-06-21 17:31:50,079 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 3.591
2024-06-21 17:31:50,490 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 3.235
2024-06-21 17:31:50,787 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 2.142
2024-06-21 17:31:51,155 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 2.734
2024-06-21 17:31:51,465 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 3.096
2024-06-21 17:31:51,878 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 2.905
2024-06-21 17:31:52,169 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 2.293
2024-06-21 17:31:52,528 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 2.403
2024-06-21 17:31:52,842 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 2.988
2024-06-21 17:31:53,276 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 3.301
2024-06-21 17:31:53,581 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 3.399
2024-06-21 17:31:53,977 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 2.776
2024-06-21 17:31:54,292 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 3.054
2024-06-21 17:31:54,720 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 2.613
2024-06-21 17:31:55,022 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 3.080
2024-06-21 17:31:55,409 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 3.262
2024-06-21 17:31:55,717 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 3.430
2024-06-21 17:31:56,135 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 2.949
2024-06-21 17:31:56,438 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 3.069
2024-06-21 17:31:56,827 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 3.334
2024-06-21 17:31:57,143 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 2.651
2024-06-21 17:31:57,560 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 3.298
2024-06-21 17:31:57,858 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 2.507
2024-06-21 17:31:58,243 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 2.596
2024-06-21 17:31:58,554 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 3.103
2024-06-21 17:31:58,767 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 3.057
2024-06-21 17:32:09,556 - INFO: 26/200 final results:
2024-06-21 17:32:09,556 - INFO: Training loss: 2.959.
2024-06-21 17:32:09,557 - INFO: Training MAE: 2.957.
2024-06-21 17:32:09,557 - INFO: Training MSE: 16.389.
2024-06-21 17:32:29,714 - INFO: Epoch: 26/200, Loss_train: 2.959226665825679, Loss_val: 3.3434333307989714
2024-06-21 17:32:29,714 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:32:29,714 - INFO: Epoch 27/200...
2024-06-21 17:32:29,714 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:32:29,714 - INFO: Batch size: 32.
2024-06-21 17:32:29,718 - INFO: Dataset:
2024-06-21 17:32:29,718 - INFO: Batch size:
2024-06-21 17:32:29,718 - INFO: Number of workers:
2024-06-21 17:32:30,782 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 2.650
2024-06-21 17:32:31,100 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 3.571
2024-06-21 17:32:31,503 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 2.789
2024-06-21 17:32:31,820 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 3.068
2024-06-21 17:32:32,218 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 2.324
2024-06-21 17:32:32,529 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 2.854
2024-06-21 17:32:32,926 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 2.723
2024-06-21 17:32:33,239 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 2.700
2024-06-21 17:32:33,627 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 2.907
2024-06-21 17:32:33,931 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 2.844
2024-06-21 17:32:34,321 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 2.972
2024-06-21 17:32:34,635 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 2.880
2024-06-21 17:32:35,037 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 2.720
2024-06-21 17:32:35,351 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 2.753
2024-06-21 17:32:35,756 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 2.766
2024-06-21 17:32:36,066 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 2.591
2024-06-21 17:32:36,467 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 3.645
2024-06-21 17:32:36,779 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 2.808
2024-06-21 17:32:37,176 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 3.186
2024-06-21 17:32:37,482 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 2.846
2024-06-21 17:32:37,877 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 2.739
2024-06-21 17:32:38,190 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 2.758
2024-06-21 17:32:38,587 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 2.707
2024-06-21 17:32:38,900 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 2.726
2024-06-21 17:32:39,292 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 3.569
2024-06-21 17:32:39,600 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 2.880
2024-06-21 17:32:39,981 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 2.552
2024-06-21 17:32:40,289 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 2.349
2024-06-21 17:32:40,506 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 2.678
2024-06-21 17:32:51,701 - INFO: 27/200 final results:
2024-06-21 17:32:51,701 - INFO: Training loss: 2.847.
2024-06-21 17:32:51,701 - INFO: Training MAE: 2.850.
2024-06-21 17:32:51,701 - INFO: Training MSE: 14.952.
2024-06-21 17:33:12,138 - INFO: Epoch: 27/200, Loss_train: 2.84676587170568, Loss_val: 3.866774090405168
2024-06-21 17:33:12,138 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:33:12,138 - INFO: Epoch 28/200...
2024-06-21 17:33:12,138 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:33:12,138 - INFO: Batch size: 32.
2024-06-21 17:33:12,141 - INFO: Dataset:
2024-06-21 17:33:12,142 - INFO: Batch size:
2024-06-21 17:33:12,142 - INFO: Number of workers:
2024-06-21 17:33:13,230 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 2.629
2024-06-21 17:33:13,537 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 2.840
2024-06-21 17:33:13,926 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 2.465
2024-06-21 17:33:14,245 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 2.659
2024-06-21 17:33:14,678 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 2.528
2024-06-21 17:33:14,980 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 2.926
2024-06-21 17:33:15,360 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 2.852
2024-06-21 17:33:15,662 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 2.587
2024-06-21 17:33:16,102 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 2.724
2024-06-21 17:33:16,396 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 2.680
2024-06-21 17:33:16,771 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 2.598
2024-06-21 17:33:17,076 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 3.462
2024-06-21 17:33:17,520 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 2.651
2024-06-21 17:33:17,825 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 3.239
2024-06-21 17:33:18,219 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 2.578
2024-06-21 17:33:18,520 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 2.638
2024-06-21 17:33:18,968 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 2.900
2024-06-21 17:33:19,269 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 3.593
2024-06-21 17:33:19,654 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 2.547
2024-06-21 17:33:19,950 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 2.905
2024-06-21 17:33:20,386 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 3.030
2024-06-21 17:33:20,690 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 2.436
2024-06-21 17:33:21,065 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 2.432
2024-06-21 17:33:21,368 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 2.897
2024-06-21 17:33:21,790 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 2.772
2024-06-21 17:33:22,086 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 2.432
2024-06-21 17:33:22,466 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 2.886
2024-06-21 17:33:22,761 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 2.416
2024-06-21 17:33:22,971 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 2.523
2024-06-21 17:33:34,049 - INFO: 28/200 final results:
2024-06-21 17:33:34,050 - INFO: Training loss: 2.753.
2024-06-21 17:33:34,050 - INFO: Training MAE: 2.757.
2024-06-21 17:33:34,050 - INFO: Training MSE: 14.201.
2024-06-21 17:33:54,125 - INFO: Epoch: 28/200, Loss_train: 2.752582212974285, Loss_val: 3.7580305707865747
2024-06-21 17:33:54,125 - INFO: Best internal validation val_loss: 3.246 at epoch: 25.
2024-06-21 17:33:54,125 - INFO: Epoch 29/200...
2024-06-21 17:33:54,125 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:33:54,126 - INFO: Batch size: 32.
2024-06-21 17:33:54,129 - INFO: Dataset:
2024-06-21 17:33:54,129 - INFO: Batch size:
2024-06-21 17:33:54,129 - INFO: Number of workers:
2024-06-21 17:33:55,221 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 2.326
2024-06-21 17:33:55,541 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 2.828
2024-06-21 17:33:55,949 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 2.912
2024-06-21 17:33:56,266 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 2.381
2024-06-21 17:33:56,669 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 2.105
2024-06-21 17:33:56,982 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 3.179
2024-06-21 17:33:57,384 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 3.009
2024-06-21 17:33:57,698 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 2.778
2024-06-21 17:33:58,087 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 2.798
2024-06-21 17:33:58,393 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 2.538
2024-06-21 17:33:58,782 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 3.075
2024-06-21 17:33:59,097 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 2.531
2024-06-21 17:33:59,504 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 3.338
2024-06-21 17:33:59,819 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 3.016
2024-06-21 17:34:00,224 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 3.013
2024-06-21 17:34:00,535 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 3.124
2024-06-21 17:34:00,940 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 2.579
2024-06-21 17:34:01,254 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 2.502
2024-06-21 17:34:01,646 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 2.418
2024-06-21 17:34:01,954 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 2.955
2024-06-21 17:34:02,337 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 2.713
2024-06-21 17:34:02,653 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 2.735
2024-06-21 17:34:03,046 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 2.914
2024-06-21 17:34:03,361 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 2.617
2024-06-21 17:34:03,754 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 2.155
2024-06-21 17:34:04,065 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 2.480
2024-06-21 17:34:04,463 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 2.851
2024-06-21 17:34:04,775 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 2.792
2024-06-21 17:34:04,992 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 2.722
2024-06-21 17:34:16,055 - INFO: 29/200 final results:
2024-06-21 17:34:16,055 - INFO: Training loss: 2.737.
2024-06-21 17:34:16,055 - INFO: Training MAE: 2.738.
2024-06-21 17:34:16,055 - INFO: Training MSE: 13.776.
2024-06-21 17:34:36,485 - INFO: Epoch: 29/200, Loss_train: 2.737426149434057, Loss_val: 3.1881279205453805
2024-06-21 17:34:36,505 - INFO: Saved new best metric model for epoch 29.
2024-06-21 17:34:36,505 - INFO: Best internal validation val_loss: 3.188 at epoch: 29.
2024-06-21 17:34:36,505 - INFO: Epoch 30/200...
2024-06-21 17:34:36,505 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:34:36,505 - INFO: Batch size: 32.
2024-06-21 17:34:36,508 - INFO: Dataset:
2024-06-21 17:34:36,509 - INFO: Batch size:
2024-06-21 17:34:36,509 - INFO: Number of workers:
2024-06-21 17:34:37,589 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 2.390
2024-06-21 17:34:37,894 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 2.603
2024-06-21 17:34:38,312 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 3.119
2024-06-21 17:34:38,616 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 3.225
2024-06-21 17:34:39,016 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 2.376
2024-06-21 17:34:39,328 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 2.868
2024-06-21 17:34:39,727 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 1.854
2024-06-21 17:34:40,030 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 2.889
2024-06-21 17:34:40,422 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 2.704
2024-06-21 17:34:40,729 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 2.466
2024-06-21 17:34:41,134 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 2.632
2024-06-21 17:34:41,438 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 3.072
2024-06-21 17:34:41,834 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 2.587
2024-06-21 17:34:42,151 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 2.523
2024-06-21 17:34:42,574 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 2.650
2024-06-21 17:34:42,874 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 2.407
2024-06-21 17:34:43,255 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 3.216
2024-06-21 17:34:43,568 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 3.215
2024-06-21 17:34:43,982 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 3.006
2024-06-21 17:34:44,274 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 2.346
2024-06-21 17:34:44,639 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 2.592
2024-06-21 17:34:44,952 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 2.909
2024-06-21 17:34:45,367 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 2.501
2024-06-21 17:34:45,667 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 3.536
2024-06-21 17:34:46,037 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 2.379
2024-06-21 17:34:46,345 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 2.775
2024-06-21 17:34:46,745 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 3.326
2024-06-21 17:34:47,042 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 2.287
2024-06-21 17:34:47,250 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 2.586
2024-06-21 17:34:58,397 - INFO: 30/200 final results:
2024-06-21 17:34:58,397 - INFO: Training loss: 2.726.
2024-06-21 17:34:58,397 - INFO: Training MAE: 2.728.
2024-06-21 17:34:58,397 - INFO: Training MSE: 13.490.
2024-06-21 17:35:18,593 - INFO: Epoch: 30/200, Loss_train: 2.725536457423506, Loss_val: 2.8623216316617768
2024-06-21 17:35:18,611 - INFO: Saved new best metric model for epoch 30.
2024-06-21 17:35:18,611 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:35:18,611 - INFO: Epoch 31/200...
2024-06-21 17:35:18,611 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:35:18,612 - INFO: Batch size: 32.
2024-06-21 17:35:18,616 - INFO: Dataset:
2024-06-21 17:35:18,616 - INFO: Batch size:
2024-06-21 17:35:18,616 - INFO: Number of workers:
2024-06-21 17:35:19,692 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 2.216
2024-06-21 17:35:19,998 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 2.590
2024-06-21 17:35:20,390 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 2.580
2024-06-21 17:35:20,707 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 2.624
2024-06-21 17:35:21,128 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 2.375
2024-06-21 17:35:21,428 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 2.866
2024-06-21 17:35:21,813 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 2.569
2024-06-21 17:35:22,125 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 2.620
2024-06-21 17:35:22,548 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 2.883
2024-06-21 17:35:22,839 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 2.339
2024-06-21 17:35:23,211 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 2.697
2024-06-21 17:35:23,527 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 2.279
2024-06-21 17:35:23,955 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 2.686
2024-06-21 17:35:24,257 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 2.743
2024-06-21 17:35:24,649 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 2.443
2024-06-21 17:35:24,960 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 2.101
2024-06-21 17:35:25,388 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 2.468
2024-06-21 17:35:25,687 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 2.860
2024-06-21 17:35:26,071 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 2.304
2024-06-21 17:35:26,377 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 2.337
2024-06-21 17:35:26,795 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 2.359
2024-06-21 17:35:27,096 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 3.000
2024-06-21 17:35:27,470 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 2.248
2024-06-21 17:35:27,784 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 2.662
2024-06-21 17:35:28,196 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 2.806
2024-06-21 17:35:28,492 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 2.249
2024-06-21 17:35:28,860 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 2.011
2024-06-21 17:35:29,168 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 2.702
2024-06-21 17:35:29,379 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 2.253
2024-06-21 17:35:40,428 - INFO: 31/200 final results:
2024-06-21 17:35:40,428 - INFO: Training loss: 2.513.
2024-06-21 17:35:40,428 - INFO: Training MAE: 2.518.
2024-06-21 17:35:40,428 - INFO: Training MSE: 11.691.
2024-06-21 17:36:00,541 - INFO: Epoch: 31/200, Loss_train: 2.5126695797361176, Loss_val: 3.4359739484458136
2024-06-21 17:36:00,542 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:36:00,542 - INFO: Epoch 32/200...
2024-06-21 17:36:00,542 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:36:00,542 - INFO: Batch size: 32.
2024-06-21 17:36:00,545 - INFO: Dataset:
2024-06-21 17:36:00,545 - INFO: Batch size:
2024-06-21 17:36:00,545 - INFO: Number of workers:
2024-06-21 17:36:01,624 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 2.792
2024-06-21 17:36:01,944 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 2.251
2024-06-21 17:36:02,340 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 2.563
2024-06-21 17:36:02,657 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 3.189
2024-06-21 17:36:03,068 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 2.677
2024-06-21 17:36:03,380 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 2.196
2024-06-21 17:36:03,764 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 2.170
2024-06-21 17:36:04,077 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 2.455
2024-06-21 17:36:04,490 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 2.546
2024-06-21 17:36:04,796 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 2.636
2024-06-21 17:36:05,170 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 2.468
2024-06-21 17:36:05,488 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 2.648
2024-06-21 17:36:05,906 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 2.619
2024-06-21 17:36:06,222 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 2.932
2024-06-21 17:36:06,615 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 3.375
2024-06-21 17:36:06,928 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 2.395
2024-06-21 17:36:07,329 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 2.172
2024-06-21 17:36:07,642 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 2.322
2024-06-21 17:36:08,024 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 2.294
2024-06-21 17:36:08,332 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 2.391
2024-06-21 17:36:08,739 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 2.396
2024-06-21 17:36:09,054 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 2.495
2024-06-21 17:36:09,441 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 2.544
2024-06-21 17:36:09,757 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 3.025
2024-06-21 17:36:10,163 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 2.793
2024-06-21 17:36:10,474 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 3.473
2024-06-21 17:36:10,851 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 2.004
2024-06-21 17:36:11,163 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 2.612
2024-06-21 17:36:11,383 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 3.184
2024-06-21 17:36:22,566 - INFO: 32/200 final results:
2024-06-21 17:36:22,566 - INFO: Training loss: 2.608.
2024-06-21 17:36:22,566 - INFO: Training MAE: 2.596.
2024-06-21 17:36:22,566 - INFO: Training MSE: 12.594.
2024-06-21 17:36:42,878 - INFO: Epoch: 32/200, Loss_train: 2.607512424732077, Loss_val: 5.320437924615268
2024-06-21 17:36:42,878 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:36:42,878 - INFO: Epoch 33/200...
2024-06-21 17:36:42,878 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:36:42,878 - INFO: Batch size: 32.
2024-06-21 17:36:42,882 - INFO: Dataset:
2024-06-21 17:36:42,882 - INFO: Batch size:
2024-06-21 17:36:42,882 - INFO: Number of workers:
2024-06-21 17:36:43,934 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 3.419
2024-06-21 17:36:44,264 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 2.639
2024-06-21 17:36:44,656 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 2.592
2024-06-21 17:36:44,972 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 2.239
2024-06-21 17:36:45,366 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 2.300
2024-06-21 17:36:45,691 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 2.238
2024-06-21 17:36:46,078 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 2.058
2024-06-21 17:36:46,393 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 2.539
2024-06-21 17:36:46,781 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 3.509
2024-06-21 17:36:47,110 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 2.001
2024-06-21 17:36:47,486 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 2.717
2024-06-21 17:36:47,803 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 2.268
2024-06-21 17:36:48,215 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 2.945
2024-06-21 17:36:48,543 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 2.352
2024-06-21 17:36:48,933 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 2.723
2024-06-21 17:36:49,244 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 2.311
2024-06-21 17:36:49,647 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 2.228
2024-06-21 17:36:49,970 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 3.098
2024-06-21 17:36:50,344 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 2.941
2024-06-21 17:36:50,648 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 3.032
2024-06-21 17:36:51,044 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 2.248
2024-06-21 17:36:51,368 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 2.550
2024-06-21 17:36:51,751 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 1.998
2024-06-21 17:36:52,063 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 2.572
2024-06-21 17:36:52,457 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 2.078
2024-06-21 17:36:52,777 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 2.563
2024-06-21 17:36:53,150 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 2.265
2024-06-21 17:36:53,458 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 2.180
2024-06-21 17:36:53,671 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 2.845
2024-06-21 17:37:04,496 - INFO: 33/200 final results:
2024-06-21 17:37:04,496 - INFO: Training loss: 2.533.
2024-06-21 17:37:04,496 - INFO: Training MAE: 2.526.
2024-06-21 17:37:04,496 - INFO: Training MSE: 11.969.
2024-06-21 17:37:24,972 - INFO: Epoch: 33/200, Loss_train: 2.532671574888558, Loss_val: 3.118515771010826
2024-06-21 17:37:24,972 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:37:24,972 - INFO: Epoch 34/200...
2024-06-21 17:37:24,972 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:37:24,972 - INFO: Batch size: 32.
2024-06-21 17:37:24,976 - INFO: Dataset:
2024-06-21 17:37:24,976 - INFO: Batch size:
2024-06-21 17:37:24,976 - INFO: Number of workers:
2024-06-21 17:37:26,031 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 2.344
2024-06-21 17:37:26,362 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 2.254
2024-06-21 17:37:26,746 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 2.420
2024-06-21 17:37:27,063 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 2.342
2024-06-21 17:37:27,454 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 2.483
2024-06-21 17:37:27,777 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 2.955
2024-06-21 17:37:28,180 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 2.129
2024-06-21 17:37:28,504 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 2.509
2024-06-21 17:37:28,912 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 2.168
2024-06-21 17:37:29,248 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 2.260
2024-06-21 17:37:29,626 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 2.529
2024-06-21 17:37:29,944 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 2.047
2024-06-21 17:37:30,353 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 2.619
2024-06-21 17:37:30,683 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 2.439
2024-06-21 17:37:31,079 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 3.850
2024-06-21 17:37:31,392 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 2.781
2024-06-21 17:37:31,797 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 2.376
2024-06-21 17:37:32,121 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 2.263
2024-06-21 17:37:32,508 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 2.318
2024-06-21 17:37:32,815 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 3.183
2024-06-21 17:37:33,212 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 2.677
2024-06-21 17:37:33,539 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 1.980
2024-06-21 17:37:33,925 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 2.090
2024-06-21 17:37:34,239 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 2.718
2024-06-21 17:37:34,635 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 2.388
2024-06-21 17:37:34,956 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 2.484
2024-06-21 17:37:35,339 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 2.542
2024-06-21 17:37:35,648 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 1.893
2024-06-21 17:37:35,869 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 2.738
2024-06-21 17:37:46,954 - INFO: 34/200 final results:
2024-06-21 17:37:46,955 - INFO: Training loss: 2.475.
2024-06-21 17:37:46,955 - INFO: Training MAE: 2.470.
2024-06-21 17:37:46,955 - INFO: Training MSE: 11.235.
2024-06-21 17:38:07,570 - INFO: Epoch: 34/200, Loss_train: 2.4751795571425865, Loss_val: 3.3945121354070205
2024-06-21 17:38:07,570 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:38:07,570 - INFO: Epoch 35/200...
2024-06-21 17:38:07,570 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:38:07,570 - INFO: Batch size: 32.
2024-06-21 17:38:07,573 - INFO: Dataset:
2024-06-21 17:38:07,574 - INFO: Batch size:
2024-06-21 17:38:07,574 - INFO: Number of workers:
2024-06-21 17:38:08,670 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 2.707
2024-06-21 17:38:08,977 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 2.534
2024-06-21 17:38:09,371 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 2.694
2024-06-21 17:38:09,691 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 2.217
2024-06-21 17:38:10,121 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 2.774
2024-06-21 17:38:10,423 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 2.111
2024-06-21 17:38:10,812 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 2.646
2024-06-21 17:38:11,128 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 3.064
2024-06-21 17:38:11,556 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 2.816
2024-06-21 17:38:11,849 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 2.301
2024-06-21 17:38:12,227 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 2.420
2024-06-21 17:38:12,545 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 2.449
2024-06-21 17:38:12,980 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 1.960
2024-06-21 17:38:13,284 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 2.371
2024-06-21 17:38:13,680 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 2.218
2024-06-21 17:38:13,994 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 2.132
2024-06-21 17:38:14,426 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 2.493
2024-06-21 17:38:14,727 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 2.260
2024-06-21 17:38:15,116 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 2.235
2024-06-21 17:38:15,424 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 2.992
2024-06-21 17:38:15,846 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 2.876
2024-06-21 17:38:16,149 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 2.091
2024-06-21 17:38:16,530 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 2.389
2024-06-21 17:38:16,847 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 2.465
2024-06-21 17:38:17,264 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 2.383
2024-06-21 17:38:17,562 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 3.014
2024-06-21 17:38:17,948 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 2.332
2024-06-21 17:38:18,260 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 2.160
2024-06-21 17:38:18,475 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 2.668
2024-06-21 17:38:29,411 - INFO: 35/200 final results:
2024-06-21 17:38:29,411 - INFO: Training loss: 2.475.
2024-06-21 17:38:29,411 - INFO: Training MAE: 2.471.
2024-06-21 17:38:29,411 - INFO: Training MSE: 11.136.
2024-06-21 17:38:49,702 - INFO: Epoch: 35/200, Loss_train: 2.4749287325760414, Loss_val: 3.485599838454148
2024-06-21 17:38:49,702 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:38:49,702 - INFO: Epoch 36/200...
2024-06-21 17:38:49,702 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:38:49,702 - INFO: Batch size: 32.
2024-06-21 17:38:49,706 - INFO: Dataset:
2024-06-21 17:38:49,706 - INFO: Batch size:
2024-06-21 17:38:49,706 - INFO: Number of workers:
2024-06-21 17:38:50,796 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 2.392
2024-06-21 17:38:51,100 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 3.001
2024-06-21 17:38:51,492 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 3.081
2024-06-21 17:38:51,808 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 2.860
2024-06-21 17:38:52,215 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 2.185
2024-06-21 17:38:52,524 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 2.059
2024-06-21 17:38:52,908 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 2.229
2024-06-21 17:38:53,220 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 2.228
2024-06-21 17:38:53,627 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 2.200
2024-06-21 17:38:53,930 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 2.090
2024-06-21 17:38:54,309 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 2.452
2024-06-21 17:38:54,626 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 2.024
2024-06-21 17:38:55,045 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 2.166
2024-06-21 17:38:55,363 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 2.472
2024-06-21 17:38:55,762 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 2.162
2024-06-21 17:38:56,075 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 2.135
2024-06-21 17:38:56,494 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 2.282
2024-06-21 17:38:56,808 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 2.467
2024-06-21 17:38:57,192 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 2.518
2024-06-21 17:38:57,501 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 2.832
2024-06-21 17:38:57,910 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 1.918
2024-06-21 17:38:58,226 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 2.533
2024-06-21 17:38:58,613 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 2.288
2024-06-21 17:38:58,929 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 2.588
2024-06-21 17:38:59,334 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 2.042
2024-06-21 17:38:59,646 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 2.666
2024-06-21 17:39:00,016 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 2.154
2024-06-21 17:39:00,326 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 2.403
2024-06-21 17:39:00,540 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 2.146
2024-06-21 17:39:11,650 - INFO: 36/200 final results:
2024-06-21 17:39:11,650 - INFO: Training loss: 2.365.
2024-06-21 17:39:11,650 - INFO: Training MAE: 2.369.
2024-06-21 17:39:11,650 - INFO: Training MSE: 10.222.
2024-06-21 17:39:31,789 - INFO: Epoch: 36/200, Loss_train: 2.364553657071344, Loss_val: 3.536565090047902
2024-06-21 17:39:31,789 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:39:31,789 - INFO: Epoch 37/200...
2024-06-21 17:39:31,789 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 17:39:31,789 - INFO: Batch size: 32.
2024-06-21 17:39:31,793 - INFO: Dataset:
2024-06-21 17:39:31,793 - INFO: Batch size:
2024-06-21 17:39:31,793 - INFO: Number of workers:
2024-06-21 17:39:32,842 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 2.701
2024-06-21 17:39:33,160 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 2.838
2024-06-21 17:39:33,559 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 2.288
2024-06-21 17:39:33,874 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 1.980
2024-06-21 17:39:34,259 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 2.378
2024-06-21 17:39:34,571 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 2.399
2024-06-21 17:39:34,967 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 2.142
2024-06-21 17:39:35,281 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 2.033
2024-06-21 17:39:35,663 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 2.939
2024-06-21 17:39:35,968 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 2.356
2024-06-21 17:39:36,347 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 2.829
2024-06-21 17:39:36,662 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 2.344
2024-06-21 17:39:37,061 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 2.143
2024-06-21 17:39:37,374 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 2.483
2024-06-21 17:39:37,774 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 2.372
2024-06-21 17:39:38,085 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 2.071
2024-06-21 17:39:38,486 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 2.235
2024-06-21 17:39:38,796 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 2.267
2024-06-21 17:39:39,186 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 2.585
2024-06-21 17:39:39,492 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 2.069
2024-06-21 17:39:39,878 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 1.962
2024-06-21 17:39:40,191 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 2.497
2024-06-21 17:39:40,585 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 2.721
2024-06-21 17:39:40,897 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 2.747
2024-06-21 17:39:41,280 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 2.618
2024-06-21 17:39:41,588 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 2.308
2024-06-21 17:39:41,976 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 2.105
2024-06-21 17:39:42,285 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 2.784
2024-06-21 17:39:42,503 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 2.172
2024-06-21 17:39:53,545 - INFO: 37/200 final results:
2024-06-21 17:39:53,545 - INFO: Training loss: 2.392.
2024-06-21 17:39:53,545 - INFO: Training MAE: 2.396.
2024-06-21 17:39:53,545 - INFO: Training MSE: 10.612.
2024-06-21 17:40:13,834 - INFO: Epoch: 37/200, Loss_train: 2.391994361219735, Loss_val: 3.3170373933068635
2024-06-21 17:40:13,834 - INFO: Best internal validation val_loss: 2.862 at epoch: 30.
2024-06-21 17:40:13,834 - INFO: Epoch 38/200...
2024-06-21 17:40:13,834 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:40:13,834 - INFO: Batch size: 32.
2024-06-21 17:40:13,838 - INFO: Dataset:
2024-06-21 17:40:13,838 - INFO: Batch size:
2024-06-21 17:40:13,838 - INFO: Number of workers:
2024-06-21 17:40:14,902 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 2.676
2024-06-21 17:40:15,207 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 2.019
2024-06-21 17:40:15,616 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 3.045
2024-06-21 17:40:15,934 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 2.163
2024-06-21 17:40:16,356 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 1.999
2024-06-21 17:40:16,656 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 2.351
2024-06-21 17:40:17,039 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 2.412
2024-06-21 17:40:17,352 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 2.287
2024-06-21 17:40:17,828 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 1.915
2024-06-21 17:40:18,121 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 1.804
2024-06-21 17:40:18,498 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 2.009
2024-06-21 17:40:18,814 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 2.024
2024-06-21 17:40:19,241 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 1.691
2024-06-21 17:40:19,543 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 1.751
2024-06-21 17:40:19,935 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 1.863
2024-06-21 17:40:20,250 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 1.756
2024-06-21 17:40:20,679 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 1.448
2024-06-21 17:40:20,979 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 1.998
2024-06-21 17:40:21,363 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 1.703
2024-06-21 17:40:21,672 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 1.924
2024-06-21 17:40:22,090 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 1.833
2024-06-21 17:40:22,395 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 1.892
2024-06-21 17:40:22,782 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 1.795
2024-06-21 17:40:23,098 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 1.439
2024-06-21 17:40:23,514 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 1.932
2024-06-21 17:40:23,810 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 1.897
2024-06-21 17:40:24,186 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 2.012
2024-06-21 17:40:24,494 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 1.664
2024-06-21 17:40:24,707 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 1.676
2024-06-21 17:40:35,816 - INFO: 38/200 final results:
2024-06-21 17:40:35,816 - INFO: Training loss: 1.965.
2024-06-21 17:40:35,816 - INFO: Training MAE: 1.970.
2024-06-21 17:40:35,816 - INFO: Training MSE: 7.507.
2024-06-21 17:40:56,455 - INFO: Epoch: 38/200, Loss_train: 1.9647427756210853, Loss_val: 2.7288301895404685
2024-06-21 17:40:56,475 - INFO: Saved new best metric model for epoch 38.
2024-06-21 17:40:56,475 - INFO: Best internal validation val_loss: 2.729 at epoch: 38.
2024-06-21 17:40:56,475 - INFO: Epoch 39/200...
2024-06-21 17:40:56,475 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:40:56,475 - INFO: Batch size: 32.
2024-06-21 17:40:56,478 - INFO: Dataset:
2024-06-21 17:40:56,479 - INFO: Batch size:
2024-06-21 17:40:56,479 - INFO: Number of workers:
2024-06-21 17:40:57,546 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 1.935
2024-06-21 17:40:57,854 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 1.829
2024-06-21 17:40:58,248 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 2.422
2024-06-21 17:40:58,568 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 1.738
2024-06-21 17:40:59,007 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 2.353
2024-06-21 17:40:59,310 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 2.224
2024-06-21 17:40:59,694 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 2.227
2024-06-21 17:40:59,998 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 1.786
2024-06-21 17:41:00,444 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 1.541
2024-06-21 17:41:00,738 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 1.730
2024-06-21 17:41:01,114 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 1.969
2024-06-21 17:41:01,419 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 1.743
2024-06-21 17:41:01,861 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 1.740
2024-06-21 17:41:02,165 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 1.633
2024-06-21 17:41:02,557 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 1.765
2024-06-21 17:41:02,857 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 1.892
2024-06-21 17:41:03,293 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 1.917
2024-06-21 17:41:03,594 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 1.983
2024-06-21 17:41:03,980 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 1.883
2024-06-21 17:41:04,274 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 1.754
2024-06-21 17:41:04,707 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 2.039
2024-06-21 17:41:05,010 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 2.048
2024-06-21 17:41:05,396 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 2.152
2024-06-21 17:41:05,699 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 1.546
2024-06-21 17:41:06,129 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 1.995
2024-06-21 17:41:06,427 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 1.631
2024-06-21 17:41:06,812 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 1.778
2024-06-21 17:41:07,110 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 2.273
2024-06-21 17:41:07,332 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 2.180
2024-06-21 17:41:18,458 - INFO: 39/200 final results:
2024-06-21 17:41:18,458 - INFO: Training loss: 1.921.
2024-06-21 17:41:18,458 - INFO: Training MAE: 1.916.
2024-06-21 17:41:18,459 - INFO: Training MSE: 6.822.
2024-06-21 17:41:38,728 - INFO: Epoch: 39/200, Loss_train: 1.92094113908965, Loss_val: 2.713525155494953
2024-06-21 17:41:38,747 - INFO: Saved new best metric model for epoch 39.
2024-06-21 17:41:38,747 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:41:38,747 - INFO: Epoch 40/200...
2024-06-21 17:41:38,747 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:41:38,747 - INFO: Batch size: 32.
2024-06-21 17:41:38,751 - INFO: Dataset:
2024-06-21 17:41:38,751 - INFO: Batch size:
2024-06-21 17:41:38,751 - INFO: Number of workers:
2024-06-21 17:41:39,825 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 1.627
2024-06-21 17:41:40,131 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 1.876
2024-06-21 17:41:40,518 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 1.690
2024-06-21 17:41:40,835 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 1.837
2024-06-21 17:41:41,250 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 1.780
2024-06-21 17:41:41,565 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 1.623
2024-06-21 17:41:41,955 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 1.525
2024-06-21 17:41:42,270 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 1.662
2024-06-21 17:41:42,692 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 1.725
2024-06-21 17:41:43,007 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 1.843
2024-06-21 17:41:43,399 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 1.775
2024-06-21 17:41:43,720 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 2.198
2024-06-21 17:41:44,130 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 1.961
2024-06-21 17:41:44,447 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 1.735
2024-06-21 17:41:44,835 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 1.646
2024-06-21 17:41:45,150 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 2.334
2024-06-21 17:41:45,563 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 1.581
2024-06-21 17:41:45,877 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 1.548
2024-06-21 17:41:46,260 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 1.736
2024-06-21 17:41:46,569 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 2.189
2024-06-21 17:41:46,976 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 1.665
2024-06-21 17:41:47,292 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 1.509
2024-06-21 17:41:47,666 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 1.810
2024-06-21 17:41:47,982 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 1.675
2024-06-21 17:41:48,380 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 1.848
2024-06-21 17:41:48,691 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 1.496
2024-06-21 17:41:49,059 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 2.324
2024-06-21 17:41:49,370 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 1.706
2024-06-21 17:41:49,580 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 1.705
2024-06-21 17:42:00,556 - INFO: 40/200 final results:
2024-06-21 17:42:00,556 - INFO: Training loss: 1.780.
2024-06-21 17:42:00,556 - INFO: Training MAE: 1.782.
2024-06-21 17:42:00,556 - INFO: Training MSE: 6.020.
2024-06-21 17:42:21,072 - INFO: Epoch: 40/200, Loss_train: 1.7803601889774716, Loss_val: 3.1620103572977
2024-06-21 17:42:21,072 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:42:21,072 - INFO: Epoch 41/200...
2024-06-21 17:42:21,073 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:42:21,073 - INFO: Batch size: 32.
2024-06-21 17:42:21,076 - INFO: Dataset:
2024-06-21 17:42:21,076 - INFO: Batch size:
2024-06-21 17:42:21,076 - INFO: Number of workers:
2024-06-21 17:42:22,121 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 2.347
2024-06-21 17:42:22,462 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 1.819
2024-06-21 17:42:22,856 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 1.991
2024-06-21 17:42:23,171 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 1.719
2024-06-21 17:42:23,571 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 1.754
2024-06-21 17:42:23,908 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 1.956
2024-06-21 17:42:24,291 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 1.415
2024-06-21 17:42:24,590 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 1.746
2024-06-21 17:42:24,980 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 1.941
2024-06-21 17:42:25,320 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 1.992
2024-06-21 17:42:25,695 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 1.889
2024-06-21 17:42:25,997 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 1.828
2024-06-21 17:42:26,400 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 1.875
2024-06-21 17:42:26,750 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 1.751
2024-06-21 17:42:27,140 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 1.879
2024-06-21 17:42:27,438 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 2.002
2024-06-21 17:42:27,821 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 2.021
2024-06-21 17:42:28,167 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 2.069
2024-06-21 17:42:28,545 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 1.918
2024-06-21 17:42:28,837 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 1.846
2024-06-21 17:42:29,216 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 2.160
2024-06-21 17:42:29,564 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 1.657
2024-06-21 17:42:29,942 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 1.624
2024-06-21 17:42:30,242 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 2.131
2024-06-21 17:42:30,613 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 1.630
2024-06-21 17:42:30,950 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 1.624
2024-06-21 17:42:31,315 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 1.603
2024-06-21 17:42:31,610 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 1.723
2024-06-21 17:42:31,810 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 2.424
2024-06-21 17:42:42,635 - INFO: 41/200 final results:
2024-06-21 17:42:42,636 - INFO: Training loss: 1.874.
2024-06-21 17:42:42,636 - INFO: Training MAE: 1.863.
2024-06-21 17:42:42,636 - INFO: Training MSE: 6.462.
2024-06-21 17:43:03,060 - INFO: Epoch: 41/200, Loss_train: 1.8735965367021232, Loss_val: 2.9025889758406014
2024-06-21 17:43:03,060 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:43:03,060 - INFO: Epoch 42/200...
2024-06-21 17:43:03,060 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:43:03,060 - INFO: Batch size: 32.
2024-06-21 17:43:03,063 - INFO: Dataset:
2024-06-21 17:43:03,064 - INFO: Batch size:
2024-06-21 17:43:03,064 - INFO: Number of workers:
2024-06-21 17:43:04,112 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 1.645
2024-06-21 17:43:04,430 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 2.213
2024-06-21 17:43:04,845 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 2.029
2024-06-21 17:43:05,148 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 1.460
2024-06-21 17:43:05,560 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 2.190
2024-06-21 17:43:05,859 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 1.740
2024-06-21 17:43:06,268 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 1.885
2024-06-21 17:43:06,568 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 1.890
2024-06-21 17:43:06,972 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 1.378
2024-06-21 17:43:07,263 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 1.588
2024-06-21 17:43:07,682 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 1.656
2024-06-21 17:43:07,984 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 1.690
2024-06-21 17:43:08,399 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 1.488
2024-06-21 17:43:08,701 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 1.462
2024-06-21 17:43:09,119 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 1.761
2024-06-21 17:43:09,417 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 1.842
2024-06-21 17:43:09,811 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 1.753
2024-06-21 17:43:10,109 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 1.587
2024-06-21 17:43:10,521 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 1.487
2024-06-21 17:43:10,813 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 1.624
2024-06-21 17:43:11,204 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 1.651
2024-06-21 17:43:11,505 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 1.757
2024-06-21 17:43:11,916 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 1.622
2024-06-21 17:43:12,215 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 1.640
2024-06-21 17:43:12,602 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 1.732
2024-06-21 17:43:12,897 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:13,302 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 1.595
2024-06-21 17:43:13,596 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 1.792
2024-06-21 17:43:13,802 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 1.766
2024-06-21 17:43:24,799 - INFO: 42/200 final results:
2024-06-21 17:43:24,800 - INFO: Training loss: 1.717.
2024-06-21 17:43:24,800 - INFO: Training MAE: 1.716.
2024-06-21 17:43:24,800 - INFO: Training MSE: 5.525.
2024-06-21 17:43:45,131 - INFO: Epoch: 42/200, Loss_train: 1.7167191094365613, Loss_val: 2.8088232319930504
2024-06-21 17:43:45,131 - INFO: Best internal validation val_loss: 2.714 at epoch: 39.
2024-06-21 17:43:45,131 - INFO: Epoch 43/200...
2024-06-21 17:43:45,131 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:43:45,131 - INFO: Batch size: 32.
2024-06-21 17:43:45,135 - INFO: Dataset:
2024-06-21 17:43:45,135 - INFO: Batch size:
2024-06-21 17:43:45,135 - INFO: Number of workers:
2024-06-21 17:43:46,201 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 2.082
2024-06-21 17:43:46,508 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 2.024
2024-06-21 17:43:46,917 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:47,235 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 1.474
2024-06-21 17:43:47,649 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 1.463
2024-06-21 17:43:47,950 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 1.587
2024-06-21 17:43:48,348 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 1.341
2024-06-21 17:43:48,662 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 1.914
2024-06-21 17:43:49,075 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 1.826
2024-06-21 17:43:49,368 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 1.652
2024-06-21 17:43:49,757 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 1.990
2024-06-21 17:43:50,072 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 1.444
2024-06-21 17:43:50,490 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 1.628
2024-06-21 17:43:50,792 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 1.487
2024-06-21 17:43:51,196 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 1.713
2024-06-21 17:43:51,506 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 1.772
2024-06-21 17:43:51,922 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 1.512
2024-06-21 17:43:52,220 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 1.501
2024-06-21 17:43:52,615 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 2.011
2024-06-21 17:43:52,920 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 1.974
2024-06-21 17:43:53,326 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 1.686
2024-06-21 17:43:53,627 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 2.438
2024-06-21 17:43:54,027 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 1.648
2024-06-21 17:43:54,341 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 1.442
2024-06-21 17:43:54,747 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 1.883
2024-06-21 17:43:55,043 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 1.859
2024-06-21 17:43:55,437 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 1.780
2024-06-21 17:43:55,745 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 1.917
2024-06-21 17:43:55,966 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 1.814
2024-06-21 17:44:06,980 - INFO: 43/200 final results:
2024-06-21 17:44:06,980 - INFO: Training loss: 1.749.
2024-06-21 17:44:06,980 - INFO: Training MAE: 1.748.
2024-06-21 17:44:06,980 - INFO: Training MSE: 5.630.
2024-06-21 17:44:27,674 - INFO: Epoch: 43/200, Loss_train: 1.749013095066465, Loss_val: 2.4302909990836836
2024-06-21 17:44:27,693 - INFO: Saved new best metric model for epoch 43.
2024-06-21 17:44:27,693 - INFO: Best internal validation val_loss: 2.430 at epoch: 43.
2024-06-21 17:44:27,693 - INFO: Epoch 44/200...
2024-06-21 17:44:27,693 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:44:27,693 - INFO: Batch size: 32.
2024-06-21 17:44:27,697 - INFO: Dataset:
2024-06-21 17:44:27,697 - INFO: Batch size:
2024-06-21 17:44:27,697 - INFO: Number of workers:
2024-06-21 17:44:28,790 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 1.673
2024-06-21 17:44:29,096 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 1.731
2024-06-21 17:44:29,488 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 1.615
2024-06-21 17:44:29,805 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 1.931
2024-06-21 17:44:30,218 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 1.775
2024-06-21 17:44:30,517 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 1.753
2024-06-21 17:44:30,903 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 1.723
2024-06-21 17:44:31,217 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 1.562
2024-06-21 17:44:31,631 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 1.874
2024-06-21 17:44:31,924 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 1.926
2024-06-21 17:44:32,299 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 1.858
2024-06-21 17:44:32,614 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 1.215
2024-06-21 17:44:33,034 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 2.145
2024-06-21 17:44:33,337 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 1.650
2024-06-21 17:44:33,729 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 1.512
2024-06-21 17:44:34,040 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 1.636
2024-06-21 17:44:34,471 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 1.704
2024-06-21 17:44:34,769 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 1.634
2024-06-21 17:44:35,150 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 1.393
2024-06-21 17:44:35,456 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 1.444
2024-06-21 17:44:35,877 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 1.769
2024-06-21 17:44:36,178 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 1.803
2024-06-21 17:44:36,560 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 1.819
2024-06-21 17:44:36,873 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 1.776
2024-06-21 17:44:37,295 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 1.739
2024-06-21 17:44:37,594 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 2.126
2024-06-21 17:44:37,981 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 1.511
2024-06-21 17:44:38,294 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 1.906
2024-06-21 17:44:38,510 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 1.677
2024-06-21 17:44:49,692 - INFO: 44/200 final results:
2024-06-21 17:44:49,692 - INFO: Training loss: 1.720.
2024-06-21 17:44:49,693 - INFO: Training MAE: 1.721.
2024-06-21 17:44:49,693 - INFO: Training MSE: 5.551.
2024-06-21 17:45:09,664 - INFO: Epoch: 44/200, Loss_train: 1.7199699796479324, Loss_val: 3.5132553906276307
2024-06-21 17:45:09,664 - INFO: Best internal validation val_loss: 2.430 at epoch: 43.
2024-06-21 17:45:09,664 - INFO: Epoch 45/200...
2024-06-21 17:45:09,664 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:45:09,664 - INFO: Batch size: 32.
2024-06-21 17:45:09,668 - INFO: Dataset:
2024-06-21 17:45:09,668 - INFO: Batch size:
2024-06-21 17:45:09,668 - INFO: Number of workers:
2024-06-21 17:45:10,724 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 2.016
2024-06-21 17:45:11,043 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 1.767
2024-06-21 17:45:11,449 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 1.624
2024-06-21 17:45:11,766 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 1.558
2024-06-21 17:45:12,163 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 1.627
2024-06-21 17:45:12,475 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 1.462
2024-06-21 17:45:12,874 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 1.615
2024-06-21 17:45:13,187 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 1.738
2024-06-21 17:45:13,573 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 1.489
2024-06-21 17:45:13,878 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 1.986
2024-06-21 17:45:14,263 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 1.959
2024-06-21 17:45:14,577 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 1.861
2024-06-21 17:45:14,979 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 1.633
2024-06-21 17:45:15,294 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 1.614
2024-06-21 17:45:15,694 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 1.577
2024-06-21 17:45:16,005 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 1.770
2024-06-21 17:45:16,407 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 1.445
2024-06-21 17:45:16,718 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 1.715
2024-06-21 17:45:17,104 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 1.584
2024-06-21 17:45:17,409 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 1.472
2024-06-21 17:45:17,795 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 1.548
2024-06-21 17:45:18,108 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 1.352
2024-06-21 17:45:18,502 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 1.643
2024-06-21 17:45:18,815 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 1.590
2024-06-21 17:45:19,207 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 1.365
2024-06-21 17:45:19,516 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 1.805
2024-06-21 17:45:19,911 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 1.834
2024-06-21 17:45:20,220 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 1.588
2024-06-21 17:45:20,433 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 1.433
2024-06-21 17:45:31,555 - INFO: 45/200 final results:
2024-06-21 17:45:31,555 - INFO: Training loss: 1.644.
2024-06-21 17:45:31,555 - INFO: Training MAE: 1.648.
2024-06-21 17:45:31,555 - INFO: Training MSE: 4.966.
2024-06-21 17:45:51,816 - INFO: Epoch: 45/200, Loss_train: 1.6437602906391537, Loss_val: 2.3645453165317405
2024-06-21 17:45:51,835 - INFO: Saved new best metric model for epoch 45.
2024-06-21 17:45:51,836 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:45:51,836 - INFO: Epoch 46/200...
2024-06-21 17:45:51,836 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:45:51,836 - INFO: Batch size: 32.
2024-06-21 17:45:51,839 - INFO: Dataset:
2024-06-21 17:45:51,840 - INFO: Batch size:
2024-06-21 17:45:51,840 - INFO: Number of workers:
2024-06-21 17:45:52,891 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 1.956
2024-06-21 17:45:53,212 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 1.990
2024-06-21 17:45:53,622 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 1.680
2024-06-21 17:45:53,941 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 1.356
2024-06-21 17:45:54,345 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 1.627
2024-06-21 17:45:54,660 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 1.317
2024-06-21 17:45:55,061 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 1.523
2024-06-21 17:45:55,378 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 1.751
2024-06-21 17:45:55,771 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 1.707
2024-06-21 17:45:56,078 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 1.743
2024-06-21 17:45:56,468 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 1.656
2024-06-21 17:45:56,787 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 1.414
2024-06-21 17:45:57,197 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 1.761
2024-06-21 17:45:57,512 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 1.186
2024-06-21 17:45:57,918 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 1.741
2024-06-21 17:45:58,229 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 1.827
2024-06-21 17:45:58,635 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 1.991
2024-06-21 17:45:58,947 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 2.018
2024-06-21 17:45:59,345 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 1.548
2024-06-21 17:45:59,652 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 1.476
2024-06-21 17:46:00,048 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 1.542
2024-06-21 17:46:00,364 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 1.434
2024-06-21 17:46:00,756 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 1.699
2024-06-21 17:46:01,071 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 1.888
2024-06-21 17:46:01,462 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 1.705
2024-06-21 17:46:01,772 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 1.495
2024-06-21 17:46:02,162 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 2.004
2024-06-21 17:46:02,473 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 1.994
2024-06-21 17:46:02,686 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 1.976
2024-06-21 17:46:13,796 - INFO: 46/200 final results:
2024-06-21 17:46:13,796 - INFO: Training loss: 1.690.
2024-06-21 17:46:13,796 - INFO: Training MAE: 1.684.
2024-06-21 17:46:13,796 - INFO: Training MSE: 5.360.
2024-06-21 17:46:34,240 - INFO: Epoch: 46/200, Loss_train: 1.6898927565278679, Loss_val: 2.428579096136422
2024-06-21 17:46:34,240 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:46:34,241 - INFO: Epoch 47/200...
2024-06-21 17:46:34,241 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:46:34,241 - INFO: Batch size: 32.
2024-06-21 17:46:34,244 - INFO: Dataset:
2024-06-21 17:46:34,245 - INFO: Batch size:
2024-06-21 17:46:34,245 - INFO: Number of workers:
2024-06-21 17:46:35,315 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 1.465
2024-06-21 17:46:35,620 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 1.464
2024-06-21 17:46:36,013 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 1.427
2024-06-21 17:46:36,331 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 1.514
2024-06-21 17:46:36,762 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 1.682
2024-06-21 17:46:37,060 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 1.663
2024-06-21 17:46:37,444 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 1.693
2024-06-21 17:46:37,746 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 1.757
2024-06-21 17:46:38,192 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 1.491
2024-06-21 17:46:38,485 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 1.653
2024-06-21 17:46:38,860 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 1.761
2024-06-21 17:46:39,165 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 1.704
2024-06-21 17:46:39,608 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 1.434
2024-06-21 17:46:39,912 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 1.810
2024-06-21 17:46:40,304 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 1.817
2024-06-21 17:46:40,605 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 1.486
2024-06-21 17:46:41,047 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 1.395
2024-06-21 17:46:41,347 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 2.306
2024-06-21 17:46:41,729 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 1.797
2024-06-21 17:46:42,025 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 1.880
2024-06-21 17:46:42,456 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 1.485
2024-06-21 17:46:42,760 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 1.317
2024-06-21 17:46:43,141 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 1.899
2024-06-21 17:46:43,444 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 1.713
2024-06-21 17:46:43,878 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 1.818
2024-06-21 17:46:44,177 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 1.912
2024-06-21 17:46:44,555 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 1.552
2024-06-21 17:46:44,853 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 1.771
2024-06-21 17:46:45,072 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 1.653
2024-06-21 17:46:56,168 - INFO: 47/200 final results:
2024-06-21 17:46:56,169 - INFO: Training loss: 1.666.
2024-06-21 17:46:56,169 - INFO: Training MAE: 1.666.
2024-06-21 17:46:56,169 - INFO: Training MSE: 5.256.
2024-06-21 17:47:16,594 - INFO: Epoch: 47/200, Loss_train: 1.6661566208148826, Loss_val: 2.6962341111281822
2024-06-21 17:47:16,594 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:47:16,594 - INFO: Epoch 48/200...
2024-06-21 17:47:16,594 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:47:16,594 - INFO: Batch size: 32.
2024-06-21 17:47:16,597 - INFO: Dataset:
2024-06-21 17:47:16,598 - INFO: Batch size:
2024-06-21 17:47:16,598 - INFO: Number of workers:
2024-06-21 17:47:17,679 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 2.447
2024-06-21 17:47:18,000 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 1.473
2024-06-21 17:47:18,402 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 1.495
2024-06-21 17:47:18,720 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 1.662
2024-06-21 17:47:19,135 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 1.598
2024-06-21 17:47:19,436 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 1.998
2024-06-21 17:47:19,821 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 1.794
2024-06-21 17:47:20,134 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 1.652
2024-06-21 17:47:20,539 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 1.614
2024-06-21 17:47:20,832 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 1.668
2024-06-21 17:47:21,208 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 1.580
2024-06-21 17:47:21,525 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 1.488
2024-06-21 17:47:21,942 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 1.414
2024-06-21 17:47:22,246 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 1.545
2024-06-21 17:47:22,640 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 1.771
2024-06-21 17:47:22,953 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 1.931
2024-06-21 17:47:23,366 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 1.872
2024-06-21 17:47:23,666 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 1.547
2024-06-21 17:47:24,052 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 1.656
2024-06-21 17:47:24,359 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 1.962
2024-06-21 17:47:24,766 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 1.636
2024-06-21 17:47:25,069 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 1.645
2024-06-21 17:47:25,467 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 1.792
2024-06-21 17:47:25,781 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 1.410
2024-06-21 17:47:26,181 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 1.671
2024-06-21 17:47:26,478 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 1.839
2024-06-21 17:47:26,861 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 1.650
2024-06-21 17:47:27,170 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 1.607
2024-06-21 17:47:27,379 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 2.085
2024-06-21 17:47:38,242 - INFO: 48/200 final results:
2024-06-21 17:47:38,242 - INFO: Training loss: 1.707.
2024-06-21 17:47:38,242 - INFO: Training MAE: 1.699.
2024-06-21 17:47:38,242 - INFO: Training MSE: 5.301.
2024-06-21 17:47:58,637 - INFO: Epoch: 48/200, Loss_train: 1.7069553753425335, Loss_val: 2.747206112434124
2024-06-21 17:47:58,637 - INFO: Best internal validation val_loss: 2.365 at epoch: 45.
2024-06-21 17:47:58,637 - INFO: Epoch 49/200...
2024-06-21 17:47:58,637 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:47:58,637 - INFO: Batch size: 32.
2024-06-21 17:47:58,641 - INFO: Dataset:
2024-06-21 17:47:58,641 - INFO: Batch size:
2024-06-21 17:47:58,641 - INFO: Number of workers:
2024-06-21 17:47:59,694 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 2.081
2024-06-21 17:48:00,027 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 1.498
2024-06-21 17:48:00,425 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 1.991
2024-06-21 17:48:00,742 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 1.428
2024-06-21 17:48:01,145 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 1.699
2024-06-21 17:48:01,471 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 1.616
2024-06-21 17:48:01,859 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 1.614
2024-06-21 17:48:02,173 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 1.193
2024-06-21 17:48:02,562 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 1.474
2024-06-21 17:48:02,888 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 1.663
2024-06-21 17:48:03,266 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 1.613
2024-06-21 17:48:03,583 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 1.913
2024-06-21 17:48:03,990 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 1.870
2024-06-21 17:48:04,319 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 2.173
2024-06-21 17:48:04,713 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 1.802
2024-06-21 17:48:05,027 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 1.351
2024-06-21 17:48:05,429 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 1.861
2024-06-21 17:48:05,755 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 1.835
2024-06-21 17:48:06,140 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 1.606
2024-06-21 17:48:06,448 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 1.738
2024-06-21 17:48:06,841 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 1.597
2024-06-21 17:48:07,169 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 1.856
2024-06-21 17:48:07,543 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 1.396
2024-06-21 17:48:07,858 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 1.811
2024-06-21 17:48:08,243 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 1.537
2024-06-21 17:48:08,565 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 1.554
2024-06-21 17:48:08,932 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 1.943
2024-06-21 17:48:09,243 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 1.667
2024-06-21 17:48:09,451 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 2.190
2024-06-21 17:48:20,465 - INFO: 49/200 final results:
2024-06-21 17:48:20,465 - INFO: Training loss: 1.709.
2024-06-21 17:48:20,465 - INFO: Training MAE: 1.700.
2024-06-21 17:48:20,465 - INFO: Training MSE: 5.306.
2024-06-21 17:48:40,871 - INFO: Epoch: 49/200, Loss_train: 1.7092792330117061, Loss_val: 2.12885398289253
2024-06-21 17:48:40,890 - INFO: Saved new best metric model for epoch 49.
2024-06-21 17:48:40,890 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:48:40,890 - INFO: Epoch 50/200...
2024-06-21 17:48:40,890 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:48:40,890 - INFO: Batch size: 32.
2024-06-21 17:48:40,894 - INFO: Dataset:
2024-06-21 17:48:40,894 - INFO: Batch size:
2024-06-21 17:48:40,894 - INFO: Number of workers:
2024-06-21 17:48:41,968 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 1.440
2024-06-21 17:48:42,276 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 1.416
2024-06-21 17:48:42,691 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 1.547
2024-06-21 17:48:43,012 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 1.609
2024-06-21 17:48:43,426 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 1.333
2024-06-21 17:48:43,728 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 2.122
2024-06-21 17:48:44,132 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 1.749
2024-06-21 17:48:44,449 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 1.556
2024-06-21 17:48:44,866 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 1.706
2024-06-21 17:48:45,160 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 1.571
2024-06-21 17:48:45,554 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 1.390
2024-06-21 17:48:45,871 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 1.481
2024-06-21 17:48:46,292 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 1.544
2024-06-21 17:48:46,596 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 1.481
2024-06-21 17:48:47,005 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 1.617
2024-06-21 17:48:47,316 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 1.433
2024-06-21 17:48:47,729 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 1.277
2024-06-21 17:48:48,027 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 1.293
2024-06-21 17:48:48,425 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 1.901
2024-06-21 17:48:48,730 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 1.636
2024-06-21 17:48:49,133 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 1.606
2024-06-21 17:48:49,434 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 1.318
2024-06-21 17:48:49,829 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 1.721
2024-06-21 17:48:50,143 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 1.441
2024-06-21 17:48:50,547 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 1.478
2024-06-21 17:48:50,845 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 1.297
2024-06-21 17:48:51,244 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 1.867
2024-06-21 17:48:51,555 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 1.996
2024-06-21 17:48:51,767 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 1.722
2024-06-21 17:49:02,629 - INFO: 50/200 final results:
2024-06-21 17:49:02,629 - INFO: Training loss: 1.571.
2024-06-21 17:49:02,629 - INFO: Training MAE: 1.568.
2024-06-21 17:49:02,629 - INFO: Training MSE: 4.570.
2024-06-21 17:49:22,883 - INFO: Epoch: 50/200, Loss_train: 1.570638660726876, Loss_val: 2.287520947127507
2024-06-21 17:49:22,883 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:49:22,883 - INFO: Epoch 51/200...
2024-06-21 17:49:22,883 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:49:22,884 - INFO: Batch size: 32.
2024-06-21 17:49:22,887 - INFO: Dataset:
2024-06-21 17:49:22,888 - INFO: Batch size:
2024-06-21 17:49:22,888 - INFO: Number of workers:
2024-06-21 17:49:23,977 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 1.627
2024-06-21 17:49:24,285 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 1.755
2024-06-21 17:49:24,684 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 1.428
2024-06-21 17:49:25,004 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 1.767
2024-06-21 17:49:25,433 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 2.070
2024-06-21 17:49:25,736 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 1.728
2024-06-21 17:49:26,125 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 1.724
2024-06-21 17:49:26,440 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 2.126
2024-06-21 17:49:26,865 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 1.392
2024-06-21 17:49:27,159 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 1.659
2024-06-21 17:49:27,538 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 1.188
2024-06-21 17:49:27,856 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 1.569
2024-06-21 17:49:28,288 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 1.731
2024-06-21 17:49:28,590 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 1.512
2024-06-21 17:49:28,982 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 1.285
2024-06-21 17:49:29,292 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 1.805
2024-06-21 17:49:29,720 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 1.878
2024-06-21 17:49:30,018 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 1.507
2024-06-21 17:49:30,402 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 1.516
2024-06-21 17:49:30,707 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 1.526
2024-06-21 17:49:31,128 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 1.617
2024-06-21 17:49:31,432 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 1.907
2024-06-21 17:49:31,806 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 1.440
2024-06-21 17:49:32,122 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 1.570
2024-06-21 17:49:32,532 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 1.540
2024-06-21 17:49:32,830 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 1.381
2024-06-21 17:49:33,199 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 1.357
2024-06-21 17:49:33,510 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 2.036
2024-06-21 17:49:33,719 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 1.883
2024-06-21 17:49:44,819 - INFO: 51/200 final results:
2024-06-21 17:49:44,819 - INFO: Training loss: 1.639.
2024-06-21 17:49:44,819 - INFO: Training MAE: 1.634.
2024-06-21 17:49:44,819 - INFO: Training MSE: 4.924.
2024-06-21 17:50:04,891 - INFO: Epoch: 51/200, Loss_train: 1.6387584990468518, Loss_val: 2.5013901981814155
2024-06-21 17:50:04,891 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:50:04,891 - INFO: Epoch 52/200...
2024-06-21 17:50:04,891 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:50:04,891 - INFO: Batch size: 32.
2024-06-21 17:50:04,895 - INFO: Dataset:
2024-06-21 17:50:04,895 - INFO: Batch size:
2024-06-21 17:50:04,895 - INFO: Number of workers:
2024-06-21 17:50:05,957 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 1.990
2024-06-21 17:50:06,276 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 1.855
2024-06-21 17:50:06,664 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 1.536
2024-06-21 17:50:06,980 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 1.879
2024-06-21 17:50:07,370 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 1.568
2024-06-21 17:50:07,694 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 1.653
2024-06-21 17:50:08,069 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 1.583
2024-06-21 17:50:08,381 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 1.520
2024-06-21 17:50:08,763 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 1.219
2024-06-21 17:50:09,086 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 1.303
2024-06-21 17:50:09,465 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 1.357
2024-06-21 17:50:09,783 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 2.081
2024-06-21 17:50:10,192 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 1.821
2024-06-21 17:50:10,522 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 2.172
2024-06-21 17:50:10,919 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 1.958
2024-06-21 17:50:11,233 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 1.496
2024-06-21 17:50:11,639 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 1.983
2024-06-21 17:50:11,966 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 1.846
2024-06-21 17:50:12,352 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 1.712
2024-06-21 17:50:12,660 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 1.504
2024-06-21 17:50:13,057 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 1.420
2024-06-21 17:50:13,385 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 1.720
2024-06-21 17:50:13,766 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 1.578
2024-06-21 17:50:14,083 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 1.429
2024-06-21 17:50:14,476 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 1.584
2024-06-21 17:50:14,799 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 1.311
2024-06-21 17:50:15,172 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 1.511
2024-06-21 17:50:15,483 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 1.685
2024-06-21 17:50:15,700 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 2.117
2024-06-21 17:50:26,896 - INFO: 52/200 final results:
2024-06-21 17:50:26,896 - INFO: Training loss: 1.669.
2024-06-21 17:50:26,896 - INFO: Training MAE: 1.660.
2024-06-21 17:50:26,896 - INFO: Training MSE: 5.100.
2024-06-21 17:50:47,359 - INFO: Epoch: 52/200, Loss_train: 1.6686521480823386, Loss_val: 2.659305531403114
2024-06-21 17:50:47,359 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:50:47,359 - INFO: Epoch 53/200...
2024-06-21 17:50:47,359 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:50:47,359 - INFO: Batch size: 32.
2024-06-21 17:50:47,363 - INFO: Dataset:
2024-06-21 17:50:47,363 - INFO: Batch size:
2024-06-21 17:50:47,363 - INFO: Number of workers:
2024-06-21 17:50:48,456 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 1.357
2024-06-21 17:50:48,761 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 1.551
2024-06-21 17:50:49,156 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 1.480
2024-06-21 17:50:49,473 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 1.904
2024-06-21 17:50:49,907 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 1.419
2024-06-21 17:50:50,206 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 1.777
2024-06-21 17:50:50,591 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 1.566
2024-06-21 17:50:50,892 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 1.865
2024-06-21 17:50:51,337 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 1.843
2024-06-21 17:50:51,628 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 1.898
2024-06-21 17:50:52,001 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 1.776
2024-06-21 17:50:52,304 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 1.615
2024-06-21 17:50:52,747 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 1.393
2024-06-21 17:50:53,051 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 1.204
2024-06-21 17:50:53,446 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 1.175
2024-06-21 17:50:53,746 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 1.482
2024-06-21 17:50:54,183 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 1.488
2024-06-21 17:50:54,480 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 1.519
2024-06-21 17:50:54,861 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 1.518
2024-06-21 17:50:55,153 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 1.444
2024-06-21 17:50:55,578 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 1.624
2024-06-21 17:50:55,879 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 1.906
2024-06-21 17:50:56,250 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 1.630
2024-06-21 17:50:56,550 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 1.630
2024-06-21 17:50:56,970 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 1.262
2024-06-21 17:50:57,266 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 1.510
2024-06-21 17:50:57,632 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 1.221
2024-06-21 17:50:57,928 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 1.553
2024-06-21 17:50:58,135 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 1.992
2024-06-21 17:51:09,274 - INFO: 53/200 final results:
2024-06-21 17:51:09,275 - INFO: Training loss: 1.573.
2024-06-21 17:51:09,275 - INFO: Training MAE: 1.564.
2024-06-21 17:51:09,275 - INFO: Training MSE: 4.500.
2024-06-21 17:51:29,197 - INFO: Epoch: 53/200, Loss_train: 1.5725214686887017, Loss_val: 2.3709471801231645
2024-06-21 17:51:29,197 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:51:29,197 - INFO: Epoch 54/200...
2024-06-21 17:51:29,197 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:51:29,197 - INFO: Batch size: 32.
2024-06-21 17:51:29,201 - INFO: Dataset:
2024-06-21 17:51:29,201 - INFO: Batch size:
2024-06-21 17:51:29,201 - INFO: Number of workers:
2024-06-21 17:51:30,288 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 1.728
2024-06-21 17:51:30,593 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 2.041
2024-06-21 17:51:30,994 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 1.916
2024-06-21 17:51:31,314 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 1.751
2024-06-21 17:51:31,749 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 1.519
2024-06-21 17:51:32,049 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 1.662
2024-06-21 17:51:32,438 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 1.506
2024-06-21 17:51:32,737 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 1.664
2024-06-21 17:51:33,173 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 1.445
2024-06-21 17:51:33,466 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 1.017
2024-06-21 17:51:33,842 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 1.503
2024-06-21 17:51:34,144 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 1.272
2024-06-21 17:51:34,878 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 1.435
2024-06-21 17:51:35,183 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 1.498
2024-06-21 17:51:35,580 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 1.611
2024-06-21 17:51:35,881 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 1.427
2024-06-21 17:51:36,320 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 1.560
2024-06-21 17:51:36,622 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 1.645
2024-06-21 17:51:37,009 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 1.369
2024-06-21 17:51:37,304 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 1.678
2024-06-21 17:51:37,736 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 1.650
2024-06-21 17:51:38,040 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 1.923
2024-06-21 17:51:38,428 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 1.490
2024-06-21 17:51:38,732 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 1.606
2024-06-21 17:51:39,163 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 1.566
2024-06-21 17:51:39,462 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 2.006
2024-06-21 17:51:39,842 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 1.480
2024-06-21 17:51:40,141 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 1.413
2024-06-21 17:51:40,362 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 1.437
2024-06-21 17:51:51,270 - INFO: 54/200 final results:
2024-06-21 17:51:51,271 - INFO: Training loss: 1.580.
2024-06-21 17:51:51,271 - INFO: Training MAE: 1.583.
2024-06-21 17:51:51,271 - INFO: Training MSE: 4.610.
2024-06-21 17:52:11,590 - INFO: Epoch: 54/200, Loss_train: 1.5799566014059658, Loss_val: 2.3920297129400847
2024-06-21 17:52:11,590 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:52:11,590 - INFO: Epoch 55/200...
2024-06-21 17:52:11,590 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:52:11,590 - INFO: Batch size: 32.
2024-06-21 17:52:11,594 - INFO: Dataset:
2024-06-21 17:52:11,594 - INFO: Batch size:
2024-06-21 17:52:11,594 - INFO: Number of workers:
2024-06-21 17:52:12,707 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 1.878
2024-06-21 17:52:13,025 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 1.922
2024-06-21 17:52:13,419 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 1.523
2024-06-21 17:52:13,735 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 1.513
2024-06-21 17:52:14,156 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 1.409
2024-06-21 17:52:14,456 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 1.421
2024-06-21 17:52:14,843 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 1.623
2024-06-21 17:52:15,155 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 1.399
2024-06-21 17:52:15,587 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 1.293
2024-06-21 17:52:15,880 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 1.426
2024-06-21 17:52:16,258 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 1.591
2024-06-21 17:52:16,573 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 1.492
2024-06-21 17:52:16,999 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 1.720
2024-06-21 17:52:17,303 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 1.776
2024-06-21 17:52:17,700 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 1.687
2024-06-21 17:52:18,013 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 1.676
2024-06-21 17:52:18,445 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 1.510
2024-06-21 17:52:18,745 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 1.419
2024-06-21 17:52:19,117 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 1.308
2024-06-21 17:52:19,425 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 1.336
2024-06-21 17:52:19,838 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 1.323
2024-06-21 17:52:20,147 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 1.373
2024-06-21 17:52:20,548 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 1.671
2024-06-21 17:52:20,872 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 1.414
2024-06-21 17:52:21,291 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 1.857
2024-06-21 17:52:21,587 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 1.904
2024-06-21 17:52:21,967 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 1.686
2024-06-21 17:52:22,275 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 1.320
2024-06-21 17:52:22,493 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 2.101
2024-06-21 17:52:33,481 - INFO: 55/200 final results:
2024-06-21 17:52:33,482 - INFO: Training loss: 1.571.
2024-06-21 17:52:33,482 - INFO: Training MAE: 1.561.
2024-06-21 17:52:33,482 - INFO: Training MSE: 4.427.
2024-06-21 17:52:53,705 - INFO: Epoch: 55/200, Loss_train: 1.5713629722595215, Loss_val: 2.234514330995494
2024-06-21 17:52:53,705 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:52:53,705 - INFO: Epoch 56/200...
2024-06-21 17:52:53,705 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 17:52:53,705 - INFO: Batch size: 32.
2024-06-21 17:52:53,709 - INFO: Dataset:
2024-06-21 17:52:53,709 - INFO: Batch size:
2024-06-21 17:52:53,709 - INFO: Number of workers:
2024-06-21 17:52:54,764 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 1.324
2024-06-21 17:52:55,085 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 1.265
2024-06-21 17:52:55,495 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 1.267
2024-06-21 17:52:55,814 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 1.215
2024-06-21 17:52:56,218 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 1.255
2024-06-21 17:52:56,517 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 1.680
2024-06-21 17:52:56,911 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 1.583
2024-06-21 17:52:57,226 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 1.504
2024-06-21 17:52:57,632 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 2.021
2024-06-21 17:52:57,926 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 1.375
2024-06-21 17:52:58,309 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 1.222
2024-06-21 17:52:58,626 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 1.080
2024-06-21 17:52:59,043 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 1.579
2024-06-21 17:52:59,348 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 1.775
2024-06-21 17:52:59,751 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 1.629
2024-06-21 17:53:00,064 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 1.334
2024-06-21 17:53:00,480 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 1.356
2024-06-21 17:53:00,780 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 1.747
2024-06-21 17:53:01,164 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 1.389
2024-06-21 17:53:01,474 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 1.671
2024-06-21 17:53:01,879 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 1.440
2024-06-21 17:53:02,183 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 1.606
2024-06-21 17:53:02,582 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 1.337
2024-06-21 17:53:02,897 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 1.581
2024-06-21 17:53:03,306 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 1.406
2024-06-21 17:53:03,604 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 1.511
2024-06-21 17:53:04,002 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 1.832
2024-06-21 17:53:04,314 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 1.423
2024-06-21 17:53:04,534 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 1.265
2024-06-21 17:53:15,660 - INFO: 56/200 final results:
2024-06-21 17:53:15,660 - INFO: Training loss: 1.471.
2024-06-21 17:53:15,660 - INFO: Training MAE: 1.475.
2024-06-21 17:53:15,660 - INFO: Training MSE: 3.901.
2024-06-21 17:53:35,781 - INFO: Epoch: 56/200, Loss_train: 1.4713498477278084, Loss_val: 2.5679900235143203
2024-06-21 17:53:35,781 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:53:35,781 - INFO: Epoch 57/200...
2024-06-21 17:53:35,781 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:53:35,781 - INFO: Batch size: 32.
2024-06-21 17:53:35,785 - INFO: Dataset:
2024-06-21 17:53:35,785 - INFO: Batch size:
2024-06-21 17:53:35,785 - INFO: Number of workers:
2024-06-21 17:53:36,870 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 1.547
2024-06-21 17:53:37,175 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 1.396
2024-06-21 17:53:37,572 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 1.536
2024-06-21 17:53:37,890 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 1.479
2024-06-21 17:53:38,314 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 1.544
2024-06-21 17:53:38,614 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 1.169
2024-06-21 17:53:38,997 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 1.176
2024-06-21 17:53:39,312 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 1.300
2024-06-21 17:53:39,724 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 1.593
2024-06-21 17:53:40,016 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 1.240
2024-06-21 17:53:40,378 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 1.208
2024-06-21 17:53:40,692 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 1.239
2024-06-21 17:53:41,127 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 1.294
2024-06-21 17:53:41,431 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 1.251
2024-06-21 17:53:41,827 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 1.569
2024-06-21 17:53:42,141 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 1.419
2024-06-21 17:53:42,573 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 1.153
2024-06-21 17:53:42,873 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 0.996
2024-06-21 17:53:43,245 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 1.307
2024-06-21 17:53:43,553 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 1.169
2024-06-21 17:53:43,972 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 1.590
2024-06-21 17:53:44,273 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 1.335
2024-06-21 17:53:44,654 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 1.641
2024-06-21 17:53:44,967 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 1.158
2024-06-21 17:53:45,378 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 1.746
2024-06-21 17:53:45,674 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 1.230
2024-06-21 17:53:46,043 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 1.200
2024-06-21 17:53:46,352 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 1.389
2024-06-21 17:53:46,571 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 1.316
2024-06-21 17:53:57,602 - INFO: 57/200 final results:
2024-06-21 17:53:57,602 - INFO: Training loss: 1.351.
2024-06-21 17:53:57,602 - INFO: Training MAE: 1.352.
2024-06-21 17:53:57,602 - INFO: Training MSE: 3.432.
2024-06-21 17:54:18,008 - INFO: Epoch: 57/200, Loss_train: 1.3514444786926796, Loss_val: 2.253265717933918
2024-06-21 17:54:18,008 - INFO: Best internal validation val_loss: 2.129 at epoch: 49.
2024-06-21 17:54:18,008 - INFO: Epoch 58/200...
2024-06-21 17:54:18,008 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:54:18,008 - INFO: Batch size: 32.
2024-06-21 17:54:18,012 - INFO: Dataset:
2024-06-21 17:54:18,012 - INFO: Batch size:
2024-06-21 17:54:18,012 - INFO: Number of workers:
2024-06-21 17:54:19,079 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 1.280
2024-06-21 17:54:19,400 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 1.056
2024-06-21 17:54:19,798 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 1.313
2024-06-21 17:54:20,116 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 1.392
2024-06-21 17:54:20,530 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 0.981
2024-06-21 17:54:20,830 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 1.035
2024-06-21 17:54:21,220 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 1.100
2024-06-21 17:54:21,533 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 1.093
2024-06-21 17:54:21,943 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 1.568
2024-06-21 17:54:22,237 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 1.515
2024-06-21 17:54:22,614 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 1.092
2024-06-21 17:54:22,931 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 1.063
2024-06-21 17:54:23,348 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 1.487
2024-06-21 17:54:23,652 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 1.229
2024-06-21 17:54:24,057 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 1.066
2024-06-21 17:54:24,371 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 1.322
2024-06-21 17:54:24,776 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 1.165
2024-06-21 17:54:25,076 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 1.075
2024-06-21 17:54:25,473 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 1.159
2024-06-21 17:54:25,781 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 1.151
2024-06-21 17:54:26,174 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 1.266
2024-06-21 17:54:26,477 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 1.207
2024-06-21 17:54:26,876 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 1.176
2024-06-21 17:54:27,198 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 1.318
2024-06-21 17:54:27,620 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 1.220
2024-06-21 17:54:27,917 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 1.558
2024-06-21 17:54:28,312 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 1.300
2024-06-21 17:54:28,622 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 1.231
2024-06-21 17:54:28,836 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 1.475
2024-06-21 17:54:39,871 - INFO: 58/200 final results:
2024-06-21 17:54:39,871 - INFO: Training loss: 1.238.
2024-06-21 17:54:39,871 - INFO: Training MAE: 1.233.
2024-06-21 17:54:39,871 - INFO: Training MSE: 2.863.
2024-06-21 17:55:00,030 - INFO: Epoch: 58/200, Loss_train: 1.2377645085597861, Loss_val: 2.097023577525698
2024-06-21 17:55:00,049 - INFO: Saved new best metric model for epoch 58.
2024-06-21 17:55:00,049 - INFO: Best internal validation val_loss: 2.097 at epoch: 58.
2024-06-21 17:55:00,049 - INFO: Epoch 59/200...
2024-06-21 17:55:00,049 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:55:00,049 - INFO: Batch size: 32.
2024-06-21 17:55:00,053 - INFO: Dataset:
2024-06-21 17:55:00,053 - INFO: Batch size:
2024-06-21 17:55:00,053 - INFO: Number of workers:
2024-06-21 17:55:01,108 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 1.119
2024-06-21 17:55:01,431 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 1.227
2024-06-21 17:55:01,842 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 1.410
2024-06-21 17:55:02,160 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 1.553
2024-06-21 17:55:02,561 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 1.279
2024-06-21 17:55:02,874 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 1.167
2024-06-21 17:55:03,256 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 1.155
2024-06-21 17:55:03,571 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 1.480
2024-06-21 17:55:03,962 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 1.078
2024-06-21 17:55:04,268 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 1.617
2024-06-21 17:55:04,659 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 1.152
2024-06-21 17:55:04,976 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 1.426
2024-06-21 17:55:05,382 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 1.277
2024-06-21 17:55:05,700 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 1.296
2024-06-21 17:55:06,095 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 1.151
2024-06-21 17:55:06,409 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 1.196
2024-06-21 17:55:06,811 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 1.205
2024-06-21 17:55:07,125 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 1.501
2024-06-21 17:55:07,508 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 1.581
2024-06-21 17:55:07,817 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 1.398
2024-06-21 17:55:08,210 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 1.167
2024-06-21 17:55:08,525 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 1.064
2024-06-21 17:55:08,911 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 1.123
2024-06-21 17:55:09,226 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 1.369
2024-06-21 17:55:09,610 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 1.225
2024-06-21 17:55:09,920 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 1.132
2024-06-21 17:55:10,299 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 1.287
2024-06-21 17:55:10,609 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 1.396
2024-06-21 17:55:10,818 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 1.138
2024-06-21 17:55:21,933 - INFO: 59/200 final results:
2024-06-21 17:55:21,933 - INFO: Training loss: 1.282.
2024-06-21 17:55:21,933 - INFO: Training MAE: 1.285.
2024-06-21 17:55:21,933 - INFO: Training MSE: 3.041.
2024-06-21 17:55:42,022 - INFO: Epoch: 59/200, Loss_train: 1.2817197462608074, Loss_val: 1.9598168710182453
2024-06-21 17:55:42,041 - INFO: Saved new best metric model for epoch 59.
2024-06-21 17:55:42,041 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:55:42,041 - INFO: Epoch 60/200...
2024-06-21 17:55:42,041 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:55:42,041 - INFO: Batch size: 32.
2024-06-21 17:55:42,045 - INFO: Dataset:
2024-06-21 17:55:42,045 - INFO: Batch size:
2024-06-21 17:55:42,045 - INFO: Number of workers:
2024-06-21 17:55:43,128 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 1.013
2024-06-21 17:55:43,432 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 1.282
2024-06-21 17:55:43,828 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 1.045
2024-06-21 17:55:44,144 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 1.071
2024-06-21 17:55:44,565 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 1.319
2024-06-21 17:55:44,863 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 1.237
2024-06-21 17:55:45,250 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 1.133
2024-06-21 17:55:45,562 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 1.092
2024-06-21 17:55:45,987 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 1.349
2024-06-21 17:55:46,279 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 1.269
2024-06-21 17:55:46,654 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 1.216
2024-06-21 17:55:46,968 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 1.279
2024-06-21 17:55:47,397 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 1.051
2024-06-21 17:55:47,699 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 1.139
2024-06-21 17:55:48,094 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 1.099
2024-06-21 17:55:48,406 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 1.324
2024-06-21 17:55:48,835 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 1.380
2024-06-21 17:55:49,133 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 1.081
2024-06-21 17:55:49,517 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 1.273
2024-06-21 17:55:49,823 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 1.294
2024-06-21 17:55:50,233 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 1.218
2024-06-21 17:55:50,534 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 1.149
2024-06-21 17:55:50,917 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 1.213
2024-06-21 17:55:51,230 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 1.214
2024-06-21 17:55:51,646 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 1.172
2024-06-21 17:55:51,942 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 1.014
2024-06-21 17:55:52,323 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 1.066
2024-06-21 17:55:52,633 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 1.240
2024-06-21 17:55:52,843 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 0.723
2024-06-21 17:56:03,862 - INFO: 60/200 final results:
2024-06-21 17:56:03,862 - INFO: Training loss: 1.171.
2024-06-21 17:56:03,862 - INFO: Training MAE: 1.180.
2024-06-21 17:56:03,862 - INFO: Training MSE: 2.637.
2024-06-21 17:56:24,248 - INFO: Epoch: 60/200, Loss_train: 1.1709334459798089, Loss_val: 2.222502724877719
2024-06-21 17:56:24,248 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:56:24,248 - INFO: Epoch 61/200...
2024-06-21 17:56:24,248 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:56:24,248 - INFO: Batch size: 32.
2024-06-21 17:56:24,252 - INFO: Dataset:
2024-06-21 17:56:24,252 - INFO: Batch size:
2024-06-21 17:56:24,252 - INFO: Number of workers:
2024-06-21 17:56:25,318 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 1.160
2024-06-21 17:56:25,659 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 1.176
2024-06-21 17:56:26,054 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 1.236
2024-06-21 17:56:26,371 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 1.370
2024-06-21 17:56:26,771 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 1.253
2024-06-21 17:56:27,108 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 1.335
2024-06-21 17:56:27,495 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 1.186
2024-06-21 17:56:27,796 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 1.056
2024-06-21 17:56:28,185 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 1.262
2024-06-21 17:56:28,524 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 0.962
2024-06-21 17:56:28,900 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 0.958
2024-06-21 17:56:29,203 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 1.384
2024-06-21 17:56:29,609 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 1.189
2024-06-21 17:56:29,949 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 1.185
2024-06-21 17:56:30,344 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 1.137
2024-06-21 17:56:30,642 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 1.090
2024-06-21 17:56:31,046 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 1.490
2024-06-21 17:56:31,383 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 0.989
2024-06-21 17:56:31,768 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 1.241
2024-06-21 17:56:32,061 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 1.090
2024-06-21 17:56:32,459 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 1.194
2024-06-21 17:56:32,796 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 1.013
2024-06-21 17:56:33,169 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 0.978
2024-06-21 17:56:33,470 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 1.314
2024-06-21 17:56:33,867 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 1.117
2024-06-21 17:56:34,200 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 1.116
2024-06-21 17:56:34,584 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 1.273
2024-06-21 17:56:34,880 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 1.047
2024-06-21 17:56:35,101 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 1.386
2024-06-21 17:56:45,883 - INFO: 61/200 final results:
2024-06-21 17:56:45,884 - INFO: Training loss: 1.179.
2024-06-21 17:56:45,884 - INFO: Training MAE: 1.175.
2024-06-21 17:56:45,884 - INFO: Training MSE: 2.617.
2024-06-21 17:57:05,980 - INFO: Epoch: 61/200, Loss_train: 1.178932331759354, Loss_val: 2.3403911426149566
2024-06-21 17:57:05,980 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:57:05,980 - INFO: Epoch 62/200...
2024-06-21 17:57:05,980 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:57:05,980 - INFO: Batch size: 32.
2024-06-21 17:57:05,983 - INFO: Dataset:
2024-06-21 17:57:05,984 - INFO: Batch size:
2024-06-21 17:57:05,984 - INFO: Number of workers:
2024-06-21 17:57:07,053 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 1.375
2024-06-21 17:57:07,372 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 1.199
2024-06-21 17:57:07,781 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 1.293
2024-06-21 17:57:08,097 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 1.402
2024-06-21 17:57:08,485 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 1.212
2024-06-21 17:57:08,797 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 1.334
2024-06-21 17:57:09,194 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 1.206
2024-06-21 17:57:09,507 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 1.225
2024-06-21 17:57:09,883 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 1.123
2024-06-21 17:57:10,188 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 0.949
2024-06-21 17:57:10,572 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 1.127
2024-06-21 17:57:10,887 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 1.188
2024-06-21 17:57:11,275 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 1.135
2024-06-21 17:57:11,590 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 1.037
2024-06-21 17:57:11,993 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 1.141
2024-06-21 17:57:12,305 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 1.259
2024-06-21 17:57:12,690 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 1.111
2024-06-21 17:57:13,001 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 0.982
2024-06-21 17:57:13,395 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 1.021
2024-06-21 17:57:13,700 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 1.083
2024-06-21 17:57:14,077 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 1.046
2024-06-21 17:57:14,391 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 1.074
2024-06-21 17:57:14,782 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 1.172
2024-06-21 17:57:15,095 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 1.313
2024-06-21 17:57:15,481 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 1.034
2024-06-21 17:57:15,789 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 1.255
2024-06-21 17:57:16,176 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 1.402
2024-06-21 17:57:16,485 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 1.152
2024-06-21 17:57:16,697 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 1.674
2024-06-21 17:57:27,755 - INFO: 62/200 final results:
2024-06-21 17:57:27,756 - INFO: Training loss: 1.190.
2024-06-21 17:57:27,756 - INFO: Training MAE: 1.181.
2024-06-21 17:57:27,756 - INFO: Training MSE: 2.599.
2024-06-21 17:57:48,191 - INFO: Epoch: 62/200, Loss_train: 1.190498514422055, Loss_val: 2.571770133643315
2024-06-21 17:57:48,191 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:57:48,191 - INFO: Epoch 63/200...
2024-06-21 17:57:48,191 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:57:48,191 - INFO: Batch size: 32.
2024-06-21 17:57:48,194 - INFO: Dataset:
2024-06-21 17:57:48,195 - INFO: Batch size:
2024-06-21 17:57:48,195 - INFO: Number of workers:
2024-06-21 17:57:49,262 - INFO: Epoch: 63/200, Batch: 1/29, Batch_Loss_Train: 1.406
2024-06-21 17:57:49,582 - INFO: Epoch: 63/200, Batch: 2/29, Batch_Loss_Train: 1.646
2024-06-21 17:57:49,985 - INFO: Epoch: 63/200, Batch: 3/29, Batch_Loss_Train: 1.391
2024-06-21 17:57:50,303 - INFO: Epoch: 63/200, Batch: 4/29, Batch_Loss_Train: 1.147
2024-06-21 17:57:50,695 - INFO: Epoch: 63/200, Batch: 5/29, Batch_Loss_Train: 0.987
2024-06-21 17:57:51,009 - INFO: Epoch: 63/200, Batch: 6/29, Batch_Loss_Train: 1.023
2024-06-21 17:57:51,409 - INFO: Epoch: 63/200, Batch: 7/29, Batch_Loss_Train: 1.281
2024-06-21 17:57:51,725 - INFO: Epoch: 63/200, Batch: 8/29, Batch_Loss_Train: 1.243
2024-06-21 17:57:52,103 - INFO: Epoch: 63/200, Batch: 9/29, Batch_Loss_Train: 1.094
2024-06-21 17:57:52,412 - INFO: Epoch: 63/200, Batch: 10/29, Batch_Loss_Train: 1.166
2024-06-21 17:57:52,795 - INFO: Epoch: 63/200, Batch: 11/29, Batch_Loss_Train: 0.968
2024-06-21 17:57:53,112 - INFO: Epoch: 63/200, Batch: 12/29, Batch_Loss_Train: 1.074
2024-06-21 17:57:53,522 - INFO: Epoch: 63/200, Batch: 13/29, Batch_Loss_Train: 1.205
2024-06-21 17:57:53,840 - INFO: Epoch: 63/200, Batch: 14/29, Batch_Loss_Train: 1.136
2024-06-21 17:57:54,240 - INFO: Epoch: 63/200, Batch: 15/29, Batch_Loss_Train: 1.177
2024-06-21 17:57:54,554 - INFO: Epoch: 63/200, Batch: 16/29, Batch_Loss_Train: 1.243
2024-06-21 17:57:54,962 - INFO: Epoch: 63/200, Batch: 17/29, Batch_Loss_Train: 1.052
2024-06-21 17:57:55,276 - INFO: Epoch: 63/200, Batch: 18/29, Batch_Loss_Train: 1.254
2024-06-21 17:57:55,669 - INFO: Epoch: 63/200, Batch: 19/29, Batch_Loss_Train: 1.027
2024-06-21 17:57:55,977 - INFO: Epoch: 63/200, Batch: 20/29, Batch_Loss_Train: 1.137
2024-06-21 17:57:56,374 - INFO: Epoch: 63/200, Batch: 21/29, Batch_Loss_Train: 0.968
2024-06-21 17:57:56,690 - INFO: Epoch: 63/200, Batch: 22/29, Batch_Loss_Train: 1.165
2024-06-21 17:57:57,090 - INFO: Epoch: 63/200, Batch: 23/29, Batch_Loss_Train: 1.126
2024-06-21 17:57:57,405 - INFO: Epoch: 63/200, Batch: 24/29, Batch_Loss_Train: 0.856
2024-06-21 17:57:57,800 - INFO: Epoch: 63/200, Batch: 25/29, Batch_Loss_Train: 0.933
2024-06-21 17:57:58,112 - INFO: Epoch: 63/200, Batch: 26/29, Batch_Loss_Train: 1.406
2024-06-21 17:57:58,505 - INFO: Epoch: 63/200, Batch: 27/29, Batch_Loss_Train: 1.221
2024-06-21 17:57:58,817 - INFO: Epoch: 63/200, Batch: 28/29, Batch_Loss_Train: 1.318
2024-06-21 17:57:59,037 - INFO: Epoch: 63/200, Batch: 29/29, Batch_Loss_Train: 1.045
2024-06-21 17:58:10,205 - INFO: 63/200 final results:
2024-06-21 17:58:10,205 - INFO: Training loss: 1.162.
2024-06-21 17:58:10,205 - INFO: Training MAE: 1.164.
2024-06-21 17:58:10,205 - INFO: Training MSE: 2.595.
2024-06-21 17:58:30,684 - INFO: Epoch: 63/200, Loss_train: 1.162010751921555, Loss_val: 2.0562038010564345
2024-06-21 17:58:30,684 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:58:30,684 - INFO: Epoch 64/200...
2024-06-21 17:58:30,684 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:58:30,685 - INFO: Batch size: 32.
2024-06-21 17:58:30,688 - INFO: Dataset:
2024-06-21 17:58:30,689 - INFO: Batch size:
2024-06-21 17:58:30,689 - INFO: Number of workers:
2024-06-21 17:58:31,759 - INFO: Epoch: 64/200, Batch: 1/29, Batch_Loss_Train: 1.228
2024-06-21 17:58:32,078 - INFO: Epoch: 64/200, Batch: 2/29, Batch_Loss_Train: 1.170
2024-06-21 17:58:32,476 - INFO: Epoch: 64/200, Batch: 3/29, Batch_Loss_Train: 0.917
2024-06-21 17:58:32,792 - INFO: Epoch: 64/200, Batch: 4/29, Batch_Loss_Train: 1.063
2024-06-21 17:58:33,185 - INFO: Epoch: 64/200, Batch: 5/29, Batch_Loss_Train: 1.183
2024-06-21 17:58:33,497 - INFO: Epoch: 64/200, Batch: 6/29, Batch_Loss_Train: 1.161
2024-06-21 17:58:33,882 - INFO: Epoch: 64/200, Batch: 7/29, Batch_Loss_Train: 0.996
2024-06-21 17:58:34,194 - INFO: Epoch: 64/200, Batch: 8/29, Batch_Loss_Train: 1.151
2024-06-21 17:58:34,578 - INFO: Epoch: 64/200, Batch: 9/29, Batch_Loss_Train: 0.917
2024-06-21 17:58:34,883 - INFO: Epoch: 64/200, Batch: 10/29, Batch_Loss_Train: 1.497
2024-06-21 17:58:35,263 - INFO: Epoch: 64/200, Batch: 11/29, Batch_Loss_Train: 1.289
2024-06-21 17:58:35,579 - INFO: Epoch: 64/200, Batch: 12/29, Batch_Loss_Train: 0.978
2024-06-21 17:58:35,982 - INFO: Epoch: 64/200, Batch: 13/29, Batch_Loss_Train: 1.192
2024-06-21 17:58:36,297 - INFO: Epoch: 64/200, Batch: 14/29, Batch_Loss_Train: 1.358
2024-06-21 17:58:36,693 - INFO: Epoch: 64/200, Batch: 15/29, Batch_Loss_Train: 1.215
2024-06-21 17:58:37,004 - INFO: Epoch: 64/200, Batch: 16/29, Batch_Loss_Train: 1.180
2024-06-21 17:58:37,407 - INFO: Epoch: 64/200, Batch: 17/29, Batch_Loss_Train: 1.108
2024-06-21 17:58:37,720 - INFO: Epoch: 64/200, Batch: 18/29, Batch_Loss_Train: 1.175
2024-06-21 17:58:38,110 - INFO: Epoch: 64/200, Batch: 19/29, Batch_Loss_Train: 1.284
2024-06-21 17:58:38,416 - INFO: Epoch: 64/200, Batch: 20/29, Batch_Loss_Train: 1.477
2024-06-21 17:58:38,804 - INFO: Epoch: 64/200, Batch: 21/29, Batch_Loss_Train: 1.147
2024-06-21 17:58:39,118 - INFO: Epoch: 64/200, Batch: 22/29, Batch_Loss_Train: 1.241
2024-06-21 17:58:39,512 - INFO: Epoch: 64/200, Batch: 23/29, Batch_Loss_Train: 1.253
2024-06-21 17:58:39,825 - INFO: Epoch: 64/200, Batch: 24/29, Batch_Loss_Train: 1.365
2024-06-21 17:58:40,213 - INFO: Epoch: 64/200, Batch: 25/29, Batch_Loss_Train: 1.236
2024-06-21 17:58:40,521 - INFO: Epoch: 64/200, Batch: 26/29, Batch_Loss_Train: 1.176
2024-06-21 17:58:40,917 - INFO: Epoch: 64/200, Batch: 27/29, Batch_Loss_Train: 1.212
2024-06-21 17:58:41,225 - INFO: Epoch: 64/200, Batch: 28/29, Batch_Loss_Train: 1.159
2024-06-21 17:58:41,443 - INFO: Epoch: 64/200, Batch: 29/29, Batch_Loss_Train: 1.362
2024-06-21 17:58:52,428 - INFO: 64/200 final results:
2024-06-21 17:58:52,428 - INFO: Training loss: 1.196.
2024-06-21 17:58:52,428 - INFO: Training MAE: 1.193.
2024-06-21 17:58:52,428 - INFO: Training MSE: 2.703.
2024-06-21 17:59:12,548 - INFO: Epoch: 64/200, Loss_train: 1.1961852558727921, Loss_val: 2.3428746174121726
2024-06-21 17:59:12,548 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:59:12,548 - INFO: Epoch 65/200...
2024-06-21 17:59:12,548 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:59:12,548 - INFO: Batch size: 32.
2024-06-21 17:59:12,552 - INFO: Dataset:
2024-06-21 17:59:12,552 - INFO: Batch size:
2024-06-21 17:59:12,552 - INFO: Number of workers:
2024-06-21 17:59:13,611 - INFO: Epoch: 65/200, Batch: 1/29, Batch_Loss_Train: 1.700
2024-06-21 17:59:13,929 - INFO: Epoch: 65/200, Batch: 2/29, Batch_Loss_Train: 1.362
2024-06-21 17:59:14,337 - INFO: Epoch: 65/200, Batch: 3/29, Batch_Loss_Train: 0.938
2024-06-21 17:59:14,654 - INFO: Epoch: 65/200, Batch: 4/29, Batch_Loss_Train: 1.188
2024-06-21 17:59:15,041 - INFO: Epoch: 65/200, Batch: 5/29, Batch_Loss_Train: 0.984
2024-06-21 17:59:15,353 - INFO: Epoch: 65/200, Batch: 6/29, Batch_Loss_Train: 0.967
2024-06-21 17:59:15,749 - INFO: Epoch: 65/200, Batch: 7/29, Batch_Loss_Train: 0.916
2024-06-21 17:59:16,062 - INFO: Epoch: 65/200, Batch: 8/29, Batch_Loss_Train: 0.977
2024-06-21 17:59:16,437 - INFO: Epoch: 65/200, Batch: 9/29, Batch_Loss_Train: 1.102
2024-06-21 17:59:16,743 - INFO: Epoch: 65/200, Batch: 10/29, Batch_Loss_Train: 1.514
2024-06-21 17:59:17,127 - INFO: Epoch: 65/200, Batch: 11/29, Batch_Loss_Train: 1.066
2024-06-21 17:59:17,441 - INFO: Epoch: 65/200, Batch: 12/29, Batch_Loss_Train: 1.414
2024-06-21 17:59:17,834 - INFO: Epoch: 65/200, Batch: 13/29, Batch_Loss_Train: 1.327
2024-06-21 17:59:18,147 - INFO: Epoch: 65/200, Batch: 14/29, Batch_Loss_Train: 1.112
2024-06-21 17:59:18,548 - INFO: Epoch: 65/200, Batch: 15/29, Batch_Loss_Train: 1.038
2024-06-21 17:59:18,860 - INFO: Epoch: 65/200, Batch: 16/29, Batch_Loss_Train: 1.175
2024-06-21 17:59:19,247 - INFO: Epoch: 65/200, Batch: 17/29, Batch_Loss_Train: 1.030
2024-06-21 17:59:19,557 - INFO: Epoch: 65/200, Batch: 18/29, Batch_Loss_Train: 1.453
2024-06-21 17:59:19,952 - INFO: Epoch: 65/200, Batch: 19/29, Batch_Loss_Train: 1.110
2024-06-21 17:59:20,257 - INFO: Epoch: 65/200, Batch: 20/29, Batch_Loss_Train: 1.274
2024-06-21 17:59:20,634 - INFO: Epoch: 65/200, Batch: 21/29, Batch_Loss_Train: 1.031
2024-06-21 17:59:20,947 - INFO: Epoch: 65/200, Batch: 22/29, Batch_Loss_Train: 1.236
2024-06-21 17:59:21,331 - INFO: Epoch: 65/200, Batch: 23/29, Batch_Loss_Train: 1.163
2024-06-21 17:59:21,643 - INFO: Epoch: 65/200, Batch: 24/29, Batch_Loss_Train: 1.035
2024-06-21 17:59:22,026 - INFO: Epoch: 65/200, Batch: 25/29, Batch_Loss_Train: 1.346
2024-06-21 17:59:22,333 - INFO: Epoch: 65/200, Batch: 26/29, Batch_Loss_Train: 1.132
2024-06-21 17:59:22,711 - INFO: Epoch: 65/200, Batch: 27/29, Batch_Loss_Train: 1.080
2024-06-21 17:59:23,019 - INFO: Epoch: 65/200, Batch: 28/29, Batch_Loss_Train: 1.237
2024-06-21 17:59:23,225 - INFO: Epoch: 65/200, Batch: 29/29, Batch_Loss_Train: 1.844
2024-06-21 17:59:33,848 - INFO: 65/200 final results:
2024-06-21 17:59:33,848 - INFO: Training loss: 1.198.
2024-06-21 17:59:33,848 - INFO: Training MAE: 1.186.
2024-06-21 17:59:33,848 - INFO: Training MSE: 2.644.
2024-06-21 17:59:54,268 - INFO: Epoch: 65/200, Loss_train: 1.198356803121238, Loss_val: 2.3052204888442467
2024-06-21 17:59:54,268 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 17:59:54,268 - INFO: Epoch 66/200...
2024-06-21 17:59:54,268 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 17:59:54,268 - INFO: Batch size: 32.
2024-06-21 17:59:54,272 - INFO: Dataset:
2024-06-21 17:59:54,272 - INFO: Batch size:
2024-06-21 17:59:54,272 - INFO: Number of workers:
2024-06-21 17:59:55,345 - INFO: Epoch: 66/200, Batch: 1/29, Batch_Loss_Train: 1.088
2024-06-21 17:59:55,663 - INFO: Epoch: 66/200, Batch: 2/29, Batch_Loss_Train: 1.218
2024-06-21 17:59:56,054 - INFO: Epoch: 66/200, Batch: 3/29, Batch_Loss_Train: 1.086
2024-06-21 17:59:56,371 - INFO: Epoch: 66/200, Batch: 4/29, Batch_Loss_Train: 1.094
2024-06-21 17:59:56,763 - INFO: Epoch: 66/200, Batch: 5/29, Batch_Loss_Train: 1.122
2024-06-21 17:59:57,090 - INFO: Epoch: 66/200, Batch: 6/29, Batch_Loss_Train: 1.178
2024-06-21 17:59:57,481 - INFO: Epoch: 66/200, Batch: 7/29, Batch_Loss_Train: 1.105
2024-06-21 17:59:57,796 - INFO: Epoch: 66/200, Batch: 8/29, Batch_Loss_Train: 1.197
2024-06-21 17:59:58,178 - INFO: Epoch: 66/200, Batch: 9/29, Batch_Loss_Train: 1.288
2024-06-21 17:59:58,511 - INFO: Epoch: 66/200, Batch: 10/29, Batch_Loss_Train: 1.184
2024-06-21 17:59:58,873 - INFO: Epoch: 66/200, Batch: 11/29, Batch_Loss_Train: 1.371
2024-06-21 17:59:59,191 - INFO: Epoch: 66/200, Batch: 12/29, Batch_Loss_Train: 0.987
2024-06-21 17:59:59,599 - INFO: Epoch: 66/200, Batch: 13/29, Batch_Loss_Train: 1.142
2024-06-21 17:59:59,928 - INFO: Epoch: 66/200, Batch: 14/29, Batch_Loss_Train: 1.095
2024-06-21 18:00:00,327 - INFO: Epoch: 66/200, Batch: 15/29, Batch_Loss_Train: 1.322
2024-06-21 18:00:00,640 - INFO: Epoch: 66/200, Batch: 16/29, Batch_Loss_Train: 1.308
2024-06-21 18:00:01,043 - INFO: Epoch: 66/200, Batch: 17/29, Batch_Loss_Train: 0.910
2024-06-21 18:00:01,369 - INFO: Epoch: 66/200, Batch: 18/29, Batch_Loss_Train: 1.101
2024-06-21 18:00:01,756 - INFO: Epoch: 66/200, Batch: 19/29, Batch_Loss_Train: 1.196
2024-06-21 18:00:02,063 - INFO: Epoch: 66/200, Batch: 20/29, Batch_Loss_Train: 1.029
2024-06-21 18:00:02,455 - INFO: Epoch: 66/200, Batch: 21/29, Batch_Loss_Train: 0.966
2024-06-21 18:00:02,782 - INFO: Epoch: 66/200, Batch: 22/29, Batch_Loss_Train: 1.043
2024-06-21 18:00:03,156 - INFO: Epoch: 66/200, Batch: 23/29, Batch_Loss_Train: 1.033
2024-06-21 18:00:03,471 - INFO: Epoch: 66/200, Batch: 24/29, Batch_Loss_Train: 1.148
2024-06-21 18:00:03,857 - INFO: Epoch: 66/200, Batch: 25/29, Batch_Loss_Train: 1.224
2024-06-21 18:00:04,179 - INFO: Epoch: 66/200, Batch: 26/29, Batch_Loss_Train: 1.101
2024-06-21 18:00:04,547 - INFO: Epoch: 66/200, Batch: 27/29, Batch_Loss_Train: 1.140
2024-06-21 18:00:04,857 - INFO: Epoch: 66/200, Batch: 28/29, Batch_Loss_Train: 1.253
2024-06-21 18:00:05,068 - INFO: Epoch: 66/200, Batch: 29/29, Batch_Loss_Train: 1.110
2024-06-21 18:00:16,218 - INFO: 66/200 final results:
2024-06-21 18:00:16,218 - INFO: Training loss: 1.139.
2024-06-21 18:00:16,218 - INFO: Training MAE: 1.140.
2024-06-21 18:00:16,218 - INFO: Training MSE: 2.429.
2024-06-21 18:00:36,806 - INFO: Epoch: 66/200, Loss_train: 1.1391984100999504, Loss_val: 2.3081327800093026
2024-06-21 18:00:36,806 - INFO: Best internal validation val_loss: 1.960 at epoch: 59.
2024-06-21 18:00:36,806 - INFO: Epoch 67/200...
2024-06-21 18:00:36,806 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:00:36,806 - INFO: Batch size: 32.
2024-06-21 18:00:36,809 - INFO: Dataset:
2024-06-21 18:00:36,810 - INFO: Batch size:
2024-06-21 18:00:36,810 - INFO: Number of workers:
2024-06-21 18:00:37,887 - INFO: Epoch: 67/200, Batch: 1/29, Batch_Loss_Train: 1.130
2024-06-21 18:00:38,204 - INFO: Epoch: 67/200, Batch: 2/29, Batch_Loss_Train: 1.307
2024-06-21 18:00:38,597 - INFO: Epoch: 67/200, Batch: 3/29, Batch_Loss_Train: 0.952
2024-06-21 18:00:38,913 - INFO: Epoch: 67/200, Batch: 4/29, Batch_Loss_Train: 1.209
2024-06-21 18:00:39,335 - INFO: Epoch: 67/200, Batch: 5/29, Batch_Loss_Train: 1.169
2024-06-21 18:00:39,633 - INFO: Epoch: 67/200, Batch: 6/29, Batch_Loss_Train: 0.999
2024-06-21 18:00:40,017 - INFO: Epoch: 67/200, Batch: 7/29, Batch_Loss_Train: 0.968
2024-06-21 18:00:40,329 - INFO: Epoch: 67/200, Batch: 8/29, Batch_Loss_Train: 0.940
2024-06-21 18:00:40,755 - INFO: Epoch: 67/200, Batch: 9/29, Batch_Loss_Train: 1.204
2024-06-21 18:00:41,046 - INFO: Epoch: 67/200, Batch: 10/29, Batch_Loss_Train: 0.905
2024-06-21 18:00:41,416 - INFO: Epoch: 67/200, Batch: 11/29, Batch_Loss_Train: 0.884
2024-06-21 18:00:41,731 - INFO: Epoch: 67/200, Batch: 12/29, Batch_Loss_Train: 0.961
2024-06-21 18:00:42,160 - INFO: Epoch: 67/200, Batch: 13/29, Batch_Loss_Train: 1.075
2024-06-21 18:00:42,464 - INFO: Epoch: 67/200, Batch: 14/29, Batch_Loss_Train: 1.107
2024-06-21 18:00:42,858 - INFO: Epoch: 67/200, Batch: 15/29, Batch_Loss_Train: 0.926
2024-06-21 18:00:43,170 - INFO: Epoch: 67/200, Batch: 16/29, Batch_Loss_Train: 1.084
2024-06-21 18:00:43,600 - INFO: Epoch: 67/200, Batch: 17/29, Batch_Loss_Train: 1.044
2024-06-21 18:00:43,901 - INFO: Epoch: 67/200, Batch: 18/29, Batch_Loss_Train: 1.111
2024-06-21 18:00:44,277 - INFO: Epoch: 67/200, Batch: 19/29, Batch_Loss_Train: 0.924
2024-06-21 18:00:44,585 - INFO: Epoch: 67/200, Batch: 20/29, Batch_Loss_Train: 0.912
2024-06-21 18:00:44,998 - INFO: Epoch: 67/200, Batch: 21/29, Batch_Loss_Train: 0.713
2024-06-21 18:00:45,301 - INFO: Epoch: 67/200, Batch: 22/29, Batch_Loss_Train: 0.962
2024-06-21 18:00:45,688 - INFO: Epoch: 67/200, Batch: 23/29, Batch_Loss_Train: 0.975
2024-06-21 18:00:46,004 - INFO: Epoch: 67/200, Batch: 24/29, Batch_Loss_Train: 0.884
2024-06-21 18:00:46,422 - INFO: Epoch: 67/200, Batch: 25/29, Batch_Loss_Train: 1.309
2024-06-21 18:00:46,720 - INFO: Epoch: 67/200, Batch: 26/29, Batch_Loss_Train: 1.122
2024-06-21 18:00:47,107 - INFO: Epoch: 67/200, Batch: 27/29, Batch_Loss_Train: 0.924
2024-06-21 18:00:47,418 - INFO: Epoch: 67/200, Batch: 28/29, Batch_Loss_Train: 1.022
2024-06-21 18:00:47,639 - INFO: Epoch: 67/200, Batch: 29/29, Batch_Loss_Train: 1.196
2024-06-21 18:00:58,789 - INFO: 67/200 final results:
2024-06-21 18:00:58,789 - INFO: Training loss: 1.032.
2024-06-21 18:00:58,789 - INFO: Training MAE: 1.028.
2024-06-21 18:00:58,789 - INFO: Training MSE: 2.091.
2024-06-21 18:01:18,772 - INFO: Epoch: 67/200, Loss_train: 1.031591670266513, Loss_val: 1.9057752222850406
2024-06-21 18:01:18,791 - INFO: Saved new best metric model for epoch 67.
2024-06-21 18:01:18,791 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:01:18,791 - INFO: Epoch 68/200...
2024-06-21 18:01:18,791 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:01:18,791 - INFO: Batch size: 32.
2024-06-21 18:01:18,795 - INFO: Dataset:
2024-06-21 18:01:18,795 - INFO: Batch size:
2024-06-21 18:01:18,795 - INFO: Number of workers:
2024-06-21 18:01:19,858 - INFO: Epoch: 68/200, Batch: 1/29, Batch_Loss_Train: 1.083
2024-06-21 18:01:20,175 - INFO: Epoch: 68/200, Batch: 2/29, Batch_Loss_Train: 1.180
2024-06-21 18:01:20,570 - INFO: Epoch: 68/200, Batch: 3/29, Batch_Loss_Train: 0.959
2024-06-21 18:01:20,884 - INFO: Epoch: 68/200, Batch: 4/29, Batch_Loss_Train: 1.053
2024-06-21 18:01:21,278 - INFO: Epoch: 68/200, Batch: 5/29, Batch_Loss_Train: 0.939
2024-06-21 18:01:21,589 - INFO: Epoch: 68/200, Batch: 6/29, Batch_Loss_Train: 1.315
2024-06-21 18:01:21,981 - INFO: Epoch: 68/200, Batch: 7/29, Batch_Loss_Train: 0.785
2024-06-21 18:01:22,293 - INFO: Epoch: 68/200, Batch: 8/29, Batch_Loss_Train: 1.096
2024-06-21 18:01:22,675 - INFO: Epoch: 68/200, Batch: 9/29, Batch_Loss_Train: 0.929
2024-06-21 18:01:22,979 - INFO: Epoch: 68/200, Batch: 10/29, Batch_Loss_Train: 0.992
2024-06-21 18:01:23,361 - INFO: Epoch: 68/200, Batch: 11/29, Batch_Loss_Train: 1.017
2024-06-21 18:01:23,676 - INFO: Epoch: 68/200, Batch: 12/29, Batch_Loss_Train: 0.912
2024-06-21 18:01:24,074 - INFO: Epoch: 68/200, Batch: 13/29, Batch_Loss_Train: 1.173
2024-06-21 18:01:24,390 - INFO: Epoch: 68/200, Batch: 14/29, Batch_Loss_Train: 0.870
2024-06-21 18:01:24,792 - INFO: Epoch: 68/200, Batch: 15/29, Batch_Loss_Train: 1.063
2024-06-21 18:01:25,103 - INFO: Epoch: 68/200, Batch: 16/29, Batch_Loss_Train: 1.066
2024-06-21 18:01:25,490 - INFO: Epoch: 68/200, Batch: 17/29, Batch_Loss_Train: 0.941
2024-06-21 18:01:25,801 - INFO: Epoch: 68/200, Batch: 18/29, Batch_Loss_Train: 0.781
2024-06-21 18:01:26,196 - INFO: Epoch: 68/200, Batch: 19/29, Batch_Loss_Train: 0.969
2024-06-21 18:01:26,501 - INFO: Epoch: 68/200, Batch: 20/29, Batch_Loss_Train: 1.188
2024-06-21 18:01:26,891 - INFO: Epoch: 68/200, Batch: 21/29, Batch_Loss_Train: 0.809
2024-06-21 18:01:27,203 - INFO: Epoch: 68/200, Batch: 22/29, Batch_Loss_Train: 1.046
2024-06-21 18:01:27,600 - INFO: Epoch: 68/200, Batch: 23/29, Batch_Loss_Train: 0.858
2024-06-21 18:01:27,913 - INFO: Epoch: 68/200, Batch: 24/29, Batch_Loss_Train: 1.005
2024-06-21 18:01:28,303 - INFO: Epoch: 68/200, Batch: 25/29, Batch_Loss_Train: 0.927
2024-06-21 18:01:28,612 - INFO: Epoch: 68/200, Batch: 26/29, Batch_Loss_Train: 1.390
2024-06-21 18:01:28,998 - INFO: Epoch: 68/200, Batch: 27/29, Batch_Loss_Train: 0.940
2024-06-21 18:01:29,306 - INFO: Epoch: 68/200, Batch: 28/29, Batch_Loss_Train: 1.012
2024-06-21 18:01:29,523 - INFO: Epoch: 68/200, Batch: 29/29, Batch_Loss_Train: 1.299
2024-06-21 18:01:40,488 - INFO: 68/200 final results:
2024-06-21 18:01:40,488 - INFO: Training loss: 1.021.
2024-06-21 18:01:40,488 - INFO: Training MAE: 1.015.
2024-06-21 18:01:40,488 - INFO: Training MSE: 1.961.
2024-06-21 18:02:00,936 - INFO: Epoch: 68/200, Loss_train: 1.0206950389105698, Loss_val: 2.039431666505748
2024-06-21 18:02:00,936 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:02:00,936 - INFO: Epoch 69/200...
2024-06-21 18:02:00,936 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:02:00,936 - INFO: Batch size: 32.
2024-06-21 18:02:00,940 - INFO: Dataset:
2024-06-21 18:02:00,940 - INFO: Batch size:
2024-06-21 18:02:00,940 - INFO: Number of workers:
2024-06-21 18:02:02,007 - INFO: Epoch: 69/200, Batch: 1/29, Batch_Loss_Train: 1.228
2024-06-21 18:02:02,328 - INFO: Epoch: 69/200, Batch: 2/29, Batch_Loss_Train: 0.915
2024-06-21 18:02:02,725 - INFO: Epoch: 69/200, Batch: 3/29, Batch_Loss_Train: 0.942
2024-06-21 18:02:03,043 - INFO: Epoch: 69/200, Batch: 4/29, Batch_Loss_Train: 1.145
2024-06-21 18:02:03,445 - INFO: Epoch: 69/200, Batch: 5/29, Batch_Loss_Train: 1.019
2024-06-21 18:02:03,761 - INFO: Epoch: 69/200, Batch: 6/29, Batch_Loss_Train: 0.939
2024-06-21 18:02:04,147 - INFO: Epoch: 69/200, Batch: 7/29, Batch_Loss_Train: 0.992
2024-06-21 18:02:04,461 - INFO: Epoch: 69/200, Batch: 8/29, Batch_Loss_Train: 1.200
2024-06-21 18:02:04,851 - INFO: Epoch: 69/200, Batch: 9/29, Batch_Loss_Train: 0.797
2024-06-21 18:02:05,158 - INFO: Epoch: 69/200, Batch: 10/29, Batch_Loss_Train: 0.977
2024-06-21 18:02:05,534 - INFO: Epoch: 69/200, Batch: 11/29, Batch_Loss_Train: 0.946
2024-06-21 18:02:05,850 - INFO: Epoch: 69/200, Batch: 12/29, Batch_Loss_Train: 1.148
2024-06-21 18:02:06,256 - INFO: Epoch: 69/200, Batch: 13/29, Batch_Loss_Train: 0.753
2024-06-21 18:02:06,574 - INFO: Epoch: 69/200, Batch: 14/29, Batch_Loss_Train: 0.894
2024-06-21 18:02:06,978 - INFO: Epoch: 69/200, Batch: 15/29, Batch_Loss_Train: 0.875
2024-06-21 18:02:07,291 - INFO: Epoch: 69/200, Batch: 16/29, Batch_Loss_Train: 0.955
2024-06-21 18:02:07,692 - INFO: Epoch: 69/200, Batch: 17/29, Batch_Loss_Train: 0.911
2024-06-21 18:02:08,006 - INFO: Epoch: 69/200, Batch: 18/29, Batch_Loss_Train: 1.186
2024-06-21 18:02:08,396 - INFO: Epoch: 69/200, Batch: 19/29, Batch_Loss_Train: 0.954
2024-06-21 18:02:08,703 - INFO: Epoch: 69/200, Batch: 20/29, Batch_Loss_Train: 1.009
2024-06-21 18:02:09,095 - INFO: Epoch: 69/200, Batch: 21/29, Batch_Loss_Train: 0.916
2024-06-21 18:02:09,411 - INFO: Epoch: 69/200, Batch: 22/29, Batch_Loss_Train: 0.765
2024-06-21 18:02:09,807 - INFO: Epoch: 69/200, Batch: 23/29, Batch_Loss_Train: 0.977
2024-06-21 18:02:10,123 - INFO: Epoch: 69/200, Batch: 24/29, Batch_Loss_Train: 1.080
2024-06-21 18:02:10,517 - INFO: Epoch: 69/200, Batch: 25/29, Batch_Loss_Train: 0.981
2024-06-21 18:02:10,828 - INFO: Epoch: 69/200, Batch: 26/29, Batch_Loss_Train: 0.916
2024-06-21 18:02:11,223 - INFO: Epoch: 69/200, Batch: 27/29, Batch_Loss_Train: 0.994
2024-06-21 18:02:11,534 - INFO: Epoch: 69/200, Batch: 28/29, Batch_Loss_Train: 0.920
2024-06-21 18:02:11,747 - INFO: Epoch: 69/200, Batch: 29/29, Batch_Loss_Train: 0.856
2024-06-21 18:02:22,932 - INFO: 69/200 final results:
2024-06-21 18:02:22,932 - INFO: Training loss: 0.972.
2024-06-21 18:02:22,933 - INFO: Training MAE: 0.974.
2024-06-21 18:02:22,933 - INFO: Training MSE: 1.873.
2024-06-21 18:02:43,165 - INFO: Epoch: 69/200, Loss_train: 0.9720834431977108, Loss_val: 1.9229647661077565
2024-06-21 18:02:43,165 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:02:43,165 - INFO: Epoch 70/200...
2024-06-21 18:02:43,165 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:02:43,165 - INFO: Batch size: 32.
2024-06-21 18:02:43,169 - INFO: Dataset:
2024-06-21 18:02:43,169 - INFO: Batch size:
2024-06-21 18:02:43,169 - INFO: Number of workers:
2024-06-21 18:02:44,244 - INFO: Epoch: 70/200, Batch: 1/29, Batch_Loss_Train: 0.886
2024-06-21 18:02:44,549 - INFO: Epoch: 70/200, Batch: 2/29, Batch_Loss_Train: 0.805
2024-06-21 18:02:44,930 - INFO: Epoch: 70/200, Batch: 3/29, Batch_Loss_Train: 0.955
2024-06-21 18:02:45,247 - INFO: Epoch: 70/200, Batch: 4/29, Batch_Loss_Train: 0.852
2024-06-21 18:02:45,658 - INFO: Epoch: 70/200, Batch: 5/29, Batch_Loss_Train: 0.930
2024-06-21 18:02:45,957 - INFO: Epoch: 70/200, Batch: 6/29, Batch_Loss_Train: 0.899
2024-06-21 18:02:46,329 - INFO: Epoch: 70/200, Batch: 7/29, Batch_Loss_Train: 1.036
2024-06-21 18:02:46,640 - INFO: Epoch: 70/200, Batch: 8/29, Batch_Loss_Train: 1.031
2024-06-21 18:02:47,051 - INFO: Epoch: 70/200, Batch: 9/29, Batch_Loss_Train: 0.958
2024-06-21 18:02:47,343 - INFO: Epoch: 70/200, Batch: 10/29, Batch_Loss_Train: 0.965
2024-06-21 18:02:47,703 - INFO: Epoch: 70/200, Batch: 11/29, Batch_Loss_Train: 1.119
2024-06-21 18:02:48,017 - INFO: Epoch: 70/200, Batch: 12/29, Batch_Loss_Train: 1.085
2024-06-21 18:02:48,443 - INFO: Epoch: 70/200, Batch: 13/29, Batch_Loss_Train: 0.918
2024-06-21 18:02:48,746 - INFO: Epoch: 70/200, Batch: 14/29, Batch_Loss_Train: 1.014
2024-06-21 18:02:49,132 - INFO: Epoch: 70/200, Batch: 15/29, Batch_Loss_Train: 1.016
2024-06-21 18:02:49,446 - INFO: Epoch: 70/200, Batch: 16/29, Batch_Loss_Train: 0.775
2024-06-21 18:02:49,860 - INFO: Epoch: 70/200, Batch: 17/29, Batch_Loss_Train: 0.892
2024-06-21 18:02:50,160 - INFO: Epoch: 70/200, Batch: 18/29, Batch_Loss_Train: 0.800
2024-06-21 18:02:50,536 - INFO: Epoch: 70/200, Batch: 19/29, Batch_Loss_Train: 0.843
2024-06-21 18:02:50,845 - INFO: Epoch: 70/200, Batch: 20/29, Batch_Loss_Train: 0.917
2024-06-21 18:02:51,249 - INFO: Epoch: 70/200, Batch: 21/29, Batch_Loss_Train: 1.070
2024-06-21 18:02:51,551 - INFO: Epoch: 70/200, Batch: 22/29, Batch_Loss_Train: 1.097
2024-06-21 18:02:51,926 - INFO: Epoch: 70/200, Batch: 23/29, Batch_Loss_Train: 0.915
2024-06-21 18:02:52,240 - INFO: Epoch: 70/200, Batch: 24/29, Batch_Loss_Train: 0.985
2024-06-21 18:02:52,651 - INFO: Epoch: 70/200, Batch: 25/29, Batch_Loss_Train: 0.978
2024-06-21 18:02:52,949 - INFO: Epoch: 70/200, Batch: 26/29, Batch_Loss_Train: 0.895
2024-06-21 18:02:53,317 - INFO: Epoch: 70/200, Batch: 27/29, Batch_Loss_Train: 0.895
2024-06-21 18:02:53,627 - INFO: Epoch: 70/200, Batch: 28/29, Batch_Loss_Train: 1.116
2024-06-21 18:02:53,835 - INFO: Epoch: 70/200, Batch: 29/29, Batch_Loss_Train: 1.126
2024-06-21 18:03:05,030 - INFO: 70/200 final results:
2024-06-21 18:03:05,030 - INFO: Training loss: 0.958.
2024-06-21 18:03:05,030 - INFO: Training MAE: 0.954.
2024-06-21 18:03:05,030 - INFO: Training MSE: 1.826.
2024-06-21 18:03:25,501 - INFO: Epoch: 70/200, Loss_train: 0.9577394292272371, Loss_val: 1.9450677633285522
2024-06-21 18:03:25,501 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:03:25,501 - INFO: Epoch 71/200...
2024-06-21 18:03:25,501 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:03:25,501 - INFO: Batch size: 32.
2024-06-21 18:03:25,505 - INFO: Dataset:
2024-06-21 18:03:25,505 - INFO: Batch size:
2024-06-21 18:03:25,505 - INFO: Number of workers:
2024-06-21 18:03:26,568 - INFO: Epoch: 71/200, Batch: 1/29, Batch_Loss_Train: 1.190
2024-06-21 18:03:26,886 - INFO: Epoch: 71/200, Batch: 2/29, Batch_Loss_Train: 0.984
2024-06-21 18:03:27,294 - INFO: Epoch: 71/200, Batch: 3/29, Batch_Loss_Train: 1.016
2024-06-21 18:03:27,610 - INFO: Epoch: 71/200, Batch: 4/29, Batch_Loss_Train: 1.039
2024-06-21 18:03:28,002 - INFO: Epoch: 71/200, Batch: 5/29, Batch_Loss_Train: 0.921
2024-06-21 18:03:28,313 - INFO: Epoch: 71/200, Batch: 6/29, Batch_Loss_Train: 1.047
2024-06-21 18:03:28,711 - INFO: Epoch: 71/200, Batch: 7/29, Batch_Loss_Train: 1.025
2024-06-21 18:03:29,023 - INFO: Epoch: 71/200, Batch: 8/29, Batch_Loss_Train: 0.827
2024-06-21 18:03:29,405 - INFO: Epoch: 71/200, Batch: 9/29, Batch_Loss_Train: 0.956
2024-06-21 18:03:29,709 - INFO: Epoch: 71/200, Batch: 10/29, Batch_Loss_Train: 0.840
2024-06-21 18:03:30,095 - INFO: Epoch: 71/200, Batch: 11/29, Batch_Loss_Train: 0.951
2024-06-21 18:03:30,408 - INFO: Epoch: 71/200, Batch: 12/29, Batch_Loss_Train: 0.918
2024-06-21 18:03:30,806 - INFO: Epoch: 71/200, Batch: 13/29, Batch_Loss_Train: 1.008
2024-06-21 18:03:31,120 - INFO: Epoch: 71/200, Batch: 14/29, Batch_Loss_Train: 0.823
2024-06-21 18:03:31,522 - INFO: Epoch: 71/200, Batch: 15/29, Batch_Loss_Train: 0.877
2024-06-21 18:03:31,833 - INFO: Epoch: 71/200, Batch: 16/29, Batch_Loss_Train: 0.907
2024-06-21 18:03:32,232 - INFO: Epoch: 71/200, Batch: 17/29, Batch_Loss_Train: 0.842
2024-06-21 18:03:32,543 - INFO: Epoch: 71/200, Batch: 18/29, Batch_Loss_Train: 0.879
2024-06-21 18:03:32,938 - INFO: Epoch: 71/200, Batch: 19/29, Batch_Loss_Train: 0.947
2024-06-21 18:03:33,244 - INFO: Epoch: 71/200, Batch: 20/29, Batch_Loss_Train: 0.898
2024-06-21 18:03:33,630 - INFO: Epoch: 71/200, Batch: 21/29, Batch_Loss_Train: 0.907
2024-06-21 18:03:33,943 - INFO: Epoch: 71/200, Batch: 22/29, Batch_Loss_Train: 0.861
2024-06-21 18:03:34,326 - INFO: Epoch: 71/200, Batch: 23/29, Batch_Loss_Train: 0.874
2024-06-21 18:03:34,638 - INFO: Epoch: 71/200, Batch: 24/29, Batch_Loss_Train: 1.159
2024-06-21 18:03:35,028 - INFO: Epoch: 71/200, Batch: 25/29, Batch_Loss_Train: 1.015
2024-06-21 18:03:35,338 - INFO: Epoch: 71/200, Batch: 26/29, Batch_Loss_Train: 0.892
2024-06-21 18:03:35,726 - INFO: Epoch: 71/200, Batch: 27/29, Batch_Loss_Train: 0.988
2024-06-21 18:03:36,035 - INFO: Epoch: 71/200, Batch: 28/29, Batch_Loss_Train: 0.842
2024-06-21 18:03:36,250 - INFO: Epoch: 71/200, Batch: 29/29, Batch_Loss_Train: 1.284
2024-06-21 18:03:47,259 - INFO: 71/200 final results:
2024-06-21 18:03:47,259 - INFO: Training loss: 0.956.
2024-06-21 18:03:47,259 - INFO: Training MAE: 0.949.
2024-06-21 18:03:47,259 - INFO: Training MSE: 1.785.
2024-06-21 18:04:07,438 - INFO: Epoch: 71/200, Loss_train: 0.9558101049784956, Loss_val: 2.0477164778216133
2024-06-21 18:04:07,438 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:04:07,438 - INFO: Epoch 72/200...
2024-06-21 18:04:07,438 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:04:07,438 - INFO: Batch size: 32.
2024-06-21 18:04:07,442 - INFO: Dataset:
2024-06-21 18:04:07,442 - INFO: Batch size:
2024-06-21 18:04:07,442 - INFO: Number of workers:
2024-06-21 18:04:08,508 - INFO: Epoch: 72/200, Batch: 1/29, Batch_Loss_Train: 1.201
2024-06-21 18:04:08,830 - INFO: Epoch: 72/200, Batch: 2/29, Batch_Loss_Train: 0.943
2024-06-21 18:04:09,251 - INFO: Epoch: 72/200, Batch: 3/29, Batch_Loss_Train: 0.953
2024-06-21 18:04:09,556 - INFO: Epoch: 72/200, Batch: 4/29, Batch_Loss_Train: 1.017
2024-06-21 18:04:09,964 - INFO: Epoch: 72/200, Batch: 5/29, Batch_Loss_Train: 0.738
2024-06-21 18:04:10,265 - INFO: Epoch: 72/200, Batch: 6/29, Batch_Loss_Train: 0.846
2024-06-21 18:04:10,674 - INFO: Epoch: 72/200, Batch: 7/29, Batch_Loss_Train: 1.107
2024-06-21 18:04:10,976 - INFO: Epoch: 72/200, Batch: 8/29, Batch_Loss_Train: 0.759
2024-06-21 18:04:11,389 - INFO: Epoch: 72/200, Batch: 9/29, Batch_Loss_Train: 0.818
2024-06-21 18:04:11,684 - INFO: Epoch: 72/200, Batch: 10/29, Batch_Loss_Train: 0.963
2024-06-21 18:04:12,088 - INFO: Epoch: 72/200, Batch: 11/29, Batch_Loss_Train: 0.872
2024-06-21 18:04:12,391 - INFO: Epoch: 72/200, Batch: 12/29, Batch_Loss_Train: 1.009
2024-06-21 18:04:12,809 - INFO: Epoch: 72/200, Batch: 13/29, Batch_Loss_Train: 0.812
2024-06-21 18:04:13,112 - INFO: Epoch: 72/200, Batch: 14/29, Batch_Loss_Train: 0.956
2024-06-21 18:04:13,536 - INFO: Epoch: 72/200, Batch: 15/29, Batch_Loss_Train: 1.086
2024-06-21 18:04:13,836 - INFO: Epoch: 72/200, Batch: 16/29, Batch_Loss_Train: 0.874
2024-06-21 18:04:14,234 - INFO: Epoch: 72/200, Batch: 17/29, Batch_Loss_Train: 0.906
2024-06-21 18:04:14,535 - INFO: Epoch: 72/200, Batch: 18/29, Batch_Loss_Train: 0.843
2024-06-21 18:04:14,953 - INFO: Epoch: 72/200, Batch: 19/29, Batch_Loss_Train: 0.806
2024-06-21 18:04:15,248 - INFO: Epoch: 72/200, Batch: 20/29, Batch_Loss_Train: 0.793
2024-06-21 18:04:15,644 - INFO: Epoch: 72/200, Batch: 21/29, Batch_Loss_Train: 1.074
2024-06-21 18:04:15,946 - INFO: Epoch: 72/200, Batch: 22/29, Batch_Loss_Train: 1.084
2024-06-21 18:04:16,367 - INFO: Epoch: 72/200, Batch: 23/29, Batch_Loss_Train: 0.904
2024-06-21 18:04:16,670 - INFO: Epoch: 72/200, Batch: 24/29, Batch_Loss_Train: 0.911
2024-06-21 18:04:17,068 - INFO: Epoch: 72/200, Batch: 25/29, Batch_Loss_Train: 0.999
2024-06-21 18:04:17,366 - INFO: Epoch: 72/200, Batch: 26/29, Batch_Loss_Train: 0.991
2024-06-21 18:04:17,778 - INFO: Epoch: 72/200, Batch: 27/29, Batch_Loss_Train: 0.945
2024-06-21 18:04:18,077 - INFO: Epoch: 72/200, Batch: 28/29, Batch_Loss_Train: 0.907
2024-06-21 18:04:18,292 - INFO: Epoch: 72/200, Batch: 29/29, Batch_Loss_Train: 1.037
2024-06-21 18:04:29,062 - INFO: 72/200 final results:
2024-06-21 18:04:29,062 - INFO: Training loss: 0.936.
2024-06-21 18:04:29,062 - INFO: Training MAE: 0.934.
2024-06-21 18:04:29,062 - INFO: Training MSE: 1.715.
2024-06-21 18:04:49,311 - INFO: Epoch: 72/200, Loss_train: 0.9363737229643196, Loss_val: 1.9617830761547745
2024-06-21 18:04:49,311 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:04:49,311 - INFO: Epoch 73/200...
2024-06-21 18:04:49,311 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:04:49,311 - INFO: Batch size: 32.
2024-06-21 18:04:49,315 - INFO: Dataset:
2024-06-21 18:04:49,315 - INFO: Batch size:
2024-06-21 18:04:49,315 - INFO: Number of workers:
2024-06-21 18:04:50,373 - INFO: Epoch: 73/200, Batch: 1/29, Batch_Loss_Train: 0.824
2024-06-21 18:04:50,689 - INFO: Epoch: 73/200, Batch: 2/29, Batch_Loss_Train: 0.883
2024-06-21 18:04:51,099 - INFO: Epoch: 73/200, Batch: 3/29, Batch_Loss_Train: 0.844
2024-06-21 18:04:51,420 - INFO: Epoch: 73/200, Batch: 4/29, Batch_Loss_Train: 0.905
2024-06-21 18:04:51,836 - INFO: Epoch: 73/200, Batch: 5/29, Batch_Loss_Train: 0.861
2024-06-21 18:04:52,139 - INFO: Epoch: 73/200, Batch: 6/29, Batch_Loss_Train: 0.942
2024-06-21 18:04:52,540 - INFO: Epoch: 73/200, Batch: 7/29, Batch_Loss_Train: 1.098
2024-06-21 18:04:52,856 - INFO: Epoch: 73/200, Batch: 8/29, Batch_Loss_Train: 0.778
2024-06-21 18:04:53,266 - INFO: Epoch: 73/200, Batch: 9/29, Batch_Loss_Train: 0.753
2024-06-21 18:04:53,560 - INFO: Epoch: 73/200, Batch: 10/29, Batch_Loss_Train: 0.855
2024-06-21 18:04:53,954 - INFO: Epoch: 73/200, Batch: 11/29, Batch_Loss_Train: 1.079
2024-06-21 18:04:54,272 - INFO: Epoch: 73/200, Batch: 12/29, Batch_Loss_Train: 1.197
2024-06-21 18:04:54,695 - INFO: Epoch: 73/200, Batch: 13/29, Batch_Loss_Train: 0.857
2024-06-21 18:04:54,999 - INFO: Epoch: 73/200, Batch: 14/29, Batch_Loss_Train: 1.001
2024-06-21 18:04:55,398 - INFO: Epoch: 73/200, Batch: 15/29, Batch_Loss_Train: 0.819
2024-06-21 18:04:55,709 - INFO: Epoch: 73/200, Batch: 16/29, Batch_Loss_Train: 0.757
2024-06-21 18:04:56,127 - INFO: Epoch: 73/200, Batch: 17/29, Batch_Loss_Train: 1.107
2024-06-21 18:04:56,426 - INFO: Epoch: 73/200, Batch: 18/29, Batch_Loss_Train: 1.052
2024-06-21 18:04:56,819 - INFO: Epoch: 73/200, Batch: 19/29, Batch_Loss_Train: 1.083
2024-06-21 18:04:57,128 - INFO: Epoch: 73/200, Batch: 20/29, Batch_Loss_Train: 0.930
2024-06-21 18:04:57,526 - INFO: Epoch: 73/200, Batch: 21/29, Batch_Loss_Train: 1.106
2024-06-21 18:04:57,831 - INFO: Epoch: 73/200, Batch: 22/29, Batch_Loss_Train: 0.926
2024-06-21 18:04:58,224 - INFO: Epoch: 73/200, Batch: 23/29, Batch_Loss_Train: 1.102
2024-06-21 18:04:58,540 - INFO: Epoch: 73/200, Batch: 24/29, Batch_Loss_Train: 0.886
2024-06-21 18:04:58,941 - INFO: Epoch: 73/200, Batch: 25/29, Batch_Loss_Train: 1.092
2024-06-21 18:04:59,240 - INFO: Epoch: 73/200, Batch: 26/29, Batch_Loss_Train: 0.879
2024-06-21 18:04:59,639 - INFO: Epoch: 73/200, Batch: 27/29, Batch_Loss_Train: 1.101
2024-06-21 18:04:59,950 - INFO: Epoch: 73/200, Batch: 28/29, Batch_Loss_Train: 0.889
2024-06-21 18:05:00,159 - INFO: Epoch: 73/200, Batch: 29/29, Batch_Loss_Train: 0.938
2024-06-21 18:05:11,286 - INFO: 73/200 final results:
2024-06-21 18:05:11,286 - INFO: Training loss: 0.950.
2024-06-21 18:05:11,286 - INFO: Training MAE: 0.950.
2024-06-21 18:05:11,286 - INFO: Training MSE: 1.809.
2024-06-21 18:05:31,808 - INFO: Epoch: 73/200, Loss_train: 0.9497855840058163, Loss_val: 2.017741051213495
2024-06-21 18:05:31,808 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:05:31,808 - INFO: Epoch 74/200...
2024-06-21 18:05:31,808 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 18:05:31,808 - INFO: Batch size: 32.
2024-06-21 18:05:31,811 - INFO: Dataset:
2024-06-21 18:05:31,812 - INFO: Batch size:
2024-06-21 18:05:31,812 - INFO: Number of workers:
2024-06-21 18:05:32,867 - INFO: Epoch: 74/200, Batch: 1/29, Batch_Loss_Train: 0.911
2024-06-21 18:05:33,185 - INFO: Epoch: 74/200, Batch: 2/29, Batch_Loss_Train: 1.043
2024-06-21 18:05:33,598 - INFO: Epoch: 74/200, Batch: 3/29, Batch_Loss_Train: 0.773
2024-06-21 18:05:33,902 - INFO: Epoch: 74/200, Batch: 4/29, Batch_Loss_Train: 1.036
2024-06-21 18:05:34,301 - INFO: Epoch: 74/200, Batch: 5/29, Batch_Loss_Train: 0.753
2024-06-21 18:05:34,613 - INFO: Epoch: 74/200, Batch: 6/29, Batch_Loss_Train: 0.837
2024-06-21 18:05:35,021 - INFO: Epoch: 74/200, Batch: 7/29, Batch_Loss_Train: 0.758
2024-06-21 18:05:35,321 - INFO: Epoch: 74/200, Batch: 8/29, Batch_Loss_Train: 0.874
2024-06-21 18:05:35,711 - INFO: Epoch: 74/200, Batch: 9/29, Batch_Loss_Train: 1.017
2024-06-21 18:05:36,017 - INFO: Epoch: 74/200, Batch: 10/29, Batch_Loss_Train: 0.798
2024-06-21 18:05:36,418 - INFO: Epoch: 74/200, Batch: 11/29, Batch_Loss_Train: 1.118
2024-06-21 18:05:36,720 - INFO: Epoch: 74/200, Batch: 12/29, Batch_Loss_Train: 0.918
2024-06-21 18:05:37,126 - INFO: Epoch: 74/200, Batch: 13/29, Batch_Loss_Train: 0.980
2024-06-21 18:05:37,440 - INFO: Epoch: 74/200, Batch: 14/29, Batch_Loss_Train: 0.896
2024-06-21 18:05:37,864 - INFO: Epoch: 74/200, Batch: 15/29, Batch_Loss_Train: 0.875
2024-06-21 18:05:38,161 - INFO: Epoch: 74/200, Batch: 16/29, Batch_Loss_Train: 1.031
2024-06-21 18:05:38,552 - INFO: Epoch: 74/200, Batch: 17/29, Batch_Loss_Train: 0.908
2024-06-21 18:05:38,862 - INFO: Epoch: 74/200, Batch: 18/29, Batch_Loss_Train: 0.796
2024-06-21 18:05:39,271 - INFO: Epoch: 74/200, Batch: 19/29, Batch_Loss_Train: 0.867
2024-06-21 18:05:39,564 - INFO: Epoch: 74/200, Batch: 20/29, Batch_Loss_Train: 0.864
2024-06-21 18:05:39,946 - INFO: Epoch: 74/200, Batch: 21/29, Batch_Loss_Train: 0.957
2024-06-21 18:05:40,258 - INFO: Epoch: 74/200, Batch: 22/29, Batch_Loss_Train: 0.867
2024-06-21 18:05:40,677 - INFO: Epoch: 74/200, Batch: 23/29, Batch_Loss_Train: 0.850
2024-06-21 18:05:40,977 - INFO: Epoch: 74/200, Batch: 24/29, Batch_Loss_Train: 1.318
2024-06-21 18:05:41,349 - INFO: Epoch: 74/200, Batch: 25/29, Batch_Loss_Train: 0.868
2024-06-21 18:05:41,658 - INFO: Epoch: 74/200, Batch: 26/29, Batch_Loss_Train: 1.068
2024-06-21 18:05:42,055 - INFO: Epoch: 74/200, Batch: 27/29, Batch_Loss_Train: 1.171
2024-06-21 18:05:42,350 - INFO: Epoch: 74/200, Batch: 28/29, Batch_Loss_Train: 1.107
2024-06-21 18:05:42,560 - INFO: Epoch: 74/200, Batch: 29/29, Batch_Loss_Train: 1.051
2024-06-21 18:05:53,633 - INFO: 74/200 final results:
2024-06-21 18:05:53,634 - INFO: Training loss: 0.942.
2024-06-21 18:05:53,634 - INFO: Training MAE: 0.940.
2024-06-21 18:05:53,634 - INFO: Training MSE: 1.704.
2024-06-21 18:06:13,920 - INFO: Epoch: 74/200, Loss_train: 0.9417317427437881, Loss_val: 1.917458073846225
2024-06-21 18:06:13,920 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:06:13,920 - INFO: Epoch 75/200...
2024-06-21 18:06:13,920 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:06:13,920 - INFO: Batch size: 32.
2024-06-21 18:06:13,923 - INFO: Dataset:
2024-06-21 18:06:13,924 - INFO: Batch size:
2024-06-21 18:06:13,924 - INFO: Number of workers:
2024-06-21 18:06:15,015 - INFO: Epoch: 75/200, Batch: 1/29, Batch_Loss_Train: 0.998
2024-06-21 18:06:15,320 - INFO: Epoch: 75/200, Batch: 2/29, Batch_Loss_Train: 1.004
2024-06-21 18:06:15,717 - INFO: Epoch: 75/200, Batch: 3/29, Batch_Loss_Train: 0.921
2024-06-21 18:06:16,037 - INFO: Epoch: 75/200, Batch: 4/29, Batch_Loss_Train: 0.852
2024-06-21 18:06:16,462 - INFO: Epoch: 75/200, Batch: 5/29, Batch_Loss_Train: 0.851
2024-06-21 18:06:16,762 - INFO: Epoch: 75/200, Batch: 6/29, Batch_Loss_Train: 0.794
2024-06-21 18:06:17,148 - INFO: Epoch: 75/200, Batch: 7/29, Batch_Loss_Train: 0.749
2024-06-21 18:06:17,462 - INFO: Epoch: 75/200, Batch: 8/29, Batch_Loss_Train: 0.968
2024-06-21 18:06:17,877 - INFO: Epoch: 75/200, Batch: 9/29, Batch_Loss_Train: 1.082
2024-06-21 18:06:18,169 - INFO: Epoch: 75/200, Batch: 10/29, Batch_Loss_Train: 1.023
2024-06-21 18:06:18,542 - INFO: Epoch: 75/200, Batch: 11/29, Batch_Loss_Train: 1.031
2024-06-21 18:06:18,860 - INFO: Epoch: 75/200, Batch: 12/29, Batch_Loss_Train: 0.952
2024-06-21 18:06:19,292 - INFO: Epoch: 75/200, Batch: 13/29, Batch_Loss_Train: 0.838
2024-06-21 18:06:19,597 - INFO: Epoch: 75/200, Batch: 14/29, Batch_Loss_Train: 0.877
2024-06-21 18:06:19,987 - INFO: Epoch: 75/200, Batch: 15/29, Batch_Loss_Train: 0.904
2024-06-21 18:06:20,300 - INFO: Epoch: 75/200, Batch: 16/29, Batch_Loss_Train: 0.896
2024-06-21 18:06:20,732 - INFO: Epoch: 75/200, Batch: 17/29, Batch_Loss_Train: 0.814
2024-06-21 18:06:21,033 - INFO: Epoch: 75/200, Batch: 18/29, Batch_Loss_Train: 0.980
2024-06-21 18:06:21,411 - INFO: Epoch: 75/200, Batch: 19/29, Batch_Loss_Train: 0.861
2024-06-21 18:06:21,720 - INFO: Epoch: 75/200, Batch: 20/29, Batch_Loss_Train: 0.785
2024-06-21 18:06:22,139 - INFO: Epoch: 75/200, Batch: 21/29, Batch_Loss_Train: 0.844
2024-06-21 18:06:22,442 - INFO: Epoch: 75/200, Batch: 22/29, Batch_Loss_Train: 0.785
2024-06-21 18:06:22,816 - INFO: Epoch: 75/200, Batch: 23/29, Batch_Loss_Train: 1.012
2024-06-21 18:06:23,131 - INFO: Epoch: 75/200, Batch: 24/29, Batch_Loss_Train: 0.982
2024-06-21 18:06:23,547 - INFO: Epoch: 75/200, Batch: 25/29, Batch_Loss_Train: 0.866
2024-06-21 18:06:23,845 - INFO: Epoch: 75/200, Batch: 26/29, Batch_Loss_Train: 0.836
2024-06-21 18:06:24,228 - INFO: Epoch: 75/200, Batch: 27/29, Batch_Loss_Train: 0.940
2024-06-21 18:06:24,539 - INFO: Epoch: 75/200, Batch: 28/29, Batch_Loss_Train: 0.922
2024-06-21 18:06:24,755 - INFO: Epoch: 75/200, Batch: 29/29, Batch_Loss_Train: 0.711
2024-06-21 18:06:35,721 - INFO: 75/200 final results:
2024-06-21 18:06:35,721 - INFO: Training loss: 0.899.
2024-06-21 18:06:35,721 - INFO: Training MAE: 0.903.
2024-06-21 18:06:35,721 - INFO: Training MSE: 1.615.
2024-06-21 18:06:55,878 - INFO: Epoch: 75/200, Loss_train: 0.8992860440550179, Loss_val: 1.911831859884591
2024-06-21 18:06:55,878 - INFO: Best internal validation val_loss: 1.906 at epoch: 67.
2024-06-21 18:06:55,878 - INFO: Epoch 76/200...
2024-06-21 18:06:55,878 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:06:55,878 - INFO: Batch size: 32.
2024-06-21 18:06:55,882 - INFO: Dataset:
2024-06-21 18:06:55,882 - INFO: Batch size:
2024-06-21 18:06:55,882 - INFO: Number of workers:
2024-06-21 18:06:56,983 - INFO: Epoch: 76/200, Batch: 1/29, Batch_Loss_Train: 0.767
2024-06-21 18:06:57,290 - INFO: Epoch: 76/200, Batch: 2/29, Batch_Loss_Train: 0.926
2024-06-21 18:06:57,685 - INFO: Epoch: 76/200, Batch: 3/29, Batch_Loss_Train: 0.900
2024-06-21 18:06:58,004 - INFO: Epoch: 76/200, Batch: 4/29, Batch_Loss_Train: 0.683
2024-06-21 18:06:58,434 - INFO: Epoch: 76/200, Batch: 5/29, Batch_Loss_Train: 0.927
2024-06-21 18:06:58,734 - INFO: Epoch: 76/200, Batch: 6/29, Batch_Loss_Train: 0.882
2024-06-21 18:06:59,117 - INFO: Epoch: 76/200, Batch: 7/29, Batch_Loss_Train: 0.777
2024-06-21 18:06:59,418 - INFO: Epoch: 76/200, Batch: 8/29, Batch_Loss_Train: 0.723
2024-06-21 18:06:59,865 - INFO: Epoch: 76/200, Batch: 9/29, Batch_Loss_Train: 0.780
2024-06-21 18:07:00,159 - INFO: Epoch: 76/200, Batch: 10/29, Batch_Loss_Train: 0.908
2024-06-21 18:07:00,525 - INFO: Epoch: 76/200, Batch: 11/29, Batch_Loss_Train: 0.850
2024-06-21 18:07:00,828 - INFO: Epoch: 76/200, Batch: 12/29, Batch_Loss_Train: 0.688
2024-06-21 18:07:01,278 - INFO: Epoch: 76/200, Batch: 13/29, Batch_Loss_Train: 0.727
2024-06-21 18:07:01,582 - INFO: Epoch: 76/200, Batch: 14/29, Batch_Loss_Train: 1.096
2024-06-21 18:07:01,974 - INFO: Epoch: 76/200, Batch: 15/29, Batch_Loss_Train: 0.803
2024-06-21 18:07:02,275 - INFO: Epoch: 76/200, Batch: 16/29, Batch_Loss_Train: 0.693
2024-06-21 18:07:02,707 - INFO: Epoch: 76/200, Batch: 17/29, Batch_Loss_Train: 0.937
2024-06-21 18:07:03,008 - INFO: Epoch: 76/200, Batch: 18/29, Batch_Loss_Train: 0.759
2024-06-21 18:07:03,389 - INFO: Epoch: 76/200, Batch: 19/29, Batch_Loss_Train: 0.946
2024-06-21 18:07:03,685 - INFO: Epoch: 76/200, Batch: 20/29, Batch_Loss_Train: 0.836
2024-06-21 18:07:04,102 - INFO: Epoch: 76/200, Batch: 21/29, Batch_Loss_Train: 0.797
2024-06-21 18:07:04,404 - INFO: Epoch: 76/200, Batch: 22/29, Batch_Loss_Train: 0.795
2024-06-21 18:07:04,786 - INFO: Epoch: 76/200, Batch: 23/29, Batch_Loss_Train: 0.879
2024-06-21 18:07:05,088 - INFO: Epoch: 76/200, Batch: 24/29, Batch_Loss_Train: 0.876
2024-06-21 18:07:05,511 - INFO: Epoch: 76/200, Batch: 25/29, Batch_Loss_Train: 0.750
2024-06-21 18:07:05,809 - INFO: Epoch: 76/200, Batch: 26/29, Batch_Loss_Train: 0.930
2024-06-21 18:07:06,176 - INFO: Epoch: 76/200, Batch: 27/29, Batch_Loss_Train: 0.789
2024-06-21 18:07:06,474 - INFO: Epoch: 76/200, Batch: 28/29, Batch_Loss_Train: 1.057
2024-06-21 18:07:06,688 - INFO: Epoch: 76/200, Batch: 29/29, Batch_Loss_Train: 0.879
2024-06-21 18:07:17,891 - INFO: 76/200 final results:
2024-06-21 18:07:17,892 - INFO: Training loss: 0.840.
2024-06-21 18:07:17,892 - INFO: Training MAE: 0.839.
2024-06-21 18:07:17,892 - INFO: Training MSE: 1.421.
2024-06-21 18:07:38,231 - INFO: Epoch: 76/200, Loss_train: 0.8400052436466875, Loss_val: 1.8852612602299657
2024-06-21 18:07:38,250 - INFO: Saved new best metric model for epoch 76.
2024-06-21 18:07:38,250 - INFO: Best internal validation val_loss: 1.885 at epoch: 76.
2024-06-21 18:07:38,251 - INFO: Epoch 77/200...
2024-06-21 18:07:38,251 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:07:38,251 - INFO: Batch size: 32.
2024-06-21 18:07:38,254 - INFO: Dataset:
2024-06-21 18:07:38,254 - INFO: Batch size:
2024-06-21 18:07:38,255 - INFO: Number of workers:
2024-06-21 18:07:39,352 - INFO: Epoch: 77/200, Batch: 1/29, Batch_Loss_Train: 0.756
2024-06-21 18:07:39,655 - INFO: Epoch: 77/200, Batch: 2/29, Batch_Loss_Train: 0.783
2024-06-21 18:07:40,061 - INFO: Epoch: 77/200, Batch: 3/29, Batch_Loss_Train: 0.905
2024-06-21 18:07:40,375 - INFO: Epoch: 77/200, Batch: 4/29, Batch_Loss_Train: 0.793
2024-06-21 18:07:40,779 - INFO: Epoch: 77/200, Batch: 5/29, Batch_Loss_Train: 0.804
2024-06-21 18:07:41,076 - INFO: Epoch: 77/200, Batch: 6/29, Batch_Loss_Train: 0.695
2024-06-21 18:07:41,470 - INFO: Epoch: 77/200, Batch: 7/29, Batch_Loss_Train: 0.721
2024-06-21 18:07:41,781 - INFO: Epoch: 77/200, Batch: 8/29, Batch_Loss_Train: 0.900
2024-06-21 18:07:42,190 - INFO: Epoch: 77/200, Batch: 9/29, Batch_Loss_Train: 0.886
2024-06-21 18:07:42,484 - INFO: Epoch: 77/200, Batch: 10/29, Batch_Loss_Train: 0.890
2024-06-21 18:07:42,871 - INFO: Epoch: 77/200, Batch: 11/29, Batch_Loss_Train: 0.674
2024-06-21 18:07:43,188 - INFO: Epoch: 77/200, Batch: 12/29, Batch_Loss_Train: 1.089
2024-06-21 18:07:43,609 - INFO: Epoch: 77/200, Batch: 13/29, Batch_Loss_Train: 1.019
2024-06-21 18:07:43,914 - INFO: Epoch: 77/200, Batch: 14/29, Batch_Loss_Train: 0.696
2024-06-21 18:07:44,319 - INFO: Epoch: 77/200, Batch: 15/29, Batch_Loss_Train: 0.712
2024-06-21 18:07:44,632 - INFO: Epoch: 77/200, Batch: 16/29, Batch_Loss_Train: 0.751
2024-06-21 18:07:45,052 - INFO: Epoch: 77/200, Batch: 17/29, Batch_Loss_Train: 0.793
2024-06-21 18:07:45,354 - INFO: Epoch: 77/200, Batch: 18/29, Batch_Loss_Train: 0.831
2024-06-21 18:07:45,747 - INFO: Epoch: 77/200, Batch: 19/29, Batch_Loss_Train: 0.826
2024-06-21 18:07:46,056 - INFO: Epoch: 77/200, Batch: 20/29, Batch_Loss_Train: 0.851
2024-06-21 18:07:46,468 - INFO: Epoch: 77/200, Batch: 21/29, Batch_Loss_Train: 0.801
2024-06-21 18:07:46,771 - INFO: Epoch: 77/200, Batch: 22/29, Batch_Loss_Train: 0.815
2024-06-21 18:07:47,164 - INFO: Epoch: 77/200, Batch: 23/29, Batch_Loss_Train: 0.882
2024-06-21 18:07:47,480 - INFO: Epoch: 77/200, Batch: 24/29, Batch_Loss_Train: 1.073
2024-06-21 18:07:47,883 - INFO: Epoch: 77/200, Batch: 25/29, Batch_Loss_Train: 1.025
2024-06-21 18:07:48,178 - INFO: Epoch: 77/200, Batch: 26/29, Batch_Loss_Train: 0.802
2024-06-21 18:07:48,565 - INFO: Epoch: 77/200, Batch: 27/29, Batch_Loss_Train: 0.767
2024-06-21 18:07:48,873 - INFO: Epoch: 77/200, Batch: 28/29, Batch_Loss_Train: 0.885
2024-06-21 18:07:49,085 - INFO: Epoch: 77/200, Batch: 29/29, Batch_Loss_Train: 0.764
2024-06-21 18:08:00,254 - INFO: 77/200 final results:
2024-06-21 18:08:00,255 - INFO: Training loss: 0.834.
2024-06-21 18:08:00,255 - INFO: Training MAE: 0.835.
2024-06-21 18:08:00,255 - INFO: Training MSE: 1.412.
2024-06-21 18:08:20,682 - INFO: Epoch: 77/200, Loss_train: 0.8340380294569607, Loss_val: 1.8750350434204628
2024-06-21 18:08:20,705 - INFO: Saved new best metric model for epoch 77.
2024-06-21 18:08:20,705 - INFO: Best internal validation val_loss: 1.875 at epoch: 77.
2024-06-21 18:08:20,705 - INFO: Epoch 78/200...
2024-06-21 18:08:20,706 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:08:20,706 - INFO: Batch size: 32.
2024-06-21 18:08:20,709 - INFO: Dataset:
2024-06-21 18:08:20,709 - INFO: Batch size:
2024-06-21 18:08:20,709 - INFO: Number of workers:
2024-06-21 18:08:21,769 - INFO: Epoch: 78/200, Batch: 1/29, Batch_Loss_Train: 0.886
2024-06-21 18:08:22,100 - INFO: Epoch: 78/200, Batch: 2/29, Batch_Loss_Train: 0.848
2024-06-21 18:08:22,499 - INFO: Epoch: 78/200, Batch: 3/29, Batch_Loss_Train: 0.802
2024-06-21 18:08:22,818 - INFO: Epoch: 78/200, Batch: 4/29, Batch_Loss_Train: 0.676
2024-06-21 18:08:23,221 - INFO: Epoch: 78/200, Batch: 5/29, Batch_Loss_Train: 0.975
2024-06-21 18:08:23,547 - INFO: Epoch: 78/200, Batch: 6/29, Batch_Loss_Train: 0.910
2024-06-21 18:08:23,935 - INFO: Epoch: 78/200, Batch: 7/29, Batch_Loss_Train: 0.851
2024-06-21 18:08:24,250 - INFO: Epoch: 78/200, Batch: 8/29, Batch_Loss_Train: 1.024
2024-06-21 18:08:24,642 - INFO: Epoch: 78/200, Batch: 9/29, Batch_Loss_Train: 0.795
2024-06-21 18:08:24,972 - INFO: Epoch: 78/200, Batch: 10/29, Batch_Loss_Train: 0.935
2024-06-21 18:08:25,347 - INFO: Epoch: 78/200, Batch: 11/29, Batch_Loss_Train: 0.920
2024-06-21 18:08:25,664 - INFO: Epoch: 78/200, Batch: 12/29, Batch_Loss_Train: 0.749
2024-06-21 18:08:26,071 - INFO: Epoch: 78/200, Batch: 13/29, Batch_Loss_Train: 0.682
2024-06-21 18:08:26,402 - INFO: Epoch: 78/200, Batch: 14/29, Batch_Loss_Train: 0.888
2024-06-21 18:08:26,796 - INFO: Epoch: 78/200, Batch: 15/29, Batch_Loss_Train: 0.967
2024-06-21 18:08:27,110 - INFO: Epoch: 78/200, Batch: 16/29, Batch_Loss_Train: 0.800
2024-06-21 18:08:27,512 - INFO: Epoch: 78/200, Batch: 17/29, Batch_Loss_Train: 0.759
2024-06-21 18:08:27,839 - INFO: Epoch: 78/200, Batch: 18/29, Batch_Loss_Train: 0.822
2024-06-21 18:08:28,224 - INFO: Epoch: 78/200, Batch: 19/29, Batch_Loss_Train: 0.904
2024-06-21 18:08:28,532 - INFO: Epoch: 78/200, Batch: 20/29, Batch_Loss_Train: 0.871
2024-06-21 18:08:28,925 - INFO: Epoch: 78/200, Batch: 21/29, Batch_Loss_Train: 0.894
2024-06-21 18:08:29,253 - INFO: Epoch: 78/200, Batch: 22/29, Batch_Loss_Train: 0.991
2024-06-21 18:08:29,627 - INFO: Epoch: 78/200, Batch: 23/29, Batch_Loss_Train: 0.783
2024-06-21 18:08:29,942 - INFO: Epoch: 78/200, Batch: 24/29, Batch_Loss_Train: 0.660
2024-06-21 18:08:30,328 - INFO: Epoch: 78/200, Batch: 25/29, Batch_Loss_Train: 0.715
2024-06-21 18:08:30,650 - INFO: Epoch: 78/200, Batch: 26/29, Batch_Loss_Train: 0.823
2024-06-21 18:08:31,018 - INFO: Epoch: 78/200, Batch: 27/29, Batch_Loss_Train: 0.974
2024-06-21 18:08:31,329 - INFO: Epoch: 78/200, Batch: 28/29, Batch_Loss_Train: 0.892
2024-06-21 18:08:31,551 - INFO: Epoch: 78/200, Batch: 29/29, Batch_Loss_Train: 0.889
2024-06-21 18:08:42,562 - INFO: 78/200 final results:
2024-06-21 18:08:42,562 - INFO: Training loss: 0.851.
2024-06-21 18:08:42,562 - INFO: Training MAE: 0.850.
2024-06-21 18:08:42,562 - INFO: Training MSE: 1.462.
2024-06-21 18:09:02,430 - INFO: Epoch: 78/200, Loss_train: 0.8512286256099569, Loss_val: 1.8516622535113632
2024-06-21 18:09:02,448 - INFO: Saved new best metric model for epoch 78.
2024-06-21 18:09:02,448 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:09:02,448 - INFO: Epoch 79/200...
2024-06-21 18:09:02,448 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:09:02,449 - INFO: Batch size: 32.
2024-06-21 18:09:02,453 - INFO: Dataset:
2024-06-21 18:09:02,453 - INFO: Batch size:
2024-06-21 18:09:02,453 - INFO: Number of workers:
2024-06-21 18:09:03,501 - INFO: Epoch: 79/200, Batch: 1/29, Batch_Loss_Train: 0.870
2024-06-21 18:09:03,818 - INFO: Epoch: 79/200, Batch: 2/29, Batch_Loss_Train: 0.662
2024-06-21 18:09:04,225 - INFO: Epoch: 79/200, Batch: 3/29, Batch_Loss_Train: 0.922
2024-06-21 18:09:04,542 - INFO: Epoch: 79/200, Batch: 4/29, Batch_Loss_Train: 0.786
2024-06-21 18:09:04,939 - INFO: Epoch: 79/200, Batch: 5/29, Batch_Loss_Train: 1.035
2024-06-21 18:09:05,249 - INFO: Epoch: 79/200, Batch: 6/29, Batch_Loss_Train: 1.102
2024-06-21 18:09:05,637 - INFO: Epoch: 79/200, Batch: 7/29, Batch_Loss_Train: 0.929
2024-06-21 18:09:05,949 - INFO: Epoch: 79/200, Batch: 8/29, Batch_Loss_Train: 0.804
2024-06-21 18:09:06,334 - INFO: Epoch: 79/200, Batch: 9/29, Batch_Loss_Train: 0.875
2024-06-21 18:09:06,640 - INFO: Epoch: 79/200, Batch: 10/29, Batch_Loss_Train: 0.895
2024-06-21 18:09:07,011 - INFO: Epoch: 79/200, Batch: 11/29, Batch_Loss_Train: 0.748
2024-06-21 18:09:07,329 - INFO: Epoch: 79/200, Batch: 12/29, Batch_Loss_Train: 0.942
2024-06-21 18:09:07,737 - INFO: Epoch: 79/200, Batch: 13/29, Batch_Loss_Train: 0.943
2024-06-21 18:09:08,354 - INFO: Epoch: 79/200, Batch: 14/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:08,762 - INFO: Epoch: 79/200, Batch: 15/29, Batch_Loss_Train: 0.763
2024-06-21 18:09:09,076 - INFO: Epoch: 79/200, Batch: 16/29, Batch_Loss_Train: 0.768
2024-06-21 18:09:09,481 - INFO: Epoch: 79/200, Batch: 17/29, Batch_Loss_Train: 0.958
2024-06-21 18:09:09,794 - INFO: Epoch: 79/200, Batch: 18/29, Batch_Loss_Train: 0.945
2024-06-21 18:09:10,181 - INFO: Epoch: 79/200, Batch: 19/29, Batch_Loss_Train: 0.777
2024-06-21 18:09:10,490 - INFO: Epoch: 79/200, Batch: 20/29, Batch_Loss_Train: 0.724
2024-06-21 18:09:10,885 - INFO: Epoch: 79/200, Batch: 21/29, Batch_Loss_Train: 0.763
2024-06-21 18:09:11,197 - INFO: Epoch: 79/200, Batch: 22/29, Batch_Loss_Train: 0.811
2024-06-21 18:09:11,593 - INFO: Epoch: 79/200, Batch: 23/29, Batch_Loss_Train: 0.796
2024-06-21 18:09:11,909 - INFO: Epoch: 79/200, Batch: 24/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:12,306 - INFO: Epoch: 79/200, Batch: 25/29, Batch_Loss_Train: 0.835
2024-06-21 18:09:12,617 - INFO: Epoch: 79/200, Batch: 26/29, Batch_Loss_Train: 0.806
2024-06-21 18:09:13,013 - INFO: Epoch: 79/200, Batch: 27/29, Batch_Loss_Train: 0.716
2024-06-21 18:09:13,325 - INFO: Epoch: 79/200, Batch: 28/29, Batch_Loss_Train: 0.834
2024-06-21 18:09:13,546 - INFO: Epoch: 79/200, Batch: 29/29, Batch_Loss_Train: 1.202
2024-06-21 18:09:24,480 - INFO: 79/200 final results:
2024-06-21 18:09:24,480 - INFO: Training loss: 0.851.
2024-06-21 18:09:24,480 - INFO: Training MAE: 0.844.
2024-06-21 18:09:24,480 - INFO: Training MSE: 1.466.
2024-06-21 18:09:44,704 - INFO: Epoch: 79/200, Loss_train: 0.8505148640994368, Loss_val: 1.8676618954231
2024-06-21 18:09:44,704 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:09:44,704 - INFO: Epoch 80/200...
2024-06-21 18:09:44,704 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:09:44,704 - INFO: Batch size: 32.
2024-06-21 18:09:44,708 - INFO: Dataset:
2024-06-21 18:09:44,708 - INFO: Batch size:
2024-06-21 18:09:44,708 - INFO: Number of workers:
2024-06-21 18:09:45,802 - INFO: Epoch: 80/200, Batch: 1/29, Batch_Loss_Train: 0.713
2024-06-21 18:09:46,110 - INFO: Epoch: 80/200, Batch: 2/29, Batch_Loss_Train: 0.758
2024-06-21 18:09:46,506 - INFO: Epoch: 80/200, Batch: 3/29, Batch_Loss_Train: 1.002
2024-06-21 18:09:46,826 - INFO: Epoch: 80/200, Batch: 4/29, Batch_Loss_Train: 0.761
2024-06-21 18:09:47,261 - INFO: Epoch: 80/200, Batch: 5/29, Batch_Loss_Train: 0.907
2024-06-21 18:09:47,560 - INFO: Epoch: 80/200, Batch: 6/29, Batch_Loss_Train: 0.969
2024-06-21 18:09:47,946 - INFO: Epoch: 80/200, Batch: 7/29, Batch_Loss_Train: 0.707
2024-06-21 18:09:48,248 - INFO: Epoch: 80/200, Batch: 8/29, Batch_Loss_Train: 0.970
2024-06-21 18:09:48,680 - INFO: Epoch: 80/200, Batch: 9/29, Batch_Loss_Train: 0.962
2024-06-21 18:09:48,971 - INFO: Epoch: 80/200, Batch: 10/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:49,343 - INFO: Epoch: 80/200, Batch: 11/29, Batch_Loss_Train: 0.754
2024-06-21 18:09:49,646 - INFO: Epoch: 80/200, Batch: 12/29, Batch_Loss_Train: 0.714
2024-06-21 18:09:50,094 - INFO: Epoch: 80/200, Batch: 13/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:50,406 - INFO: Epoch: 80/200, Batch: 14/29, Batch_Loss_Train: 1.010
2024-06-21 18:09:50,813 - INFO: Epoch: 80/200, Batch: 15/29, Batch_Loss_Train: 0.797
2024-06-21 18:09:51,121 - INFO: Epoch: 80/200, Batch: 16/29, Batch_Loss_Train: 0.906
2024-06-21 18:09:51,580 - INFO: Epoch: 80/200, Batch: 17/29, Batch_Loss_Train: 0.711
2024-06-21 18:09:51,881 - INFO: Epoch: 80/200, Batch: 18/29, Batch_Loss_Train: 0.796
2024-06-21 18:09:52,265 - INFO: Epoch: 80/200, Batch: 19/29, Batch_Loss_Train: 0.891
2024-06-21 18:09:52,561 - INFO: Epoch: 80/200, Batch: 20/29, Batch_Loss_Train: 0.862
2024-06-21 18:09:52,985 - INFO: Epoch: 80/200, Batch: 21/29, Batch_Loss_Train: 0.853
2024-06-21 18:09:53,286 - INFO: Epoch: 80/200, Batch: 22/29, Batch_Loss_Train: 1.264
2024-06-21 18:09:53,668 - INFO: Epoch: 80/200, Batch: 23/29, Batch_Loss_Train: 0.786
2024-06-21 18:09:53,969 - INFO: Epoch: 80/200, Batch: 24/29, Batch_Loss_Train: 0.854
2024-06-21 18:09:54,403 - INFO: Epoch: 80/200, Batch: 25/29, Batch_Loss_Train: 0.743
2024-06-21 18:09:54,702 - INFO: Epoch: 80/200, Batch: 26/29, Batch_Loss_Train: 1.012
2024-06-21 18:09:55,088 - INFO: Epoch: 80/200, Batch: 27/29, Batch_Loss_Train: 0.734
2024-06-21 18:09:55,387 - INFO: Epoch: 80/200, Batch: 28/29, Batch_Loss_Train: 0.728
2024-06-21 18:09:55,608 - INFO: Epoch: 80/200, Batch: 29/29, Batch_Loss_Train: 1.113
2024-06-21 18:10:06,642 - INFO: 80/200 final results:
2024-06-21 18:10:06,643 - INFO: Training loss: 0.853.
2024-06-21 18:10:06,643 - INFO: Training MAE: 0.848.
2024-06-21 18:10:06,643 - INFO: Training MSE: 1.436.
2024-06-21 18:10:27,387 - INFO: Epoch: 80/200, Loss_train: 0.852845901045306, Loss_val: 1.896312635520409
2024-06-21 18:10:27,387 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:10:27,387 - INFO: Epoch 81/200...
2024-06-21 18:10:27,387 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:10:27,387 - INFO: Batch size: 32.
2024-06-21 18:10:27,391 - INFO: Dataset:
2024-06-21 18:10:27,391 - INFO: Batch size:
2024-06-21 18:10:27,391 - INFO: Number of workers:
2024-06-21 18:10:28,476 - INFO: Epoch: 81/200, Batch: 1/29, Batch_Loss_Train: 0.796
2024-06-21 18:10:28,779 - INFO: Epoch: 81/200, Batch: 2/29, Batch_Loss_Train: 0.812
2024-06-21 18:10:29,160 - INFO: Epoch: 81/200, Batch: 3/29, Batch_Loss_Train: 1.062
2024-06-21 18:10:29,474 - INFO: Epoch: 81/200, Batch: 4/29, Batch_Loss_Train: 0.936
2024-06-21 18:10:29,888 - INFO: Epoch: 81/200, Batch: 5/29, Batch_Loss_Train: 0.751
2024-06-21 18:10:30,185 - INFO: Epoch: 81/200, Batch: 6/29, Batch_Loss_Train: 0.779
2024-06-21 18:10:30,555 - INFO: Epoch: 81/200, Batch: 7/29, Batch_Loss_Train: 0.785
2024-06-21 18:10:30,866 - INFO: Epoch: 81/200, Batch: 8/29, Batch_Loss_Train: 0.857
2024-06-21 18:10:31,276 - INFO: Epoch: 81/200, Batch: 9/29, Batch_Loss_Train: 0.771
2024-06-21 18:10:31,570 - INFO: Epoch: 81/200, Batch: 10/29, Batch_Loss_Train: 0.774
2024-06-21 18:10:31,949 - INFO: Epoch: 81/200, Batch: 11/29, Batch_Loss_Train: 0.876
2024-06-21 18:10:32,264 - INFO: Epoch: 81/200, Batch: 12/29, Batch_Loss_Train: 0.758
2024-06-21 18:10:32,694 - INFO: Epoch: 81/200, Batch: 13/29, Batch_Loss_Train: 0.819
2024-06-21 18:10:32,996 - INFO: Epoch: 81/200, Batch: 14/29, Batch_Loss_Train: 0.682
2024-06-21 18:10:33,388 - INFO: Epoch: 81/200, Batch: 15/29, Batch_Loss_Train: 0.797
2024-06-21 18:10:33,699 - INFO: Epoch: 81/200, Batch: 16/29, Batch_Loss_Train: 1.177
2024-06-21 18:10:34,135 - INFO: Epoch: 81/200, Batch: 17/29, Batch_Loss_Train: 0.718
2024-06-21 18:10:34,436 - INFO: Epoch: 81/200, Batch: 18/29, Batch_Loss_Train: 1.051
2024-06-21 18:10:34,823 - INFO: Epoch: 81/200, Batch: 19/29, Batch_Loss_Train: 0.764
2024-06-21 18:10:35,131 - INFO: Epoch: 81/200, Batch: 20/29, Batch_Loss_Train: 0.877
2024-06-21 18:10:35,556 - INFO: Epoch: 81/200, Batch: 21/29, Batch_Loss_Train: 0.899
2024-06-21 18:10:35,859 - INFO: Epoch: 81/200, Batch: 22/29, Batch_Loss_Train: 0.893
2024-06-21 18:10:36,236 - INFO: Epoch: 81/200, Batch: 23/29, Batch_Loss_Train: 0.696
2024-06-21 18:10:36,553 - INFO: Epoch: 81/200, Batch: 24/29, Batch_Loss_Train: 0.913
2024-06-21 18:10:36,968 - INFO: Epoch: 81/200, Batch: 25/29, Batch_Loss_Train: 0.747
2024-06-21 18:10:37,266 - INFO: Epoch: 81/200, Batch: 26/29, Batch_Loss_Train: 0.889
2024-06-21 18:10:37,644 - INFO: Epoch: 81/200, Batch: 27/29, Batch_Loss_Train: 0.833
2024-06-21 18:10:37,957 - INFO: Epoch: 81/200, Batch: 28/29, Batch_Loss_Train: 0.936
2024-06-21 18:10:38,176 - INFO: Epoch: 81/200, Batch: 29/29, Batch_Loss_Train: 0.975
2024-06-21 18:10:49,332 - INFO: 81/200 final results:
2024-06-21 18:10:49,333 - INFO: Training loss: 0.849.
2024-06-21 18:10:49,333 - INFO: Training MAE: 0.847.
2024-06-21 18:10:49,333 - INFO: Training MSE: 1.447.
2024-06-21 18:11:09,888 - INFO: Epoch: 81/200, Loss_train: 0.8491239588836144, Loss_val: 1.856054663658142
2024-06-21 18:11:09,889 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:11:09,889 - INFO: Epoch 82/200...
2024-06-21 18:11:09,889 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:11:09,889 - INFO: Batch size: 32.
2024-06-21 18:11:09,892 - INFO: Dataset:
2024-06-21 18:11:09,893 - INFO: Batch size:
2024-06-21 18:11:09,893 - INFO: Number of workers:
2024-06-21 18:11:10,990 - INFO: Epoch: 82/200, Batch: 1/29, Batch_Loss_Train: 0.822
2024-06-21 18:11:11,294 - INFO: Epoch: 82/200, Batch: 2/29, Batch_Loss_Train: 0.897
2024-06-21 18:11:11,689 - INFO: Epoch: 82/200, Batch: 3/29, Batch_Loss_Train: 0.840
2024-06-21 18:11:12,005 - INFO: Epoch: 82/200, Batch: 4/29, Batch_Loss_Train: 0.722
2024-06-21 18:11:12,439 - INFO: Epoch: 82/200, Batch: 5/29, Batch_Loss_Train: 0.761
2024-06-21 18:11:12,738 - INFO: Epoch: 82/200, Batch: 6/29, Batch_Loss_Train: 0.758
2024-06-21 18:11:13,112 - INFO: Epoch: 82/200, Batch: 7/29, Batch_Loss_Train: 0.928
2024-06-21 18:11:13,414 - INFO: Epoch: 82/200, Batch: 8/29, Batch_Loss_Train: 0.808
2024-06-21 18:11:13,860 - INFO: Epoch: 82/200, Batch: 9/29, Batch_Loss_Train: 0.730
2024-06-21 18:11:14,152 - INFO: Epoch: 82/200, Batch: 10/29, Batch_Loss_Train: 0.729
2024-06-21 18:11:14,528 - INFO: Epoch: 82/200, Batch: 11/29, Batch_Loss_Train: 0.806
2024-06-21 18:11:14,829 - INFO: Epoch: 82/200, Batch: 12/29, Batch_Loss_Train: 0.811
2024-06-21 18:11:15,273 - INFO: Epoch: 82/200, Batch: 13/29, Batch_Loss_Train: 0.735
2024-06-21 18:11:15,578 - INFO: Epoch: 82/200, Batch: 14/29, Batch_Loss_Train: 0.768
2024-06-21 18:11:15,962 - INFO: Epoch: 82/200, Batch: 15/29, Batch_Loss_Train: 0.771
2024-06-21 18:11:16,263 - INFO: Epoch: 82/200, Batch: 16/29, Batch_Loss_Train: 0.884
2024-06-21 18:11:16,702 - INFO: Epoch: 82/200, Batch: 17/29, Batch_Loss_Train: 0.858
2024-06-21 18:11:16,998 - INFO: Epoch: 82/200, Batch: 18/29, Batch_Loss_Train: 1.018
2024-06-21 18:11:17,382 - INFO: Epoch: 82/200, Batch: 19/29, Batch_Loss_Train: 0.676
2024-06-21 18:11:17,673 - INFO: Epoch: 82/200, Batch: 20/29, Batch_Loss_Train: 0.843
2024-06-21 18:11:18,098 - INFO: Epoch: 82/200, Batch: 21/29, Batch_Loss_Train: 0.830
2024-06-21 18:11:18,398 - INFO: Epoch: 82/200, Batch: 22/29, Batch_Loss_Train: 0.773
2024-06-21 18:11:18,781 - INFO: Epoch: 82/200, Batch: 23/29, Batch_Loss_Train: 1.000
2024-06-21 18:11:19,080 - INFO: Epoch: 82/200, Batch: 24/29, Batch_Loss_Train: 0.617
2024-06-21 18:11:19,509 - INFO: Epoch: 82/200, Batch: 25/29, Batch_Loss_Train: 0.790
2024-06-21 18:11:19,804 - INFO: Epoch: 82/200, Batch: 26/29, Batch_Loss_Train: 0.714
2024-06-21 18:11:20,184 - INFO: Epoch: 82/200, Batch: 27/29, Batch_Loss_Train: 0.677
2024-06-21 18:11:20,479 - INFO: Epoch: 82/200, Batch: 28/29, Batch_Loss_Train: 0.615
2024-06-21 18:11:20,698 - INFO: Epoch: 82/200, Batch: 29/29, Batch_Loss_Train: 0.752
2024-06-21 18:11:31,634 - INFO: 82/200 final results:
2024-06-21 18:11:31,634 - INFO: Training loss: 0.791.
2024-06-21 18:11:31,634 - INFO: Training MAE: 0.792.
2024-06-21 18:11:31,634 - INFO: Training MSE: 1.350.
2024-06-21 18:11:51,648 - INFO: Epoch: 82/200, Loss_train: 0.7908163892811743, Loss_val: 1.9593568875871856
2024-06-21 18:11:51,648 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:11:51,648 - INFO: Epoch 83/200...
2024-06-21 18:11:51,648 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:11:51,648 - INFO: Batch size: 32.
2024-06-21 18:11:51,652 - INFO: Dataset:
2024-06-21 18:11:51,652 - INFO: Batch size:
2024-06-21 18:11:51,652 - INFO: Number of workers:
2024-06-21 18:11:52,716 - INFO: Epoch: 83/200, Batch: 1/29, Batch_Loss_Train: 0.781
2024-06-21 18:11:53,020 - INFO: Epoch: 83/200, Batch: 2/29, Batch_Loss_Train: 0.744
2024-06-21 18:11:53,427 - INFO: Epoch: 83/200, Batch: 3/29, Batch_Loss_Train: 0.796
2024-06-21 18:11:53,742 - INFO: Epoch: 83/200, Batch: 4/29, Batch_Loss_Train: 0.768
2024-06-21 18:11:54,155 - INFO: Epoch: 83/200, Batch: 5/29, Batch_Loss_Train: 0.670
2024-06-21 18:11:54,453 - INFO: Epoch: 83/200, Batch: 6/29, Batch_Loss_Train: 0.925
2024-06-21 18:11:54,840 - INFO: Epoch: 83/200, Batch: 7/29, Batch_Loss_Train: 0.746
2024-06-21 18:11:55,151 - INFO: Epoch: 83/200, Batch: 8/29, Batch_Loss_Train: 1.030
2024-06-21 18:11:55,575 - INFO: Epoch: 83/200, Batch: 9/29, Batch_Loss_Train: 0.729
2024-06-21 18:11:55,866 - INFO: Epoch: 83/200, Batch: 10/29, Batch_Loss_Train: 0.794
2024-06-21 18:11:56,236 - INFO: Epoch: 83/200, Batch: 11/29, Batch_Loss_Train: 0.662
2024-06-21 18:11:56,550 - INFO: Epoch: 83/200, Batch: 12/29, Batch_Loss_Train: 0.715
2024-06-21 18:11:56,967 - INFO: Epoch: 83/200, Batch: 13/29, Batch_Loss_Train: 0.842
2024-06-21 18:11:57,267 - INFO: Epoch: 83/200, Batch: 14/29, Batch_Loss_Train: 0.730
2024-06-21 18:11:57,657 - INFO: Epoch: 83/200, Batch: 15/29, Batch_Loss_Train: 0.718
2024-06-21 18:11:57,967 - INFO: Epoch: 83/200, Batch: 16/29, Batch_Loss_Train: 0.642
2024-06-21 18:11:58,384 - INFO: Epoch: 83/200, Batch: 17/29, Batch_Loss_Train: 0.731
2024-06-21 18:11:58,681 - INFO: Epoch: 83/200, Batch: 18/29, Batch_Loss_Train: 1.013
2024-06-21 18:11:59,060 - INFO: Epoch: 83/200, Batch: 19/29, Batch_Loss_Train: 0.908
2024-06-21 18:11:59,366 - INFO: Epoch: 83/200, Batch: 20/29, Batch_Loss_Train: 0.883
2024-06-21 18:11:59,776 - INFO: Epoch: 83/200, Batch: 21/29, Batch_Loss_Train: 0.741
2024-06-21 18:12:00,076 - INFO: Epoch: 83/200, Batch: 22/29, Batch_Loss_Train: 0.819
2024-06-21 18:12:00,450 - INFO: Epoch: 83/200, Batch: 23/29, Batch_Loss_Train: 0.856
2024-06-21 18:12:00,762 - INFO: Epoch: 83/200, Batch: 24/29, Batch_Loss_Train: 0.706
2024-06-21 18:12:01,173 - INFO: Epoch: 83/200, Batch: 25/29, Batch_Loss_Train: 0.894
2024-06-21 18:12:01,468 - INFO: Epoch: 83/200, Batch: 26/29, Batch_Loss_Train: 0.865
2024-06-21 18:12:01,838 - INFO: Epoch: 83/200, Batch: 27/29, Batch_Loss_Train: 0.775
2024-06-21 18:12:02,145 - INFO: Epoch: 83/200, Batch: 28/29, Batch_Loss_Train: 0.799
2024-06-21 18:12:02,359 - INFO: Epoch: 83/200, Batch: 29/29, Batch_Loss_Train: 0.791
2024-06-21 18:12:13,468 - INFO: 83/200 final results:
2024-06-21 18:12:13,469 - INFO: Training loss: 0.796.
2024-06-21 18:12:13,469 - INFO: Training MAE: 0.796.
2024-06-21 18:12:13,469 - INFO: Training MSE: 1.300.
2024-06-21 18:12:33,589 - INFO: Epoch: 83/200, Loss_train: 0.7956108738636148, Loss_val: 1.9049080322528709
2024-06-21 18:12:33,589 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:12:33,589 - INFO: Epoch 84/200...
2024-06-21 18:12:33,589 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:12:33,589 - INFO: Batch size: 32.
2024-06-21 18:12:33,592 - INFO: Dataset:
2024-06-21 18:12:33,593 - INFO: Batch size:
2024-06-21 18:12:33,593 - INFO: Number of workers:
2024-06-21 18:12:34,651 - INFO: Epoch: 84/200, Batch: 1/29, Batch_Loss_Train: 0.669
2024-06-21 18:12:34,969 - INFO: Epoch: 84/200, Batch: 2/29, Batch_Loss_Train: 0.712
2024-06-21 18:12:35,374 - INFO: Epoch: 84/200, Batch: 3/29, Batch_Loss_Train: 0.796
2024-06-21 18:12:35,692 - INFO: Epoch: 84/200, Batch: 4/29, Batch_Loss_Train: 0.827
2024-06-21 18:12:36,105 - INFO: Epoch: 84/200, Batch: 5/29, Batch_Loss_Train: 0.629
2024-06-21 18:12:36,406 - INFO: Epoch: 84/200, Batch: 6/29, Batch_Loss_Train: 0.871
2024-06-21 18:12:36,809 - INFO: Epoch: 84/200, Batch: 7/29, Batch_Loss_Train: 0.762
2024-06-21 18:12:37,125 - INFO: Epoch: 84/200, Batch: 8/29, Batch_Loss_Train: 0.702
2024-06-21 18:12:37,538 - INFO: Epoch: 84/200, Batch: 9/29, Batch_Loss_Train: 0.899
2024-06-21 18:12:37,833 - INFO: Epoch: 84/200, Batch: 10/29, Batch_Loss_Train: 0.853
2024-06-21 18:12:38,222 - INFO: Epoch: 84/200, Batch: 11/29, Batch_Loss_Train: 0.690
2024-06-21 18:12:38,539 - INFO: Epoch: 84/200, Batch: 12/29, Batch_Loss_Train: 0.819
2024-06-21 18:12:38,959 - INFO: Epoch: 84/200, Batch: 13/29, Batch_Loss_Train: 0.639
2024-06-21 18:12:39,264 - INFO: Epoch: 84/200, Batch: 14/29, Batch_Loss_Train: 0.657
2024-06-21 18:12:39,672 - INFO: Epoch: 84/200, Batch: 15/29, Batch_Loss_Train: 0.799
2024-06-21 18:12:39,984 - INFO: Epoch: 84/200, Batch: 16/29, Batch_Loss_Train: 0.667
2024-06-21 18:12:40,401 - INFO: Epoch: 84/200, Batch: 17/29, Batch_Loss_Train: 1.009
2024-06-21 18:12:40,703 - INFO: Epoch: 84/200, Batch: 18/29, Batch_Loss_Train: 0.761
2024-06-21 18:12:41,099 - INFO: Epoch: 84/200, Batch: 19/29, Batch_Loss_Train: 0.731
2024-06-21 18:12:41,406 - INFO: Epoch: 84/200, Batch: 20/29, Batch_Loss_Train: 0.759
2024-06-21 18:12:41,810 - INFO: Epoch: 84/200, Batch: 21/29, Batch_Loss_Train: 0.712
2024-06-21 18:12:42,111 - INFO: Epoch: 84/200, Batch: 22/29, Batch_Loss_Train: 0.840
2024-06-21 18:12:42,499 - INFO: Epoch: 84/200, Batch: 23/29, Batch_Loss_Train: 0.718
2024-06-21 18:12:42,812 - INFO: Epoch: 84/200, Batch: 24/29, Batch_Loss_Train: 0.986
2024-06-21 18:12:43,206 - INFO: Epoch: 84/200, Batch: 25/29, Batch_Loss_Train: 0.671
2024-06-21 18:12:43,501 - INFO: Epoch: 84/200, Batch: 26/29, Batch_Loss_Train: 0.851
2024-06-21 18:12:43,878 - INFO: Epoch: 84/200, Batch: 27/29, Batch_Loss_Train: 0.952
2024-06-21 18:12:44,185 - INFO: Epoch: 84/200, Batch: 28/29, Batch_Loss_Train: 0.936
2024-06-21 18:12:44,401 - INFO: Epoch: 84/200, Batch: 29/29, Batch_Loss_Train: 0.811
2024-06-21 18:12:55,540 - INFO: 84/200 final results:
2024-06-21 18:12:55,540 - INFO: Training loss: 0.784.
2024-06-21 18:12:55,540 - INFO: Training MAE: 0.783.
2024-06-21 18:12:55,540 - INFO: Training MSE: 1.261.
2024-06-21 18:13:15,858 - INFO: Epoch: 84/200, Loss_train: 0.7837535204558537, Loss_val: 1.8577203134010578
2024-06-21 18:13:15,858 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:13:15,858 - INFO: Epoch 85/200...
2024-06-21 18:13:15,858 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 18:13:15,858 - INFO: Batch size: 32.
2024-06-21 18:13:15,861 - INFO: Dataset:
2024-06-21 18:13:15,862 - INFO: Batch size:
2024-06-21 18:13:15,862 - INFO: Number of workers:
2024-06-21 18:13:16,936 - INFO: Epoch: 85/200, Batch: 1/29, Batch_Loss_Train: 0.825
2024-06-21 18:13:17,241 - INFO: Epoch: 85/200, Batch: 2/29, Batch_Loss_Train: 0.947
2024-06-21 18:13:17,650 - INFO: Epoch: 85/200, Batch: 3/29, Batch_Loss_Train: 0.845
2024-06-21 18:13:17,967 - INFO: Epoch: 85/200, Batch: 4/29, Batch_Loss_Train: 0.784
2024-06-21 18:13:18,386 - INFO: Epoch: 85/200, Batch: 5/29, Batch_Loss_Train: 0.893
2024-06-21 18:13:18,685 - INFO: Epoch: 85/200, Batch: 6/29, Batch_Loss_Train: 0.901
2024-06-21 18:13:19,073 - INFO: Epoch: 85/200, Batch: 7/29, Batch_Loss_Train: 0.821
2024-06-21 18:13:19,386 - INFO: Epoch: 85/200, Batch: 8/29, Batch_Loss_Train: 0.758
2024-06-21 18:13:19,818 - INFO: Epoch: 85/200, Batch: 9/29, Batch_Loss_Train: 0.684
2024-06-21 18:13:20,109 - INFO: Epoch: 85/200, Batch: 10/29, Batch_Loss_Train: 0.929
2024-06-21 18:13:20,485 - INFO: Epoch: 85/200, Batch: 11/29, Batch_Loss_Train: 0.825
2024-06-21 18:13:20,800 - INFO: Epoch: 85/200, Batch: 12/29, Batch_Loss_Train: 0.750
2024-06-21 18:13:21,228 - INFO: Epoch: 85/200, Batch: 13/29, Batch_Loss_Train: 0.973
2024-06-21 18:13:21,530 - INFO: Epoch: 85/200, Batch: 14/29, Batch_Loss_Train: 0.753
2024-06-21 18:13:21,923 - INFO: Epoch: 85/200, Batch: 15/29, Batch_Loss_Train: 0.929
2024-06-21 18:13:22,233 - INFO: Epoch: 85/200, Batch: 16/29, Batch_Loss_Train: 0.539
2024-06-21 18:13:22,659 - INFO: Epoch: 85/200, Batch: 17/29, Batch_Loss_Train: 0.637
2024-06-21 18:13:22,957 - INFO: Epoch: 85/200, Batch: 18/29, Batch_Loss_Train: 0.617
2024-06-21 18:13:23,337 - INFO: Epoch: 85/200, Batch: 19/29, Batch_Loss_Train: 0.731
2024-06-21 18:13:23,643 - INFO: Epoch: 85/200, Batch: 20/29, Batch_Loss_Train: 1.090
2024-06-21 18:13:24,050 - INFO: Epoch: 85/200, Batch: 21/29, Batch_Loss_Train: 0.815
2024-06-21 18:13:24,349 - INFO: Epoch: 85/200, Batch: 22/29, Batch_Loss_Train: 0.672
2024-06-21 18:13:24,720 - INFO: Epoch: 85/200, Batch: 23/29, Batch_Loss_Train: 1.027
2024-06-21 18:13:25,032 - INFO: Epoch: 85/200, Batch: 24/29, Batch_Loss_Train: 0.851
2024-06-21 18:13:25,438 - INFO: Epoch: 85/200, Batch: 25/29, Batch_Loss_Train: 0.833
2024-06-21 18:13:25,734 - INFO: Epoch: 85/200, Batch: 26/29, Batch_Loss_Train: 0.701
2024-06-21 18:13:26,116 - INFO: Epoch: 85/200, Batch: 27/29, Batch_Loss_Train: 0.925
2024-06-21 18:13:26,424 - INFO: Epoch: 85/200, Batch: 28/29, Batch_Loss_Train: 0.869
2024-06-21 18:13:26,633 - INFO: Epoch: 85/200, Batch: 29/29, Batch_Loss_Train: 0.891
2024-06-21 18:13:37,697 - INFO: 85/200 final results:
2024-06-21 18:13:37,697 - INFO: Training loss: 0.821.
2024-06-21 18:13:37,697 - INFO: Training MAE: 0.820.
2024-06-21 18:13:37,697 - INFO: Training MSE: 1.343.
2024-06-21 18:13:58,130 - INFO: Epoch: 85/200, Loss_train: 0.8212578831047848, Loss_val: 1.9186989603371456
2024-06-21 18:13:58,130 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:13:58,130 - INFO: Epoch 86/200...
2024-06-21 18:13:58,130 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:13:58,130 - INFO: Batch size: 32.
2024-06-21 18:13:58,134 - INFO: Dataset:
2024-06-21 18:13:58,134 - INFO: Batch size:
2024-06-21 18:13:58,134 - INFO: Number of workers:
2024-06-21 18:13:59,190 - INFO: Epoch: 86/200, Batch: 1/29, Batch_Loss_Train: 1.062
2024-06-21 18:13:59,510 - INFO: Epoch: 86/200, Batch: 2/29, Batch_Loss_Train: 0.804
2024-06-21 18:13:59,920 - INFO: Epoch: 86/200, Batch: 3/29, Batch_Loss_Train: 0.816
2024-06-21 18:14:00,238 - INFO: Epoch: 86/200, Batch: 4/29, Batch_Loss_Train: 0.724
2024-06-21 18:14:00,641 - INFO: Epoch: 86/200, Batch: 5/29, Batch_Loss_Train: 0.828
2024-06-21 18:14:00,941 - INFO: Epoch: 86/200, Batch: 6/29, Batch_Loss_Train: 0.633
2024-06-21 18:14:01,339 - INFO: Epoch: 86/200, Batch: 7/29, Batch_Loss_Train: 0.724
2024-06-21 18:14:01,655 - INFO: Epoch: 86/200, Batch: 8/29, Batch_Loss_Train: 0.761
2024-06-21 18:14:02,056 - INFO: Epoch: 86/200, Batch: 9/29, Batch_Loss_Train: 0.799
2024-06-21 18:14:02,349 - INFO: Epoch: 86/200, Batch: 10/29, Batch_Loss_Train: 0.673
2024-06-21 18:14:02,736 - INFO: Epoch: 86/200, Batch: 11/29, Batch_Loss_Train: 0.738
2024-06-21 18:14:03,052 - INFO: Epoch: 86/200, Batch: 12/29, Batch_Loss_Train: 0.758
2024-06-21 18:14:03,461 - INFO: Epoch: 86/200, Batch: 13/29, Batch_Loss_Train: 0.777
2024-06-21 18:14:03,762 - INFO: Epoch: 86/200, Batch: 14/29, Batch_Loss_Train: 0.617
2024-06-21 18:14:04,166 - INFO: Epoch: 86/200, Batch: 15/29, Batch_Loss_Train: 0.782
2024-06-21 18:14:04,477 - INFO: Epoch: 86/200, Batch: 16/29, Batch_Loss_Train: 0.587
2024-06-21 18:14:04,880 - INFO: Epoch: 86/200, Batch: 17/29, Batch_Loss_Train: 0.812
2024-06-21 18:14:05,178 - INFO: Epoch: 86/200, Batch: 18/29, Batch_Loss_Train: 0.798
2024-06-21 18:14:05,571 - INFO: Epoch: 86/200, Batch: 19/29, Batch_Loss_Train: 0.779
2024-06-21 18:14:05,878 - INFO: Epoch: 86/200, Batch: 20/29, Batch_Loss_Train: 0.742
2024-06-21 18:14:06,271 - INFO: Epoch: 86/200, Batch: 21/29, Batch_Loss_Train: 0.911
2024-06-21 18:14:06,572 - INFO: Epoch: 86/200, Batch: 22/29, Batch_Loss_Train: 0.891
2024-06-21 18:14:06,956 - INFO: Epoch: 86/200, Batch: 23/29, Batch_Loss_Train: 0.856
2024-06-21 18:14:07,269 - INFO: Epoch: 86/200, Batch: 24/29, Batch_Loss_Train: 0.638
2024-06-21 18:14:07,667 - INFO: Epoch: 86/200, Batch: 25/29, Batch_Loss_Train: 0.645
2024-06-21 18:14:07,964 - INFO: Epoch: 86/200, Batch: 26/29, Batch_Loss_Train: 0.821
2024-06-21 18:14:08,344 - INFO: Epoch: 86/200, Batch: 27/29, Batch_Loss_Train: 0.832
2024-06-21 18:14:08,653 - INFO: Epoch: 86/200, Batch: 28/29, Batch_Loss_Train: 0.795
2024-06-21 18:14:08,876 - INFO: Epoch: 86/200, Batch: 29/29, Batch_Loss_Train: 0.863
2024-06-21 18:14:20,025 - INFO: 86/200 final results:
2024-06-21 18:14:20,025 - INFO: Training loss: 0.775.
2024-06-21 18:14:20,025 - INFO: Training MAE: 0.773.
2024-06-21 18:14:20,025 - INFO: Training MSE: 1.247.
2024-06-21 18:14:39,964 - INFO: Epoch: 86/200, Loss_train: 0.7746825999227064, Loss_val: 1.872085587731723
2024-06-21 18:14:39,965 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:14:39,965 - INFO: Epoch 87/200...
2024-06-21 18:14:39,965 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:14:39,965 - INFO: Batch size: 32.
2024-06-21 18:14:39,968 - INFO: Dataset:
2024-06-21 18:14:39,969 - INFO: Batch size:
2024-06-21 18:14:39,969 - INFO: Number of workers:
2024-06-21 18:14:41,018 - INFO: Epoch: 87/200, Batch: 1/29, Batch_Loss_Train: 0.884
2024-06-21 18:14:41,339 - INFO: Epoch: 87/200, Batch: 2/29, Batch_Loss_Train: 0.710
2024-06-21 18:14:41,750 - INFO: Epoch: 87/200, Batch: 3/29, Batch_Loss_Train: 0.698
2024-06-21 18:14:42,069 - INFO: Epoch: 87/200, Batch: 4/29, Batch_Loss_Train: 0.753
2024-06-21 18:14:42,480 - INFO: Epoch: 87/200, Batch: 5/29, Batch_Loss_Train: 0.811
2024-06-21 18:14:42,783 - INFO: Epoch: 87/200, Batch: 6/29, Batch_Loss_Train: 0.825
2024-06-21 18:14:43,179 - INFO: Epoch: 87/200, Batch: 7/29, Batch_Loss_Train: 0.582
2024-06-21 18:14:43,492 - INFO: Epoch: 87/200, Batch: 8/29, Batch_Loss_Train: 0.804
2024-06-21 18:14:43,908 - INFO: Epoch: 87/200, Batch: 9/29, Batch_Loss_Train: 0.866
2024-06-21 18:14:44,202 - INFO: Epoch: 87/200, Batch: 10/29, Batch_Loss_Train: 0.908
2024-06-21 18:14:44,582 - INFO: Epoch: 87/200, Batch: 11/29, Batch_Loss_Train: 0.812
2024-06-21 18:14:44,899 - INFO: Epoch: 87/200, Batch: 12/29, Batch_Loss_Train: 0.668
2024-06-21 18:14:45,318 - INFO: Epoch: 87/200, Batch: 13/29, Batch_Loss_Train: 0.831
2024-06-21 18:14:45,620 - INFO: Epoch: 87/200, Batch: 14/29, Batch_Loss_Train: 0.856
2024-06-21 18:14:46,019 - INFO: Epoch: 87/200, Batch: 15/29, Batch_Loss_Train: 0.719
2024-06-21 18:14:46,330 - INFO: Epoch: 87/200, Batch: 16/29, Batch_Loss_Train: 0.644
2024-06-21 18:14:46,744 - INFO: Epoch: 87/200, Batch: 17/29, Batch_Loss_Train: 0.816
2024-06-21 18:14:47,042 - INFO: Epoch: 87/200, Batch: 18/29, Batch_Loss_Train: 0.639
2024-06-21 18:14:47,434 - INFO: Epoch: 87/200, Batch: 19/29, Batch_Loss_Train: 0.720
2024-06-21 18:14:47,739 - INFO: Epoch: 87/200, Batch: 20/29, Batch_Loss_Train: 0.686
2024-06-21 18:14:48,143 - INFO: Epoch: 87/200, Batch: 21/29, Batch_Loss_Train: 0.924
2024-06-21 18:14:48,444 - INFO: Epoch: 87/200, Batch: 22/29, Batch_Loss_Train: 0.721
2024-06-21 18:14:49,128 - INFO: Epoch: 87/200, Batch: 23/29, Batch_Loss_Train: 0.900
2024-06-21 18:14:49,441 - INFO: Epoch: 87/200, Batch: 24/29, Batch_Loss_Train: 0.768
2024-06-21 18:14:49,841 - INFO: Epoch: 87/200, Batch: 25/29, Batch_Loss_Train: 0.560
2024-06-21 18:14:50,137 - INFO: Epoch: 87/200, Batch: 26/29, Batch_Loss_Train: 0.775
2024-06-21 18:14:50,524 - INFO: Epoch: 87/200, Batch: 27/29, Batch_Loss_Train: 0.745
2024-06-21 18:14:50,832 - INFO: Epoch: 87/200, Batch: 28/29, Batch_Loss_Train: 0.633
2024-06-21 18:14:51,044 - INFO: Epoch: 87/200, Batch: 29/29, Batch_Loss_Train: 1.197
2024-06-21 18:15:01,769 - INFO: 87/200 final results:
2024-06-21 18:15:01,769 - INFO: Training loss: 0.774.
2024-06-21 18:15:01,769 - INFO: Training MAE: 0.766.
2024-06-21 18:15:01,769 - INFO: Training MSE: 1.227.
2024-06-21 18:15:22,393 - INFO: Epoch: 87/200, Loss_train: 0.7743224069989961, Loss_val: 1.871932181818732
2024-06-21 18:15:22,393 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:15:22,393 - INFO: Epoch 88/200...
2024-06-21 18:15:22,393 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 18:15:22,394 - INFO: Batch size: 32.
2024-06-21 18:15:22,397 - INFO: Dataset:
2024-06-21 18:15:22,398 - INFO: Batch size:
2024-06-21 18:15:22,398 - INFO: Number of workers:
2024-06-21 18:15:23,476 - INFO: Epoch: 88/200, Batch: 1/29, Batch_Loss_Train: 0.825
2024-06-21 18:15:23,782 - INFO: Epoch: 88/200, Batch: 2/29, Batch_Loss_Train: 0.779
2024-06-21 18:15:24,184 - INFO: Epoch: 88/200, Batch: 3/29, Batch_Loss_Train: 0.684
2024-06-21 18:15:24,501 - INFO: Epoch: 88/200, Batch: 4/29, Batch_Loss_Train: 0.875
2024-06-21 18:15:24,909 - INFO: Epoch: 88/200, Batch: 5/29, Batch_Loss_Train: 0.780
2024-06-21 18:15:25,208 - INFO: Epoch: 88/200, Batch: 6/29, Batch_Loss_Train: 0.877
2024-06-21 18:15:25,603 - INFO: Epoch: 88/200, Batch: 7/29, Batch_Loss_Train: 0.763
2024-06-21 18:15:25,916 - INFO: Epoch: 88/200, Batch: 8/29, Batch_Loss_Train: 0.773
2024-06-21 18:15:26,314 - INFO: Epoch: 88/200, Batch: 9/29, Batch_Loss_Train: 0.807
2024-06-21 18:15:26,609 - INFO: Epoch: 88/200, Batch: 10/29, Batch_Loss_Train: 0.679
2024-06-21 18:15:27,004 - INFO: Epoch: 88/200, Batch: 11/29, Batch_Loss_Train: 0.736
2024-06-21 18:15:27,321 - INFO: Epoch: 88/200, Batch: 12/29, Batch_Loss_Train: 0.687
2024-06-21 18:15:27,741 - INFO: Epoch: 88/200, Batch: 13/29, Batch_Loss_Train: 0.879
2024-06-21 18:15:28,046 - INFO: Epoch: 88/200, Batch: 14/29, Batch_Loss_Train: 0.872
2024-06-21 18:15:28,457 - INFO: Epoch: 88/200, Batch: 15/29, Batch_Loss_Train: 0.679
2024-06-21 18:15:28,771 - INFO: Epoch: 88/200, Batch: 16/29, Batch_Loss_Train: 0.796
2024-06-21 18:15:29,192 - INFO: Epoch: 88/200, Batch: 17/29, Batch_Loss_Train: 0.705
2024-06-21 18:15:29,493 - INFO: Epoch: 88/200, Batch: 18/29, Batch_Loss_Train: 0.777
2024-06-21 18:15:29,891 - INFO: Epoch: 88/200, Batch: 19/29, Batch_Loss_Train: 0.745
2024-06-21 18:15:30,200 - INFO: Epoch: 88/200, Batch: 20/29, Batch_Loss_Train: 0.820
2024-06-21 18:15:30,609 - INFO: Epoch: 88/200, Batch: 21/29, Batch_Loss_Train: 0.822
2024-06-21 18:15:30,911 - INFO: Epoch: 88/200, Batch: 22/29, Batch_Loss_Train: 0.720
2024-06-21 18:15:31,299 - INFO: Epoch: 88/200, Batch: 23/29, Batch_Loss_Train: 0.817
2024-06-21 18:15:31,612 - INFO: Epoch: 88/200, Batch: 24/29, Batch_Loss_Train: 0.768
2024-06-21 18:15:32,010 - INFO: Epoch: 88/200, Batch: 25/29, Batch_Loss_Train: 0.742
2024-06-21 18:15:32,306 - INFO: Epoch: 88/200, Batch: 26/29, Batch_Loss_Train: 0.682
2024-06-21 18:15:32,688 - INFO: Epoch: 88/200, Batch: 27/29, Batch_Loss_Train: 0.662
2024-06-21 18:15:32,995 - INFO: Epoch: 88/200, Batch: 28/29, Batch_Loss_Train: 0.713
2024-06-21 18:15:33,208 - INFO: Epoch: 88/200, Batch: 29/29, Batch_Loss_Train: 0.979
2024-06-21 18:15:44,374 - INFO: 88/200 final results:
2024-06-21 18:15:44,374 - INFO: Training loss: 0.774.
2024-06-21 18:15:44,374 - INFO: Training MAE: 0.770.
2024-06-21 18:15:44,374 - INFO: Training MSE: 1.220.
2024-06-21 18:16:04,675 - INFO: Epoch: 88/200, Loss_train: 0.7739076367739973, Loss_val: 1.8797403861736428
2024-06-21 18:16:04,676 - INFO: Best internal validation val_loss: 1.852 at epoch: 78.
2024-06-21 18:16:04,676 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
2024-06-21 18:16:48,872 - INFO: Experiment has been saved in 20240621_181648_341_Dual_DCNN_LReLu_0_78_tr_0.631_val_1.852_test_3.951 folder.
2024-06-21 18:16:48,872 - INFO: Fold: 1
2024-06-21 18:16:48,873 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 18:16:48,873 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 18:16:48,873 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 18:16:49,005 - INFO: To_device: False.
2024-06-21 18:16:49,006 - INFO: Transformers have been made successfully.
2024-06-21 18:16:49,007 - INFO: Dataset type: cache.
2024-06-21 18:16:49,007 - INFO: Dataloader type: standard.
2024-06-21 18:18:38,508 - INFO: Train dataloader arguments.
2024-06-21 18:18:38,508 - INFO: 	Batch_size: 32.
2024-06-21 18:18:38,508 - INFO: 	Shuffle: True.
2024-06-21 18:18:38,508 - INFO: 	Sampler: None.
2024-06-21 18:18:38,508 - INFO: 	Num_workers: 4.
2024-06-21 18:18:38,508 - INFO: 	Drop_last: False.
2024-06-21 18:18:38,742 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (4): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=65536, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 18:18:38,747 - INFO: Weight init name: kaiming_uniform.
2024-06-21 18:18:39,484 - INFO: Number of training iterations per epoch: 29.
2024-06-21 18:18:39,484 - INFO: Epoch 1/200...
2024-06-21 18:18:39,484 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:18:39,484 - INFO: Batch size: 32.
2024-06-21 18:18:39,484 - INFO: Dataset:
2024-06-21 18:18:39,484 - INFO: Batch size:
2024-06-21 18:18:39,484 - INFO: Number of workers:
2024-06-21 18:18:40,703 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 7.090
2024-06-21 18:18:41,004 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 7.456
2024-06-21 18:18:41,393 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 8.148
2024-06-21 18:18:41,693 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 7.594
2024-06-21 18:18:42,079 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 8.477
2024-06-21 18:18:42,412 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 7.083
2024-06-21 18:18:42,786 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 9.231
2024-06-21 18:18:43,083 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 7.014
2024-06-21 18:18:43,453 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 7.537
2024-06-21 18:18:43,785 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 7.965
2024-06-21 18:18:44,159 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 7.083
2024-06-21 18:18:44,459 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 6.847
2024-06-21 18:18:44,851 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 6.972
2024-06-21 18:18:45,196 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 7.758
2024-06-21 18:18:45,585 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 7.062
2024-06-21 18:18:45,879 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 6.189
2024-06-21 18:18:46,283 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 7.344
2024-06-21 18:18:46,618 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 6.475
2024-06-21 18:18:46,994 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 6.116
2024-06-21 18:18:47,287 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 6.412
2024-06-21 18:18:47,682 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 5.371
2024-06-21 18:18:48,016 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 6.443
2024-06-21 18:18:48,394 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 5.748
2024-06-21 18:18:48,692 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 5.658
2024-06-21 18:18:49,076 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 5.140
2024-06-21 18:18:49,409 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 6.367
2024-06-21 18:18:49,793 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 5.755
2024-06-21 18:18:50,091 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 6.470
2024-06-21 18:18:50,313 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 6.956
2024-06-21 18:19:01,624 - INFO: 1/200 final results:
2024-06-21 18:19:01,624 - INFO: Training loss: 6.888.
2024-06-21 18:19:01,624 - INFO: Training MAE: 6.887.
2024-06-21 18:19:01,625 - INFO: Training MSE: 68.550.
2024-06-21 18:19:21,924 - INFO: Epoch: 1/200, Loss_train: 6.888368359927473, Loss_val: 7.327792907583302
2024-06-21 18:19:21,924 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 18:19:21,924 - INFO: Epoch 2/200...
2024-06-21 18:19:21,924 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:19:21,924 - INFO: Batch size: 32.
2024-06-21 18:19:21,928 - INFO: Dataset:
2024-06-21 18:19:21,928 - INFO: Batch size:
2024-06-21 18:19:21,928 - INFO: Number of workers:
2024-06-21 18:19:23,084 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 5.376
2024-06-21 18:19:23,412 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 5.660
2024-06-21 18:19:23,801 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 5.764
2024-06-21 18:19:24,115 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 6.230
2024-06-21 18:19:24,527 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 6.250
2024-06-21 18:19:24,841 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 6.380
2024-06-21 18:19:25,236 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 6.134
2024-06-21 18:19:25,546 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 6.106
2024-06-21 18:19:25,957 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 6.572
2024-06-21 18:19:26,261 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 6.046
2024-06-21 18:19:26,632 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 5.599
2024-06-21 18:19:26,944 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 5.762
2024-06-21 18:19:27,358 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 5.683
2024-06-21 18:19:27,672 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 5.733
2024-06-21 18:19:28,061 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 5.838
2024-06-21 18:19:28,371 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 5.290
2024-06-21 18:19:28,778 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 5.541
2024-06-21 18:19:29,089 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 6.010
2024-06-21 18:19:29,467 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 5.851
2024-06-21 18:19:29,771 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 6.078
2024-06-21 18:19:30,173 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 5.477
2024-06-21 18:19:30,485 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 5.794
2024-06-21 18:19:30,860 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 6.122
2024-06-21 18:19:31,170 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 6.641
2024-06-21 18:19:31,567 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 6.178
2024-06-21 18:19:31,874 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 5.747
2024-06-21 18:19:32,247 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 6.117
2024-06-21 18:19:32,554 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 6.342
2024-06-21 18:19:32,774 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 6.210
2024-06-21 18:19:43,805 - INFO: 2/200 final results:
2024-06-21 18:19:43,805 - INFO: Training loss: 5.949.
2024-06-21 18:19:43,805 - INFO: Training MAE: 5.944.
2024-06-21 18:19:43,805 - INFO: Training MSE: 53.416.
2024-06-21 18:20:04,437 - INFO: Epoch: 2/200, Loss_train: 5.949333273131272, Loss_val: 13.538698360837738
2024-06-21 18:20:04,456 - INFO: Saved new best metric model for epoch 2.
2024-06-21 18:20:04,456 - INFO: Best internal validation val_loss: 13.539 at epoch: 2.
2024-06-21 18:20:04,456 - INFO: Epoch 3/200...
2024-06-21 18:20:04,456 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:20:04,456 - INFO: Batch size: 32.
2024-06-21 18:20:04,461 - INFO: Dataset:
2024-06-21 18:20:04,461 - INFO: Batch size:
2024-06-21 18:20:04,461 - INFO: Number of workers:
2024-06-21 18:20:05,625 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 5.256
2024-06-21 18:20:05,943 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 5.711
2024-06-21 18:20:06,351 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 6.025
2024-06-21 18:20:06,667 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 5.989
2024-06-21 18:20:07,063 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 6.181
2024-06-21 18:20:07,360 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 7.213
2024-06-21 18:20:07,756 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 6.635
2024-06-21 18:20:08,067 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 5.455
2024-06-21 18:20:08,470 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 6.379
2024-06-21 18:20:08,762 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 5.745
2024-06-21 18:20:09,149 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 5.746
2024-06-21 18:20:09,461 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 6.116
2024-06-21 18:20:09,873 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 5.707
2024-06-21 18:20:10,177 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 5.592
2024-06-21 18:20:10,577 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 5.991
2024-06-21 18:20:10,890 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 5.713
2024-06-21 18:20:11,310 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 5.772
2024-06-21 18:20:11,610 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 5.662
2024-06-21 18:20:11,994 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 5.879
2024-06-21 18:20:12,303 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 5.808
2024-06-21 18:20:12,705 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 6.581
2024-06-21 18:20:13,006 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 5.516
2024-06-21 18:20:13,390 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 5.272
2024-06-21 18:20:13,703 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 6.025
2024-06-21 18:20:14,099 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 5.382
2024-06-21 18:20:14,396 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 5.716
2024-06-21 18:20:14,776 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 5.915
2024-06-21 18:20:15,085 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 5.718
2024-06-21 18:20:15,294 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 6.038
2024-06-21 18:20:26,367 - INFO: 3/200 final results:
2024-06-21 18:20:26,367 - INFO: Training loss: 5.887.
2024-06-21 18:20:26,367 - INFO: Training MAE: 5.884.
2024-06-21 18:20:26,367 - INFO: Training MSE: 52.713.
2024-06-21 18:20:46,513 - INFO: Epoch: 3/200, Loss_train: 5.88746059351954, Loss_val: 6.093751118100923
2024-06-21 18:20:46,532 - INFO: Saved new best metric model for epoch 3.
2024-06-21 18:20:46,532 - INFO: Best internal validation val_loss: 6.094 at epoch: 3.
2024-06-21 18:20:46,532 - INFO: Epoch 4/200...
2024-06-21 18:20:46,532 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:20:46,532 - INFO: Batch size: 32.
2024-06-21 18:20:46,536 - INFO: Dataset:
2024-06-21 18:20:46,536 - INFO: Batch size:
2024-06-21 18:20:46,536 - INFO: Number of workers:
2024-06-21 18:20:47,706 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 4.814
2024-06-21 18:20:48,024 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 5.693
2024-06-21 18:20:48,417 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 5.874
2024-06-21 18:20:48,731 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 5.771
2024-06-21 18:20:49,144 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 5.603
2024-06-21 18:20:49,443 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 5.799
2024-06-21 18:20:49,830 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 5.769
2024-06-21 18:20:50,144 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 5.395
2024-06-21 18:20:50,558 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 5.986
2024-06-21 18:20:50,850 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 5.143
2024-06-21 18:20:51,230 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 4.850
2024-06-21 18:20:51,547 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 5.271
2024-06-21 18:20:51,964 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 6.655
2024-06-21 18:20:52,269 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 5.174
2024-06-21 18:20:52,664 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 6.174
2024-06-21 18:20:52,978 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 6.369
2024-06-21 18:20:53,403 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 5.454
2024-06-21 18:20:53,701 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 6.412
2024-06-21 18:20:54,070 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 5.810
2024-06-21 18:20:54,376 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 5.612
2024-06-21 18:20:54,777 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 5.273
2024-06-21 18:20:55,076 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 5.861
2024-06-21 18:20:55,449 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 5.732
2024-06-21 18:20:55,761 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 6.265
2024-06-21 18:20:56,177 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 5.183
2024-06-21 18:20:56,472 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 6.629
2024-06-21 18:20:56,844 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 5.503
2024-06-21 18:20:57,153 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 5.642
2024-06-21 18:20:57,360 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 4.716
2024-06-21 18:21:08,448 - INFO: 4/200 final results:
2024-06-21 18:21:08,448 - INFO: Training loss: 5.670.
2024-06-21 18:21:08,448 - INFO: Training MAE: 5.689.
2024-06-21 18:21:08,448 - INFO: Training MSE: 50.682.
2024-06-21 18:21:28,920 - INFO: Epoch: 4/200, Loss_train: 5.670090790452628, Loss_val: 5.731989564566777
2024-06-21 18:21:28,938 - INFO: Saved new best metric model for epoch 4.
2024-06-21 18:21:28,939 - INFO: Best internal validation val_loss: 5.732 at epoch: 4.
2024-06-21 18:21:28,939 - INFO: Epoch 5/200...
2024-06-21 18:21:28,939 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:21:28,939 - INFO: Batch size: 32.
2024-06-21 18:21:28,943 - INFO: Dataset:
2024-06-21 18:21:28,943 - INFO: Batch size:
2024-06-21 18:21:28,943 - INFO: Number of workers:
2024-06-21 18:21:30,112 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 5.850
2024-06-21 18:21:30,416 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 4.994
2024-06-21 18:21:30,823 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 5.637
2024-06-21 18:21:31,140 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 4.833
2024-06-21 18:21:31,560 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 5.958
2024-06-21 18:21:31,859 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 5.445
2024-06-21 18:21:32,243 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 5.540
2024-06-21 18:21:32,556 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 5.212
2024-06-21 18:21:32,978 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 4.774
2024-06-21 18:21:33,270 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 6.252
2024-06-21 18:21:33,643 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 5.388
2024-06-21 18:21:33,957 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 5.118
2024-06-21 18:21:34,390 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 5.512
2024-06-21 18:21:34,694 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 5.409
2024-06-21 18:21:35,091 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 5.451
2024-06-21 18:21:35,405 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 5.868
2024-06-21 18:21:35,837 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 5.525
2024-06-21 18:21:36,138 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 5.859
2024-06-21 18:21:36,521 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 6.929
2024-06-21 18:21:36,830 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 5.797
2024-06-21 18:21:37,244 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 5.610
2024-06-21 18:21:37,543 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 5.156
2024-06-21 18:21:37,912 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 5.187
2024-06-21 18:21:38,223 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 5.738
2024-06-21 18:21:38,628 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 5.617
2024-06-21 18:21:38,923 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 4.507
2024-06-21 18:21:39,297 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 5.642
2024-06-21 18:21:39,605 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 4.964
2024-06-21 18:21:39,823 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 5.031
2024-06-21 18:21:50,952 - INFO: 5/200 final results:
2024-06-21 18:21:50,952 - INFO: Training loss: 5.476.
2024-06-21 18:21:50,952 - INFO: Training MAE: 5.485.
2024-06-21 18:21:50,952 - INFO: Training MSE: 47.587.
2024-06-21 18:22:11,131 - INFO: Epoch: 5/200, Loss_train: 5.475910203210239, Loss_val: 5.443807930781923
2024-06-21 18:22:11,150 - INFO: Saved new best metric model for epoch 5.
2024-06-21 18:22:11,150 - INFO: Best internal validation val_loss: 5.444 at epoch: 5.
2024-06-21 18:22:11,150 - INFO: Epoch 6/200...
2024-06-21 18:22:11,150 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:22:11,150 - INFO: Batch size: 32.
2024-06-21 18:22:11,154 - INFO: Dataset:
2024-06-21 18:22:11,155 - INFO: Batch size:
2024-06-21 18:22:11,155 - INFO: Number of workers:
2024-06-21 18:22:12,337 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 4.394
2024-06-21 18:22:12,641 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 5.287
2024-06-21 18:22:13,036 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 4.989
2024-06-21 18:22:13,352 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 6.319
2024-06-21 18:22:13,786 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 5.226
2024-06-21 18:22:14,084 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 6.589
2024-06-21 18:22:14,469 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 4.943
2024-06-21 18:22:14,769 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 5.790
2024-06-21 18:22:15,496 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 5.181
2024-06-21 18:22:15,787 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 5.882
2024-06-21 18:22:16,160 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 5.076
2024-06-21 18:22:16,463 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 5.033
2024-06-21 18:22:16,902 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 5.520
2024-06-21 18:22:17,202 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 4.023
2024-06-21 18:22:17,595 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 4.726
2024-06-21 18:22:17,894 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 5.459
2024-06-21 18:22:18,326 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 5.014
2024-06-21 18:22:18,623 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 5.600
2024-06-21 18:22:19,006 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 5.167
2024-06-21 18:22:19,298 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 5.223
2024-06-21 18:22:19,727 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 5.452
2024-06-21 18:22:20,026 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 5.153
2024-06-21 18:22:20,410 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 6.620
2024-06-21 18:22:20,710 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 5.565
2024-06-21 18:22:21,131 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 4.911
2024-06-21 18:22:21,427 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 5.403
2024-06-21 18:22:21,810 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 4.948
2024-06-21 18:22:22,106 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 4.379
2024-06-21 18:22:22,316 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 5.083
2024-06-21 18:22:33,095 - INFO: 6/200 final results:
2024-06-21 18:22:33,095 - INFO: Training loss: 5.274.
2024-06-21 18:22:33,096 - INFO: Training MAE: 5.278.
2024-06-21 18:22:33,096 - INFO: Training MSE: 45.363.
2024-06-21 18:22:53,797 - INFO: Epoch: 6/200, Loss_train: 5.274312002905484, Loss_val: 4.963181479223843
2024-06-21 18:22:53,816 - INFO: Saved new best metric model for epoch 6.
2024-06-21 18:22:53,816 - INFO: Best internal validation val_loss: 4.963 at epoch: 6.
2024-06-21 18:22:53,817 - INFO: Epoch 7/200...
2024-06-21 18:22:53,817 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:22:53,817 - INFO: Batch size: 32.
2024-06-21 18:22:53,820 - INFO: Dataset:
2024-06-21 18:22:53,820 - INFO: Batch size:
2024-06-21 18:22:53,820 - INFO: Number of workers:
2024-06-21 18:22:54,968 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 4.291
2024-06-21 18:22:55,287 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 4.565
2024-06-21 18:22:55,680 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 4.177
2024-06-21 18:22:55,997 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 4.843
2024-06-21 18:22:56,390 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 4.565
2024-06-21 18:22:56,708 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 5.558
2024-06-21 18:22:57,105 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 4.130
2024-06-21 18:22:57,418 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 4.556
2024-06-21 18:22:57,805 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 5.155
2024-06-21 18:22:58,109 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 5.051
2024-06-21 18:22:58,493 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 4.500
2024-06-21 18:22:58,807 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 4.530
2024-06-21 18:22:59,212 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 4.626
2024-06-21 18:22:59,526 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 4.598
2024-06-21 18:22:59,930 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 4.773
2024-06-21 18:23:00,240 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 5.089
2024-06-21 18:23:00,634 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 5.160
2024-06-21 18:23:00,948 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 5.976
2024-06-21 18:23:01,348 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 5.843
2024-06-21 18:23:01,656 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 5.267
2024-06-21 18:23:02,054 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 5.405
2024-06-21 18:23:02,369 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 4.708
2024-06-21 18:23:02,770 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 5.124
2024-06-21 18:23:03,085 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 4.723
2024-06-21 18:23:03,482 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 5.338
2024-06-21 18:23:03,792 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 5.288
2024-06-21 18:23:04,190 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 4.795
2024-06-21 18:23:04,502 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 4.895
2024-06-21 18:23:04,724 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 5.352
2024-06-21 18:23:15,997 - INFO: 7/200 final results:
2024-06-21 18:23:15,997 - INFO: Training loss: 4.927.
2024-06-21 18:23:15,997 - INFO: Training MAE: 4.918.
2024-06-21 18:23:15,997 - INFO: Training MSE: 40.393.
2024-06-21 18:23:36,416 - INFO: Epoch: 7/200, Loss_train: 4.926877317757442, Loss_val: 5.3519438217426165
2024-06-21 18:23:36,416 - INFO: Best internal validation val_loss: 4.963 at epoch: 6.
2024-06-21 18:23:36,416 - INFO: Epoch 8/200...
2024-06-21 18:23:36,416 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:23:36,416 - INFO: Batch size: 32.
2024-06-21 18:23:36,420 - INFO: Dataset:
2024-06-21 18:23:36,420 - INFO: Batch size:
2024-06-21 18:23:36,421 - INFO: Number of workers:
2024-06-21 18:23:37,577 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 5.507
2024-06-21 18:23:37,898 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 5.983
2024-06-21 18:23:38,320 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 4.761
2024-06-21 18:23:38,627 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 4.916
2024-06-21 18:23:39,041 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 4.293
2024-06-21 18:23:39,343 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 4.583
2024-06-21 18:23:39,757 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 4.993
2024-06-21 18:23:40,060 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 4.335
2024-06-21 18:23:40,451 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 4.684
2024-06-21 18:23:40,744 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 4.358
2024-06-21 18:23:41,152 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 4.374
2024-06-21 18:23:41,455 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 4.422
2024-06-21 18:23:41,877 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 4.445
2024-06-21 18:23:42,180 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 4.652
2024-06-21 18:23:42,604 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 4.784
2024-06-21 18:23:42,903 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 3.944
2024-06-21 18:23:43,306 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 6.206
2024-06-21 18:23:43,605 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 4.878
2024-06-21 18:23:44,025 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 4.743
2024-06-21 18:23:44,318 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 4.381
2024-06-21 18:23:44,715 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 4.398
2024-06-21 18:23:45,017 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 4.544
2024-06-21 18:23:45,438 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 4.078
2024-06-21 18:23:45,740 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 4.372
2024-06-21 18:23:46,139 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 4.914
2024-06-21 18:23:46,436 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 4.919
2024-06-21 18:23:46,853 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 5.414
2024-06-21 18:23:47,153 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 4.110
2024-06-21 18:23:47,367 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 5.313
2024-06-21 18:23:58,555 - INFO: 8/200 final results:
2024-06-21 18:23:58,555 - INFO: Training loss: 4.735.
2024-06-21 18:23:58,555 - INFO: Training MAE: 4.723.
2024-06-21 18:23:58,555 - INFO: Training MSE: 37.264.
2024-06-21 18:24:18,795 - INFO: Epoch: 8/200, Loss_train: 4.734710808457999, Loss_val: 4.725787976692462
2024-06-21 18:24:18,814 - INFO: Saved new best metric model for epoch 8.
2024-06-21 18:24:18,814 - INFO: Best internal validation val_loss: 4.726 at epoch: 8.
2024-06-21 18:24:18,814 - INFO: Epoch 9/200...
2024-06-21 18:24:18,814 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:24:18,814 - INFO: Batch size: 32.
2024-06-21 18:24:18,818 - INFO: Dataset:
2024-06-21 18:24:18,818 - INFO: Batch size:
2024-06-21 18:24:18,818 - INFO: Number of workers:
2024-06-21 18:24:19,990 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 4.459
2024-06-21 18:24:20,308 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 4.218
2024-06-21 18:24:20,702 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 4.095
2024-06-21 18:24:21,017 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 4.644
2024-06-21 18:24:21,426 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 4.410
2024-06-21 18:24:21,738 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 4.925
2024-06-21 18:24:22,124 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 5.219
2024-06-21 18:24:22,437 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 4.206
2024-06-21 18:24:22,845 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 4.721
2024-06-21 18:24:23,150 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 4.241
2024-06-21 18:24:23,526 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 4.369
2024-06-21 18:24:23,841 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 4.149
2024-06-21 18:24:24,251 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 4.350
2024-06-21 18:24:24,569 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 4.216
2024-06-21 18:24:24,966 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 4.675
2024-06-21 18:24:25,280 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 4.121
2024-06-21 18:24:25,685 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 4.214
2024-06-21 18:24:25,998 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 4.403
2024-06-21 18:24:26,386 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 4.585
2024-06-21 18:24:26,694 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 3.994
2024-06-21 18:24:27,094 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 4.356
2024-06-21 18:24:27,410 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 4.335
2024-06-21 18:24:27,795 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 3.744
2024-06-21 18:24:28,111 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 3.780
2024-06-21 18:24:28,516 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 3.970
2024-06-21 18:24:28,827 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 4.343
2024-06-21 18:24:29,211 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 5.241
2024-06-21 18:24:29,523 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 4.391
2024-06-21 18:24:29,741 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 5.016
2024-06-21 18:24:40,785 - INFO: 9/200 final results:
2024-06-21 18:24:40,785 - INFO: Training loss: 4.393.
2024-06-21 18:24:40,785 - INFO: Training MAE: 4.380.
2024-06-21 18:24:40,785 - INFO: Training MSE: 33.176.
2024-06-21 18:25:01,116 - INFO: Epoch: 9/200, Loss_train: 4.39270595846505, Loss_val: 8.554491273288068
2024-06-21 18:25:01,116 - INFO: Best internal validation val_loss: 4.726 at epoch: 8.
2024-06-21 18:25:01,116 - INFO: Epoch 10/200...
2024-06-21 18:25:01,116 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:25:01,116 - INFO: Batch size: 32.
2024-06-21 18:25:01,120 - INFO: Dataset:
2024-06-21 18:25:01,120 - INFO: Batch size:
2024-06-21 18:25:01,120 - INFO: Number of workers:
2024-06-21 18:25:02,283 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 6.062
2024-06-21 18:25:02,606 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 5.254
2024-06-21 18:25:03,020 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 5.128
2024-06-21 18:25:03,327 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 4.530
2024-06-21 18:25:03,730 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 4.605
2024-06-21 18:25:04,031 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 4.549
2024-06-21 18:25:04,440 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 3.921
2024-06-21 18:25:04,741 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 4.439
2024-06-21 18:25:05,144 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 4.562
2024-06-21 18:25:05,437 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 4.091
2024-06-21 18:25:05,839 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 4.492
2024-06-21 18:25:06,142 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 4.054
2024-06-21 18:25:06,562 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 4.640
2024-06-21 18:25:06,865 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 4.335
2024-06-21 18:25:07,293 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 4.555
2024-06-21 18:25:07,592 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 4.342
2024-06-21 18:25:07,998 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 4.445
2024-06-21 18:25:08,297 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 4.494
2024-06-21 18:25:08,720 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 4.234
2024-06-21 18:25:09,014 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 5.600
2024-06-21 18:25:09,413 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 4.426
2024-06-21 18:25:09,715 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 4.352
2024-06-21 18:25:10,137 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 4.460
2024-06-21 18:25:10,438 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 3.495
2024-06-21 18:25:10,836 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 4.450
2024-06-21 18:25:11,133 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 4.583
2024-06-21 18:25:11,548 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 4.876
2024-06-21 18:25:11,846 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 3.979
2024-06-21 18:25:12,060 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 4.371
2024-06-21 18:25:23,174 - INFO: 10/200 final results:
2024-06-21 18:25:23,174 - INFO: Training loss: 4.528.
2024-06-21 18:25:23,174 - INFO: Training MAE: 4.532.
2024-06-21 18:25:23,174 - INFO: Training MSE: 34.887.
2024-06-21 18:25:43,457 - INFO: Epoch: 10/200, Loss_train: 4.528474577542009, Loss_val: 5.253462824328192
2024-06-21 18:25:43,457 - INFO: Best internal validation val_loss: 4.726 at epoch: 8.
2024-06-21 18:25:43,457 - INFO: Epoch 11/200...
2024-06-21 18:25:43,457 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:25:43,457 - INFO: Batch size: 32.
2024-06-21 18:25:43,461 - INFO: Dataset:
2024-06-21 18:25:43,461 - INFO: Batch size:
2024-06-21 18:25:43,461 - INFO: Number of workers:
2024-06-21 18:25:44,619 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 4.279
2024-06-21 18:25:44,953 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 4.072
2024-06-21 18:25:45,353 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 4.289
2024-06-21 18:25:45,673 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 4.473
2024-06-21 18:25:46,077 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 4.455
2024-06-21 18:25:46,403 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 4.844
2024-06-21 18:25:46,796 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 4.591
2024-06-21 18:25:47,112 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 4.315
2024-06-21 18:25:47,504 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 4.013
2024-06-21 18:25:47,832 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 4.284
2024-06-21 18:25:48,210 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 4.406
2024-06-21 18:25:48,527 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 4.175
2024-06-21 18:25:48,938 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 5.300
2024-06-21 18:25:49,267 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 5.145
2024-06-21 18:25:49,665 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 5.090
2024-06-21 18:25:49,980 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 4.354
2024-06-21 18:25:50,386 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 4.539
2024-06-21 18:25:50,712 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 4.709
2024-06-21 18:25:51,097 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 3.628
2024-06-21 18:25:51,405 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 3.694
2024-06-21 18:25:51,796 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 4.349
2024-06-21 18:25:52,121 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 3.899
2024-06-21 18:25:52,494 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 3.709
2024-06-21 18:25:52,806 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 3.967
2024-06-21 18:25:53,195 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 4.081
2024-06-21 18:25:53,518 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 3.925
2024-06-21 18:25:53,902 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 5.809
2024-06-21 18:25:54,212 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 4.400
2024-06-21 18:25:54,430 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 4.280
2024-06-21 18:26:05,560 - INFO: 11/200 final results:
2024-06-21 18:26:05,560 - INFO: Training loss: 4.382.
2024-06-21 18:26:05,560 - INFO: Training MAE: 4.384.
2024-06-21 18:26:05,560 - INFO: Training MSE: 32.898.
2024-06-21 18:26:26,294 - INFO: Epoch: 11/200, Loss_train: 4.381894399379862, Loss_val: 11.376858974325247
2024-06-21 18:26:26,294 - INFO: Best internal validation val_loss: 4.726 at epoch: 8.
2024-06-21 18:26:26,294 - INFO: Epoch 12/200...
2024-06-21 18:26:26,294 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:26:26,294 - INFO: Batch size: 32.
2024-06-21 18:26:26,299 - INFO: Dataset:
2024-06-21 18:26:26,299 - INFO: Batch size:
2024-06-21 18:26:26,299 - INFO: Number of workers:
2024-06-21 18:26:27,469 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 4.421
2024-06-21 18:26:27,774 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 3.955
2024-06-21 18:26:28,193 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 3.735
2024-06-21 18:26:28,497 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 4.074
2024-06-21 18:26:28,903 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 4.239
2024-06-21 18:26:29,201 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 4.863
2024-06-21 18:26:29,611 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 3.957
2024-06-21 18:26:29,912 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 4.318
2024-06-21 18:26:30,320 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 3.727
2024-06-21 18:26:30,612 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 4.317
2024-06-21 18:26:31,015 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 4.629
2024-06-21 18:26:31,316 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 4.297
2024-06-21 18:26:31,728 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 3.925
2024-06-21 18:26:32,029 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 4.112
2024-06-21 18:26:32,455 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 4.442
2024-06-21 18:26:32,752 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 4.375
2024-06-21 18:26:33,147 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 4.419
2024-06-21 18:26:33,443 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 4.263
2024-06-21 18:26:33,859 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 4.663
2024-06-21 18:26:34,151 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 4.021
2024-06-21 18:26:34,538 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 4.395
2024-06-21 18:26:34,838 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 4.063
2024-06-21 18:26:35,248 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 3.934
2024-06-21 18:26:35,546 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 3.889
2024-06-21 18:26:35,934 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 3.878
2024-06-21 18:26:36,229 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 4.334
2024-06-21 18:26:36,637 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 3.921
2024-06-21 18:26:36,932 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 3.648
2024-06-21 18:26:37,139 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 4.693
2024-06-21 18:26:48,208 - INFO: 12/200 final results:
2024-06-21 18:26:48,208 - INFO: Training loss: 4.190.
2024-06-21 18:26:48,208 - INFO: Training MAE: 4.180.
2024-06-21 18:26:48,208 - INFO: Training MSE: 30.068.
2024-06-21 18:27:08,376 - INFO: Epoch: 12/200, Loss_train: 4.189846466327536, Loss_val: 4.397306072300878
2024-06-21 18:27:08,394 - INFO: Saved new best metric model for epoch 12.
2024-06-21 18:27:08,395 - INFO: Best internal validation val_loss: 4.397 at epoch: 12.
2024-06-21 18:27:08,395 - INFO: Epoch 13/200...
2024-06-21 18:27:08,395 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:27:08,395 - INFO: Batch size: 32.
2024-06-21 18:27:08,399 - INFO: Dataset:
2024-06-21 18:27:08,399 - INFO: Batch size:
2024-06-21 18:27:08,399 - INFO: Number of workers:
2024-06-21 18:27:09,569 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 3.356
2024-06-21 18:27:09,887 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 4.877
2024-06-21 18:27:10,283 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 3.893
2024-06-21 18:27:10,599 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 3.791
2024-06-21 18:27:11,002 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 3.800
2024-06-21 18:27:11,313 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 4.320
2024-06-21 18:27:11,690 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 4.270
2024-06-21 18:27:12,005 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 3.943
2024-06-21 18:27:12,408 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 4.230
2024-06-21 18:27:12,714 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 3.399
2024-06-21 18:27:13,090 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 3.910
2024-06-21 18:27:13,406 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 3.611
2024-06-21 18:27:13,820 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 4.386
2024-06-21 18:27:14,137 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 3.542
2024-06-21 18:27:14,535 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 3.994
2024-06-21 18:27:14,848 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 4.216
2024-06-21 18:27:15,252 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 4.882
2024-06-21 18:27:15,563 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 3.571
2024-06-21 18:27:15,941 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 3.864
2024-06-21 18:27:16,245 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 4.333
2024-06-21 18:27:16,641 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 3.768
2024-06-21 18:27:16,952 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 4.565
2024-06-21 18:27:17,324 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 4.430
2024-06-21 18:27:17,635 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 5.176
2024-06-21 18:27:18,036 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 3.577
2024-06-21 18:27:18,347 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 3.800
2024-06-21 18:27:18,732 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 5.133
2024-06-21 18:27:19,043 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 4.443
2024-06-21 18:27:19,252 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 3.007
2024-06-21 18:27:30,550 - INFO: 13/200 final results:
2024-06-21 18:27:30,550 - INFO: Training loss: 4.072.
2024-06-21 18:27:30,550 - INFO: Training MAE: 4.093.
2024-06-21 18:27:30,550 - INFO: Training MSE: 29.157.
2024-06-21 18:27:50,616 - INFO: Epoch: 13/200, Loss_train: 4.0719442203127105, Loss_val: 9.521311431095517
2024-06-21 18:27:50,616 - INFO: Best internal validation val_loss: 4.397 at epoch: 12.
2024-06-21 18:27:50,616 - INFO: Epoch 14/200...
2024-06-21 18:27:50,616 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:27:50,616 - INFO: Batch size: 32.
2024-06-21 18:27:50,621 - INFO: Dataset:
2024-06-21 18:27:50,621 - INFO: Batch size:
2024-06-21 18:27:50,621 - INFO: Number of workers:
2024-06-21 18:27:51,789 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 4.435
2024-06-21 18:27:52,096 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 4.182
2024-06-21 18:27:52,507 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 4.882
2024-06-21 18:27:52,812 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 4.243
2024-06-21 18:27:53,216 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 5.093
2024-06-21 18:27:53,518 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 5.107
2024-06-21 18:27:53,923 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 4.096
2024-06-21 18:27:54,225 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 3.773
2024-06-21 18:27:54,622 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 4.375
2024-06-21 18:27:54,916 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 4.363
2024-06-21 18:27:55,304 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 3.500
2024-06-21 18:27:55,608 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 3.580
2024-06-21 18:27:56,012 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 3.327
2024-06-21 18:27:56,316 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 3.659
2024-06-21 18:27:56,742 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 3.498
2024-06-21 18:27:57,042 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 3.966
2024-06-21 18:27:57,433 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 4.082
2024-06-21 18:27:57,733 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 3.549
2024-06-21 18:27:58,153 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 4.064
2024-06-21 18:27:58,448 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 3.977
2024-06-21 18:27:58,849 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 3.937
2024-06-21 18:27:59,153 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 3.905
2024-06-21 18:27:59,571 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 3.239
2024-06-21 18:27:59,875 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 4.040
2024-06-21 18:28:00,266 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 3.205
2024-06-21 18:28:00,565 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 3.664
2024-06-21 18:28:00,982 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 3.573
2024-06-21 18:28:01,281 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 3.815
2024-06-21 18:28:01,494 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 3.346
2024-06-21 18:28:12,665 - INFO: 14/200 final results:
2024-06-21 18:28:12,665 - INFO: Training loss: 3.948.
2024-06-21 18:28:12,665 - INFO: Training MAE: 3.959.
2024-06-21 18:28:12,665 - INFO: Training MSE: 27.185.
2024-06-21 18:28:33,431 - INFO: Epoch: 14/200, Loss_train: 3.9475208890849145, Loss_val: 5.071633766437399
2024-06-21 18:28:33,431 - INFO: Best internal validation val_loss: 4.397 at epoch: 12.
2024-06-21 18:28:33,431 - INFO: Epoch 15/200...
2024-06-21 18:28:33,431 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:28:33,431 - INFO: Batch size: 32.
2024-06-21 18:28:33,435 - INFO: Dataset:
2024-06-21 18:28:33,435 - INFO: Batch size:
2024-06-21 18:28:33,435 - INFO: Number of workers:
2024-06-21 18:28:34,614 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 4.869
2024-06-21 18:28:34,919 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 3.992
2024-06-21 18:28:35,329 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 4.296
2024-06-21 18:28:35,646 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 4.502
2024-06-21 18:28:36,053 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 4.171
2024-06-21 18:28:36,351 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 4.220
2024-06-21 18:28:36,747 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 3.502
2024-06-21 18:28:37,059 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 3.452
2024-06-21 18:28:37,459 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 3.914
2024-06-21 18:28:37,751 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 3.563
2024-06-21 18:28:38,134 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 4.064
2024-06-21 18:28:38,447 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 4.126
2024-06-21 18:28:38,861 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 3.881
2024-06-21 18:28:39,162 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 3.457
2024-06-21 18:28:39,566 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 3.995
2024-06-21 18:28:39,875 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 3.838
2024-06-21 18:28:40,292 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 3.927
2024-06-21 18:28:40,593 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 3.765
2024-06-21 18:28:40,978 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 3.406
2024-06-21 18:28:41,287 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 3.819
2024-06-21 18:28:41,691 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 3.894
2024-06-21 18:28:41,993 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 3.321
2024-06-21 18:28:42,390 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 3.922
2024-06-21 18:28:42,705 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 4.007
2024-06-21 18:28:43,108 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 3.694
2024-06-21 18:28:43,406 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 4.175
2024-06-21 18:28:43,785 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 3.735
2024-06-21 18:28:44,095 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 4.032
2024-06-21 18:28:44,308 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 3.635
2024-06-21 18:28:55,563 - INFO: 15/200 final results:
2024-06-21 18:28:55,563 - INFO: Training loss: 3.903.
2024-06-21 18:28:55,563 - INFO: Training MAE: 3.908.
2024-06-21 18:28:55,563 - INFO: Training MSE: 26.850.
2024-06-21 18:29:15,941 - INFO: Epoch: 15/200, Loss_train: 3.902545139707368, Loss_val: 4.508009737935559
2024-06-21 18:29:15,941 - INFO: Best internal validation val_loss: 4.397 at epoch: 12.
2024-06-21 18:29:15,941 - INFO: Epoch 16/200...
2024-06-21 18:29:15,941 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:29:15,941 - INFO: Batch size: 32.
2024-06-21 18:29:15,945 - INFO: Dataset:
2024-06-21 18:29:15,946 - INFO: Batch size:
2024-06-21 18:29:15,946 - INFO: Number of workers:
2024-06-21 18:29:17,143 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 3.739
2024-06-21 18:29:17,448 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 3.861
2024-06-21 18:29:17,833 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 3.505
2024-06-21 18:29:18,150 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 3.602
2024-06-21 18:29:18,568 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 3.874
2024-06-21 18:29:18,867 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 3.452
2024-06-21 18:29:19,241 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 2.860
2024-06-21 18:29:19,552 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 4.151
2024-06-21 18:29:19,974 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 3.368
2024-06-21 18:29:20,265 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 3.566
2024-06-21 18:29:20,630 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 4.025
2024-06-21 18:29:20,948 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 3.486
2024-06-21 18:29:21,383 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 3.600
2024-06-21 18:29:21,688 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 3.789
2024-06-21 18:29:22,086 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 4.122
2024-06-21 18:29:22,399 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 3.468
2024-06-21 18:29:22,831 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 3.023
2024-06-21 18:29:23,131 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 3.471
2024-06-21 18:29:23,514 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 3.992
2024-06-21 18:29:23,822 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 4.496
2024-06-21 18:29:24,240 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 3.561
2024-06-21 18:29:24,543 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 3.614
2024-06-21 18:29:24,929 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 3.449
2024-06-21 18:29:25,245 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 3.706
2024-06-21 18:29:25,666 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 3.236
2024-06-21 18:29:25,965 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 4.539
2024-06-21 18:29:26,343 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 3.245
2024-06-21 18:29:26,653 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 4.047
2024-06-21 18:29:26,869 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 4.925
2024-06-21 18:29:38,074 - INFO: 16/200 final results:
2024-06-21 18:29:38,074 - INFO: Training loss: 3.716.
2024-06-21 18:29:38,074 - INFO: Training MAE: 3.692.
2024-06-21 18:29:38,074 - INFO: Training MSE: 24.058.
2024-06-21 18:29:58,672 - INFO: Epoch: 16/200, Loss_train: 3.7162722143633613, Loss_val: 247.29786945211475
2024-06-21 18:29:58,672 - INFO: Best internal validation val_loss: 4.397 at epoch: 12.
2024-06-21 18:29:58,672 - INFO: Epoch 17/200...
2024-06-21 18:29:58,672 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:29:58,672 - INFO: Batch size: 32.
2024-06-21 18:29:58,676 - INFO: Dataset:
2024-06-21 18:29:58,676 - INFO: Batch size:
2024-06-21 18:29:58,676 - INFO: Number of workers:
2024-06-21 18:29:59,857 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 7.491
2024-06-21 18:30:00,178 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 4.380
2024-06-21 18:30:00,562 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 5.414
2024-06-21 18:30:00,881 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 4.113
2024-06-21 18:30:01,275 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 4.803
2024-06-21 18:30:01,603 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 4.759
2024-06-21 18:30:01,975 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 4.544
2024-06-21 18:30:02,289 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 3.993
2024-06-21 18:30:02,702 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 3.731
2024-06-21 18:30:02,996 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 3.582
2024-06-21 18:30:03,358 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 3.213
2024-06-21 18:30:03,675 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 3.120
2024-06-21 18:30:04,097 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 3.600
2024-06-21 18:30:04,401 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 4.079
2024-06-21 18:30:04,785 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 4.117
2024-06-21 18:30:05,098 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 3.790
2024-06-21 18:30:05,512 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 4.115
2024-06-21 18:30:05,812 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 3.775
2024-06-21 18:30:06,183 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 3.544
2024-06-21 18:30:06,490 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 3.543
2024-06-21 18:30:06,895 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 3.213
2024-06-21 18:30:07,197 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 3.511
2024-06-21 18:30:07,571 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 3.534
2024-06-21 18:30:07,886 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 4.571
2024-06-21 18:30:08,301 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 3.994
2024-06-21 18:30:08,598 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 3.240
2024-06-21 18:30:08,966 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 4.498
2024-06-21 18:30:09,275 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 3.278
2024-06-21 18:30:09,483 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 3.451
2024-06-21 18:30:20,674 - INFO: 17/200 final results:
2024-06-21 18:30:20,674 - INFO: Training loss: 4.034.
2024-06-21 18:30:20,674 - INFO: Training MAE: 4.046.
2024-06-21 18:30:20,674 - INFO: Training MSE: 28.857.
2024-06-21 18:30:40,735 - INFO: Epoch: 17/200, Loss_train: 4.034337800124596, Loss_val: 4.272346513024692
2024-06-21 18:30:40,755 - INFO: Saved new best metric model for epoch 17.
2024-06-21 18:30:40,755 - INFO: Best internal validation val_loss: 4.272 at epoch: 17.
2024-06-21 18:30:40,755 - INFO: Epoch 18/200...
2024-06-21 18:30:40,755 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:30:40,755 - INFO: Batch size: 32.
2024-06-21 18:30:40,759 - INFO: Dataset:
2024-06-21 18:30:40,759 - INFO: Batch size:
2024-06-21 18:30:40,760 - INFO: Number of workers:
2024-06-21 18:30:41,946 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 3.932
2024-06-21 18:30:42,253 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 3.546
2024-06-21 18:30:42,644 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 3.437
2024-06-21 18:30:42,962 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 3.063
2024-06-21 18:30:43,397 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 3.291
2024-06-21 18:30:43,697 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 2.988
2024-06-21 18:30:44,080 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 3.632
2024-06-21 18:30:44,380 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 3.420
2024-06-21 18:30:44,827 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 2.808
2024-06-21 18:30:45,119 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 3.076
2024-06-21 18:30:45,492 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 3.585
2024-06-21 18:30:45,797 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 3.276
2024-06-21 18:30:46,242 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 3.722
2024-06-21 18:30:46,547 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 3.633
2024-06-21 18:30:46,945 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 4.057
2024-06-21 18:30:47,247 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 3.522
2024-06-21 18:30:47,687 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 3.848
2024-06-21 18:30:47,988 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 3.196
2024-06-21 18:30:48,374 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 3.005
2024-06-21 18:30:48,668 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 3.547
2024-06-21 18:30:49,097 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 3.047
2024-06-21 18:30:49,401 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 3.391
2024-06-21 18:30:49,775 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 4.194
2024-06-21 18:30:50,077 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 3.352
2024-06-21 18:30:50,496 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 3.849
2024-06-21 18:30:50,791 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 3.443
2024-06-21 18:30:51,157 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 3.530
2024-06-21 18:30:51,453 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 3.213
2024-06-21 18:30:51,665 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 2.979
2024-06-21 18:31:02,792 - INFO: 18/200 final results:
2024-06-21 18:31:02,792 - INFO: Training loss: 3.434.
2024-06-21 18:31:02,792 - INFO: Training MAE: 3.443.
2024-06-21 18:31:02,792 - INFO: Training MSE: 21.737.
2024-06-21 18:31:23,222 - INFO: Epoch: 18/200, Loss_train: 3.4339756143504174, Loss_val: 4.096361604230157
2024-06-21 18:31:23,240 - INFO: Saved new best metric model for epoch 18.
2024-06-21 18:31:23,240 - INFO: Best internal validation val_loss: 4.096 at epoch: 18.
2024-06-21 18:31:23,240 - INFO: Epoch 19/200...
2024-06-21 18:31:23,241 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:31:23,241 - INFO: Batch size: 32.
2024-06-21 18:31:23,245 - INFO: Dataset:
2024-06-21 18:31:23,245 - INFO: Batch size:
2024-06-21 18:31:23,245 - INFO: Number of workers:
2024-06-21 18:31:24,434 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 3.022
2024-06-21 18:31:24,771 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 3.037
2024-06-21 18:31:25,168 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 3.558
2024-06-21 18:31:25,487 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 3.130
2024-06-21 18:31:25,903 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 2.598
2024-06-21 18:31:26,218 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 3.123
2024-06-21 18:31:26,608 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 2.933
2024-06-21 18:31:26,923 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 4.745
2024-06-21 18:31:27,341 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 3.484
2024-06-21 18:31:27,646 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 3.585
2024-06-21 18:31:28,026 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 2.768
2024-06-21 18:31:28,343 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 3.234
2024-06-21 18:31:28,765 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 3.196
2024-06-21 18:31:29,082 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 3.288
2024-06-21 18:31:29,481 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 3.572
2024-06-21 18:31:29,794 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 3.243
2024-06-21 18:31:30,214 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 3.638
2024-06-21 18:31:30,527 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 3.706
2024-06-21 18:31:30,916 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 3.300
2024-06-21 18:31:31,223 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 2.860
2024-06-21 18:31:31,631 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 3.959
2024-06-21 18:31:31,947 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 3.054
2024-06-21 18:31:32,331 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 3.425
2024-06-21 18:31:32,646 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 2.921
2024-06-21 18:31:33,050 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 3.202
2024-06-21 18:31:33,361 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 3.126
2024-06-21 18:31:33,747 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 2.647
2024-06-21 18:31:34,057 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 3.170
2024-06-21 18:31:34,279 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 2.656
2024-06-21 18:31:45,431 - INFO: 19/200 final results:
2024-06-21 18:31:45,431 - INFO: Training loss: 3.248.
2024-06-21 18:31:45,431 - INFO: Training MAE: 3.259.
2024-06-21 18:31:45,431 - INFO: Training MSE: 19.342.
2024-06-21 18:32:05,830 - INFO: Epoch: 19/200, Loss_train: 3.247633539397141, Loss_val: 3.875271303900357
2024-06-21 18:32:05,848 - INFO: Saved new best metric model for epoch 19.
2024-06-21 18:32:05,848 - INFO: Best internal validation val_loss: 3.875 at epoch: 19.
2024-06-21 18:32:05,849 - INFO: Epoch 20/200...
2024-06-21 18:32:05,849 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:32:05,849 - INFO: Batch size: 32.
2024-06-21 18:32:05,853 - INFO: Dataset:
2024-06-21 18:32:05,853 - INFO: Batch size:
2024-06-21 18:32:05,853 - INFO: Number of workers:
2024-06-21 18:32:07,012 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 2.802
2024-06-21 18:32:07,329 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 3.477
2024-06-21 18:32:07,739 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 3.149
2024-06-21 18:32:08,059 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 3.539
2024-06-21 18:32:08,463 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 3.558
2024-06-21 18:32:08,778 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 3.549
2024-06-21 18:32:09,179 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 2.672
2024-06-21 18:32:09,495 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 3.255
2024-06-21 18:32:09,889 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 3.450
2024-06-21 18:32:10,196 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 3.948
2024-06-21 18:32:10,584 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 2.959
2024-06-21 18:32:10,902 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 3.617
2024-06-21 18:32:11,308 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 4.037
2024-06-21 18:32:11,623 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 3.526
2024-06-21 18:32:12,026 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 3.124
2024-06-21 18:32:12,338 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 2.763
2024-06-21 18:32:12,742 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 3.237
2024-06-21 18:32:13,052 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 2.958
2024-06-21 18:32:13,442 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 3.052
2024-06-21 18:32:13,751 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 3.079
2024-06-21 18:32:14,151 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 3.558
2024-06-21 18:32:14,467 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 3.066
2024-06-21 18:32:14,854 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 2.666
2024-06-21 18:32:15,169 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 3.580
2024-06-21 18:32:15,560 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 2.975
2024-06-21 18:32:15,872 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 3.541
2024-06-21 18:32:16,252 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 2.991
2024-06-21 18:32:16,562 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 2.866
2024-06-21 18:32:16,780 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 2.919
2024-06-21 18:32:27,907 - INFO: 20/200 final results:
2024-06-21 18:32:27,907 - INFO: Training loss: 3.238.
2024-06-21 18:32:27,907 - INFO: Training MAE: 3.245.
2024-06-21 18:32:27,907 - INFO: Training MSE: 19.686.
2024-06-21 18:32:48,358 - INFO: Epoch: 20/200, Loss_train: 3.2384461863287566, Loss_val: 4.227133117873093
2024-06-21 18:32:48,358 - INFO: Best internal validation val_loss: 3.875 at epoch: 19.
2024-06-21 18:32:48,358 - INFO: Epoch 21/200...
2024-06-21 18:32:48,358 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:32:48,358 - INFO: Batch size: 32.
2024-06-21 18:32:48,363 - INFO: Dataset:
2024-06-21 18:32:48,363 - INFO: Batch size:
2024-06-21 18:32:48,363 - INFO: Number of workers:
2024-06-21 18:32:49,548 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 3.673
2024-06-21 18:32:49,854 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 3.330
2024-06-21 18:32:50,252 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 3.199
2024-06-21 18:32:50,570 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 3.219
2024-06-21 18:32:51,000 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 3.389
2024-06-21 18:32:51,302 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 2.781
2024-06-21 18:32:51,691 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 3.101
2024-06-21 18:32:52,005 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 2.997
2024-06-21 18:32:52,437 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 3.300
2024-06-21 18:32:52,730 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 2.871
2024-06-21 18:32:53,109 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 2.693
2024-06-21 18:32:53,425 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 3.170
2024-06-21 18:32:53,858 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 3.371
2024-06-21 18:32:54,160 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 2.894
2024-06-21 18:32:54,556 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 3.536
2024-06-21 18:32:54,867 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 2.794
2024-06-21 18:32:55,297 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 3.769
2024-06-21 18:32:55,596 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 3.415
2024-06-21 18:32:55,982 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 3.046
2024-06-21 18:32:56,287 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 3.346
2024-06-21 18:32:56,709 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 2.939
2024-06-21 18:32:57,009 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 2.640
2024-06-21 18:32:57,391 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 3.053
2024-06-21 18:32:57,703 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 2.639
2024-06-21 18:32:58,115 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 3.362
2024-06-21 18:32:58,410 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 3.603
2024-06-21 18:32:58,792 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 3.025
2024-06-21 18:32:59,101 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 2.972
2024-06-21 18:32:59,320 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 3.295
2024-06-21 18:33:10,478 - INFO: 21/200 final results:
2024-06-21 18:33:10,478 - INFO: Training loss: 3.152.
2024-06-21 18:33:10,478 - INFO: Training MAE: 3.150.
2024-06-21 18:33:10,478 - INFO: Training MSE: 18.584.
2024-06-21 18:33:30,721 - INFO: Epoch: 21/200, Loss_train: 3.1523892386206267, Loss_val: 3.9128456773429083
2024-06-21 18:33:30,721 - INFO: Best internal validation val_loss: 3.875 at epoch: 19.
2024-06-21 18:33:30,721 - INFO: Epoch 22/200...
2024-06-21 18:33:30,721 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:33:30,721 - INFO: Batch size: 32.
2024-06-21 18:33:30,725 - INFO: Dataset:
2024-06-21 18:33:30,725 - INFO: Batch size:
2024-06-21 18:33:30,725 - INFO: Number of workers:
2024-06-21 18:33:31,901 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 3.707
2024-06-21 18:33:32,208 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 3.496
2024-06-21 18:33:32,620 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 2.420
2024-06-21 18:33:32,939 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 2.428
2024-06-21 18:33:33,370 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 2.718
2024-06-21 18:33:33,673 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 2.220
2024-06-21 18:33:34,064 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 2.693
2024-06-21 18:33:34,380 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 3.212
2024-06-21 18:33:34,802 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 2.758
2024-06-21 18:33:35,096 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 3.430
2024-06-21 18:33:35,476 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 3.586
2024-06-21 18:33:35,794 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 2.596
2024-06-21 18:33:36,230 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 3.533
2024-06-21 18:33:36,535 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 3.131
2024-06-21 18:33:36,934 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 2.615
2024-06-21 18:33:37,247 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 2.828
2024-06-21 18:33:37,679 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 3.011
2024-06-21 18:33:37,979 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 2.934
2024-06-21 18:33:38,368 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 2.945
2024-06-21 18:33:38,675 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 2.535
2024-06-21 18:33:39,097 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 3.758
2024-06-21 18:33:39,399 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 2.409
2024-06-21 18:33:39,784 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 2.502
2024-06-21 18:33:40,100 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 3.027
2024-06-21 18:33:40,519 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 2.943
2024-06-21 18:33:40,816 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 2.570
2024-06-21 18:33:41,195 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 2.981
2024-06-21 18:33:41,505 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 3.151
2024-06-21 18:33:41,725 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 2.582
2024-06-21 18:33:52,995 - INFO: 22/200 final results:
2024-06-21 18:33:52,996 - INFO: Training loss: 2.922.
2024-06-21 18:33:52,996 - INFO: Training MAE: 2.928.
2024-06-21 18:33:52,996 - INFO: Training MSE: 16.521.
2024-06-21 18:34:13,341 - INFO: Epoch: 22/200, Loss_train: 2.9215062157861116, Loss_val: 3.786108296493004
2024-06-21 18:34:13,360 - INFO: Saved new best metric model for epoch 22.
2024-06-21 18:34:13,360 - INFO: Best internal validation val_loss: 3.786 at epoch: 22.
2024-06-21 18:34:13,360 - INFO: Epoch 23/200...
2024-06-21 18:34:13,360 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:34:13,360 - INFO: Batch size: 32.
2024-06-21 18:34:13,364 - INFO: Dataset:
2024-06-21 18:34:13,364 - INFO: Batch size:
2024-06-21 18:34:13,364 - INFO: Number of workers:
2024-06-21 18:34:14,523 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 2.564
2024-06-21 18:34:14,845 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 3.247
2024-06-21 18:34:15,253 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 2.354
2024-06-21 18:34:15,572 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 2.777
2024-06-21 18:34:15,982 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 3.114
2024-06-21 18:34:16,283 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 3.395
2024-06-21 18:34:16,677 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 3.080
2024-06-21 18:34:16,992 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 3.899
2024-06-21 18:34:17,390 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 3.154
2024-06-21 18:34:17,683 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 3.155
2024-06-21 18:34:18,071 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 3.840
2024-06-21 18:34:18,387 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 3.206
2024-06-21 18:34:18,791 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 2.799
2024-06-21 18:34:19,096 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 3.290
2024-06-21 18:34:19,502 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 3.685
2024-06-21 18:34:19,816 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 3.468
2024-06-21 18:34:20,219 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 2.839
2024-06-21 18:34:20,520 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 3.140
2024-06-21 18:34:20,913 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 2.454
2024-06-21 18:34:21,221 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 2.389
2024-06-21 18:34:21,632 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 2.694
2024-06-21 18:34:21,935 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 3.109
2024-06-21 18:34:22,333 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 3.929
2024-06-21 18:34:22,649 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 2.694
2024-06-21 18:34:23,048 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 3.351
2024-06-21 18:34:23,346 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 2.843
2024-06-21 18:34:23,738 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 3.545
2024-06-21 18:34:24,050 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 3.449
2024-06-21 18:34:24,265 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 2.684
2024-06-21 18:34:35,307 - INFO: 23/200 final results:
2024-06-21 18:34:35,307 - INFO: Training loss: 3.109.
2024-06-21 18:34:35,307 - INFO: Training MAE: 3.117.
2024-06-21 18:34:35,307 - INFO: Training MSE: 18.049.
2024-06-21 18:34:56,045 - INFO: Epoch: 23/200, Loss_train: 3.1085280796577193, Loss_val: 4.197509749182339
2024-06-21 18:34:56,045 - INFO: Best internal validation val_loss: 3.786 at epoch: 22.
2024-06-21 18:34:56,045 - INFO: Epoch 24/200...
2024-06-21 18:34:56,045 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:34:56,045 - INFO: Batch size: 32.
2024-06-21 18:34:56,049 - INFO: Dataset:
2024-06-21 18:34:56,050 - INFO: Batch size:
2024-06-21 18:34:56,050 - INFO: Number of workers:
2024-06-21 18:34:57,232 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 3.756
2024-06-21 18:34:57,537 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 2.684
2024-06-21 18:34:57,948 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 3.195
2024-06-21 18:34:58,265 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 2.373
2024-06-21 18:34:58,674 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 3.712
2024-06-21 18:34:58,973 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 3.221
2024-06-21 18:34:59,361 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 2.596
2024-06-21 18:34:59,676 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 2.691
2024-06-21 18:35:00,078 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 2.900
2024-06-21 18:35:00,369 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 3.299
2024-06-21 18:35:00,748 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 4.384
2024-06-21 18:35:01,063 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 2.907
2024-06-21 18:35:01,482 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 2.886
2024-06-21 18:35:01,784 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 2.917
2024-06-21 18:35:02,183 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 2.645
2024-06-21 18:35:02,493 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 2.829
2024-06-21 18:35:02,908 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 2.555
2024-06-21 18:35:03,209 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 2.798
2024-06-21 18:35:03,599 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 2.464
2024-06-21 18:35:03,908 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 2.714
2024-06-21 18:35:04,314 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 2.614
2024-06-21 18:35:04,617 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 2.722
2024-06-21 18:35:05,017 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 2.790
2024-06-21 18:35:05,332 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 2.662
2024-06-21 18:35:05,730 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 3.803
2024-06-21 18:35:06,027 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 2.902
2024-06-21 18:35:06,409 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 3.041
2024-06-21 18:35:06,720 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 2.356
2024-06-21 18:35:06,936 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 2.073
2024-06-21 18:35:18,049 - INFO: 24/200 final results:
2024-06-21 18:35:18,049 - INFO: Training loss: 2.913.
2024-06-21 18:35:18,049 - INFO: Training MAE: 2.930.
2024-06-21 18:35:18,049 - INFO: Training MSE: 16.211.
2024-06-21 18:35:38,244 - INFO: Epoch: 24/200, Loss_train: 2.913472635992642, Loss_val: 3.9043644461138496
2024-06-21 18:35:38,244 - INFO: Best internal validation val_loss: 3.786 at epoch: 22.
2024-06-21 18:35:38,244 - INFO: Epoch 25/200...
2024-06-21 18:35:38,244 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:35:38,244 - INFO: Batch size: 32.
2024-06-21 18:35:38,248 - INFO: Dataset:
2024-06-21 18:35:38,248 - INFO: Batch size:
2024-06-21 18:35:38,248 - INFO: Number of workers:
2024-06-21 18:35:39,412 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 3.175
2024-06-21 18:35:39,720 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 2.635
2024-06-21 18:35:40,118 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 2.196
2024-06-21 18:35:40,437 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 2.837
2024-06-21 18:35:40,864 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 3.211
2024-06-21 18:35:41,163 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 2.528
2024-06-21 18:35:41,550 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 2.817
2024-06-21 18:35:42,160 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 3.148
2024-06-21 18:35:42,591 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 3.027
2024-06-21 18:35:42,885 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 2.972
2024-06-21 18:35:43,262 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 2.568
2024-06-21 18:35:43,580 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 2.970
2024-06-21 18:35:44,011 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 3.017
2024-06-21 18:35:44,316 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 2.438
2024-06-21 18:35:44,714 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 2.612
2024-06-21 18:35:45,026 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 2.803
2024-06-21 18:35:45,452 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 3.776
2024-06-21 18:35:45,753 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 3.882
2024-06-21 18:35:46,140 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 2.567
2024-06-21 18:35:46,447 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 3.325
2024-06-21 18:35:46,865 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 2.717
2024-06-21 18:35:47,168 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 2.759
2024-06-21 18:35:47,550 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 2.854
2024-06-21 18:35:47,866 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 2.882
2024-06-21 18:35:48,283 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 2.652
2024-06-21 18:35:48,580 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 3.312
2024-06-21 18:35:48,948 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 2.912
2024-06-21 18:35:49,260 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 2.883
2024-06-21 18:35:49,481 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 2.533
2024-06-21 18:36:00,275 - INFO: 25/200 final results:
2024-06-21 18:36:00,275 - INFO: Training loss: 2.897.
2024-06-21 18:36:00,275 - INFO: Training MAE: 2.904.
2024-06-21 18:36:00,275 - INFO: Training MSE: 15.851.
2024-06-21 18:36:20,867 - INFO: Epoch: 25/200, Loss_train: 2.896798306498034, Loss_val: 3.4774267426852523
2024-06-21 18:36:20,886 - INFO: Saved new best metric model for epoch 25.
2024-06-21 18:36:20,886 - INFO: Best internal validation val_loss: 3.477 at epoch: 25.
2024-06-21 18:36:20,886 - INFO: Epoch 26/200...
2024-06-21 18:36:20,886 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:36:20,886 - INFO: Batch size: 32.
2024-06-21 18:36:20,890 - INFO: Dataset:
2024-06-21 18:36:20,890 - INFO: Batch size:
2024-06-21 18:36:20,890 - INFO: Number of workers:
2024-06-21 18:36:22,044 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 3.658
2024-06-21 18:36:22,366 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 3.008
2024-06-21 18:36:22,773 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 2.850
2024-06-21 18:36:23,093 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 2.375
2024-06-21 18:36:23,502 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 2.536
2024-06-21 18:36:23,803 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 2.955
2024-06-21 18:36:24,203 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 3.025
2024-06-21 18:36:24,518 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 3.111
2024-06-21 18:36:24,918 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 2.270
2024-06-21 18:36:25,212 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 2.854
2024-06-21 18:36:25,600 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 2.768
2024-06-21 18:36:25,918 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 2.843
2024-06-21 18:36:26,323 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 2.891
2024-06-21 18:36:26,624 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 2.782
2024-06-21 18:36:27,028 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 2.453
2024-06-21 18:36:27,338 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 3.087
2024-06-21 18:36:27,776 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 2.445
2024-06-21 18:36:28,088 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 3.266
2024-06-21 18:36:28,477 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 3.221
2024-06-21 18:36:28,785 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 2.683
2024-06-21 18:36:29,189 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 2.587
2024-06-21 18:36:29,493 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 3.736
2024-06-21 18:36:29,885 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 2.933
2024-06-21 18:36:30,201 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 2.754
2024-06-21 18:36:30,607 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 2.554
2024-06-21 18:36:30,906 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 2.883
2024-06-21 18:36:31,295 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 2.182
2024-06-21 18:36:31,606 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 2.997
2024-06-21 18:36:31,827 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 2.627
2024-06-21 18:36:42,900 - INFO: 26/200 final results:
2024-06-21 18:36:42,900 - INFO: Training loss: 2.839.
2024-06-21 18:36:42,900 - INFO: Training MAE: 2.843.
2024-06-21 18:36:42,900 - INFO: Training MSE: 15.723.
2024-06-21 18:37:03,455 - INFO: Epoch: 26/200, Loss_train: 2.8390915887109163, Loss_val: 3.2113576017577072
2024-06-21 18:37:03,474 - INFO: Saved new best metric model for epoch 26.
2024-06-21 18:37:03,475 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:37:03,475 - INFO: Epoch 27/200...
2024-06-21 18:37:03,475 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:37:03,475 - INFO: Batch size: 32.
2024-06-21 18:37:03,478 - INFO: Dataset:
2024-06-21 18:37:03,479 - INFO: Batch size:
2024-06-21 18:37:03,479 - INFO: Number of workers:
2024-06-21 18:37:04,644 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 2.834
2024-06-21 18:37:04,964 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 2.879
2024-06-21 18:37:05,364 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 3.230
2024-06-21 18:37:05,683 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 2.452
2024-06-21 18:37:06,110 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 3.128
2024-06-21 18:37:06,412 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 2.857
2024-06-21 18:37:06,799 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 2.550
2024-06-21 18:37:07,115 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 2.926
2024-06-21 18:37:07,547 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 2.451
2024-06-21 18:37:07,841 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 2.228
2024-06-21 18:37:08,217 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 2.644
2024-06-21 18:37:08,535 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 3.571
2024-06-21 18:37:08,967 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 2.422
2024-06-21 18:37:09,271 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 2.507
2024-06-21 18:37:09,667 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 3.167
2024-06-21 18:37:09,978 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 2.393
2024-06-21 18:37:10,406 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 2.633
2024-06-21 18:37:10,705 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 2.279
2024-06-21 18:37:11,085 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 2.252
2024-06-21 18:37:11,390 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 2.707
2024-06-21 18:37:11,805 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 2.880
2024-06-21 18:37:12,106 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 2.381
2024-06-21 18:37:12,485 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 2.334
2024-06-21 18:37:12,799 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 2.718
2024-06-21 18:37:13,214 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 2.318
2024-06-21 18:37:13,510 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 2.900
2024-06-21 18:37:13,880 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 2.767
2024-06-21 18:37:14,188 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 2.680
2024-06-21 18:37:14,404 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 2.970
2024-06-21 18:37:25,484 - INFO: 27/200 final results:
2024-06-21 18:37:25,484 - INFO: Training loss: 2.692.
2024-06-21 18:37:25,484 - INFO: Training MAE: 2.686.
2024-06-21 18:37:25,484 - INFO: Training MSE: 13.825.
2024-06-21 18:37:46,150 - INFO: Epoch: 27/200, Loss_train: 2.691573669170511, Loss_val: 3.5441129783104204
2024-06-21 18:37:46,150 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:37:46,150 - INFO: Epoch 28/200...
2024-06-21 18:37:46,150 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:37:46,150 - INFO: Batch size: 32.
2024-06-21 18:37:46,154 - INFO: Dataset:
2024-06-21 18:37:46,154 - INFO: Batch size:
2024-06-21 18:37:46,154 - INFO: Number of workers:
2024-06-21 18:37:47,326 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 2.831
2024-06-21 18:37:47,631 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 2.173
2024-06-21 18:37:48,046 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 2.699
2024-06-21 18:37:48,350 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 2.728
2024-06-21 18:37:48,751 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 2.835
2024-06-21 18:37:49,062 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 2.708
2024-06-21 18:37:49,472 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 2.481
2024-06-21 18:37:49,776 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 2.490
2024-06-21 18:37:50,154 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 2.583
2024-06-21 18:37:50,461 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 2.438
2024-06-21 18:37:50,861 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 2.504
2024-06-21 18:37:51,165 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 2.932
2024-06-21 18:37:51,562 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 2.686
2024-06-21 18:37:51,877 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 2.974
2024-06-21 18:37:52,288 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 2.359
2024-06-21 18:37:52,586 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 3.014
2024-06-21 18:37:52,975 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 2.587
2024-06-21 18:37:53,287 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 2.744
2024-06-21 18:37:53,692 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 2.757
2024-06-21 18:37:53,986 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 2.681
2024-06-21 18:37:54,365 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 2.709
2024-06-21 18:37:54,679 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 2.820
2024-06-21 18:37:55,076 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 2.669
2024-06-21 18:37:55,377 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 2.577
2024-06-21 18:37:55,765 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 3.907
2024-06-21 18:37:56,074 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 2.062
2024-06-21 18:37:56,481 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 2.291
2024-06-21 18:37:56,777 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 3.051
2024-06-21 18:37:56,977 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 2.066
2024-06-21 18:38:08,243 - INFO: 28/200 final results:
2024-06-21 18:38:08,243 - INFO: Training loss: 2.667.
2024-06-21 18:38:08,243 - INFO: Training MAE: 2.679.
2024-06-21 18:38:08,243 - INFO: Training MSE: 13.582.
2024-06-21 18:38:28,593 - INFO: Epoch: 28/200, Loss_train: 2.6674467777383737, Loss_val: 3.61149210765444
2024-06-21 18:38:28,593 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:38:28,593 - INFO: Epoch 29/200...
2024-06-21 18:38:28,593 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:38:28,593 - INFO: Batch size: 32.
2024-06-21 18:38:28,597 - INFO: Dataset:
2024-06-21 18:38:28,597 - INFO: Batch size:
2024-06-21 18:38:28,597 - INFO: Number of workers:
2024-06-21 18:38:29,759 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 4.044
2024-06-21 18:38:30,090 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 2.664
2024-06-21 18:38:30,487 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 2.690
2024-06-21 18:38:30,804 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 2.332
2024-06-21 18:38:31,198 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 2.861
2024-06-21 18:38:31,523 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 2.374
2024-06-21 18:38:31,911 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 2.992
2024-06-21 18:38:32,223 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 3.343
2024-06-21 18:38:32,609 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 2.649
2024-06-21 18:38:32,940 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 1.944
2024-06-21 18:38:33,317 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 2.758
2024-06-21 18:38:33,632 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 3.120
2024-06-21 18:38:34,031 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 3.227
2024-06-21 18:38:34,358 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 2.511
2024-06-21 18:38:34,751 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 2.756
2024-06-21 18:38:35,062 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 2.305
2024-06-21 18:38:35,460 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 2.177
2024-06-21 18:38:35,784 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 2.783
2024-06-21 18:38:36,169 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 2.496
2024-06-21 18:38:36,475 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 3.229
2024-06-21 18:38:36,861 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 2.529
2024-06-21 18:38:37,185 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 2.467
2024-06-21 18:38:37,569 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 3.241
2024-06-21 18:38:37,882 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 3.296
2024-06-21 18:38:38,271 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 2.264
2024-06-21 18:38:38,592 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 2.235
2024-06-21 18:38:38,974 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 2.361
2024-06-21 18:38:39,282 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 2.682
2024-06-21 18:38:39,500 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 2.379
2024-06-21 18:38:50,587 - INFO: 29/200 final results:
2024-06-21 18:38:50,587 - INFO: Training loss: 2.714.
2024-06-21 18:38:50,587 - INFO: Training MAE: 2.721.
2024-06-21 18:38:50,587 - INFO: Training MSE: 14.135.
2024-06-21 18:39:11,289 - INFO: Epoch: 29/200, Loss_train: 2.7141048168313913, Loss_val: 3.476116443502492
2024-06-21 18:39:11,289 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:39:11,289 - INFO: Epoch 30/200...
2024-06-21 18:39:11,289 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:39:11,289 - INFO: Batch size: 32.
2024-06-21 18:39:11,294 - INFO: Dataset:
2024-06-21 18:39:11,294 - INFO: Batch size:
2024-06-21 18:39:11,294 - INFO: Number of workers:
2024-06-21 18:39:12,453 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 2.615
2024-06-21 18:39:12,758 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 2.630
2024-06-21 18:39:13,154 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 1.976
2024-06-21 18:39:13,472 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 2.336
2024-06-21 18:39:13,880 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 2.602
2024-06-21 18:39:14,180 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 2.774
2024-06-21 18:39:14,578 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 3.046
2024-06-21 18:39:14,892 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 1.948
2024-06-21 18:39:15,292 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 2.859
2024-06-21 18:39:15,584 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 2.234
2024-06-21 18:39:15,970 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 2.723
2024-06-21 18:39:16,285 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 2.611
2024-06-21 18:39:16,696 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 2.641
2024-06-21 18:39:16,999 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 3.046
2024-06-21 18:39:17,407 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 2.615
2024-06-21 18:39:17,718 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 2.730
2024-06-21 18:39:18,125 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 2.547
2024-06-21 18:39:18,424 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 2.507
2024-06-21 18:39:18,818 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 2.824
2024-06-21 18:39:19,124 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 3.143
2024-06-21 18:39:19,517 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 2.624
2024-06-21 18:39:19,818 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 2.728
2024-06-21 18:39:20,210 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 2.452
2024-06-21 18:39:20,524 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 2.520
2024-06-21 18:39:20,923 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 3.001
2024-06-21 18:39:21,219 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 2.861
2024-06-21 18:39:21,597 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 3.226
2024-06-21 18:39:21,905 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 2.618
2024-06-21 18:39:22,118 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 3.184
2024-06-21 18:39:33,256 - INFO: 30/200 final results:
2024-06-21 18:39:33,256 - INFO: Training loss: 2.677.
2024-06-21 18:39:33,256 - INFO: Training MAE: 2.667.
2024-06-21 18:39:33,256 - INFO: Training MSE: 13.792.
2024-06-21 18:39:53,336 - INFO: Epoch: 30/200, Loss_train: 2.676614966885797, Loss_val: 3.6350641086183746
2024-06-21 18:39:53,336 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:39:53,336 - INFO: Epoch 31/200...
2024-06-21 18:39:53,336 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:39:53,336 - INFO: Batch size: 32.
2024-06-21 18:39:53,340 - INFO: Dataset:
2024-06-21 18:39:53,340 - INFO: Batch size:
2024-06-21 18:39:53,340 - INFO: Number of workers:
2024-06-21 18:39:54,500 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 3.701
2024-06-21 18:39:54,804 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 2.041
2024-06-21 18:39:55,214 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 2.452
2024-06-21 18:39:55,531 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 2.150
2024-06-21 18:39:55,936 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 2.246
2024-06-21 18:39:56,237 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 2.257
2024-06-21 18:39:56,638 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 1.983
2024-06-21 18:39:56,954 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 2.278
2024-06-21 18:39:57,360 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 2.561
2024-06-21 18:39:57,654 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 2.689
2024-06-21 18:39:58,045 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 2.628
2024-06-21 18:39:58,362 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 2.759
2024-06-21 18:39:58,775 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 2.075
2024-06-21 18:39:59,079 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 2.803
2024-06-21 18:39:59,490 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 2.210
2024-06-21 18:39:59,803 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 2.760
2024-06-21 18:40:00,216 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 2.436
2024-06-21 18:40:00,516 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 2.966
2024-06-21 18:40:00,915 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 2.488
2024-06-21 18:40:01,223 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 2.136
2024-06-21 18:40:01,629 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 2.636
2024-06-21 18:40:01,931 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 2.465
2024-06-21 18:40:02,326 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 2.210
2024-06-21 18:40:02,641 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 2.973
2024-06-21 18:40:03,048 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 2.761
2024-06-21 18:40:03,346 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 2.159
2024-06-21 18:40:03,743 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 2.554
2024-06-21 18:40:04,054 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 3.050
2024-06-21 18:40:04,275 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 2.602
2024-06-21 18:40:15,416 - INFO: 31/200 final results:
2024-06-21 18:40:15,416 - INFO: Training loss: 2.518.
2024-06-21 18:40:15,416 - INFO: Training MAE: 2.517.
2024-06-21 18:40:15,416 - INFO: Training MSE: 11.912.
2024-06-21 18:40:36,044 - INFO: Epoch: 31/200, Loss_train: 2.518208565383122, Loss_val: 4.669815310116472
2024-06-21 18:40:36,044 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:40:36,044 - INFO: Epoch 32/200...
2024-06-21 18:40:36,044 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:40:36,044 - INFO: Batch size: 32.
2024-06-21 18:40:36,048 - INFO: Dataset:
2024-06-21 18:40:36,048 - INFO: Batch size:
2024-06-21 18:40:36,048 - INFO: Number of workers:
2024-06-21 18:40:37,237 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 3.279
2024-06-21 18:40:37,541 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 2.406
2024-06-21 18:40:37,932 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 3.122
2024-06-21 18:40:38,247 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 2.251
2024-06-21 18:40:38,664 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 2.835
2024-06-21 18:40:38,962 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 2.422
2024-06-21 18:40:39,341 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 2.916
2024-06-21 18:40:39,653 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 2.176
2024-06-21 18:40:40,068 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 1.976
2024-06-21 18:40:40,359 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 2.370
2024-06-21 18:40:40,719 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 2.933
2024-06-21 18:40:41,032 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 2.858
2024-06-21 18:40:41,451 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 2.526
2024-06-21 18:40:41,751 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 2.358
2024-06-21 18:40:42,138 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 2.408
2024-06-21 18:40:42,448 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 2.763
2024-06-21 18:40:42,865 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 2.584
2024-06-21 18:40:43,162 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 1.849
2024-06-21 18:40:43,536 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 2.003
2024-06-21 18:40:43,840 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 2.522
2024-06-21 18:40:44,254 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 2.102
2024-06-21 18:40:44,552 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 2.584
2024-06-21 18:40:44,933 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 2.140
2024-06-21 18:40:45,244 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 2.674
2024-06-21 18:40:45,658 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 2.677
2024-06-21 18:40:45,953 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 3.468
2024-06-21 18:40:46,335 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 3.184
2024-06-21 18:40:46,643 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 2.357
2024-06-21 18:40:46,859 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 2.145
2024-06-21 18:40:57,959 - INFO: 32/200 final results:
2024-06-21 18:40:57,959 - INFO: Training loss: 2.548.
2024-06-21 18:40:57,959 - INFO: Training MAE: 2.556.
2024-06-21 18:40:57,959 - INFO: Training MSE: 12.182.
2024-06-21 18:41:18,648 - INFO: Epoch: 32/200, Loss_train: 2.54784186955156, Loss_val: 3.7589341525373787
2024-06-21 18:41:18,648 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:41:18,648 - INFO: Epoch 33/200...
2024-06-21 18:41:18,648 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 18:41:18,648 - INFO: Batch size: 32.
2024-06-21 18:41:18,652 - INFO: Dataset:
2024-06-21 18:41:18,652 - INFO: Batch size:
2024-06-21 18:41:18,652 - INFO: Number of workers:
2024-06-21 18:41:19,825 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 2.550
2024-06-21 18:41:20,132 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 2.492
2024-06-21 18:41:20,540 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 2.526
2024-06-21 18:41:20,860 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 2.712
2024-06-21 18:41:21,275 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 2.209
2024-06-21 18:41:21,576 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 2.204
2024-06-21 18:41:21,974 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 2.678
2024-06-21 18:41:22,290 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 2.732
2024-06-21 18:41:22,701 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 2.401
2024-06-21 18:41:22,995 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 2.749
2024-06-21 18:41:23,381 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 2.210
2024-06-21 18:41:23,698 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 1.958
2024-06-21 18:41:24,104 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 2.219
2024-06-21 18:41:24,407 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 2.246
2024-06-21 18:41:24,813 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 2.684
2024-06-21 18:41:25,127 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 2.199
2024-06-21 18:41:25,529 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 2.240
2024-06-21 18:41:25,828 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 2.877
2024-06-21 18:41:26,221 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 2.985
2024-06-21 18:41:26,529 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 2.812
2024-06-21 18:41:26,929 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 2.450
2024-06-21 18:41:27,233 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 2.425
2024-06-21 18:41:27,620 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 2.257
2024-06-21 18:41:27,935 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 2.509
2024-06-21 18:41:28,340 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 2.279
2024-06-21 18:41:28,638 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 2.681
2024-06-21 18:41:29,028 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 2.647
2024-06-21 18:41:29,339 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 2.393
2024-06-21 18:41:29,555 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 2.691
2024-06-21 18:41:40,406 - INFO: 33/200 final results:
2024-06-21 18:41:40,407 - INFO: Training loss: 2.483.
2024-06-21 18:41:40,407 - INFO: Training MAE: 2.479.
2024-06-21 18:41:40,407 - INFO: Training MSE: 11.789.
2024-06-21 18:42:01,001 - INFO: Epoch: 33/200, Loss_train: 2.483267743012001, Loss_val: 3.4339749237586714
2024-06-21 18:42:01,001 - INFO: Best internal validation val_loss: 3.211 at epoch: 26.
2024-06-21 18:42:01,001 - INFO: Epoch 34/200...
2024-06-21 18:42:01,001 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:42:01,001 - INFO: Batch size: 32.
2024-06-21 18:42:01,005 - INFO: Dataset:
2024-06-21 18:42:01,005 - INFO: Batch size:
2024-06-21 18:42:01,005 - INFO: Number of workers:
2024-06-21 18:42:02,198 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 2.765
2024-06-21 18:42:02,502 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 1.552
2024-06-21 18:42:02,893 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 2.123
2024-06-21 18:42:03,197 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 2.137
2024-06-21 18:42:03,633 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 1.862
2024-06-21 18:42:03,933 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 2.122
2024-06-21 18:42:04,320 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 1.977
2024-06-21 18:42:04,621 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 2.218
2024-06-21 18:42:05,068 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 1.947
2024-06-21 18:42:05,360 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 1.843
2024-06-21 18:42:05,728 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 2.284
2024-06-21 18:42:06,030 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 1.975
2024-06-21 18:42:06,475 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 2.234
2024-06-21 18:42:06,780 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 2.137
2024-06-21 18:42:07,177 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 2.060
2024-06-21 18:42:07,478 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 2.064
2024-06-21 18:42:07,923 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 2.011
2024-06-21 18:42:08,224 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 1.947
2024-06-21 18:42:08,598 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 2.145
2024-06-21 18:42:08,894 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 1.815
2024-06-21 18:42:09,321 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 2.140
2024-06-21 18:42:09,623 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 1.935
2024-06-21 18:42:10,004 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 2.266
2024-06-21 18:42:10,307 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 2.290
2024-06-21 18:42:10,730 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 1.910
2024-06-21 18:42:11,025 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 2.115
2024-06-21 18:42:11,389 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 2.201
2024-06-21 18:42:11,684 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 1.863
2024-06-21 18:42:11,901 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 1.386
2024-06-21 18:42:22,958 - INFO: 34/200 final results:
2024-06-21 18:42:22,958 - INFO: Training loss: 2.046.
2024-06-21 18:42:22,958 - INFO: Training MAE: 2.059.
2024-06-21 18:42:22,958 - INFO: Training MSE: 8.759.
2024-06-21 18:42:43,359 - INFO: Epoch: 34/200, Loss_train: 2.045686668363111, Loss_val: 3.0719779606523185
2024-06-21 18:42:43,378 - INFO: Saved new best metric model for epoch 34.
2024-06-21 18:42:43,378 - INFO: Best internal validation val_loss: 3.072 at epoch: 34.
2024-06-21 18:42:43,378 - INFO: Epoch 35/200...
2024-06-21 18:42:43,378 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:42:43,378 - INFO: Batch size: 32.
2024-06-21 18:42:43,382 - INFO: Dataset:
2024-06-21 18:42:43,382 - INFO: Batch size:
2024-06-21 18:42:43,382 - INFO: Number of workers:
2024-06-21 18:42:44,547 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 2.138
2024-06-21 18:42:44,867 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 1.597
2024-06-21 18:42:45,273 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 1.781
2024-06-21 18:42:45,589 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 2.031
2024-06-21 18:42:45,996 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 2.089
2024-06-21 18:42:46,295 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 1.945
2024-06-21 18:42:46,685 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 2.038
2024-06-21 18:42:47,002 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 1.969
2024-06-21 18:42:47,408 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 2.135
2024-06-21 18:42:47,703 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 1.753
2024-06-21 18:42:48,089 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 1.985
2024-06-21 18:42:48,406 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 2.149
2024-06-21 18:42:48,821 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 2.111
2024-06-21 18:42:49,126 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 1.935
2024-06-21 18:42:49,532 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 1.548
2024-06-21 18:42:49,846 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 1.902
2024-06-21 18:42:50,258 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 2.186
2024-06-21 18:42:50,558 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 1.771
2024-06-21 18:42:50,956 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 1.747
2024-06-21 18:42:51,264 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 2.064
2024-06-21 18:42:51,670 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 1.972
2024-06-21 18:42:51,973 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 2.059
2024-06-21 18:42:52,371 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 2.010
2024-06-21 18:42:52,686 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 2.034
2024-06-21 18:42:53,095 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 2.193
2024-06-21 18:42:53,393 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 1.880
2024-06-21 18:42:53,785 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 2.180
2024-06-21 18:42:54,096 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 1.914
2024-06-21 18:42:54,316 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 1.519
2024-06-21 18:43:05,429 - INFO: 35/200 final results:
2024-06-21 18:43:05,429 - INFO: Training loss: 1.953.
2024-06-21 18:43:05,429 - INFO: Training MAE: 1.962.
2024-06-21 18:43:05,429 - INFO: Training MSE: 7.924.
2024-06-21 18:43:26,012 - INFO: Epoch: 35/200, Loss_train: 1.9529469835347142, Loss_val: 2.6479215046455122
2024-06-21 18:43:26,031 - INFO: Saved new best metric model for epoch 35.
2024-06-21 18:43:26,031 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:43:26,031 - INFO: Epoch 36/200...
2024-06-21 18:43:26,032 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:43:26,032 - INFO: Batch size: 32.
2024-06-21 18:43:26,036 - INFO: Dataset:
2024-06-21 18:43:26,036 - INFO: Batch size:
2024-06-21 18:43:26,036 - INFO: Number of workers:
2024-06-21 18:43:27,212 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 1.545
2024-06-21 18:43:27,518 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 1.927
2024-06-21 18:43:27,913 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 1.876
2024-06-21 18:43:28,231 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 1.934
2024-06-21 18:43:28,657 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 2.095
2024-06-21 18:43:28,958 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 1.961
2024-06-21 18:43:29,345 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 1.727
2024-06-21 18:43:29,660 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 2.117
2024-06-21 18:43:30,091 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 1.903
2024-06-21 18:43:30,385 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 2.069
2024-06-21 18:43:30,764 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 2.139
2024-06-21 18:43:31,081 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 1.708
2024-06-21 18:43:31,512 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 2.392
2024-06-21 18:43:31,816 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 1.834
2024-06-21 18:43:32,210 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 2.275
2024-06-21 18:43:32,524 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 2.274
2024-06-21 18:43:32,956 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 1.943
2024-06-21 18:43:33,256 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 1.982
2024-06-21 18:43:33,641 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 2.150
2024-06-21 18:43:33,949 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 2.167
2024-06-21 18:43:34,368 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 1.873
2024-06-21 18:43:34,670 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 2.086
2024-06-21 18:43:35,055 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 1.654
2024-06-21 18:43:35,370 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 2.053
2024-06-21 18:43:35,789 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 2.001
2024-06-21 18:43:36,087 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 1.622
2024-06-21 18:43:36,459 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 2.663
2024-06-21 18:43:36,770 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 2.022
2024-06-21 18:43:36,986 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 1.709
2024-06-21 18:43:48,076 - INFO: 36/200 final results:
2024-06-21 18:43:48,077 - INFO: Training loss: 1.990.
2024-06-21 18:43:48,077 - INFO: Training MAE: 1.995.
2024-06-21 18:43:48,077 - INFO: Training MSE: 8.071.
2024-06-21 18:44:08,712 - INFO: Epoch: 36/200, Loss_train: 1.989803803378138, Loss_val: 2.868094197634993
2024-06-21 18:44:08,712 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:44:08,712 - INFO: Epoch 37/200...
2024-06-21 18:44:08,712 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:44:08,712 - INFO: Batch size: 32.
2024-06-21 18:44:08,716 - INFO: Dataset:
2024-06-21 18:44:08,717 - INFO: Batch size:
2024-06-21 18:44:08,717 - INFO: Number of workers:
2024-06-21 18:44:09,888 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 2.197
2024-06-21 18:44:10,193 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 2.191
2024-06-21 18:44:10,602 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 1.989
2024-06-21 18:44:10,919 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 1.693
2024-06-21 18:44:11,346 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 1.963
2024-06-21 18:44:11,647 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 1.510
2024-06-21 18:44:12,034 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 1.787
2024-06-21 18:44:12,351 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 2.182
2024-06-21 18:44:12,782 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 1.872
2024-06-21 18:44:13,074 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 1.601
2024-06-21 18:44:13,447 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 1.735
2024-06-21 18:44:13,764 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 1.917
2024-06-21 18:44:14,196 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 1.761
2024-06-21 18:44:14,500 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 1.926
2024-06-21 18:44:14,891 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 2.030
2024-06-21 18:44:15,204 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 2.002
2024-06-21 18:44:15,636 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 1.765
2024-06-21 18:44:15,936 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 1.636
2024-06-21 18:44:16,312 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 2.228
2024-06-21 18:44:16,621 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 1.951
2024-06-21 18:44:17,036 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 1.813
2024-06-21 18:44:17,339 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 1.678
2024-06-21 18:44:17,718 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 1.655
2024-06-21 18:44:18,033 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 1.649
2024-06-21 18:44:18,447 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 2.093
2024-06-21 18:44:18,742 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 1.941
2024-06-21 18:44:19,111 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 1.903
2024-06-21 18:44:19,419 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 2.102
2024-06-21 18:44:19,626 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 1.965
2024-06-21 18:44:30,659 - INFO: 37/200 final results:
2024-06-21 18:44:30,659 - INFO: Training loss: 1.887.
2024-06-21 18:44:30,659 - INFO: Training MAE: 1.886.
2024-06-21 18:44:30,659 - INFO: Training MSE: 7.308.
2024-06-21 18:44:51,443 - INFO: Epoch: 37/200, Loss_train: 1.887417513748695, Loss_val: 2.973750517286103
2024-06-21 18:44:51,443 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:44:51,443 - INFO: Epoch 38/200...
2024-06-21 18:44:51,443 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:44:51,443 - INFO: Batch size: 32.
2024-06-21 18:44:51,446 - INFO: Dataset:
2024-06-21 18:44:51,447 - INFO: Batch size:
2024-06-21 18:44:51,447 - INFO: Number of workers:
2024-06-21 18:44:52,620 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 1.605
2024-06-21 18:44:52,926 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 2.038
2024-06-21 18:44:53,334 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 1.703
2024-06-21 18:44:53,652 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 2.026
2024-06-21 18:44:54,053 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 1.354
2024-06-21 18:44:54,353 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 1.844
2024-06-21 18:44:54,750 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 2.021
2024-06-21 18:44:55,064 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 1.666
2024-06-21 18:44:55,474 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 1.979
2024-06-21 18:44:55,768 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 2.275
2024-06-21 18:44:56,157 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 1.740
2024-06-21 18:44:56,473 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 1.719
2024-06-21 18:44:56,885 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 1.943
2024-06-21 18:44:57,187 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 1.682
2024-06-21 18:44:57,596 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 1.985
2024-06-21 18:44:57,907 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 1.640
2024-06-21 18:44:58,322 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 2.082
2024-06-21 18:44:58,620 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 1.783
2024-06-21 18:44:59,018 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 2.023
2024-06-21 18:44:59,324 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 1.960
2024-06-21 18:44:59,729 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 1.900
2024-06-21 18:45:00,032 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 2.110
2024-06-21 18:45:00,426 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 1.687
2024-06-21 18:45:00,742 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 1.881
2024-06-21 18:45:01,148 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 1.917
2024-06-21 18:45:01,445 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 1.946
2024-06-21 18:45:01,842 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 2.038
2024-06-21 18:45:02,152 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 1.965
2024-06-21 18:45:02,371 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 1.311
2024-06-21 18:45:13,488 - INFO: 38/200 final results:
2024-06-21 18:45:13,488 - INFO: Training loss: 1.856.
2024-06-21 18:45:13,488 - INFO: Training MAE: 1.867.
2024-06-21 18:45:13,488 - INFO: Training MSE: 7.295.
2024-06-21 18:45:33,786 - INFO: Epoch: 38/200, Loss_train: 1.8559654992202232, Loss_val: 2.85527281103463
2024-06-21 18:45:33,786 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:45:33,786 - INFO: Epoch 39/200...
2024-06-21 18:45:33,786 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:45:33,786 - INFO: Batch size: 32.
2024-06-21 18:45:33,790 - INFO: Dataset:
2024-06-21 18:45:33,790 - INFO: Batch size:
2024-06-21 18:45:33,790 - INFO: Number of workers:
2024-06-21 18:45:34,958 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 2.337
2024-06-21 18:45:35,264 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 1.884
2024-06-21 18:45:35,669 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 1.796
2024-06-21 18:45:35,986 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 1.978
2024-06-21 18:45:36,398 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 1.859
2024-06-21 18:45:36,698 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 1.747
2024-06-21 18:45:37,098 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 1.701
2024-06-21 18:45:37,411 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 1.962
2024-06-21 18:45:37,844 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 1.784
2024-06-21 18:45:38,136 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 1.438
2024-06-21 18:45:38,527 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 1.632
2024-06-21 18:45:38,842 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 1.954
2024-06-21 18:45:39,259 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 1.981
2024-06-21 18:45:39,561 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 1.811
2024-06-21 18:45:39,969 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 1.665
2024-06-21 18:45:40,279 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 2.025
2024-06-21 18:45:40,694 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 2.201
2024-06-21 18:45:40,992 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 1.809
2024-06-21 18:45:41,390 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 1.886
2024-06-21 18:45:41,696 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 1.605
2024-06-21 18:45:42,101 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 1.595
2024-06-21 18:45:42,402 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 1.820
2024-06-21 18:45:42,801 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 1.519
2024-06-21 18:45:43,115 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 1.595
2024-06-21 18:45:43,514 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 1.556
2024-06-21 18:45:43,811 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 1.853
2024-06-21 18:45:44,204 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 1.668
2024-06-21 18:45:44,513 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 1.499
2024-06-21 18:45:44,730 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 1.936
2024-06-21 18:45:55,841 - INFO: 39/200 final results:
2024-06-21 18:45:55,841 - INFO: Training loss: 1.796.
2024-06-21 18:45:55,841 - INFO: Training MAE: 1.794.
2024-06-21 18:45:55,841 - INFO: Training MSE: 6.714.
2024-06-21 18:46:16,278 - INFO: Epoch: 39/200, Loss_train: 1.796280762244915, Loss_val: 2.917814139662118
2024-06-21 18:46:16,278 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:46:16,278 - INFO: Epoch 40/200...
2024-06-21 18:46:16,278 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:46:16,278 - INFO: Batch size: 32.
2024-06-21 18:46:16,282 - INFO: Dataset:
2024-06-21 18:46:16,282 - INFO: Batch size:
2024-06-21 18:46:16,282 - INFO: Number of workers:
2024-06-21 18:46:17,442 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 1.755
2024-06-21 18:46:17,761 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 1.642
2024-06-21 18:46:18,159 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 2.331
2024-06-21 18:46:18,475 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 1.865
2024-06-21 18:46:18,889 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 2.318
2024-06-21 18:46:19,200 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 1.634
2024-06-21 18:46:19,588 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 1.928
2024-06-21 18:46:19,900 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 1.603
2024-06-21 18:46:20,308 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 1.508
2024-06-21 18:46:20,611 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 2.088
2024-06-21 18:46:20,989 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 1.611
2024-06-21 18:46:21,304 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 2.435
2024-06-21 18:46:21,722 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 1.649
2024-06-21 18:46:22,037 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 1.491
2024-06-21 18:46:22,432 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 2.146
2024-06-21 18:46:22,743 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 1.782
2024-06-21 18:46:23,161 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 2.182
2024-06-21 18:46:23,472 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 1.840
2024-06-21 18:46:23,858 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 1.852
2024-06-21 18:46:24,164 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 2.231
2024-06-21 18:46:24,574 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 1.947
2024-06-21 18:46:24,887 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 2.280
2024-06-21 18:46:25,271 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 1.835
2024-06-21 18:46:25,584 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 1.801
2024-06-21 18:46:25,989 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 1.653
2024-06-21 18:46:26,298 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 1.714
2024-06-21 18:46:26,680 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 1.750
2024-06-21 18:46:26,990 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 1.379
2024-06-21 18:46:27,204 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 2.852
2024-06-21 18:46:38,271 - INFO: 40/200 final results:
2024-06-21 18:46:38,272 - INFO: Training loss: 1.900.
2024-06-21 18:46:38,272 - INFO: Training MAE: 1.881.
2024-06-21 18:46:38,272 - INFO: Training MSE: 7.208.
2024-06-21 18:46:58,959 - INFO: Epoch: 40/200, Loss_train: 1.9000080741685013, Loss_val: 3.3706003550825447
2024-06-21 18:46:58,959 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:46:58,959 - INFO: Epoch 41/200...
2024-06-21 18:46:58,959 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:46:58,959 - INFO: Batch size: 32.
2024-06-21 18:46:58,963 - INFO: Dataset:
2024-06-21 18:46:58,963 - INFO: Batch size:
2024-06-21 18:46:58,964 - INFO: Number of workers:
2024-06-21 18:47:00,148 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 2.628
2024-06-21 18:47:00,454 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 2.209
2024-06-21 18:47:00,844 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 1.809
2024-06-21 18:47:01,160 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 2.031
2024-06-21 18:47:01,575 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 1.768
2024-06-21 18:47:01,874 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 1.882
2024-06-21 18:47:02,249 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 1.892
2024-06-21 18:47:02,560 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 1.526
2024-06-21 18:47:02,977 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 1.742
2024-06-21 18:47:03,267 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 1.740
2024-06-21 18:47:03,628 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 1.561
2024-06-21 18:47:03,941 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 1.614
2024-06-21 18:47:04,361 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 2.196
2024-06-21 18:47:04,666 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 1.852
2024-06-21 18:47:05,062 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 1.745
2024-06-21 18:47:05,374 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 1.731
2024-06-21 18:47:05,803 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 1.684
2024-06-21 18:47:06,101 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 1.666
2024-06-21 18:47:06,485 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 1.616
2024-06-21 18:47:06,791 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 1.681
2024-06-21 18:47:07,208 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 1.980
2024-06-21 18:47:07,509 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 1.417
2024-06-21 18:47:07,881 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 2.197
2024-06-21 18:47:08,194 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 2.269
2024-06-21 18:47:08,613 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 2.034
2024-06-21 18:47:08,912 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 1.936
2024-06-21 18:47:09,292 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 1.811
2024-06-21 18:47:09,604 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 1.958
2024-06-21 18:47:09,824 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 1.357
2024-06-21 18:47:21,029 - INFO: 41/200 final results:
2024-06-21 18:47:21,029 - INFO: Training loss: 1.846.
2024-06-21 18:47:21,029 - INFO: Training MAE: 1.856.
2024-06-21 18:47:21,029 - INFO: Training MSE: 6.978.
2024-06-21 18:47:41,601 - INFO: Epoch: 41/200, Loss_train: 1.8459761389370621, Loss_val: 2.8591629883338667
2024-06-21 18:47:41,602 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:47:41,602 - INFO: Epoch 42/200...
2024-06-21 18:47:41,602 - INFO: Learning rate: 0.00021826633340806532.
2024-06-21 18:47:41,602 - INFO: Batch size: 32.
2024-06-21 18:47:41,606 - INFO: Dataset:
2024-06-21 18:47:41,606 - INFO: Batch size:
2024-06-21 18:47:41,606 - INFO: Number of workers:
2024-06-21 18:47:42,795 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 2.027
2024-06-21 18:47:43,101 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 1.763
2024-06-21 18:47:43,497 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 1.670
2024-06-21 18:47:43,816 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 1.721
2024-06-21 18:47:44,252 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 1.829
2024-06-21 18:47:44,554 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 1.739
2024-06-21 18:47:44,940 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 1.700
2024-06-21 18:47:45,244 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 1.669
2024-06-21 18:47:45,689 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 1.625
2024-06-21 18:47:45,984 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 2.048
2024-06-21 18:47:46,358 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 1.479
2024-06-21 18:47:46,662 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 1.808
2024-06-21 18:47:47,108 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 2.108
2024-06-21 18:47:47,413 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 1.781
2024-06-21 18:47:47,811 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 1.823
2024-06-21 18:47:48,111 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 1.493
2024-06-21 18:47:48,544 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 1.852
2024-06-21 18:47:48,845 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 1.891
2024-06-21 18:47:49,222 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 2.331
2024-06-21 18:47:49,517 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 1.567
2024-06-21 18:47:49,931 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 1.876
2024-06-21 18:47:50,231 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 1.757
2024-06-21 18:47:50,606 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 1.757
2024-06-21 18:47:50,906 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 1.513
2024-06-21 18:47:51,335 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 1.667
2024-06-21 18:47:51,633 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 1.962
2024-06-21 18:47:52,001 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 1.898
2024-06-21 18:47:52,299 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 1.416
2024-06-21 18:47:52,510 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 1.707
2024-06-21 18:48:03,666 - INFO: 42/200 final results:
2024-06-21 18:48:03,666 - INFO: Training loss: 1.775.
2024-06-21 18:48:03,666 - INFO: Training MAE: 1.776.
2024-06-21 18:48:03,666 - INFO: Training MSE: 6.258.
2024-06-21 18:48:23,930 - INFO: Epoch: 42/200, Loss_train: 1.7750468994009083, Loss_val: 2.9322743498045822
2024-06-21 18:48:23,930 - INFO: Best internal validation val_loss: 2.648 at epoch: 35.
2024-06-21 18:48:23,930 - INFO: Epoch 43/200...
2024-06-21 18:48:23,930 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:48:23,930 - INFO: Batch size: 32.
2024-06-21 18:48:23,934 - INFO: Dataset:
2024-06-21 18:48:23,935 - INFO: Batch size:
2024-06-21 18:48:23,935 - INFO: Number of workers:
2024-06-21 18:48:25,102 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 1.559
2024-06-21 18:48:25,407 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 1.588
2024-06-21 18:48:25,817 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 1.767
2024-06-21 18:48:26,134 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 1.541
2024-06-21 18:48:26,548 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 1.707
2024-06-21 18:48:26,848 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 1.627
2024-06-21 18:48:27,245 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 1.466
2024-06-21 18:48:27,559 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 1.692
2024-06-21 18:48:27,977 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 1.186
2024-06-21 18:48:28,269 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 1.386
2024-06-21 18:48:28,657 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 1.501
2024-06-21 18:48:28,972 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 1.463
2024-06-21 18:48:29,390 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 1.595
2024-06-21 18:48:29,692 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 1.573
2024-06-21 18:48:30,097 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 1.757
2024-06-21 18:48:30,408 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 1.579
2024-06-21 18:48:30,824 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 1.229
2024-06-21 18:48:31,122 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 1.759
2024-06-21 18:48:31,514 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 1.523
2024-06-21 18:48:31,820 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 1.433
2024-06-21 18:48:32,227 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 1.527
2024-06-21 18:48:32,528 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 1.399
2024-06-21 18:48:32,925 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 1.541
2024-06-21 18:48:33,238 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 1.434
2024-06-21 18:48:33,634 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 1.247
2024-06-21 18:48:33,931 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 1.357
2024-06-21 18:48:34,322 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 1.309
2024-06-21 18:48:34,631 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 1.449
2024-06-21 18:48:34,842 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 1.323
2024-06-21 18:48:45,969 - INFO: 43/200 final results:
2024-06-21 18:48:45,969 - INFO: Training loss: 1.501.
2024-06-21 18:48:45,969 - INFO: Training MAE: 1.504.
2024-06-21 18:48:45,969 - INFO: Training MSE: 4.881.
2024-06-21 18:49:06,367 - INFO: Epoch: 43/200, Loss_train: 1.5006025207453761, Loss_val: 2.467441513620574
2024-06-21 18:49:06,386 - INFO: Saved new best metric model for epoch 43.
2024-06-21 18:49:06,386 - INFO: Best internal validation val_loss: 2.467 at epoch: 43.
2024-06-21 18:49:06,386 - INFO: Epoch 44/200...
2024-06-21 18:49:06,386 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:49:06,386 - INFO: Batch size: 32.
2024-06-21 18:49:06,391 - INFO: Dataset:
2024-06-21 18:49:06,391 - INFO: Batch size:
2024-06-21 18:49:06,391 - INFO: Number of workers:
2024-06-21 18:49:07,581 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 1.388
2024-06-21 18:49:07,885 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 1.513
2024-06-21 18:49:08,278 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 1.303
2024-06-21 18:49:08,594 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 1.376
2024-06-21 18:49:09,008 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 1.586
2024-06-21 18:49:09,309 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 1.264
2024-06-21 18:49:09,706 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 1.448
2024-06-21 18:49:10,019 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 1.457
2024-06-21 18:49:10,469 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 1.248
2024-06-21 18:49:10,761 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 1.551
2024-06-21 18:49:11,145 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 1.353
2024-06-21 18:49:11,461 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 1.533
2024-06-21 18:49:11,886 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 1.559
2024-06-21 18:49:12,190 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 1.562
2024-06-21 18:49:12,581 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 1.421
2024-06-21 18:49:12,894 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 1.474
2024-06-21 18:49:13,315 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 1.330
2024-06-21 18:49:13,615 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 1.194
2024-06-21 18:49:13,997 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 1.541
2024-06-21 18:49:14,303 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 1.403
2024-06-21 18:49:14,724 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 1.543
2024-06-21 18:49:15,027 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 1.413
2024-06-21 18:49:15,413 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 1.549
2024-06-21 18:49:15,728 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 1.188
2024-06-21 18:49:16,141 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 1.507
2024-06-21 18:49:16,438 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 1.948
2024-06-21 18:49:16,814 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 1.234
2024-06-21 18:49:17,125 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 1.379
2024-06-21 18:49:17,340 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 1.295
2024-06-21 18:49:28,562 - INFO: 44/200 final results:
2024-06-21 18:49:28,562 - INFO: Training loss: 1.433.
2024-06-21 18:49:28,562 - INFO: Training MAE: 1.436.
2024-06-21 18:49:28,562 - INFO: Training MSE: 4.474.
2024-06-21 18:49:48,967 - INFO: Epoch: 44/200, Loss_train: 1.4331614560094372, Loss_val: 2.216076078086064
2024-06-21 18:49:48,985 - INFO: Saved new best metric model for epoch 44.
2024-06-21 18:49:48,985 - INFO: Best internal validation val_loss: 2.216 at epoch: 44.
2024-06-21 18:49:48,985 - INFO: Epoch 45/200...
2024-06-21 18:49:48,985 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:49:48,985 - INFO: Batch size: 32.
2024-06-21 18:49:48,989 - INFO: Dataset:
2024-06-21 18:49:48,989 - INFO: Batch size:
2024-06-21 18:49:48,989 - INFO: Number of workers:
2024-06-21 18:49:50,138 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 1.163
2024-06-21 18:49:50,457 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 1.632
2024-06-21 18:49:50,864 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 1.304
2024-06-21 18:49:51,180 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 1.678
2024-06-21 18:49:51,579 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 1.571
2024-06-21 18:49:51,877 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 1.421
2024-06-21 18:49:52,273 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 1.187
2024-06-21 18:49:52,585 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 1.425
2024-06-21 18:49:52,985 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 1.130
2024-06-21 18:49:53,275 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 1.325
2024-06-21 18:49:53,662 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 1.472
2024-06-21 18:49:53,976 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 1.206
2024-06-21 18:49:54,387 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 1.345
2024-06-21 18:49:54,688 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 1.557
2024-06-21 18:49:55,091 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 1.238
2024-06-21 18:49:55,401 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 1.476
2024-06-21 18:49:55,798 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 1.581
2024-06-21 18:49:56,094 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 1.354
2024-06-21 18:49:56,488 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 1.881
2024-06-21 18:49:56,794 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 1.594
2024-06-21 18:49:57,185 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 1.365
2024-06-21 18:49:57,484 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 1.434
2024-06-21 18:49:57,879 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 1.532
2024-06-21 18:49:58,191 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 1.349
2024-06-21 18:49:58,593 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 1.673
2024-06-21 18:49:58,888 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 1.779
2024-06-21 18:49:59,277 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 1.588
2024-06-21 18:49:59,586 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 1.147
2024-06-21 18:49:59,795 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 2.100
2024-06-21 18:50:10,889 - INFO: 45/200 final results:
2024-06-21 18:50:10,889 - INFO: Training loss: 1.466.
2024-06-21 18:50:10,889 - INFO: Training MAE: 1.453.
2024-06-21 18:50:10,889 - INFO: Training MSE: 4.633.
2024-06-21 18:50:31,611 - INFO: Epoch: 45/200, Loss_train: 1.4657449311223523, Loss_val: 2.306061498050032
2024-06-21 18:50:31,611 - INFO: Best internal validation val_loss: 2.216 at epoch: 44.
2024-06-21 18:50:31,611 - INFO: Epoch 46/200...
2024-06-21 18:50:31,611 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:50:31,611 - INFO: Batch size: 32.
2024-06-21 18:50:31,615 - INFO: Dataset:
2024-06-21 18:50:31,615 - INFO: Batch size:
2024-06-21 18:50:31,615 - INFO: Number of workers:
2024-06-21 18:50:32,785 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 1.440
2024-06-21 18:50:33,119 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 1.091
2024-06-21 18:50:33,516 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 1.335
2024-06-21 18:50:33,836 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 1.570
2024-06-21 18:50:34,251 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 1.174
2024-06-21 18:50:34,565 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 1.292
2024-06-21 18:50:34,948 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 1.379
2024-06-21 18:50:35,264 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 1.840
2024-06-21 18:50:35,674 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 1.554
2024-06-21 18:50:35,982 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 1.483
2024-06-21 18:50:36,352 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 1.361
2024-06-21 18:50:36,669 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 1.416
2024-06-21 18:50:37,086 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 1.474
2024-06-21 18:50:37,399 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 1.203
2024-06-21 18:50:37,788 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 1.443
2024-06-21 18:50:38,097 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 1.500
2024-06-21 18:50:38,511 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 1.582
2024-06-21 18:50:38,820 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 1.224
2024-06-21 18:50:39,200 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 1.306
2024-06-21 18:50:39,504 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 1.220
2024-06-21 18:50:39,911 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 1.482
2024-06-21 18:50:40,222 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 1.290
2024-06-21 18:50:40,603 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 1.799
2024-06-21 18:50:40,914 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 1.476
2024-06-21 18:50:41,315 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 1.385
2024-06-21 18:50:41,622 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 1.436
2024-06-21 18:50:41,990 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 1.229
2024-06-21 18:50:42,297 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 1.669
2024-06-21 18:50:42,503 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 1.308
2024-06-21 18:50:53,573 - INFO: 46/200 final results:
2024-06-21 18:50:53,573 - INFO: Training loss: 1.412.
2024-06-21 18:50:53,573 - INFO: Training MAE: 1.415.
2024-06-21 18:50:53,573 - INFO: Training MSE: 4.332.
2024-06-21 18:51:13,867 - INFO: Epoch: 46/200, Loss_train: 1.4124584485744607, Loss_val: 2.281958510135782
2024-06-21 18:51:13,868 - INFO: Best internal validation val_loss: 2.216 at epoch: 44.
2024-06-21 18:51:13,868 - INFO: Epoch 47/200...
2024-06-21 18:51:13,868 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:51:13,868 - INFO: Batch size: 32.
2024-06-21 18:51:13,872 - INFO: Dataset:
2024-06-21 18:51:13,872 - INFO: Batch size:
2024-06-21 18:51:13,872 - INFO: Number of workers:
2024-06-21 18:51:15,038 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 1.123
2024-06-21 18:51:15,359 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 1.440
2024-06-21 18:51:15,759 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 1.243
2024-06-21 18:51:16,078 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 1.356
2024-06-21 18:51:16,504 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 1.446
2024-06-21 18:51:16,804 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 1.174
2024-06-21 18:51:17,193 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 1.298
2024-06-21 18:51:17,506 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 1.121
2024-06-21 18:51:17,926 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 1.544
2024-06-21 18:51:18,218 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 1.399
2024-06-21 18:51:18,596 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 1.444
2024-06-21 18:51:18,912 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 1.403
2024-06-21 18:51:19,341 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 1.191
2024-06-21 18:51:19,643 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 1.188
2024-06-21 18:51:20,036 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 1.537
2024-06-21 18:51:20,349 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 1.258
2024-06-21 18:51:20,777 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 1.519
2024-06-21 18:51:21,078 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 1.158
2024-06-21 18:51:21,464 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 1.406
2024-06-21 18:51:21,774 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 1.215
2024-06-21 18:51:22,190 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 1.396
2024-06-21 18:51:22,492 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 1.617
2024-06-21 18:51:22,878 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 1.233
2024-06-21 18:51:23,193 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 1.443
2024-06-21 18:51:23,611 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 1.507
2024-06-21 18:51:23,909 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 1.582
2024-06-21 18:51:24,295 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 1.524
2024-06-21 18:51:24,607 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 1.259
2024-06-21 18:51:24,823 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 1.165
2024-06-21 18:51:35,864 - INFO: 47/200 final results:
2024-06-21 18:51:35,865 - INFO: Training loss: 1.351.
2024-06-21 18:51:35,865 - INFO: Training MAE: 1.355.
2024-06-21 18:51:35,865 - INFO: Training MSE: 4.013.
2024-06-21 18:51:56,373 - INFO: Epoch: 47/200, Loss_train: 1.3513494195609257, Loss_val: 2.6464184810375344
2024-06-21 18:51:56,373 - INFO: Best internal validation val_loss: 2.216 at epoch: 44.
2024-06-21 18:51:56,373 - INFO: Epoch 48/200...
2024-06-21 18:51:56,373 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:51:56,373 - INFO: Batch size: 32.
2024-06-21 18:51:56,377 - INFO: Dataset:
2024-06-21 18:51:56,377 - INFO: Batch size:
2024-06-21 18:51:56,377 - INFO: Number of workers:
2024-06-21 18:51:57,567 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 1.584
2024-06-21 18:51:57,874 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 1.896
2024-06-21 18:51:58,276 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 1.326
2024-06-21 18:51:58,596 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 1.278
2024-06-21 18:51:59,013 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 1.552
2024-06-21 18:51:59,327 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 1.396
2024-06-21 18:51:59,714 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 1.191
2024-06-21 18:52:00,030 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 1.187
2024-06-21 18:52:00,434 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 1.203
2024-06-21 18:52:00,742 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 1.422
2024-06-21 18:52:01,117 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 1.179
2024-06-21 18:52:01,434 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 1.634
2024-06-21 18:52:01,855 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 1.373
2024-06-21 18:52:02,172 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 1.218
2024-06-21 18:52:02,568 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 1.243
2024-06-21 18:52:02,881 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 1.525
2024-06-21 18:52:03,295 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 1.538
2024-06-21 18:52:03,609 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 1.303
2024-06-21 18:52:03,993 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 1.691
2024-06-21 18:52:04,301 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 1.298
2024-06-21 18:52:04,706 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 1.503
2024-06-21 18:52:05,021 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 1.049
2024-06-21 18:52:05,394 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 1.307
2024-06-21 18:52:05,709 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 1.447
2024-06-21 18:52:06,109 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 1.467
2024-06-21 18:52:06,419 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 1.662
2024-06-21 18:52:06,786 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 1.163
2024-06-21 18:52:07,097 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 1.242
2024-06-21 18:52:07,306 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 2.066
2024-06-21 18:52:18,525 - INFO: 48/200 final results:
2024-06-21 18:52:18,525 - INFO: Training loss: 1.412.
2024-06-21 18:52:18,525 - INFO: Training MAE: 1.399.
2024-06-21 18:52:18,525 - INFO: Training MSE: 4.237.
2024-06-21 18:52:38,836 - INFO: Epoch: 48/200, Loss_train: 1.4118173081299354, Loss_val: 2.20848037456644
2024-06-21 18:52:38,855 - INFO: Saved new best metric model for epoch 48.
2024-06-21 18:52:38,855 - INFO: Best internal validation val_loss: 2.208 at epoch: 48.
2024-06-21 18:52:38,855 - INFO: Epoch 49/200...
2024-06-21 18:52:38,855 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:52:38,855 - INFO: Batch size: 32.
2024-06-21 18:52:38,860 - INFO: Dataset:
2024-06-21 18:52:38,860 - INFO: Batch size:
2024-06-21 18:52:38,860 - INFO: Number of workers:
2024-06-21 18:52:40,036 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 0.981
2024-06-21 18:52:40,343 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 1.260
2024-06-21 18:52:40,754 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 1.233
2024-06-21 18:52:41,073 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 1.339
2024-06-21 18:52:41,486 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 1.173
2024-06-21 18:52:41,787 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 1.277
2024-06-21 18:52:42,189 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 1.301
2024-06-21 18:52:42,504 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 1.434
2024-06-21 18:52:42,921 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 1.612
2024-06-21 18:52:43,215 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 1.515
2024-06-21 18:52:43,608 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 1.339
2024-06-21 18:52:43,925 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 1.112
2024-06-21 18:52:44,343 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 1.262
2024-06-21 18:52:44,647 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 0.960
2024-06-21 18:52:45,056 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 1.457
2024-06-21 18:52:45,369 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 1.337
2024-06-21 18:52:45,785 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 1.526
2024-06-21 18:52:46,083 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 1.365
2024-06-21 18:52:46,479 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 1.579
2024-06-21 18:52:46,785 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 1.011
2024-06-21 18:52:47,191 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 1.724
2024-06-21 18:52:47,494 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 1.368
2024-06-21 18:52:47,894 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 1.349
2024-06-21 18:52:48,210 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 1.367
2024-06-21 18:52:48,618 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 1.245
2024-06-21 18:52:48,916 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 1.513
2024-06-21 18:52:49,313 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 1.772
2024-06-21 18:52:49,624 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 1.109
2024-06-21 18:52:49,844 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 2.011
2024-06-21 18:53:00,981 - INFO: 49/200 final results:
2024-06-21 18:53:00,982 - INFO: Training loss: 1.363.
2024-06-21 18:53:00,982 - INFO: Training MAE: 1.350.
2024-06-21 18:53:00,982 - INFO: Training MSE: 3.968.
2024-06-21 18:53:21,333 - INFO: Epoch: 49/200, Loss_train: 1.3630527175706009, Loss_val: 2.1719679544711936
2024-06-21 18:53:21,351 - INFO: Saved new best metric model for epoch 49.
2024-06-21 18:53:21,352 - INFO: Best internal validation val_loss: 2.172 at epoch: 49.
2024-06-21 18:53:21,352 - INFO: Epoch 50/200...
2024-06-21 18:53:21,352 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:53:21,352 - INFO: Batch size: 32.
2024-06-21 18:53:21,356 - INFO: Dataset:
2024-06-21 18:53:21,356 - INFO: Batch size:
2024-06-21 18:53:21,356 - INFO: Number of workers:
2024-06-21 18:53:22,520 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 1.100
2024-06-21 18:53:22,824 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 1.280
2024-06-21 18:53:23,221 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 1.465
2024-06-21 18:53:23,536 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 0.896
2024-06-21 18:53:23,936 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 1.167
2024-06-21 18:53:24,234 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 1.200
2024-06-21 18:53:24,636 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 1.262
2024-06-21 18:53:24,951 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 1.404
2024-06-21 18:53:25,357 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 1.119
2024-06-21 18:53:25,651 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 1.609
2024-06-21 18:53:26,041 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 1.112
2024-06-21 18:53:26,359 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 1.659
2024-06-21 18:53:26,778 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 1.657
2024-06-21 18:53:27,081 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 1.363
2024-06-21 18:53:27,488 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 1.035
2024-06-21 18:53:27,800 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 1.274
2024-06-21 18:53:28,219 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 1.377
2024-06-21 18:53:28,518 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 1.396
2024-06-21 18:53:28,915 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 1.298
2024-06-21 18:53:29,221 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 1.111
2024-06-21 18:53:29,630 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 1.222
2024-06-21 18:53:29,931 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 1.278
2024-06-21 18:53:30,331 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 1.483
2024-06-21 18:53:30,644 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 1.796
2024-06-21 18:53:31,051 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 1.177
2024-06-21 18:53:31,348 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 1.064
2024-06-21 18:53:31,745 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 1.368
2024-06-21 18:53:32,055 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 1.279
2024-06-21 18:53:32,276 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 1.589
2024-06-21 18:53:43,393 - INFO: 50/200 final results:
2024-06-21 18:53:43,394 - INFO: Training loss: 1.312.
2024-06-21 18:53:43,394 - INFO: Training MAE: 1.306.
2024-06-21 18:53:43,394 - INFO: Training MSE: 3.761.
2024-06-21 18:54:04,122 - INFO: Epoch: 50/200, Loss_train: 1.3116832396079754, Loss_val: 2.1554755506844354
2024-06-21 18:54:04,141 - INFO: Saved new best metric model for epoch 50.
2024-06-21 18:54:04,141 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:54:04,141 - INFO: Epoch 51/200...
2024-06-21 18:54:04,141 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:54:04,141 - INFO: Batch size: 32.
2024-06-21 18:54:04,145 - INFO: Dataset:
2024-06-21 18:54:04,145 - INFO: Batch size:
2024-06-21 18:54:04,145 - INFO: Number of workers:
2024-06-21 18:54:05,290 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 1.068
2024-06-21 18:54:05,607 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 1.030
2024-06-21 18:54:06,015 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 1.349
2024-06-21 18:54:06,332 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 1.355
2024-06-21 18:54:06,733 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 1.185
2024-06-21 18:54:07,044 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 1.142
2024-06-21 18:54:07,441 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 1.447
2024-06-21 18:54:07,754 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 1.218
2024-06-21 18:54:08,138 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 1.297
2024-06-21 18:54:08,442 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 1.426
2024-06-21 18:54:08,828 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 1.276
2024-06-21 18:54:09,141 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 1.355
2024-06-21 18:54:09,546 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 1.101
2024-06-21 18:54:09,859 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 1.055
2024-06-21 18:54:10,255 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 1.846
2024-06-21 18:54:10,569 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 1.279
2024-06-21 18:54:10,975 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 0.930
2024-06-21 18:54:11,288 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 1.345
2024-06-21 18:54:11,688 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 1.560
2024-06-21 18:54:11,996 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 1.283
2024-06-21 18:54:12,390 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 1.228
2024-06-21 18:54:12,706 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 1.248
2024-06-21 18:54:13,105 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 1.609
2024-06-21 18:54:13,421 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 1.483
2024-06-21 18:54:13,818 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 1.269
2024-06-21 18:54:14,129 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 1.289
2024-06-21 18:54:14,521 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 1.249
2024-06-21 18:54:14,832 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 1.222
2024-06-21 18:54:15,047 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 1.291
2024-06-21 18:54:26,175 - INFO: 51/200 final results:
2024-06-21 18:54:26,175 - INFO: Training loss: 1.291.
2024-06-21 18:54:26,175 - INFO: Training MAE: 1.291.
2024-06-21 18:54:26,175 - INFO: Training MSE: 3.634.
2024-06-21 18:54:46,717 - INFO: Epoch: 51/200, Loss_train: 1.2908815638772373, Loss_val: 2.294893540185073
2024-06-21 18:54:46,717 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:54:46,717 - INFO: Epoch 52/200...
2024-06-21 18:54:46,717 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:54:46,717 - INFO: Batch size: 32.
2024-06-21 18:54:46,722 - INFO: Dataset:
2024-06-21 18:54:46,722 - INFO: Batch size:
2024-06-21 18:54:46,722 - INFO: Number of workers:
2024-06-21 18:54:47,914 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 1.219
2024-06-21 18:54:48,220 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 1.504
2024-06-21 18:54:48,613 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 1.222
2024-06-21 18:54:48,933 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 1.220
2024-06-21 18:54:49,350 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 1.313
2024-06-21 18:54:49,652 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 1.460
2024-06-21 18:54:50,050 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 1.153
2024-06-21 18:54:50,366 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 1.260
2024-06-21 18:54:50,774 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 1.088
2024-06-21 18:54:51,067 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 1.524
2024-06-21 18:54:51,452 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 1.161
2024-06-21 18:54:51,770 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 1.219
2024-06-21 18:54:52,193 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 1.463
2024-06-21 18:54:52,497 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 1.356
2024-06-21 18:54:52,894 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 1.161
2024-06-21 18:54:53,208 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 1.277
2024-06-21 18:54:53,629 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 1.416
2024-06-21 18:54:53,930 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 1.269
2024-06-21 18:54:54,325 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 1.268
2024-06-21 18:54:54,634 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 1.365
2024-06-21 18:54:55,042 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 1.039
2024-06-21 18:54:55,345 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 1.267
2024-06-21 18:54:55,736 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 1.140
2024-06-21 18:54:56,051 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 1.292
2024-06-21 18:54:56,452 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 1.768
2024-06-21 18:54:56,747 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 1.335
2024-06-21 18:54:57,141 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 0.945
2024-06-21 18:54:57,450 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 1.415
2024-06-21 18:54:57,663 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 2.132
2024-06-21 18:55:08,861 - INFO: 52/200 final results:
2024-06-21 18:55:08,862 - INFO: Training loss: 1.319.
2024-06-21 18:55:08,862 - INFO: Training MAE: 1.303.
2024-06-21 18:55:08,862 - INFO: Training MSE: 3.667.
2024-06-21 18:55:28,973 - INFO: Epoch: 52/200, Loss_train: 1.3189640332912576, Loss_val: 2.1893589414399246
2024-06-21 18:55:28,973 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:55:28,973 - INFO: Epoch 53/200...
2024-06-21 18:55:28,973 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:55:28,974 - INFO: Batch size: 32.
2024-06-21 18:55:28,978 - INFO: Dataset:
2024-06-21 18:55:28,978 - INFO: Batch size:
2024-06-21 18:55:28,978 - INFO: Number of workers:
2024-06-21 18:55:30,165 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 1.136
2024-06-21 18:55:30,469 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 1.288
2024-06-21 18:55:30,849 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 1.234
2024-06-21 18:55:31,165 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 0.992
2024-06-21 18:55:31,597 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 1.248
2024-06-21 18:55:31,896 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 1.200
2024-06-21 18:55:32,266 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 1.176
2024-06-21 18:55:32,565 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 1.366
2024-06-21 18:55:33,016 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 1.445
2024-06-21 18:55:33,308 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 1.070
2024-06-21 18:55:33,669 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 1.062
2024-06-21 18:55:33,972 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 1.368
2024-06-21 18:55:34,412 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 1.266
2024-06-21 18:55:34,714 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 1.020
2024-06-21 18:55:35,098 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 1.257
2024-06-21 18:55:35,397 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 1.326
2024-06-21 18:55:35,830 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 1.261
2024-06-21 18:55:36,128 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 1.238
2024-06-21 18:55:36,502 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 1.145
2024-06-21 18:55:36,795 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 1.504
2024-06-21 18:55:37,225 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 1.572
2024-06-21 18:55:37,525 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 1.588
2024-06-21 18:55:37,907 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 1.472
2024-06-21 18:55:38,207 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 1.437
2024-06-21 18:55:38,624 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 1.232
2024-06-21 18:55:38,918 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 1.569
2024-06-21 18:55:39,293 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 1.264
2024-06-21 18:55:39,588 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 1.107
2024-06-21 18:55:39,804 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 0.987
2024-06-21 18:55:50,811 - INFO: 53/200 final results:
2024-06-21 18:55:50,812 - INFO: Training loss: 1.270.
2024-06-21 18:55:50,812 - INFO: Training MAE: 1.276.
2024-06-21 18:55:50,812 - INFO: Training MSE: 3.568.
2024-06-21 18:56:11,557 - INFO: Epoch: 53/200, Loss_train: 1.2699698579722438, Loss_val: 2.6135799967009445
2024-06-21 18:56:11,557 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:56:11,557 - INFO: Epoch 54/200...
2024-06-21 18:56:11,557 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:56:11,557 - INFO: Batch size: 32.
2024-06-21 18:56:11,562 - INFO: Dataset:
2024-06-21 18:56:11,562 - INFO: Batch size:
2024-06-21 18:56:11,562 - INFO: Number of workers:
2024-06-21 18:56:12,751 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 1.460
2024-06-21 18:56:13,092 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 1.144
2024-06-21 18:56:13,487 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 1.526
2024-06-21 18:56:13,803 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 1.084
2024-06-21 18:56:14,204 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 1.063
2024-06-21 18:56:14,542 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 1.134
2024-06-21 18:56:14,931 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 1.533
2024-06-21 18:56:15,231 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 1.288
2024-06-21 18:56:15,619 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 0.942
2024-06-21 18:56:15,963 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 1.294
2024-06-21 18:56:16,340 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 1.259
2024-06-21 18:56:16,642 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 1.450
2024-06-21 18:56:17,051 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 1.076
2024-06-21 18:56:17,403 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 1.167
2024-06-21 18:56:17,799 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 0.989
2024-06-21 18:56:18,097 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 1.180
2024-06-21 18:56:18,492 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 1.423
2024-06-21 18:56:18,842 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 1.384
2024-06-21 18:56:19,224 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 1.510
2024-06-21 18:56:19,516 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 1.544
2024-06-21 18:56:19,902 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 1.179
2024-06-21 18:56:20,250 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 1.506
2024-06-21 18:56:20,632 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 1.263
2024-06-21 18:56:20,933 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 1.512
2024-06-21 18:56:21,316 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 1.451
2024-06-21 18:56:21,657 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 0.980
2024-06-21 18:56:22,041 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 1.065
2024-06-21 18:56:22,338 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 1.337
2024-06-21 18:56:22,551 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 1.504
2024-06-21 18:56:33,707 - INFO: 54/200 final results:
2024-06-21 18:56:33,707 - INFO: Training loss: 1.284.
2024-06-21 18:56:33,707 - INFO: Training MAE: 1.280.
2024-06-21 18:56:33,707 - INFO: Training MSE: 3.576.
2024-06-21 18:56:53,970 - INFO: Epoch: 54/200, Loss_train: 1.2843304934172795, Loss_val: 2.156179037587396
2024-06-21 18:56:53,971 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:56:53,971 - INFO: Epoch 55/200...
2024-06-21 18:56:53,971 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:56:53,971 - INFO: Batch size: 32.
2024-06-21 18:56:53,975 - INFO: Dataset:
2024-06-21 18:56:53,975 - INFO: Batch size:
2024-06-21 18:56:53,975 - INFO: Number of workers:
2024-06-21 18:56:55,143 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 1.169
2024-06-21 18:56:55,450 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 1.257
2024-06-21 18:56:55,838 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 1.286
2024-06-21 18:56:56,157 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 1.240
2024-06-21 18:56:56,582 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 1.265
2024-06-21 18:56:56,885 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 1.264
2024-06-21 18:56:57,275 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 1.198
2024-06-21 18:56:57,577 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 1.159
2024-06-21 18:56:58,012 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 1.340
2024-06-21 18:56:58,307 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 1.369
2024-06-21 18:56:58,669 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 1.306
2024-06-21 18:56:58,974 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 1.074
2024-06-21 18:56:59,418 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 1.391
2024-06-21 18:56:59,720 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 1.390
2024-06-21 18:57:00,109 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 1.497
2024-06-21 18:57:00,410 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 1.344
2024-06-21 18:57:00,834 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 1.351
2024-06-21 18:57:01,132 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 0.928
2024-06-21 18:57:01,506 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 1.208
2024-06-21 18:57:01,798 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 1.073
2024-06-21 18:57:02,221 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 1.378
2024-06-21 18:57:02,524 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 1.216
2024-06-21 18:57:02,906 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 1.737
2024-06-21 18:57:03,209 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 1.318
2024-06-21 18:57:03,631 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 1.293
2024-06-21 18:57:03,926 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 1.516
2024-06-21 18:57:04,291 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 1.237
2024-06-21 18:57:04,586 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 0.872
2024-06-21 18:57:04,793 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 1.393
2024-06-21 18:57:15,960 - INFO: 55/200 final results:
2024-06-21 18:57:15,961 - INFO: Training loss: 1.278.
2024-06-21 18:57:15,961 - INFO: Training MAE: 1.276.
2024-06-21 18:57:15,961 - INFO: Training MSE: 3.486.
2024-06-21 18:57:36,269 - INFO: Epoch: 55/200, Loss_train: 1.2782655173334583, Loss_val: 2.3726858147259415
2024-06-21 18:57:36,269 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:57:36,269 - INFO: Epoch 56/200...
2024-06-21 18:57:36,269 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:57:36,269 - INFO: Batch size: 32.
2024-06-21 18:57:36,273 - INFO: Dataset:
2024-06-21 18:57:36,273 - INFO: Batch size:
2024-06-21 18:57:36,273 - INFO: Number of workers:
2024-06-21 18:57:37,419 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 1.565
2024-06-21 18:57:37,765 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 1.425
2024-06-21 18:57:38,152 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 1.272
2024-06-21 18:57:38,471 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 1.260
2024-06-21 18:57:38,882 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 1.257
2024-06-21 18:57:39,208 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 1.008
2024-06-21 18:57:39,579 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 1.295
2024-06-21 18:57:39,882 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 1.237
2024-06-21 18:57:40,283 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 1.070
2024-06-21 18:57:40,611 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 1.020
2024-06-21 18:57:40,976 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 0.931
2024-06-21 18:57:41,280 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 1.414
2024-06-21 18:57:41,691 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 1.427
2024-06-21 18:57:42,017 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 1.401
2024-06-21 18:57:42,399 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 1.294
2024-06-21 18:57:42,696 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 1.675
2024-06-21 18:57:43,093 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 1.088
2024-06-21 18:57:43,414 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 1.108
2024-06-21 18:57:43,782 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 1.045
2024-06-21 18:57:44,074 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 1.372
2024-06-21 18:57:44,471 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 1.335
2024-06-21 18:57:44,794 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 1.398
2024-06-21 18:57:45,171 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 1.071
2024-06-21 18:57:45,470 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 1.437
2024-06-21 18:57:45,869 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 1.297
2024-06-21 18:57:46,188 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 1.183
2024-06-21 18:57:46,553 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 1.464
2024-06-21 18:57:46,848 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 1.096
2024-06-21 18:57:47,065 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 1.111
2024-06-21 18:57:58,096 - INFO: 56/200 final results:
2024-06-21 18:57:58,096 - INFO: Training loss: 1.261.
2024-06-21 18:57:58,096 - INFO: Training MAE: 1.264.
2024-06-21 18:57:58,096 - INFO: Training MSE: 3.458.
2024-06-21 18:58:18,663 - INFO: Epoch: 56/200, Loss_train: 1.2605811193071563, Loss_val: 2.1954963042818267
2024-06-21 18:58:18,663 - INFO: Best internal validation val_loss: 2.155 at epoch: 50.
2024-06-21 18:58:18,663 - INFO: Epoch 57/200...
2024-06-21 18:58:18,663 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:58:18,663 - INFO: Batch size: 32.
2024-06-21 18:58:18,667 - INFO: Dataset:
2024-06-21 18:58:18,667 - INFO: Batch size:
2024-06-21 18:58:18,667 - INFO: Number of workers:
2024-06-21 18:58:19,810 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 1.279
2024-06-21 18:58:20,155 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 1.214
2024-06-21 18:58:20,555 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 1.392
2024-06-21 18:58:20,860 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 1.140
2024-06-21 18:58:21,262 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 1.082
2024-06-21 18:58:21,601 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 1.145
2024-06-21 18:58:21,987 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 1.174
2024-06-21 18:58:22,290 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 1.207
2024-06-21 18:58:22,685 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 1.280
2024-06-21 18:58:23,031 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 1.520
2024-06-21 18:58:23,402 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 0.987
2024-06-21 18:58:23,706 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 1.008
2024-06-21 18:58:24,117 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 1.083
2024-06-21 18:58:24,459 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 1.262
2024-06-21 18:58:24,855 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 1.168
2024-06-21 18:58:25,156 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 1.181
2024-06-21 18:58:25,563 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 1.125
2024-06-21 18:58:25,898 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 1.423
2024-06-21 18:58:26,282 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 0.903
2024-06-21 18:58:26,574 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 1.299
2024-06-21 18:58:26,966 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 1.517
2024-06-21 18:58:27,302 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 1.485
2024-06-21 18:58:27,683 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 1.024
2024-06-21 18:58:27,983 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 1.288
2024-06-21 18:58:28,369 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 1.036
2024-06-21 18:58:28,703 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 1.101
2024-06-21 18:58:29,090 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 1.398
2024-06-21 18:58:29,388 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 1.557
2024-06-21 18:58:29,610 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 1.564
2024-06-21 18:58:40,791 - INFO: 57/200 final results:
2024-06-21 18:58:40,791 - INFO: Training loss: 1.236.
2024-06-21 18:58:40,791 - INFO: Training MAE: 1.229.
2024-06-21 18:58:40,791 - INFO: Training MSE: 3.330.
2024-06-21 18:59:00,843 - INFO: Epoch: 57/200, Loss_train: 1.2359919609694645, Loss_val: 2.1433561875902374
2024-06-21 18:59:00,863 - INFO: Saved new best metric model for epoch 57.
2024-06-21 18:59:00,863 - INFO: Best internal validation val_loss: 2.143 at epoch: 57.
2024-06-21 18:59:00,863 - INFO: Epoch 58/200...
2024-06-21 18:59:00,863 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:59:00,863 - INFO: Batch size: 32.
2024-06-21 18:59:00,867 - INFO: Dataset:
2024-06-21 18:59:00,867 - INFO: Batch size:
2024-06-21 18:59:00,867 - INFO: Number of workers:
2024-06-21 18:59:02,009 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 1.413
2024-06-21 18:59:02,339 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 1.031
2024-06-21 18:59:02,735 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 1.681
2024-06-21 18:59:03,051 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 1.197
2024-06-21 18:59:03,467 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 1.295
2024-06-21 18:59:03,779 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 1.168
2024-06-21 18:59:04,166 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 1.136
2024-06-21 18:59:04,478 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 1.100
2024-06-21 18:59:04,892 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 1.587
2024-06-21 18:59:05,196 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 1.300
2024-06-21 18:59:05,572 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 1.160
2024-06-21 18:59:05,889 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 1.133
2024-06-21 18:59:06,309 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 1.059
2024-06-21 18:59:06,624 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 1.055
2024-06-21 18:59:07,019 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 1.040
2024-06-21 18:59:07,331 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 1.250
2024-06-21 18:59:07,750 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 0.999
2024-06-21 18:59:08,061 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 0.900
2024-06-21 18:59:08,447 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 1.234
2024-06-21 18:59:08,753 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 1.180
2024-06-21 18:59:09,161 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 1.156
2024-06-21 18:59:09,474 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 1.444
2024-06-21 18:59:09,857 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 1.347
2024-06-21 18:59:10,170 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 1.142
2024-06-21 18:59:10,578 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 1.341
2024-06-21 18:59:10,886 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 1.349
2024-06-21 18:59:11,267 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 1.359
2024-06-21 18:59:11,577 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 1.109
2024-06-21 18:59:11,795 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 1.441
2024-06-21 18:59:22,899 - INFO: 58/200 final results:
2024-06-21 18:59:22,899 - INFO: Training loss: 1.228.
2024-06-21 18:59:22,899 - INFO: Training MAE: 1.224.
2024-06-21 18:59:22,899 - INFO: Training MSE: 3.244.
2024-06-21 18:59:43,344 - INFO: Epoch: 58/200, Loss_train: 1.2277952288759166, Loss_val: 2.2900714545414367
2024-06-21 18:59:43,344 - INFO: Best internal validation val_loss: 2.143 at epoch: 57.
2024-06-21 18:59:43,344 - INFO: Epoch 59/200...
2024-06-21 18:59:43,344 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 18:59:43,344 - INFO: Batch size: 32.
2024-06-21 18:59:43,349 - INFO: Dataset:
2024-06-21 18:59:43,349 - INFO: Batch size:
2024-06-21 18:59:43,349 - INFO: Number of workers:
2024-06-21 18:59:44,507 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 0.979
2024-06-21 18:59:44,814 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 1.133
2024-06-21 18:59:45,227 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 1.035
2024-06-21 18:59:45,533 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 1.187
2024-06-21 18:59:45,940 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 1.085
2024-06-21 18:59:46,241 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 1.315
2024-06-21 18:59:46,640 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 1.267
2024-06-21 18:59:46,941 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 1.484
2024-06-21 18:59:47,341 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 1.164
2024-06-21 18:59:47,633 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 1.080
2024-06-21 18:59:48,044 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 1.174
2024-06-21 18:59:48,349 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 1.064
2024-06-21 18:59:48,769 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 1.306
2024-06-21 18:59:49,073 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 1.426
2024-06-21 18:59:49,504 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 1.274
2024-06-21 18:59:49,804 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 1.205
2024-06-21 18:59:50,209 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 1.386
2024-06-21 18:59:50,509 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 1.024
2024-06-21 18:59:50,927 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 1.246
2024-06-21 18:59:51,220 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 1.311
2024-06-21 18:59:51,616 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 0.961
2024-06-21 18:59:51,917 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 1.163
2024-06-21 18:59:52,344 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 1.128
2024-06-21 18:59:52,648 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 1.140
2024-06-21 18:59:53,057 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 1.200
2024-06-21 18:59:53,362 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 1.172
2024-06-21 18:59:53,791 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 1.280
2024-06-21 18:59:54,096 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 1.230
2024-06-21 18:59:54,334 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 1.314
2024-06-21 19:00:05,518 - INFO: 59/200 final results:
2024-06-21 19:00:05,518 - INFO: Training loss: 1.198.
2024-06-21 19:00:05,518 - INFO: Training MAE: 1.195.
2024-06-21 19:00:05,518 - INFO: Training MSE: 3.101.
2024-06-21 19:00:26,034 - INFO: Epoch: 59/200, Loss_train: 1.197643908961066, Loss_val: 2.115948220779156
2024-06-21 19:00:26,053 - INFO: Saved new best metric model for epoch 59.
2024-06-21 19:00:26,053 - INFO: Best internal validation val_loss: 2.116 at epoch: 59.
2024-06-21 19:00:26,053 - INFO: Epoch 60/200...
2024-06-21 19:00:26,053 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:00:26,053 - INFO: Batch size: 32.
2024-06-21 19:00:26,057 - INFO: Dataset:
2024-06-21 19:00:26,057 - INFO: Batch size:
2024-06-21 19:00:26,057 - INFO: Number of workers:
2024-06-21 19:00:27,253 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 1.011
2024-06-21 19:00:27,572 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 1.212
2024-06-21 19:00:27,966 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 1.269
2024-06-21 19:00:28,283 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 1.088
2024-06-21 19:00:28,708 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 1.208
2024-06-21 19:00:29,008 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 1.372
2024-06-21 19:00:29,390 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 1.190
2024-06-21 19:00:29,703 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 0.931
2024-06-21 19:00:30,129 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 1.117
2024-06-21 19:00:30,420 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 0.822
2024-06-21 19:00:30,792 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 1.279
2024-06-21 19:00:31,106 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 0.997
2024-06-21 19:00:31,538 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 0.908
2024-06-21 19:00:31,841 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 0.954
2024-06-21 19:00:32,230 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 1.287
2024-06-21 19:00:32,542 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 1.472
2024-06-21 19:00:32,970 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 1.336
2024-06-21 19:00:33,269 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 1.044
2024-06-21 19:00:33,648 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 1.197
2024-06-21 19:00:33,954 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 1.267
2024-06-21 19:00:34,374 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 1.215
2024-06-21 19:00:34,675 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 1.064
2024-06-21 19:00:35,054 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 1.067
2024-06-21 19:00:35,368 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 1.456
2024-06-21 19:00:35,783 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 1.359
2024-06-21 19:00:36,079 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 1.209
2024-06-21 19:00:36,452 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 1.247
2024-06-21 19:00:36,761 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 1.365
2024-06-21 19:00:36,981 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 1.327
2024-06-21 19:00:48,089 - INFO: 60/200 final results:
2024-06-21 19:00:48,089 - INFO: Training loss: 1.182.
2024-06-21 19:00:48,089 - INFO: Training MAE: 1.179.
2024-06-21 19:00:48,089 - INFO: Training MSE: 3.037.
2024-06-21 19:01:08,538 - INFO: Epoch: 60/200, Loss_train: 1.1817836576494678, Loss_val: 2.3335086970493712
2024-06-21 19:01:08,538 - INFO: Best internal validation val_loss: 2.116 at epoch: 59.
2024-06-21 19:01:08,538 - INFO: Epoch 61/200...
2024-06-21 19:01:08,538 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:01:08,538 - INFO: Batch size: 32.
2024-06-21 19:01:08,542 - INFO: Dataset:
2024-06-21 19:01:08,542 - INFO: Batch size:
2024-06-21 19:01:08,542 - INFO: Number of workers:
2024-06-21 19:01:09,715 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 1.013
2024-06-21 19:01:10,021 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 1.087
2024-06-21 19:01:10,435 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 1.261
2024-06-21 19:01:10,754 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 1.278
2024-06-21 19:01:11,165 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 1.233
2024-06-21 19:01:11,465 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 1.099
2024-06-21 19:01:11,863 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 1.335
2024-06-21 19:01:12,179 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 1.078
2024-06-21 19:01:12,619 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 1.147
2024-06-21 19:01:12,913 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 1.214
2024-06-21 19:01:13,301 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 1.026
2024-06-21 19:01:13,619 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 1.248
2024-06-21 19:01:14,038 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 1.057
2024-06-21 19:01:14,341 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 1.139
2024-06-21 19:01:14,747 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 1.269
2024-06-21 19:01:15,062 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 1.350
2024-06-21 19:01:15,478 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 1.194
2024-06-21 19:01:15,778 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 0.917
2024-06-21 19:01:16,173 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 1.281
2024-06-21 19:01:16,481 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 1.198
2024-06-21 19:01:16,876 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 1.079
2024-06-21 19:01:17,179 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 1.268
2024-06-21 19:01:17,570 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 1.034
2024-06-21 19:01:17,884 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 1.029
2024-06-21 19:01:18,292 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 1.193
2024-06-21 19:01:18,591 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 1.500
2024-06-21 19:01:18,989 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 1.305
2024-06-21 19:01:19,301 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 1.106
2024-06-21 19:01:19,511 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 1.223
2024-06-21 19:01:30,678 - INFO: 61/200 final results:
2024-06-21 19:01:30,679 - INFO: Training loss: 1.178.
2024-06-21 19:01:30,679 - INFO: Training MAE: 1.177.
2024-06-21 19:01:30,679 - INFO: Training MSE: 3.039.
2024-06-21 19:01:51,417 - INFO: Epoch: 61/200, Loss_train: 1.1778967935463478, Loss_val: 2.039825838187645
2024-06-21 19:01:51,435 - INFO: Saved new best metric model for epoch 61.
2024-06-21 19:01:51,435 - INFO: Best internal validation val_loss: 2.040 at epoch: 61.
2024-06-21 19:01:51,435 - INFO: Epoch 62/200...
2024-06-21 19:01:51,435 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:01:51,435 - INFO: Batch size: 32.
2024-06-21 19:01:51,440 - INFO: Dataset:
2024-06-21 19:01:51,440 - INFO: Batch size:
2024-06-21 19:01:51,440 - INFO: Number of workers:
2024-06-21 19:01:52,607 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 0.988
2024-06-21 19:01:52,926 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 1.058
2024-06-21 19:01:53,335 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 1.310
2024-06-21 19:01:53,651 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 1.276
2024-06-21 19:01:54,068 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 1.174
2024-06-21 19:01:54,368 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 1.054
2024-06-21 19:01:54,766 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 1.119
2024-06-21 19:01:55,080 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 0.994
2024-06-21 19:01:55,495 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 1.320
2024-06-21 19:01:55,787 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 1.153
2024-06-21 19:01:56,175 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 1.060
2024-06-21 19:01:56,490 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 0.950
2024-06-21 19:01:56,908 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 0.970
2024-06-21 19:01:57,210 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 1.236
2024-06-21 19:01:57,616 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 0.968
2024-06-21 19:01:57,928 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 1.250
2024-06-21 19:01:58,342 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 1.137
2024-06-21 19:01:58,640 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 1.425
2024-06-21 19:01:59,039 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 1.307
2024-06-21 19:01:59,345 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 1.537
2024-06-21 19:01:59,750 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 0.976
2024-06-21 19:02:00,051 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 1.338
2024-06-21 19:02:00,439 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 1.381
2024-06-21 19:02:00,751 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 1.443
2024-06-21 19:02:01,149 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 1.356
2024-06-21 19:02:01,445 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 1.184
2024-06-21 19:02:01,833 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 1.268
2024-06-21 19:02:02,141 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 0.966
2024-06-21 19:02:02,350 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 0.924
2024-06-21 19:02:13,541 - INFO: 62/200 final results:
2024-06-21 19:02:13,541 - INFO: Training loss: 1.177.
2024-06-21 19:02:13,542 - INFO: Training MAE: 1.182.
2024-06-21 19:02:13,542 - INFO: Training MSE: 3.042.
2024-06-21 19:02:33,877 - INFO: Epoch: 62/200, Loss_train: 1.1765690733646523, Loss_val: 2.4800083308384337
2024-06-21 19:02:33,877 - INFO: Best internal validation val_loss: 2.040 at epoch: 61.
2024-06-21 19:02:33,877 - INFO: Epoch 63/200...
2024-06-21 19:02:33,877 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:02:33,877 - INFO: Batch size: 32.
2024-06-21 19:02:33,881 - INFO: Dataset:
2024-06-21 19:02:33,881 - INFO: Batch size:
2024-06-21 19:02:33,881 - INFO: Number of workers:
2024-06-21 19:02:35,039 - INFO: Epoch: 63/200, Batch: 1/29, Batch_Loss_Train: 1.498
2024-06-21 19:02:35,344 - INFO: Epoch: 63/200, Batch: 2/29, Batch_Loss_Train: 1.229
2024-06-21 19:02:35,739 - INFO: Epoch: 63/200, Batch: 3/29, Batch_Loss_Train: 1.313
2024-06-21 19:02:36,054 - INFO: Epoch: 63/200, Batch: 4/29, Batch_Loss_Train: 1.399
2024-06-21 19:02:36,472 - INFO: Epoch: 63/200, Batch: 5/29, Batch_Loss_Train: 1.155
2024-06-21 19:02:36,770 - INFO: Epoch: 63/200, Batch: 6/29, Batch_Loss_Train: 1.270
2024-06-21 19:02:37,149 - INFO: Epoch: 63/200, Batch: 7/29, Batch_Loss_Train: 0.929
2024-06-21 19:02:37,461 - INFO: Epoch: 63/200, Batch: 8/29, Batch_Loss_Train: 1.045
2024-06-21 19:02:37,893 - INFO: Epoch: 63/200, Batch: 9/29, Batch_Loss_Train: 1.226
2024-06-21 19:02:38,185 - INFO: Epoch: 63/200, Batch: 10/29, Batch_Loss_Train: 1.461
2024-06-21 19:02:38,554 - INFO: Epoch: 63/200, Batch: 11/29, Batch_Loss_Train: 1.281
2024-06-21 19:02:38,869 - INFO: Epoch: 63/200, Batch: 12/29, Batch_Loss_Train: 1.043
2024-06-21 19:02:39,291 - INFO: Epoch: 63/200, Batch: 13/29, Batch_Loss_Train: 1.069
2024-06-21 19:02:39,592 - INFO: Epoch: 63/200, Batch: 14/29, Batch_Loss_Train: 1.235
2024-06-21 19:02:39,984 - INFO: Epoch: 63/200, Batch: 15/29, Batch_Loss_Train: 1.164
2024-06-21 19:02:40,294 - INFO: Epoch: 63/200, Batch: 16/29, Batch_Loss_Train: 1.068
2024-06-21 19:02:40,718 - INFO: Epoch: 63/200, Batch: 17/29, Batch_Loss_Train: 1.172
2024-06-21 19:02:41,016 - INFO: Epoch: 63/200, Batch: 18/29, Batch_Loss_Train: 1.446
2024-06-21 19:02:41,399 - INFO: Epoch: 63/200, Batch: 19/29, Batch_Loss_Train: 1.003
2024-06-21 19:02:41,705 - INFO: Epoch: 63/200, Batch: 20/29, Batch_Loss_Train: 1.261
2024-06-21 19:02:42,121 - INFO: Epoch: 63/200, Batch: 21/29, Batch_Loss_Train: 1.166
2024-06-21 19:02:42,421 - INFO: Epoch: 63/200, Batch: 22/29, Batch_Loss_Train: 1.336
2024-06-21 19:02:42,804 - INFO: Epoch: 63/200, Batch: 23/29, Batch_Loss_Train: 1.117
2024-06-21 19:02:43,118 - INFO: Epoch: 63/200, Batch: 24/29, Batch_Loss_Train: 1.230
2024-06-21 19:02:43,538 - INFO: Epoch: 63/200, Batch: 25/29, Batch_Loss_Train: 1.009
2024-06-21 19:02:43,835 - INFO: Epoch: 63/200, Batch: 26/29, Batch_Loss_Train: 1.112
2024-06-21 19:02:44,213 - INFO: Epoch: 63/200, Batch: 27/29, Batch_Loss_Train: 0.979
2024-06-21 19:02:44,522 - INFO: Epoch: 63/200, Batch: 28/29, Batch_Loss_Train: 1.209
2024-06-21 19:02:44,738 - INFO: Epoch: 63/200, Batch: 29/29, Batch_Loss_Train: 1.353
2024-06-21 19:02:55,822 - INFO: 63/200 final results:
2024-06-21 19:02:55,822 - INFO: Training loss: 1.199.
2024-06-21 19:02:55,822 - INFO: Training MAE: 1.196.
2024-06-21 19:02:55,822 - INFO: Training MSE: 2.963.
2024-06-21 19:03:16,358 - INFO: Epoch: 63/200, Loss_train: 1.199177705008408, Loss_val: 2.120638210197975
2024-06-21 19:03:16,358 - INFO: Best internal validation val_loss: 2.040 at epoch: 61.
2024-06-21 19:03:16,358 - INFO: Epoch 64/200...
2024-06-21 19:03:16,358 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:03:16,358 - INFO: Batch size: 32.
2024-06-21 19:03:16,362 - INFO: Dataset:
2024-06-21 19:03:16,362 - INFO: Batch size:
2024-06-21 19:03:16,362 - INFO: Number of workers:
2024-06-21 19:03:17,542 - INFO: Epoch: 64/200, Batch: 1/29, Batch_Loss_Train: 1.612
2024-06-21 19:03:17,850 - INFO: Epoch: 64/200, Batch: 2/29, Batch_Loss_Train: 1.018
2024-06-21 19:03:18,239 - INFO: Epoch: 64/200, Batch: 3/29, Batch_Loss_Train: 1.192
2024-06-21 19:03:18,559 - INFO: Epoch: 64/200, Batch: 4/29, Batch_Loss_Train: 1.145
2024-06-21 19:03:18,996 - INFO: Epoch: 64/200, Batch: 5/29, Batch_Loss_Train: 1.200
2024-06-21 19:03:19,295 - INFO: Epoch: 64/200, Batch: 6/29, Batch_Loss_Train: 0.965
2024-06-21 19:03:19,669 - INFO: Epoch: 64/200, Batch: 7/29, Batch_Loss_Train: 1.226
2024-06-21 19:03:19,969 - INFO: Epoch: 64/200, Batch: 8/29, Batch_Loss_Train: 1.173
2024-06-21 19:03:20,409 - INFO: Epoch: 64/200, Batch: 9/29, Batch_Loss_Train: 1.013
2024-06-21 19:03:20,700 - INFO: Epoch: 64/200, Batch: 10/29, Batch_Loss_Train: 1.054
2024-06-21 19:03:21,069 - INFO: Epoch: 64/200, Batch: 11/29, Batch_Loss_Train: 1.231
2024-06-21 19:03:21,372 - INFO: Epoch: 64/200, Batch: 12/29, Batch_Loss_Train: 1.122
2024-06-21 19:03:21,813 - INFO: Epoch: 64/200, Batch: 13/29, Batch_Loss_Train: 1.138
2024-06-21 19:03:22,115 - INFO: Epoch: 64/200, Batch: 14/29, Batch_Loss_Train: 1.098
2024-06-21 19:03:22,505 - INFO: Epoch: 64/200, Batch: 15/29, Batch_Loss_Train: 0.966
2024-06-21 19:03:22,804 - INFO: Epoch: 64/200, Batch: 16/29, Batch_Loss_Train: 1.124
2024-06-21 19:03:23,246 - INFO: Epoch: 64/200, Batch: 17/29, Batch_Loss_Train: 1.500
2024-06-21 19:03:23,545 - INFO: Epoch: 64/200, Batch: 18/29, Batch_Loss_Train: 1.136
2024-06-21 19:03:23,919 - INFO: Epoch: 64/200, Batch: 19/29, Batch_Loss_Train: 0.926
2024-06-21 19:03:24,214 - INFO: Epoch: 64/200, Batch: 20/29, Batch_Loss_Train: 0.943
2024-06-21 19:03:24,646 - INFO: Epoch: 64/200, Batch: 21/29, Batch_Loss_Train: 0.984
2024-06-21 19:03:24,946 - INFO: Epoch: 64/200, Batch: 22/29, Batch_Loss_Train: 1.149
2024-06-21 19:03:25,330 - INFO: Epoch: 64/200, Batch: 23/29, Batch_Loss_Train: 0.927
2024-06-21 19:03:25,631 - INFO: Epoch: 64/200, Batch: 24/29, Batch_Loss_Train: 1.017
2024-06-21 19:03:26,061 - INFO: Epoch: 64/200, Batch: 25/29, Batch_Loss_Train: 1.529
2024-06-21 19:03:26,356 - INFO: Epoch: 64/200, Batch: 26/29, Batch_Loss_Train: 1.056
2024-06-21 19:03:26,737 - INFO: Epoch: 64/200, Batch: 27/29, Batch_Loss_Train: 1.620
2024-06-21 19:03:27,033 - INFO: Epoch: 64/200, Batch: 28/29, Batch_Loss_Train: 1.020
2024-06-21 19:03:27,250 - INFO: Epoch: 64/200, Batch: 29/29, Batch_Loss_Train: 1.012
2024-06-21 19:03:38,340 - INFO: 64/200 final results:
2024-06-21 19:03:38,341 - INFO: Training loss: 1.141.
2024-06-21 19:03:38,341 - INFO: Training MAE: 1.144.
2024-06-21 19:03:38,341 - INFO: Training MSE: 2.765.
2024-06-21 19:03:58,691 - INFO: Epoch: 64/200, Loss_train: 1.141190387051681, Loss_val: 2.1924105964857956
2024-06-21 19:03:58,691 - INFO: Best internal validation val_loss: 2.040 at epoch: 61.
2024-06-21 19:03:58,691 - INFO: Epoch 65/200...
2024-06-21 19:03:58,691 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:03:58,691 - INFO: Batch size: 32.
2024-06-21 19:03:58,695 - INFO: Dataset:
2024-06-21 19:03:58,695 - INFO: Batch size:
2024-06-21 19:03:58,695 - INFO: Number of workers:
2024-06-21 19:03:59,841 - INFO: Epoch: 65/200, Batch: 1/29, Batch_Loss_Train: 1.107
2024-06-21 19:04:00,161 - INFO: Epoch: 65/200, Batch: 2/29, Batch_Loss_Train: 1.171
2024-06-21 19:04:00,571 - INFO: Epoch: 65/200, Batch: 3/29, Batch_Loss_Train: 1.017
2024-06-21 19:04:00,889 - INFO: Epoch: 65/200, Batch: 4/29, Batch_Loss_Train: 1.050
2024-06-21 19:04:01,300 - INFO: Epoch: 65/200, Batch: 5/29, Batch_Loss_Train: 1.055
2024-06-21 19:04:01,601 - INFO: Epoch: 65/200, Batch: 6/29, Batch_Loss_Train: 1.101
2024-06-21 19:04:02,001 - INFO: Epoch: 65/200, Batch: 7/29, Batch_Loss_Train: 1.149
2024-06-21 19:04:02,314 - INFO: Epoch: 65/200, Batch: 8/29, Batch_Loss_Train: 1.195
2024-06-21 19:04:03,013 - INFO: Epoch: 65/200, Batch: 9/29, Batch_Loss_Train: 1.073
2024-06-21 19:04:03,307 - INFO: Epoch: 65/200, Batch: 10/29, Batch_Loss_Train: 1.461
2024-06-21 19:04:03,687 - INFO: Epoch: 65/200, Batch: 11/29, Batch_Loss_Train: 1.232
2024-06-21 19:04:04,004 - INFO: Epoch: 65/200, Batch: 12/29, Batch_Loss_Train: 1.015
2024-06-21 19:04:04,415 - INFO: Epoch: 65/200, Batch: 13/29, Batch_Loss_Train: 1.104
2024-06-21 19:04:04,717 - INFO: Epoch: 65/200, Batch: 14/29, Batch_Loss_Train: 1.170
2024-06-21 19:04:05,121 - INFO: Epoch: 65/200, Batch: 15/29, Batch_Loss_Train: 0.948
2024-06-21 19:04:05,433 - INFO: Epoch: 65/200, Batch: 16/29, Batch_Loss_Train: 0.976
2024-06-21 19:04:05,842 - INFO: Epoch: 65/200, Batch: 17/29, Batch_Loss_Train: 1.473
2024-06-21 19:04:06,141 - INFO: Epoch: 65/200, Batch: 18/29, Batch_Loss_Train: 1.059
2024-06-21 19:04:06,538 - INFO: Epoch: 65/200, Batch: 19/29, Batch_Loss_Train: 1.354
2024-06-21 19:04:06,845 - INFO: Epoch: 65/200, Batch: 20/29, Batch_Loss_Train: 1.050
2024-06-21 19:04:07,244 - INFO: Epoch: 65/200, Batch: 21/29, Batch_Loss_Train: 1.302
2024-06-21 19:04:07,545 - INFO: Epoch: 65/200, Batch: 22/29, Batch_Loss_Train: 1.100
2024-06-21 19:04:07,939 - INFO: Epoch: 65/200, Batch: 23/29, Batch_Loss_Train: 1.078
2024-06-21 19:04:08,254 - INFO: Epoch: 65/200, Batch: 24/29, Batch_Loss_Train: 1.080
2024-06-21 19:04:08,653 - INFO: Epoch: 65/200, Batch: 25/29, Batch_Loss_Train: 1.106
2024-06-21 19:04:08,950 - INFO: Epoch: 65/200, Batch: 26/29, Batch_Loss_Train: 0.983
2024-06-21 19:04:09,336 - INFO: Epoch: 65/200, Batch: 27/29, Batch_Loss_Train: 0.963
2024-06-21 19:04:09,645 - INFO: Epoch: 65/200, Batch: 28/29, Batch_Loss_Train: 1.065
2024-06-21 19:04:09,865 - INFO: Epoch: 65/200, Batch: 29/29, Batch_Loss_Train: 1.066
2024-06-21 19:04:20,681 - INFO: 65/200 final results:
2024-06-21 19:04:20,681 - INFO: Training loss: 1.121.
2024-06-21 19:04:20,682 - INFO: Training MAE: 1.122.
2024-06-21 19:04:20,682 - INFO: Training MSE: 2.777.
2024-06-21 19:04:41,084 - INFO: Epoch: 65/200, Loss_train: 1.1207940516800716, Loss_val: 2.295667048158317
2024-06-21 19:04:41,084 - INFO: Best internal validation val_loss: 2.040 at epoch: 61.
2024-06-21 19:04:41,084 - INFO: Epoch 66/200...
2024-06-21 19:04:41,084 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:04:41,084 - INFO: Batch size: 32.
2024-06-21 19:04:41,088 - INFO: Dataset:
2024-06-21 19:04:41,088 - INFO: Batch size:
2024-06-21 19:04:41,088 - INFO: Number of workers:
2024-06-21 19:04:42,261 - INFO: Epoch: 66/200, Batch: 1/29, Batch_Loss_Train: 1.009
2024-06-21 19:04:42,579 - INFO: Epoch: 66/200, Batch: 2/29, Batch_Loss_Train: 0.928
2024-06-21 19:04:42,968 - INFO: Epoch: 66/200, Batch: 3/29, Batch_Loss_Train: 1.194
2024-06-21 19:04:43,286 - INFO: Epoch: 66/200, Batch: 4/29, Batch_Loss_Train: 1.381
2024-06-21 19:04:43,684 - INFO: Epoch: 66/200, Batch: 5/29, Batch_Loss_Train: 1.099
2024-06-21 19:04:44,010 - INFO: Epoch: 66/200, Batch: 6/29, Batch_Loss_Train: 1.142
2024-06-21 19:04:44,398 - INFO: Epoch: 66/200, Batch: 7/29, Batch_Loss_Train: 1.160
2024-06-21 19:04:44,714 - INFO: Epoch: 66/200, Batch: 8/29, Batch_Loss_Train: 0.920
2024-06-21 19:04:45,102 - INFO: Epoch: 66/200, Batch: 9/29, Batch_Loss_Train: 1.357
2024-06-21 19:04:45,425 - INFO: Epoch: 66/200, Batch: 10/29, Batch_Loss_Train: 1.171
2024-06-21 19:04:45,799 - INFO: Epoch: 66/200, Batch: 11/29, Batch_Loss_Train: 1.194
2024-06-21 19:04:46,117 - INFO: Epoch: 66/200, Batch: 12/29, Batch_Loss_Train: 1.174
2024-06-21 19:04:46,524 - INFO: Epoch: 66/200, Batch: 13/29, Batch_Loss_Train: 1.287
2024-06-21 19:04:46,855 - INFO: Epoch: 66/200, Batch: 14/29, Batch_Loss_Train: 1.332
2024-06-21 19:04:47,248 - INFO: Epoch: 66/200, Batch: 15/29, Batch_Loss_Train: 1.424
2024-06-21 19:04:47,562 - INFO: Epoch: 66/200, Batch: 16/29, Batch_Loss_Train: 1.010
2024-06-21 19:04:47,964 - INFO: Epoch: 66/200, Batch: 17/29, Batch_Loss_Train: 1.312
2024-06-21 19:04:48,290 - INFO: Epoch: 66/200, Batch: 18/29, Batch_Loss_Train: 0.982
2024-06-21 19:04:48,674 - INFO: Epoch: 66/200, Batch: 19/29, Batch_Loss_Train: 1.033
2024-06-21 19:04:48,982 - INFO: Epoch: 66/200, Batch: 20/29, Batch_Loss_Train: 1.100
2024-06-21 19:04:49,372 - INFO: Epoch: 66/200, Batch: 21/29, Batch_Loss_Train: 0.939
2024-06-21 19:04:49,702 - INFO: Epoch: 66/200, Batch: 22/29, Batch_Loss_Train: 0.971
2024-06-21 19:04:50,079 - INFO: Epoch: 66/200, Batch: 23/29, Batch_Loss_Train: 1.335
2024-06-21 19:04:50,394 - INFO: Epoch: 66/200, Batch: 24/29, Batch_Loss_Train: 1.228
2024-06-21 19:04:50,782 - INFO: Epoch: 66/200, Batch: 25/29, Batch_Loss_Train: 1.022
2024-06-21 19:04:51,105 - INFO: Epoch: 66/200, Batch: 26/29, Batch_Loss_Train: 1.213
2024-06-21 19:04:51,477 - INFO: Epoch: 66/200, Batch: 27/29, Batch_Loss_Train: 1.300
2024-06-21 19:04:51,788 - INFO: Epoch: 66/200, Batch: 28/29, Batch_Loss_Train: 1.264
2024-06-21 19:04:51,999 - INFO: Epoch: 66/200, Batch: 29/29, Batch_Loss_Train: 1.213
2024-06-21 19:05:03,150 - INFO: 66/200 final results:
2024-06-21 19:05:03,150 - INFO: Training loss: 1.162.
2024-06-21 19:05:03,150 - INFO: Training MAE: 1.161.
2024-06-21 19:05:03,150 - INFO: Training MSE: 2.779.
2024-06-21 19:05:23,806 - INFO: Epoch: 66/200, Loss_train: 1.1618669875736893, Loss_val: 2.0362584796445122
2024-06-21 19:05:23,825 - INFO: Saved new best metric model for epoch 66.
2024-06-21 19:05:23,825 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:05:23,825 - INFO: Epoch 67/200...
2024-06-21 19:05:23,825 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:05:23,825 - INFO: Batch size: 32.
2024-06-21 19:05:23,829 - INFO: Dataset:
2024-06-21 19:05:23,829 - INFO: Batch size:
2024-06-21 19:05:23,829 - INFO: Number of workers:
2024-06-21 19:05:25,013 - INFO: Epoch: 67/200, Batch: 1/29, Batch_Loss_Train: 1.104
2024-06-21 19:05:25,318 - INFO: Epoch: 67/200, Batch: 2/29, Batch_Loss_Train: 1.041
2024-06-21 19:05:25,710 - INFO: Epoch: 67/200, Batch: 3/29, Batch_Loss_Train: 1.087
2024-06-21 19:05:26,025 - INFO: Epoch: 67/200, Batch: 4/29, Batch_Loss_Train: 1.171
2024-06-21 19:05:26,446 - INFO: Epoch: 67/200, Batch: 5/29, Batch_Loss_Train: 0.920
2024-06-21 19:05:26,747 - INFO: Epoch: 67/200, Batch: 6/29, Batch_Loss_Train: 1.109
2024-06-21 19:05:27,132 - INFO: Epoch: 67/200, Batch: 7/29, Batch_Loss_Train: 1.098
2024-06-21 19:05:27,445 - INFO: Epoch: 67/200, Batch: 8/29, Batch_Loss_Train: 1.100
2024-06-21 19:05:27,873 - INFO: Epoch: 67/200, Batch: 9/29, Batch_Loss_Train: 1.409
2024-06-21 19:05:28,166 - INFO: Epoch: 67/200, Batch: 10/29, Batch_Loss_Train: 1.014
2024-06-21 19:05:28,545 - INFO: Epoch: 67/200, Batch: 11/29, Batch_Loss_Train: 0.870
2024-06-21 19:05:28,862 - INFO: Epoch: 67/200, Batch: 12/29, Batch_Loss_Train: 1.111
2024-06-21 19:05:29,294 - INFO: Epoch: 67/200, Batch: 13/29, Batch_Loss_Train: 1.233
2024-06-21 19:05:29,599 - INFO: Epoch: 67/200, Batch: 14/29, Batch_Loss_Train: 0.948
2024-06-21 19:05:29,991 - INFO: Epoch: 67/200, Batch: 15/29, Batch_Loss_Train: 0.981
2024-06-21 19:05:30,306 - INFO: Epoch: 67/200, Batch: 16/29, Batch_Loss_Train: 1.060
2024-06-21 19:05:30,728 - INFO: Epoch: 67/200, Batch: 17/29, Batch_Loss_Train: 1.077
2024-06-21 19:05:31,027 - INFO: Epoch: 67/200, Batch: 18/29, Batch_Loss_Train: 1.208
2024-06-21 19:05:31,408 - INFO: Epoch: 67/200, Batch: 19/29, Batch_Loss_Train: 1.328
2024-06-21 19:05:31,714 - INFO: Epoch: 67/200, Batch: 20/29, Batch_Loss_Train: 0.917
2024-06-21 19:05:32,127 - INFO: Epoch: 67/200, Batch: 21/29, Batch_Loss_Train: 1.043
2024-06-21 19:05:32,429 - INFO: Epoch: 67/200, Batch: 22/29, Batch_Loss_Train: 1.066
2024-06-21 19:05:32,817 - INFO: Epoch: 67/200, Batch: 23/29, Batch_Loss_Train: 0.903
2024-06-21 19:05:33,131 - INFO: Epoch: 67/200, Batch: 24/29, Batch_Loss_Train: 1.184
2024-06-21 19:05:33,552 - INFO: Epoch: 67/200, Batch: 25/29, Batch_Loss_Train: 1.101
2024-06-21 19:05:33,849 - INFO: Epoch: 67/200, Batch: 26/29, Batch_Loss_Train: 0.958
2024-06-21 19:05:34,221 - INFO: Epoch: 67/200, Batch: 27/29, Batch_Loss_Train: 1.535
2024-06-21 19:05:34,531 - INFO: Epoch: 67/200, Batch: 28/29, Batch_Loss_Train: 1.051
2024-06-21 19:05:34,751 - INFO: Epoch: 67/200, Batch: 29/29, Batch_Loss_Train: 1.378
2024-06-21 19:05:45,894 - INFO: 67/200 final results:
2024-06-21 19:05:45,894 - INFO: Training loss: 1.104.
2024-06-21 19:05:45,894 - INFO: Training MAE: 1.098.
2024-06-21 19:05:45,894 - INFO: Training MSE: 2.612.
2024-06-21 19:06:06,240 - INFO: Epoch: 67/200, Loss_train: 1.103601794818352, Loss_val: 2.039808149995475
2024-06-21 19:06:06,240 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:06:06,240 - INFO: Epoch 68/200...
2024-06-21 19:06:06,240 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:06:06,241 - INFO: Batch size: 32.
2024-06-21 19:06:06,244 - INFO: Dataset:
2024-06-21 19:06:06,245 - INFO: Batch size:
2024-06-21 19:06:06,245 - INFO: Number of workers:
2024-06-21 19:06:07,417 - INFO: Epoch: 68/200, Batch: 1/29, Batch_Loss_Train: 1.045
2024-06-21 19:06:07,721 - INFO: Epoch: 68/200, Batch: 2/29, Batch_Loss_Train: 1.197
2024-06-21 19:06:08,121 - INFO: Epoch: 68/200, Batch: 3/29, Batch_Loss_Train: 1.111
2024-06-21 19:06:08,440 - INFO: Epoch: 68/200, Batch: 4/29, Batch_Loss_Train: 1.092
2024-06-21 19:06:08,855 - INFO: Epoch: 68/200, Batch: 5/29, Batch_Loss_Train: 1.022
2024-06-21 19:06:09,157 - INFO: Epoch: 68/200, Batch: 6/29, Batch_Loss_Train: 1.244
2024-06-21 19:06:09,558 - INFO: Epoch: 68/200, Batch: 7/29, Batch_Loss_Train: 1.005
2024-06-21 19:06:09,874 - INFO: Epoch: 68/200, Batch: 8/29, Batch_Loss_Train: 1.101
2024-06-21 19:06:10,281 - INFO: Epoch: 68/200, Batch: 9/29, Batch_Loss_Train: 1.295
2024-06-21 19:06:10,575 - INFO: Epoch: 68/200, Batch: 10/29, Batch_Loss_Train: 1.040
2024-06-21 19:06:10,963 - INFO: Epoch: 68/200, Batch: 11/29, Batch_Loss_Train: 1.583
2024-06-21 19:06:11,281 - INFO: Epoch: 68/200, Batch: 12/29, Batch_Loss_Train: 1.429
2024-06-21 19:06:11,695 - INFO: Epoch: 68/200, Batch: 13/29, Batch_Loss_Train: 1.317
2024-06-21 19:06:11,996 - INFO: Epoch: 68/200, Batch: 14/29, Batch_Loss_Train: 1.167
2024-06-21 19:06:12,397 - INFO: Epoch: 68/200, Batch: 15/29, Batch_Loss_Train: 1.316
2024-06-21 19:06:12,708 - INFO: Epoch: 68/200, Batch: 16/29, Batch_Loss_Train: 0.961
2024-06-21 19:06:13,121 - INFO: Epoch: 68/200, Batch: 17/29, Batch_Loss_Train: 1.165
2024-06-21 19:06:13,422 - INFO: Epoch: 68/200, Batch: 18/29, Batch_Loss_Train: 1.380
2024-06-21 19:06:13,820 - INFO: Epoch: 68/200, Batch: 19/29, Batch_Loss_Train: 1.268
2024-06-21 19:06:14,129 - INFO: Epoch: 68/200, Batch: 20/29, Batch_Loss_Train: 1.027
2024-06-21 19:06:14,535 - INFO: Epoch: 68/200, Batch: 21/29, Batch_Loss_Train: 0.888
2024-06-21 19:06:14,837 - INFO: Epoch: 68/200, Batch: 22/29, Batch_Loss_Train: 0.833
2024-06-21 19:06:15,237 - INFO: Epoch: 68/200, Batch: 23/29, Batch_Loss_Train: 1.025
2024-06-21 19:06:15,552 - INFO: Epoch: 68/200, Batch: 24/29, Batch_Loss_Train: 1.028
2024-06-21 19:06:15,956 - INFO: Epoch: 68/200, Batch: 25/29, Batch_Loss_Train: 0.966
2024-06-21 19:06:16,255 - INFO: Epoch: 68/200, Batch: 26/29, Batch_Loss_Train: 1.190
2024-06-21 19:06:16,652 - INFO: Epoch: 68/200, Batch: 27/29, Batch_Loss_Train: 0.881
2024-06-21 19:06:16,964 - INFO: Epoch: 68/200, Batch: 28/29, Batch_Loss_Train: 1.061
2024-06-21 19:06:17,178 - INFO: Epoch: 68/200, Batch: 29/29, Batch_Loss_Train: 1.061
2024-06-21 19:06:28,332 - INFO: 68/200 final results:
2024-06-21 19:06:28,332 - INFO: Training loss: 1.127.
2024-06-21 19:06:28,332 - INFO: Training MAE: 1.129.
2024-06-21 19:06:28,332 - INFO: Training MSE: 2.669.
2024-06-21 19:06:48,940 - INFO: Epoch: 68/200, Loss_train: 1.127434808632423, Loss_val: 2.213637191673805
2024-06-21 19:06:48,941 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:06:48,941 - INFO: Epoch 69/200...
2024-06-21 19:06:48,941 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:06:48,941 - INFO: Batch size: 32.
2024-06-21 19:06:48,945 - INFO: Dataset:
2024-06-21 19:06:48,945 - INFO: Batch size:
2024-06-21 19:06:48,945 - INFO: Number of workers:
2024-06-21 19:06:50,113 - INFO: Epoch: 69/200, Batch: 1/29, Batch_Loss_Train: 0.886
2024-06-21 19:06:50,431 - INFO: Epoch: 69/200, Batch: 2/29, Batch_Loss_Train: 1.091
2024-06-21 19:06:50,829 - INFO: Epoch: 69/200, Batch: 3/29, Batch_Loss_Train: 1.129
2024-06-21 19:06:51,149 - INFO: Epoch: 69/200, Batch: 4/29, Batch_Loss_Train: 1.570
2024-06-21 19:06:51,552 - INFO: Epoch: 69/200, Batch: 5/29, Batch_Loss_Train: 0.832
2024-06-21 19:06:51,876 - INFO: Epoch: 69/200, Batch: 6/29, Batch_Loss_Train: 1.331
2024-06-21 19:06:52,261 - INFO: Epoch: 69/200, Batch: 7/29, Batch_Loss_Train: 1.017
2024-06-21 19:06:52,575 - INFO: Epoch: 69/200, Batch: 8/29, Batch_Loss_Train: 1.368
2024-06-21 19:06:52,962 - INFO: Epoch: 69/200, Batch: 9/29, Batch_Loss_Train: 1.125
2024-06-21 19:06:53,294 - INFO: Epoch: 69/200, Batch: 10/29, Batch_Loss_Train: 1.083
2024-06-21 19:06:53,666 - INFO: Epoch: 69/200, Batch: 11/29, Batch_Loss_Train: 1.137
2024-06-21 19:06:53,981 - INFO: Epoch: 69/200, Batch: 12/29, Batch_Loss_Train: 1.065
2024-06-21 19:06:54,386 - INFO: Epoch: 69/200, Batch: 13/29, Batch_Loss_Train: 0.959
2024-06-21 19:06:54,713 - INFO: Epoch: 69/200, Batch: 14/29, Batch_Loss_Train: 0.972
2024-06-21 19:06:55,107 - INFO: Epoch: 69/200, Batch: 15/29, Batch_Loss_Train: 1.218
2024-06-21 19:06:55,418 - INFO: Epoch: 69/200, Batch: 16/29, Batch_Loss_Train: 1.334
2024-06-21 19:06:55,818 - INFO: Epoch: 69/200, Batch: 17/29, Batch_Loss_Train: 0.946
2024-06-21 19:06:56,142 - INFO: Epoch: 69/200, Batch: 18/29, Batch_Loss_Train: 1.509
2024-06-21 19:06:56,525 - INFO: Epoch: 69/200, Batch: 19/29, Batch_Loss_Train: 1.349
2024-06-21 19:06:56,831 - INFO: Epoch: 69/200, Batch: 20/29, Batch_Loss_Train: 1.303
2024-06-21 19:06:57,220 - INFO: Epoch: 69/200, Batch: 21/29, Batch_Loss_Train: 1.264
2024-06-21 19:06:57,546 - INFO: Epoch: 69/200, Batch: 22/29, Batch_Loss_Train: 1.272
2024-06-21 19:06:57,917 - INFO: Epoch: 69/200, Batch: 23/29, Batch_Loss_Train: 1.011
2024-06-21 19:06:58,229 - INFO: Epoch: 69/200, Batch: 24/29, Batch_Loss_Train: 1.021
2024-06-21 19:06:58,614 - INFO: Epoch: 69/200, Batch: 25/29, Batch_Loss_Train: 1.148
2024-06-21 19:06:58,934 - INFO: Epoch: 69/200, Batch: 26/29, Batch_Loss_Train: 0.890
2024-06-21 19:06:59,300 - INFO: Epoch: 69/200, Batch: 27/29, Batch_Loss_Train: 1.318
2024-06-21 19:06:59,607 - INFO: Epoch: 69/200, Batch: 28/29, Batch_Loss_Train: 1.168
2024-06-21 19:06:59,815 - INFO: Epoch: 69/200, Batch: 29/29, Batch_Loss_Train: 1.333
2024-06-21 19:07:10,999 - INFO: 69/200 final results:
2024-06-21 19:07:10,999 - INFO: Training loss: 1.160.
2024-06-21 19:07:10,999 - INFO: Training MAE: 1.157.
2024-06-21 19:07:10,999 - INFO: Training MSE: 2.851.
2024-06-21 19:07:31,367 - INFO: Epoch: 69/200, Loss_train: 1.1603028692048172, Loss_val: 2.150170383782222
2024-06-21 19:07:31,367 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:07:31,367 - INFO: Epoch 70/200...
2024-06-21 19:07:31,367 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:07:31,367 - INFO: Batch size: 32.
2024-06-21 19:07:31,371 - INFO: Dataset:
2024-06-21 19:07:31,371 - INFO: Batch size:
2024-06-21 19:07:31,371 - INFO: Number of workers:
2024-06-21 19:07:32,557 - INFO: Epoch: 70/200, Batch: 1/29, Batch_Loss_Train: 1.466
2024-06-21 19:07:32,863 - INFO: Epoch: 70/200, Batch: 2/29, Batch_Loss_Train: 0.876
2024-06-21 19:07:33,261 - INFO: Epoch: 70/200, Batch: 3/29, Batch_Loss_Train: 1.122
2024-06-21 19:07:33,577 - INFO: Epoch: 70/200, Batch: 4/29, Batch_Loss_Train: 1.158
2024-06-21 19:07:34,007 - INFO: Epoch: 70/200, Batch: 5/29, Batch_Loss_Train: 1.025
2024-06-21 19:07:34,307 - INFO: Epoch: 70/200, Batch: 6/29, Batch_Loss_Train: 1.123
2024-06-21 19:07:34,697 - INFO: Epoch: 70/200, Batch: 7/29, Batch_Loss_Train: 1.161
2024-06-21 19:07:35,010 - INFO: Epoch: 70/200, Batch: 8/29, Batch_Loss_Train: 1.158
2024-06-21 19:07:35,435 - INFO: Epoch: 70/200, Batch: 9/29, Batch_Loss_Train: 0.981
2024-06-21 19:07:35,726 - INFO: Epoch: 70/200, Batch: 10/29, Batch_Loss_Train: 1.296
2024-06-21 19:07:36,103 - INFO: Epoch: 70/200, Batch: 11/29, Batch_Loss_Train: 1.023
2024-06-21 19:07:36,418 - INFO: Epoch: 70/200, Batch: 12/29, Batch_Loss_Train: 1.171
2024-06-21 19:07:36,851 - INFO: Epoch: 70/200, Batch: 13/29, Batch_Loss_Train: 1.154
2024-06-21 19:07:37,154 - INFO: Epoch: 70/200, Batch: 14/29, Batch_Loss_Train: 0.882
2024-06-21 19:07:37,552 - INFO: Epoch: 70/200, Batch: 15/29, Batch_Loss_Train: 1.132
2024-06-21 19:07:37,865 - INFO: Epoch: 70/200, Batch: 16/29, Batch_Loss_Train: 1.222
2024-06-21 19:07:38,296 - INFO: Epoch: 70/200, Batch: 17/29, Batch_Loss_Train: 0.949
2024-06-21 19:07:38,595 - INFO: Epoch: 70/200, Batch: 18/29, Batch_Loss_Train: 1.042
2024-06-21 19:07:38,983 - INFO: Epoch: 70/200, Batch: 19/29, Batch_Loss_Train: 1.170
2024-06-21 19:07:39,289 - INFO: Epoch: 70/200, Batch: 20/29, Batch_Loss_Train: 1.215
2024-06-21 19:07:39,707 - INFO: Epoch: 70/200, Batch: 21/29, Batch_Loss_Train: 1.235
2024-06-21 19:07:40,008 - INFO: Epoch: 70/200, Batch: 22/29, Batch_Loss_Train: 1.139
2024-06-21 19:07:40,394 - INFO: Epoch: 70/200, Batch: 23/29, Batch_Loss_Train: 1.154
2024-06-21 19:07:40,708 - INFO: Epoch: 70/200, Batch: 24/29, Batch_Loss_Train: 0.987
2024-06-21 19:07:41,122 - INFO: Epoch: 70/200, Batch: 25/29, Batch_Loss_Train: 1.177
2024-06-21 19:07:41,418 - INFO: Epoch: 70/200, Batch: 26/29, Batch_Loss_Train: 1.477
2024-06-21 19:07:41,800 - INFO: Epoch: 70/200, Batch: 27/29, Batch_Loss_Train: 1.030
2024-06-21 19:07:42,109 - INFO: Epoch: 70/200, Batch: 28/29, Batch_Loss_Train: 1.291
2024-06-21 19:07:42,329 - INFO: Epoch: 70/200, Batch: 29/29, Batch_Loss_Train: 0.980
2024-06-21 19:07:53,430 - INFO: 70/200 final results:
2024-06-21 19:07:53,431 - INFO: Training loss: 1.131.
2024-06-21 19:07:53,431 - INFO: Training MAE: 1.134.
2024-06-21 19:07:53,431 - INFO: Training MSE: 2.699.
2024-06-21 19:08:13,846 - INFO: Epoch: 70/200, Loss_train: 1.1308175962546776, Loss_val: 2.321188133338402
2024-06-21 19:08:13,847 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:08:13,847 - INFO: Epoch 71/200...
2024-06-21 19:08:13,847 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:08:13,847 - INFO: Batch size: 32.
2024-06-21 19:08:13,850 - INFO: Dataset:
2024-06-21 19:08:13,851 - INFO: Batch size:
2024-06-21 19:08:13,851 - INFO: Number of workers:
2024-06-21 19:08:15,026 - INFO: Epoch: 71/200, Batch: 1/29, Batch_Loss_Train: 1.325
2024-06-21 19:08:15,332 - INFO: Epoch: 71/200, Batch: 2/29, Batch_Loss_Train: 1.288
2024-06-21 19:08:15,730 - INFO: Epoch: 71/200, Batch: 3/29, Batch_Loss_Train: 1.098
2024-06-21 19:08:16,048 - INFO: Epoch: 71/200, Batch: 4/29, Batch_Loss_Train: 1.337
2024-06-21 19:08:16,491 - INFO: Epoch: 71/200, Batch: 5/29, Batch_Loss_Train: 1.077
2024-06-21 19:08:16,793 - INFO: Epoch: 71/200, Batch: 6/29, Batch_Loss_Train: 0.927
2024-06-21 19:08:17,186 - INFO: Epoch: 71/200, Batch: 7/29, Batch_Loss_Train: 1.008
2024-06-21 19:08:17,489 - INFO: Epoch: 71/200, Batch: 8/29, Batch_Loss_Train: 1.023
2024-06-21 19:08:17,935 - INFO: Epoch: 71/200, Batch: 9/29, Batch_Loss_Train: 1.038
2024-06-21 19:08:18,229 - INFO: Epoch: 71/200, Batch: 10/29, Batch_Loss_Train: 1.046
2024-06-21 19:08:18,610 - INFO: Epoch: 71/200, Batch: 11/29, Batch_Loss_Train: 0.924
2024-06-21 19:08:18,916 - INFO: Epoch: 71/200, Batch: 12/29, Batch_Loss_Train: 0.990
2024-06-21 19:08:19,359 - INFO: Epoch: 71/200, Batch: 13/29, Batch_Loss_Train: 1.051
2024-06-21 19:08:19,663 - INFO: Epoch: 71/200, Batch: 14/29, Batch_Loss_Train: 1.070
2024-06-21 19:08:20,059 - INFO: Epoch: 71/200, Batch: 15/29, Batch_Loss_Train: 1.034
2024-06-21 19:08:20,359 - INFO: Epoch: 71/200, Batch: 16/29, Batch_Loss_Train: 1.154
2024-06-21 19:08:20,803 - INFO: Epoch: 71/200, Batch: 17/29, Batch_Loss_Train: 1.268
2024-06-21 19:08:21,102 - INFO: Epoch: 71/200, Batch: 18/29, Batch_Loss_Train: 0.920
2024-06-21 19:08:21,487 - INFO: Epoch: 71/200, Batch: 19/29, Batch_Loss_Train: 0.790
2024-06-21 19:08:21,780 - INFO: Epoch: 71/200, Batch: 20/29, Batch_Loss_Train: 1.095
2024-06-21 19:08:22,216 - INFO: Epoch: 71/200, Batch: 21/29, Batch_Loss_Train: 1.282
2024-06-21 19:08:22,520 - INFO: Epoch: 71/200, Batch: 22/29, Batch_Loss_Train: 0.966
2024-06-21 19:08:22,899 - INFO: Epoch: 71/200, Batch: 23/29, Batch_Loss_Train: 0.935
2024-06-21 19:08:23,202 - INFO: Epoch: 71/200, Batch: 24/29, Batch_Loss_Train: 1.025
2024-06-21 19:08:23,632 - INFO: Epoch: 71/200, Batch: 25/29, Batch_Loss_Train: 1.190
2024-06-21 19:08:23,931 - INFO: Epoch: 71/200, Batch: 26/29, Batch_Loss_Train: 1.236
2024-06-21 19:08:24,317 - INFO: Epoch: 71/200, Batch: 27/29, Batch_Loss_Train: 1.331
2024-06-21 19:08:24,617 - INFO: Epoch: 71/200, Batch: 28/29, Batch_Loss_Train: 1.101
2024-06-21 19:08:24,831 - INFO: Epoch: 71/200, Batch: 29/29, Batch_Loss_Train: 0.828
2024-06-21 19:08:36,001 - INFO: 71/200 final results:
2024-06-21 19:08:36,001 - INFO: Training loss: 1.081.
2024-06-21 19:08:36,001 - INFO: Training MAE: 1.086.
2024-06-21 19:08:36,001 - INFO: Training MSE: 2.468.
2024-06-21 19:08:56,485 - INFO: Epoch: 71/200, Loss_train: 1.0811145613933433, Loss_val: 2.0807539512371194
2024-06-21 19:08:56,486 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:08:56,486 - INFO: Epoch 72/200...
2024-06-21 19:08:56,486 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:08:56,486 - INFO: Batch size: 32.
2024-06-21 19:08:56,490 - INFO: Dataset:
2024-06-21 19:08:56,490 - INFO: Batch size:
2024-06-21 19:08:56,490 - INFO: Number of workers:
2024-06-21 19:08:57,711 - INFO: Epoch: 72/200, Batch: 1/29, Batch_Loss_Train: 1.208
2024-06-21 19:08:58,032 - INFO: Epoch: 72/200, Batch: 2/29, Batch_Loss_Train: 1.043
2024-06-21 19:08:58,442 - INFO: Epoch: 72/200, Batch: 3/29, Batch_Loss_Train: 1.231
2024-06-21 19:08:58,760 - INFO: Epoch: 72/200, Batch: 4/29, Batch_Loss_Train: 1.056
2024-06-21 19:08:59,166 - INFO: Epoch: 72/200, Batch: 5/29, Batch_Loss_Train: 0.950
2024-06-21 19:08:59,468 - INFO: Epoch: 72/200, Batch: 6/29, Batch_Loss_Train: 1.007
2024-06-21 19:08:59,869 - INFO: Epoch: 72/200, Batch: 7/29, Batch_Loss_Train: 1.011
2024-06-21 19:09:00,184 - INFO: Epoch: 72/200, Batch: 8/29, Batch_Loss_Train: 1.039
2024-06-21 19:09:00,598 - INFO: Epoch: 72/200, Batch: 9/29, Batch_Loss_Train: 0.919
2024-06-21 19:09:00,892 - INFO: Epoch: 72/200, Batch: 10/29, Batch_Loss_Train: 0.921
2024-06-21 19:09:01,284 - INFO: Epoch: 72/200, Batch: 11/29, Batch_Loss_Train: 1.045
2024-06-21 19:09:01,601 - INFO: Epoch: 72/200, Batch: 12/29, Batch_Loss_Train: 1.024
2024-06-21 19:09:02,020 - INFO: Epoch: 72/200, Batch: 13/29, Batch_Loss_Train: 1.123
2024-06-21 19:09:02,325 - INFO: Epoch: 72/200, Batch: 14/29, Batch_Loss_Train: 0.885
2024-06-21 19:09:02,735 - INFO: Epoch: 72/200, Batch: 15/29, Batch_Loss_Train: 1.050
2024-06-21 19:09:03,048 - INFO: Epoch: 72/200, Batch: 16/29, Batch_Loss_Train: 1.080
2024-06-21 19:09:03,458 - INFO: Epoch: 72/200, Batch: 17/29, Batch_Loss_Train: 0.962
2024-06-21 19:09:03,759 - INFO: Epoch: 72/200, Batch: 18/29, Batch_Loss_Train: 1.092
2024-06-21 19:09:04,158 - INFO: Epoch: 72/200, Batch: 19/29, Batch_Loss_Train: 1.148
2024-06-21 19:09:04,465 - INFO: Epoch: 72/200, Batch: 20/29, Batch_Loss_Train: 1.275
2024-06-21 19:09:04,864 - INFO: Epoch: 72/200, Batch: 21/29, Batch_Loss_Train: 1.127
2024-06-21 19:09:05,167 - INFO: Epoch: 72/200, Batch: 22/29, Batch_Loss_Train: 0.967
2024-06-21 19:09:05,568 - INFO: Epoch: 72/200, Batch: 23/29, Batch_Loss_Train: 1.223
2024-06-21 19:09:05,883 - INFO: Epoch: 72/200, Batch: 24/29, Batch_Loss_Train: 1.099
2024-06-21 19:09:06,289 - INFO: Epoch: 72/200, Batch: 25/29, Batch_Loss_Train: 1.170
2024-06-21 19:09:06,587 - INFO: Epoch: 72/200, Batch: 26/29, Batch_Loss_Train: 1.065
2024-06-21 19:09:06,984 - INFO: Epoch: 72/200, Batch: 27/29, Batch_Loss_Train: 0.873
2024-06-21 19:09:07,295 - INFO: Epoch: 72/200, Batch: 28/29, Batch_Loss_Train: 1.235
2024-06-21 19:09:07,514 - INFO: Epoch: 72/200, Batch: 29/29, Batch_Loss_Train: 1.181
2024-06-21 19:09:18,617 - INFO: 72/200 final results:
2024-06-21 19:09:18,617 - INFO: Training loss: 1.069.
2024-06-21 19:09:18,617 - INFO: Training MAE: 1.067.
2024-06-21 19:09:18,617 - INFO: Training MSE: 2.437.
2024-06-21 19:09:38,788 - INFO: Epoch: 72/200, Loss_train: 1.0692775948294277, Loss_val: 2.1108511069725298
2024-06-21 19:09:38,788 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:09:38,788 - INFO: Epoch 73/200...
2024-06-21 19:09:38,788 - INFO: Learning rate: 0.00010913316670403266.
2024-06-21 19:09:38,788 - INFO: Batch size: 32.
2024-06-21 19:09:38,791 - INFO: Dataset:
2024-06-21 19:09:38,792 - INFO: Batch size:
2024-06-21 19:09:38,792 - INFO: Number of workers:
2024-06-21 19:09:39,965 - INFO: Epoch: 73/200, Batch: 1/29, Batch_Loss_Train: 1.257
2024-06-21 19:09:40,269 - INFO: Epoch: 73/200, Batch: 2/29, Batch_Loss_Train: 0.925
2024-06-21 19:09:40,662 - INFO: Epoch: 73/200, Batch: 3/29, Batch_Loss_Train: 0.959
2024-06-21 19:09:40,978 - INFO: Epoch: 73/200, Batch: 4/29, Batch_Loss_Train: 1.012
2024-06-21 19:09:41,394 - INFO: Epoch: 73/200, Batch: 5/29, Batch_Loss_Train: 0.845
2024-06-21 19:09:41,692 - INFO: Epoch: 73/200, Batch: 6/29, Batch_Loss_Train: 1.212
2024-06-21 19:09:42,072 - INFO: Epoch: 73/200, Batch: 7/29, Batch_Loss_Train: 1.266
2024-06-21 19:09:42,383 - INFO: Epoch: 73/200, Batch: 8/29, Batch_Loss_Train: 1.215
2024-06-21 19:09:42,803 - INFO: Epoch: 73/200, Batch: 9/29, Batch_Loss_Train: 1.126
2024-06-21 19:09:43,094 - INFO: Epoch: 73/200, Batch: 10/29, Batch_Loss_Train: 0.907
2024-06-21 19:09:43,462 - INFO: Epoch: 73/200, Batch: 11/29, Batch_Loss_Train: 1.037
2024-06-21 19:09:43,780 - INFO: Epoch: 73/200, Batch: 12/29, Batch_Loss_Train: 1.221
2024-06-21 19:09:44,215 - INFO: Epoch: 73/200, Batch: 13/29, Batch_Loss_Train: 0.987
2024-06-21 19:09:44,520 - INFO: Epoch: 73/200, Batch: 14/29, Batch_Loss_Train: 0.888
2024-06-21 19:09:44,918 - INFO: Epoch: 73/200, Batch: 15/29, Batch_Loss_Train: 1.035
2024-06-21 19:09:45,232 - INFO: Epoch: 73/200, Batch: 16/29, Batch_Loss_Train: 1.091
2024-06-21 19:09:45,661 - INFO: Epoch: 73/200, Batch: 17/29, Batch_Loss_Train: 1.013
2024-06-21 19:09:45,962 - INFO: Epoch: 73/200, Batch: 18/29, Batch_Loss_Train: 0.921
2024-06-21 19:09:46,348 - INFO: Epoch: 73/200, Batch: 19/29, Batch_Loss_Train: 1.000
2024-06-21 19:09:46,656 - INFO: Epoch: 73/200, Batch: 20/29, Batch_Loss_Train: 1.124
2024-06-21 19:09:47,077 - INFO: Epoch: 73/200, Batch: 21/29, Batch_Loss_Train: 1.481
2024-06-21 19:09:47,381 - INFO: Epoch: 73/200, Batch: 22/29, Batch_Loss_Train: 1.002
2024-06-21 19:09:47,763 - INFO: Epoch: 73/200, Batch: 23/29, Batch_Loss_Train: 0.931
2024-06-21 19:09:48,079 - INFO: Epoch: 73/200, Batch: 24/29, Batch_Loss_Train: 1.036
2024-06-21 19:09:48,497 - INFO: Epoch: 73/200, Batch: 25/29, Batch_Loss_Train: 1.129
2024-06-21 19:09:48,796 - INFO: Epoch: 73/200, Batch: 26/29, Batch_Loss_Train: 1.296
2024-06-21 19:09:49,165 - INFO: Epoch: 73/200, Batch: 27/29, Batch_Loss_Train: 1.362
2024-06-21 19:09:49,477 - INFO: Epoch: 73/200, Batch: 28/29, Batch_Loss_Train: 1.272
2024-06-21 19:09:49,686 - INFO: Epoch: 73/200, Batch: 29/29, Batch_Loss_Train: 1.070
2024-06-21 19:10:00,882 - INFO: 73/200 final results:
2024-06-21 19:10:00,883 - INFO: Training loss: 1.090.
2024-06-21 19:10:00,883 - INFO: Training MAE: 1.091.
2024-06-21 19:10:00,883 - INFO: Training MSE: 2.434.
2024-06-21 19:10:21,300 - INFO: Epoch: 73/200, Loss_train: 1.090434442306387, Loss_val: 2.0512189166299226
2024-06-21 19:10:21,301 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:10:21,301 - INFO: Epoch 74/200...
2024-06-21 19:10:21,301 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:10:21,301 - INFO: Batch size: 32.
2024-06-21 19:10:21,305 - INFO: Dataset:
2024-06-21 19:10:21,305 - INFO: Batch size:
2024-06-21 19:10:21,305 - INFO: Number of workers:
2024-06-21 19:10:22,492 - INFO: Epoch: 74/200, Batch: 1/29, Batch_Loss_Train: 1.202
2024-06-21 19:10:22,798 - INFO: Epoch: 74/200, Batch: 2/29, Batch_Loss_Train: 1.186
2024-06-21 19:10:23,181 - INFO: Epoch: 74/200, Batch: 3/29, Batch_Loss_Train: 0.831
2024-06-21 19:10:23,497 - INFO: Epoch: 74/200, Batch: 4/29, Batch_Loss_Train: 0.794
2024-06-21 19:10:23,920 - INFO: Epoch: 74/200, Batch: 5/29, Batch_Loss_Train: 1.047
2024-06-21 19:10:24,219 - INFO: Epoch: 74/200, Batch: 6/29, Batch_Loss_Train: 0.806
2024-06-21 19:10:24,595 - INFO: Epoch: 74/200, Batch: 7/29, Batch_Loss_Train: 0.868
2024-06-21 19:10:24,908 - INFO: Epoch: 74/200, Batch: 8/29, Batch_Loss_Train: 0.913
2024-06-21 19:10:25,331 - INFO: Epoch: 74/200, Batch: 9/29, Batch_Loss_Train: 0.970
2024-06-21 19:10:25,622 - INFO: Epoch: 74/200, Batch: 10/29, Batch_Loss_Train: 0.884
2024-06-21 19:10:25,988 - INFO: Epoch: 74/200, Batch: 11/29, Batch_Loss_Train: 0.906
2024-06-21 19:10:26,304 - INFO: Epoch: 74/200, Batch: 12/29, Batch_Loss_Train: 0.916
2024-06-21 19:10:26,739 - INFO: Epoch: 74/200, Batch: 13/29, Batch_Loss_Train: 0.984
2024-06-21 19:10:27,044 - INFO: Epoch: 74/200, Batch: 14/29, Batch_Loss_Train: 1.011
2024-06-21 19:10:27,441 - INFO: Epoch: 74/200, Batch: 15/29, Batch_Loss_Train: 0.830
2024-06-21 19:10:27,755 - INFO: Epoch: 74/200, Batch: 16/29, Batch_Loss_Train: 0.966
2024-06-21 19:10:28,186 - INFO: Epoch: 74/200, Batch: 17/29, Batch_Loss_Train: 0.819
2024-06-21 19:10:28,487 - INFO: Epoch: 74/200, Batch: 18/29, Batch_Loss_Train: 0.913
2024-06-21 19:10:28,877 - INFO: Epoch: 74/200, Batch: 19/29, Batch_Loss_Train: 0.968
2024-06-21 19:10:29,185 - INFO: Epoch: 74/200, Batch: 20/29, Batch_Loss_Train: 1.070
2024-06-21 19:10:29,604 - INFO: Epoch: 74/200, Batch: 21/29, Batch_Loss_Train: 0.991
2024-06-21 19:10:29,905 - INFO: Epoch: 74/200, Batch: 22/29, Batch_Loss_Train: 0.930
2024-06-21 19:10:30,288 - INFO: Epoch: 74/200, Batch: 23/29, Batch_Loss_Train: 1.159
2024-06-21 19:10:30,602 - INFO: Epoch: 74/200, Batch: 24/29, Batch_Loss_Train: 0.828
2024-06-21 19:10:31,022 - INFO: Epoch: 74/200, Batch: 25/29, Batch_Loss_Train: 1.022
2024-06-21 19:10:31,318 - INFO: Epoch: 74/200, Batch: 26/29, Batch_Loss_Train: 0.958
2024-06-21 19:10:31,702 - INFO: Epoch: 74/200, Batch: 27/29, Batch_Loss_Train: 0.967
2024-06-21 19:10:32,012 - INFO: Epoch: 74/200, Batch: 28/29, Batch_Loss_Train: 0.911
2024-06-21 19:10:32,231 - INFO: Epoch: 74/200, Batch: 29/29, Batch_Loss_Train: 0.803
2024-06-21 19:10:43,378 - INFO: 74/200 final results:
2024-06-21 19:10:43,378 - INFO: Training loss: 0.947.
2024-06-21 19:10:43,378 - INFO: Training MAE: 0.949.
2024-06-21 19:10:43,378 - INFO: Training MSE: 2.007.
2024-06-21 19:11:04,038 - INFO: Epoch: 74/200, Loss_train: 0.9466605946935457, Loss_val: 2.1107625837983757
2024-06-21 19:11:04,038 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:11:04,038 - INFO: Epoch 75/200...
2024-06-21 19:11:04,038 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:11:04,038 - INFO: Batch size: 32.
2024-06-21 19:11:04,042 - INFO: Dataset:
2024-06-21 19:11:04,042 - INFO: Batch size:
2024-06-21 19:11:04,042 - INFO: Number of workers:
2024-06-21 19:11:05,222 - INFO: Epoch: 75/200, Batch: 1/29, Batch_Loss_Train: 0.791
2024-06-21 19:11:05,526 - INFO: Epoch: 75/200, Batch: 2/29, Batch_Loss_Train: 0.911
2024-06-21 19:11:05,919 - INFO: Epoch: 75/200, Batch: 3/29, Batch_Loss_Train: 0.828
2024-06-21 19:11:06,235 - INFO: Epoch: 75/200, Batch: 4/29, Batch_Loss_Train: 0.832
2024-06-21 19:11:06,659 - INFO: Epoch: 75/200, Batch: 5/29, Batch_Loss_Train: 0.927
2024-06-21 19:11:06,958 - INFO: Epoch: 75/200, Batch: 6/29, Batch_Loss_Train: 0.932
2024-06-21 19:11:07,355 - INFO: Epoch: 75/200, Batch: 7/29, Batch_Loss_Train: 0.850
2024-06-21 19:11:07,654 - INFO: Epoch: 75/200, Batch: 8/29, Batch_Loss_Train: 0.939
2024-06-21 19:11:08,080 - INFO: Epoch: 75/200, Batch: 9/29, Batch_Loss_Train: 0.748
2024-06-21 19:11:08,371 - INFO: Epoch: 75/200, Batch: 10/29, Batch_Loss_Train: 0.837
2024-06-21 19:11:08,759 - INFO: Epoch: 75/200, Batch: 11/29, Batch_Loss_Train: 0.859
2024-06-21 19:11:09,060 - INFO: Epoch: 75/200, Batch: 12/29, Batch_Loss_Train: 0.823
2024-06-21 19:11:09,488 - INFO: Epoch: 75/200, Batch: 13/29, Batch_Loss_Train: 0.804
2024-06-21 19:11:09,790 - INFO: Epoch: 75/200, Batch: 14/29, Batch_Loss_Train: 0.948
2024-06-21 19:11:10,194 - INFO: Epoch: 75/200, Batch: 15/29, Batch_Loss_Train: 0.750
2024-06-21 19:11:10,492 - INFO: Epoch: 75/200, Batch: 16/29, Batch_Loss_Train: 1.102
2024-06-21 19:11:10,918 - INFO: Epoch: 75/200, Batch: 17/29, Batch_Loss_Train: 0.972
2024-06-21 19:11:11,215 - INFO: Epoch: 75/200, Batch: 18/29, Batch_Loss_Train: 1.053
2024-06-21 19:11:11,608 - INFO: Epoch: 75/200, Batch: 19/29, Batch_Loss_Train: 1.069
2024-06-21 19:11:11,899 - INFO: Epoch: 75/200, Batch: 20/29, Batch_Loss_Train: 0.807
2024-06-21 19:11:12,314 - INFO: Epoch: 75/200, Batch: 21/29, Batch_Loss_Train: 0.993
2024-06-21 19:11:12,613 - INFO: Epoch: 75/200, Batch: 22/29, Batch_Loss_Train: 0.877
2024-06-21 19:11:13,010 - INFO: Epoch: 75/200, Batch: 23/29, Batch_Loss_Train: 1.165
2024-06-21 19:11:13,309 - INFO: Epoch: 75/200, Batch: 24/29, Batch_Loss_Train: 0.834
2024-06-21 19:11:13,725 - INFO: Epoch: 75/200, Batch: 25/29, Batch_Loss_Train: 1.003
2024-06-21 19:11:14,020 - INFO: Epoch: 75/200, Batch: 26/29, Batch_Loss_Train: 0.988
2024-06-21 19:11:14,413 - INFO: Epoch: 75/200, Batch: 27/29, Batch_Loss_Train: 1.088
2024-06-21 19:11:14,708 - INFO: Epoch: 75/200, Batch: 28/29, Batch_Loss_Train: 0.860
2024-06-21 19:11:14,927 - INFO: Epoch: 75/200, Batch: 29/29, Batch_Loss_Train: 1.079
2024-06-21 19:11:26,011 - INFO: 75/200 final results:
2024-06-21 19:11:26,011 - INFO: Training loss: 0.920.
2024-06-21 19:11:26,011 - INFO: Training MAE: 0.916.
2024-06-21 19:11:26,011 - INFO: Training MSE: 1.836.
2024-06-21 19:11:46,393 - INFO: Epoch: 75/200, Loss_train: 0.9195899305672481, Loss_val: 2.069677759861124
2024-06-21 19:11:46,393 - INFO: Best internal validation val_loss: 2.036 at epoch: 66.
2024-06-21 19:11:46,393 - INFO: Epoch 76/200...
2024-06-21 19:11:46,393 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:11:46,393 - INFO: Batch size: 32.
2024-06-21 19:11:46,397 - INFO: Dataset:
2024-06-21 19:11:46,397 - INFO: Batch size:
2024-06-21 19:11:46,397 - INFO: Number of workers:
2024-06-21 19:11:47,582 - INFO: Epoch: 76/200, Batch: 1/29, Batch_Loss_Train: 1.066
2024-06-21 19:11:47,888 - INFO: Epoch: 76/200, Batch: 2/29, Batch_Loss_Train: 0.951
2024-06-21 19:11:48,283 - INFO: Epoch: 76/200, Batch: 3/29, Batch_Loss_Train: 0.816
2024-06-21 19:11:48,601 - INFO: Epoch: 76/200, Batch: 4/29, Batch_Loss_Train: 0.871
2024-06-21 19:11:49,029 - INFO: Epoch: 76/200, Batch: 5/29, Batch_Loss_Train: 0.841
2024-06-21 19:11:49,331 - INFO: Epoch: 76/200, Batch: 6/29, Batch_Loss_Train: 0.728
2024-06-21 19:11:49,719 - INFO: Epoch: 76/200, Batch: 7/29, Batch_Loss_Train: 0.761
2024-06-21 19:11:50,023 - INFO: Epoch: 76/200, Batch: 8/29, Batch_Loss_Train: 0.853
2024-06-21 19:11:50,459 - INFO: Epoch: 76/200, Batch: 9/29, Batch_Loss_Train: 0.868
2024-06-21 19:11:50,753 - INFO: Epoch: 76/200, Batch: 10/29, Batch_Loss_Train: 0.901
2024-06-21 19:11:51,126 - INFO: Epoch: 76/200, Batch: 11/29, Batch_Loss_Train: 1.024
2024-06-21 19:11:51,431 - INFO: Epoch: 76/200, Batch: 12/29, Batch_Loss_Train: 0.967
2024-06-21 19:11:51,863 - INFO: Epoch: 76/200, Batch: 13/29, Batch_Loss_Train: 0.793
2024-06-21 19:11:52,167 - INFO: Epoch: 76/200, Batch: 14/29, Batch_Loss_Train: 0.973
2024-06-21 19:11:52,563 - INFO: Epoch: 76/200, Batch: 15/29, Batch_Loss_Train: 0.715
2024-06-21 19:11:52,863 - INFO: Epoch: 76/200, Batch: 16/29, Batch_Loss_Train: 1.083
2024-06-21 19:11:53,291 - INFO: Epoch: 76/200, Batch: 17/29, Batch_Loss_Train: 0.903
2024-06-21 19:11:53,591 - INFO: Epoch: 76/200, Batch: 18/29, Batch_Loss_Train: 0.727
2024-06-21 19:11:53,974 - INFO: Epoch: 76/200, Batch: 19/29, Batch_Loss_Train: 0.906
2024-06-21 19:11:54,270 - INFO: Epoch: 76/200, Batch: 20/29, Batch_Loss_Train: 0.899
2024-06-21 19:11:54,691 - INFO: Epoch: 76/200, Batch: 21/29, Batch_Loss_Train: 0.840
2024-06-21 19:11:54,993 - INFO: Epoch: 76/200, Batch: 22/29, Batch_Loss_Train: 1.023
2024-06-21 19:11:55,376 - INFO: Epoch: 76/200, Batch: 23/29, Batch_Loss_Train: 0.965
2024-06-21 19:11:55,678 - INFO: Epoch: 76/200, Batch: 24/29, Batch_Loss_Train: 0.981
2024-06-21 19:11:56,105 - INFO: Epoch: 76/200, Batch: 25/29, Batch_Loss_Train: 0.899
2024-06-21 19:11:56,403 - INFO: Epoch: 76/200, Batch: 26/29, Batch_Loss_Train: 0.856
2024-06-21 19:11:56,771 - INFO: Epoch: 76/200, Batch: 27/29, Batch_Loss_Train: 0.760
2024-06-21 19:11:57,068 - INFO: Epoch: 76/200, Batch: 28/29, Batch_Loss_Train: 1.001
2024-06-21 19:11:57,280 - INFO: Epoch: 76/200, Batch: 29/29, Batch_Loss_Train: 1.045
2024-06-21 19:12:08,560 - INFO: 76/200 final results:
2024-06-21 19:12:08,560 - INFO: Training loss: 0.897.
2024-06-21 19:12:08,560 - INFO: Training MAE: 0.894.
2024-06-21 19:12:08,560 - INFO: Training MSE: 1.790.
2024-06-21 19:12:29,055 - INFO: Epoch: 76/200, Loss_train: 0.8971867951853522, Loss_val: 1.9604956528236126
2024-06-21 19:12:29,074 - INFO: Saved new best metric model for epoch 76.
2024-06-21 19:12:29,074 - INFO: Best internal validation val_loss: 1.960 at epoch: 76.
2024-06-21 19:12:29,074 - INFO: Epoch 77/200...
2024-06-21 19:12:29,074 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:12:29,074 - INFO: Batch size: 32.
2024-06-21 19:12:29,078 - INFO: Dataset:
2024-06-21 19:12:29,078 - INFO: Batch size:
2024-06-21 19:12:29,078 - INFO: Number of workers:
2024-06-21 19:12:30,225 - INFO: Epoch: 77/200, Batch: 1/29, Batch_Loss_Train: 0.965
2024-06-21 19:12:30,547 - INFO: Epoch: 77/200, Batch: 2/29, Batch_Loss_Train: 0.792
2024-06-21 19:12:30,957 - INFO: Epoch: 77/200, Batch: 3/29, Batch_Loss_Train: 0.732
2024-06-21 19:12:31,276 - INFO: Epoch: 77/200, Batch: 4/29, Batch_Loss_Train: 0.839
2024-06-21 19:12:31,681 - INFO: Epoch: 77/200, Batch: 5/29, Batch_Loss_Train: 0.755
2024-06-21 19:12:31,996 - INFO: Epoch: 77/200, Batch: 6/29, Batch_Loss_Train: 0.860
2024-06-21 19:12:32,396 - INFO: Epoch: 77/200, Batch: 7/29, Batch_Loss_Train: 0.777
2024-06-21 19:12:32,712 - INFO: Epoch: 77/200, Batch: 8/29, Batch_Loss_Train: 0.931
2024-06-21 19:12:33,101 - INFO: Epoch: 77/200, Batch: 9/29, Batch_Loss_Train: 0.929
2024-06-21 19:12:33,408 - INFO: Epoch: 77/200, Batch: 10/29, Batch_Loss_Train: 0.852
2024-06-21 19:12:33,796 - INFO: Epoch: 77/200, Batch: 11/29, Batch_Loss_Train: 0.786
2024-06-21 19:12:34,113 - INFO: Epoch: 77/200, Batch: 12/29, Batch_Loss_Train: 0.748
2024-06-21 19:12:34,520 - INFO: Epoch: 77/200, Batch: 13/29, Batch_Loss_Train: 0.850
2024-06-21 19:12:34,838 - INFO: Epoch: 77/200, Batch: 14/29, Batch_Loss_Train: 1.035
2024-06-21 19:12:35,240 - INFO: Epoch: 77/200, Batch: 15/29, Batch_Loss_Train: 0.997
2024-06-21 19:12:35,555 - INFO: Epoch: 77/200, Batch: 16/29, Batch_Loss_Train: 0.923
2024-06-21 19:12:35,962 - INFO: Epoch: 77/200, Batch: 17/29, Batch_Loss_Train: 0.920
2024-06-21 19:12:36,276 - INFO: Epoch: 77/200, Batch: 18/29, Batch_Loss_Train: 0.853
2024-06-21 19:12:36,678 - INFO: Epoch: 77/200, Batch: 19/29, Batch_Loss_Train: 1.099
2024-06-21 19:12:36,987 - INFO: Epoch: 77/200, Batch: 20/29, Batch_Loss_Train: 0.975
2024-06-21 19:12:37,384 - INFO: Epoch: 77/200, Batch: 21/29, Batch_Loss_Train: 0.839
2024-06-21 19:12:37,701 - INFO: Epoch: 77/200, Batch: 22/29, Batch_Loss_Train: 0.885
2024-06-21 19:12:38,112 - INFO: Epoch: 77/200, Batch: 23/29, Batch_Loss_Train: 0.714
2024-06-21 19:12:38,436 - INFO: Epoch: 77/200, Batch: 24/29, Batch_Loss_Train: 0.819
2024-06-21 19:12:38,844 - INFO: Epoch: 77/200, Batch: 25/29, Batch_Loss_Train: 0.862
2024-06-21 19:12:39,164 - INFO: Epoch: 77/200, Batch: 26/29, Batch_Loss_Train: 0.873
2024-06-21 19:12:39,563 - INFO: Epoch: 77/200, Batch: 27/29, Batch_Loss_Train: 0.785
2024-06-21 19:12:39,875 - INFO: Epoch: 77/200, Batch: 28/29, Batch_Loss_Train: 0.867
2024-06-21 19:12:40,099 - INFO: Epoch: 77/200, Batch: 29/29, Batch_Loss_Train: 0.819
2024-06-21 19:12:51,149 - INFO: 77/200 final results:
2024-06-21 19:12:51,149 - INFO: Training loss: 0.865.
2024-06-21 19:12:51,149 - INFO: Training MAE: 0.866.
2024-06-21 19:12:51,149 - INFO: Training MSE: 1.757.
2024-06-21 19:13:11,707 - INFO: Epoch: 77/200, Loss_train: 0.8647881540758856, Loss_val: 1.8849394568081559
2024-06-21 19:13:11,725 - INFO: Saved new best metric model for epoch 77.
2024-06-21 19:13:11,725 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:13:11,725 - INFO: Epoch 78/200...
2024-06-21 19:13:11,725 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:13:11,725 - INFO: Batch size: 32.
2024-06-21 19:13:11,729 - INFO: Dataset:
2024-06-21 19:13:11,730 - INFO: Batch size:
2024-06-21 19:13:11,730 - INFO: Number of workers:
2024-06-21 19:13:12,906 - INFO: Epoch: 78/200, Batch: 1/29, Batch_Loss_Train: 0.914
2024-06-21 19:13:13,212 - INFO: Epoch: 78/200, Batch: 2/29, Batch_Loss_Train: 0.966
2024-06-21 19:13:13,608 - INFO: Epoch: 78/200, Batch: 3/29, Batch_Loss_Train: 0.868
2024-06-21 19:13:13,924 - INFO: Epoch: 78/200, Batch: 4/29, Batch_Loss_Train: 0.864
2024-06-21 19:13:14,352 - INFO: Epoch: 78/200, Batch: 5/29, Batch_Loss_Train: 0.908
2024-06-21 19:13:14,655 - INFO: Epoch: 78/200, Batch: 6/29, Batch_Loss_Train: 0.780
2024-06-21 19:13:15,045 - INFO: Epoch: 78/200, Batch: 7/29, Batch_Loss_Train: 1.010
2024-06-21 19:13:15,361 - INFO: Epoch: 78/200, Batch: 8/29, Batch_Loss_Train: 0.840
2024-06-21 19:13:15,793 - INFO: Epoch: 78/200, Batch: 9/29, Batch_Loss_Train: 0.959
2024-06-21 19:13:16,087 - INFO: Epoch: 78/200, Batch: 10/29, Batch_Loss_Train: 0.941
2024-06-21 19:13:16,462 - INFO: Epoch: 78/200, Batch: 11/29, Batch_Loss_Train: 0.875
2024-06-21 19:13:16,780 - INFO: Epoch: 78/200, Batch: 12/29, Batch_Loss_Train: 0.846
2024-06-21 19:13:17,214 - INFO: Epoch: 78/200, Batch: 13/29, Batch_Loss_Train: 0.766
2024-06-21 19:13:17,518 - INFO: Epoch: 78/200, Batch: 14/29, Batch_Loss_Train: 0.688
2024-06-21 19:13:17,909 - INFO: Epoch: 78/200, Batch: 15/29, Batch_Loss_Train: 0.914
2024-06-21 19:13:18,223 - INFO: Epoch: 78/200, Batch: 16/29, Batch_Loss_Train: 0.982
2024-06-21 19:13:18,649 - INFO: Epoch: 78/200, Batch: 17/29, Batch_Loss_Train: 0.981
2024-06-21 19:13:18,950 - INFO: Epoch: 78/200, Batch: 18/29, Batch_Loss_Train: 1.020
2024-06-21 19:13:19,331 - INFO: Epoch: 78/200, Batch: 19/29, Batch_Loss_Train: 0.826
2024-06-21 19:13:19,639 - INFO: Epoch: 78/200, Batch: 20/29, Batch_Loss_Train: 0.861
2024-06-21 19:13:20,062 - INFO: Epoch: 78/200, Batch: 21/29, Batch_Loss_Train: 0.672
2024-06-21 19:13:20,366 - INFO: Epoch: 78/200, Batch: 22/29, Batch_Loss_Train: 0.767
2024-06-21 19:13:20,755 - INFO: Epoch: 78/200, Batch: 23/29, Batch_Loss_Train: 0.883
2024-06-21 19:13:21,071 - INFO: Epoch: 78/200, Batch: 24/29, Batch_Loss_Train: 0.976
2024-06-21 19:13:21,494 - INFO: Epoch: 78/200, Batch: 25/29, Batch_Loss_Train: 1.074
2024-06-21 19:13:21,793 - INFO: Epoch: 78/200, Batch: 26/29, Batch_Loss_Train: 0.827
2024-06-21 19:13:22,179 - INFO: Epoch: 78/200, Batch: 27/29, Batch_Loss_Train: 0.773
2024-06-21 19:13:22,492 - INFO: Epoch: 78/200, Batch: 28/29, Batch_Loss_Train: 0.758
2024-06-21 19:13:22,712 - INFO: Epoch: 78/200, Batch: 29/29, Batch_Loss_Train: 0.915
2024-06-21 19:13:33,765 - INFO: 78/200 final results:
2024-06-21 19:13:33,765 - INFO: Training loss: 0.878.
2024-06-21 19:13:33,765 - INFO: Training MAE: 0.877.
2024-06-21 19:13:33,765 - INFO: Training MSE: 1.751.
2024-06-21 19:13:53,817 - INFO: Epoch: 78/200, Loss_train: 0.8777824393634138, Loss_val: 1.9348978502997036
2024-06-21 19:13:53,817 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:13:53,817 - INFO: Epoch 79/200...
2024-06-21 19:13:53,817 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:13:53,817 - INFO: Batch size: 32.
2024-06-21 19:13:53,821 - INFO: Dataset:
2024-06-21 19:13:53,821 - INFO: Batch size:
2024-06-21 19:13:53,821 - INFO: Number of workers:
2024-06-21 19:13:54,974 - INFO: Epoch: 79/200, Batch: 1/29, Batch_Loss_Train: 0.770
2024-06-21 19:13:55,277 - INFO: Epoch: 79/200, Batch: 2/29, Batch_Loss_Train: 0.821
2024-06-21 19:13:55,682 - INFO: Epoch: 79/200, Batch: 3/29, Batch_Loss_Train: 0.781
2024-06-21 19:13:55,998 - INFO: Epoch: 79/200, Batch: 4/29, Batch_Loss_Train: 0.911
2024-06-21 19:13:56,422 - INFO: Epoch: 79/200, Batch: 5/29, Batch_Loss_Train: 0.881
2024-06-21 19:13:56,721 - INFO: Epoch: 79/200, Batch: 6/29, Batch_Loss_Train: 0.804
2024-06-21 19:13:57,105 - INFO: Epoch: 79/200, Batch: 7/29, Batch_Loss_Train: 0.715
2024-06-21 19:13:57,421 - INFO: Epoch: 79/200, Batch: 8/29, Batch_Loss_Train: 0.749
2024-06-21 19:13:57,861 - INFO: Epoch: 79/200, Batch: 9/29, Batch_Loss_Train: 0.934
2024-06-21 19:13:58,155 - INFO: Epoch: 79/200, Batch: 10/29, Batch_Loss_Train: 0.864
2024-06-21 19:13:58,527 - INFO: Epoch: 79/200, Batch: 11/29, Batch_Loss_Train: 1.079
2024-06-21 19:13:58,845 - INFO: Epoch: 79/200, Batch: 12/29, Batch_Loss_Train: 0.831
2024-06-21 19:13:59,280 - INFO: Epoch: 79/200, Batch: 13/29, Batch_Loss_Train: 0.746
2024-06-21 19:13:59,584 - INFO: Epoch: 79/200, Batch: 14/29, Batch_Loss_Train: 0.988
2024-06-21 19:13:59,979 - INFO: Epoch: 79/200, Batch: 15/29, Batch_Loss_Train: 0.682
2024-06-21 19:14:00,294 - INFO: Epoch: 79/200, Batch: 16/29, Batch_Loss_Train: 0.875
2024-06-21 19:14:00,720 - INFO: Epoch: 79/200, Batch: 17/29, Batch_Loss_Train: 0.853
2024-06-21 19:14:01,022 - INFO: Epoch: 79/200, Batch: 18/29, Batch_Loss_Train: 0.820
2024-06-21 19:14:01,405 - INFO: Epoch: 79/200, Batch: 19/29, Batch_Loss_Train: 0.803
2024-06-21 19:14:01,715 - INFO: Epoch: 79/200, Batch: 20/29, Batch_Loss_Train: 0.869
2024-06-21 19:14:02,132 - INFO: Epoch: 79/200, Batch: 21/29, Batch_Loss_Train: 0.684
2024-06-21 19:14:02,434 - INFO: Epoch: 79/200, Batch: 22/29, Batch_Loss_Train: 0.957
2024-06-21 19:14:02,817 - INFO: Epoch: 79/200, Batch: 23/29, Batch_Loss_Train: 0.796
2024-06-21 19:14:03,133 - INFO: Epoch: 79/200, Batch: 24/29, Batch_Loss_Train: 0.655
2024-06-21 19:14:03,551 - INFO: Epoch: 79/200, Batch: 25/29, Batch_Loss_Train: 0.998
2024-06-21 19:14:03,850 - INFO: Epoch: 79/200, Batch: 26/29, Batch_Loss_Train: 0.729
2024-06-21 19:14:04,231 - INFO: Epoch: 79/200, Batch: 27/29, Batch_Loss_Train: 0.914
2024-06-21 19:14:04,543 - INFO: Epoch: 79/200, Batch: 28/29, Batch_Loss_Train: 0.891
2024-06-21 19:14:04,762 - INFO: Epoch: 79/200, Batch: 29/29, Batch_Loss_Train: 0.836
2024-06-21 19:14:15,847 - INFO: 79/200 final results:
2024-06-21 19:14:15,847 - INFO: Training loss: 0.836.
2024-06-21 19:14:15,847 - INFO: Training MAE: 0.836.
2024-06-21 19:14:15,847 - INFO: Training MSE: 1.669.
2024-06-21 19:14:36,154 - INFO: Epoch: 79/200, Loss_train: 0.8357148026597911, Loss_val: 1.9782550787103588
2024-06-21 19:14:36,154 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:14:36,154 - INFO: Epoch 80/200...
2024-06-21 19:14:36,154 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:14:36,154 - INFO: Batch size: 32.
2024-06-21 19:14:36,158 - INFO: Dataset:
2024-06-21 19:14:36,158 - INFO: Batch size:
2024-06-21 19:14:36,158 - INFO: Number of workers:
2024-06-21 19:14:37,345 - INFO: Epoch: 80/200, Batch: 1/29, Batch_Loss_Train: 0.798
2024-06-21 19:14:37,649 - INFO: Epoch: 80/200, Batch: 2/29, Batch_Loss_Train: 0.741
2024-06-21 19:14:38,045 - INFO: Epoch: 80/200, Batch: 3/29, Batch_Loss_Train: 0.718
2024-06-21 19:14:38,361 - INFO: Epoch: 80/200, Batch: 4/29, Batch_Loss_Train: 0.849
2024-06-21 19:14:38,798 - INFO: Epoch: 80/200, Batch: 5/29, Batch_Loss_Train: 0.821
2024-06-21 19:14:39,099 - INFO: Epoch: 80/200, Batch: 6/29, Batch_Loss_Train: 0.955
2024-06-21 19:14:39,489 - INFO: Epoch: 80/200, Batch: 7/29, Batch_Loss_Train: 0.807
2024-06-21 19:14:39,792 - INFO: Epoch: 80/200, Batch: 8/29, Batch_Loss_Train: 0.850
2024-06-21 19:14:40,229 - INFO: Epoch: 80/200, Batch: 9/29, Batch_Loss_Train: 0.788
2024-06-21 19:14:40,521 - INFO: Epoch: 80/200, Batch: 10/29, Batch_Loss_Train: 0.912
2024-06-21 19:14:40,889 - INFO: Epoch: 80/200, Batch: 11/29, Batch_Loss_Train: 1.077
2024-06-21 19:14:41,191 - INFO: Epoch: 80/200, Batch: 12/29, Batch_Loss_Train: 0.858
2024-06-21 19:14:41,629 - INFO: Epoch: 80/200, Batch: 13/29, Batch_Loss_Train: 0.675
2024-06-21 19:14:41,933 - INFO: Epoch: 80/200, Batch: 14/29, Batch_Loss_Train: 0.855
2024-06-21 19:14:42,328 - INFO: Epoch: 80/200, Batch: 15/29, Batch_Loss_Train: 0.821
2024-06-21 19:14:42,629 - INFO: Epoch: 80/200, Batch: 16/29, Batch_Loss_Train: 0.842
2024-06-21 19:14:43,066 - INFO: Epoch: 80/200, Batch: 17/29, Batch_Loss_Train: 0.647
2024-06-21 19:14:43,366 - INFO: Epoch: 80/200, Batch: 18/29, Batch_Loss_Train: 0.946
2024-06-21 19:14:43,750 - INFO: Epoch: 80/200, Batch: 19/29, Batch_Loss_Train: 0.969
2024-06-21 19:14:44,046 - INFO: Epoch: 80/200, Batch: 20/29, Batch_Loss_Train: 0.838
2024-06-21 19:14:44,477 - INFO: Epoch: 80/200, Batch: 21/29, Batch_Loss_Train: 0.798
2024-06-21 19:14:44,780 - INFO: Epoch: 80/200, Batch: 22/29, Batch_Loss_Train: 0.707
2024-06-21 19:14:45,168 - INFO: Epoch: 80/200, Batch: 23/29, Batch_Loss_Train: 0.911
2024-06-21 19:14:45,472 - INFO: Epoch: 80/200, Batch: 24/29, Batch_Loss_Train: 0.731
2024-06-21 19:14:45,900 - INFO: Epoch: 80/200, Batch: 25/29, Batch_Loss_Train: 0.877
2024-06-21 19:14:46,199 - INFO: Epoch: 80/200, Batch: 26/29, Batch_Loss_Train: 0.853
2024-06-21 19:14:46,580 - INFO: Epoch: 80/200, Batch: 27/29, Batch_Loss_Train: 0.791
2024-06-21 19:14:46,879 - INFO: Epoch: 80/200, Batch: 28/29, Batch_Loss_Train: 0.900
2024-06-21 19:14:47,097 - INFO: Epoch: 80/200, Batch: 29/29, Batch_Loss_Train: 0.781
2024-06-21 19:14:58,338 - INFO: 80/200 final results:
2024-06-21 19:14:58,338 - INFO: Training loss: 0.832.
2024-06-21 19:14:58,338 - INFO: Training MAE: 0.833.
2024-06-21 19:14:58,338 - INFO: Training MSE: 1.592.
2024-06-21 19:15:18,980 - INFO: Epoch: 80/200, Loss_train: 0.8316192668059776, Loss_val: 2.1400039935934134
2024-06-21 19:15:18,980 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:15:18,980 - INFO: Epoch 81/200...
2024-06-21 19:15:18,980 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:15:18,980 - INFO: Batch size: 32.
2024-06-21 19:15:18,984 - INFO: Dataset:
2024-06-21 19:15:18,985 - INFO: Batch size:
2024-06-21 19:15:18,985 - INFO: Number of workers:
2024-06-21 19:15:20,165 - INFO: Epoch: 81/200, Batch: 1/29, Batch_Loss_Train: 0.812
2024-06-21 19:15:20,470 - INFO: Epoch: 81/200, Batch: 2/29, Batch_Loss_Train: 1.132
2024-06-21 19:15:20,864 - INFO: Epoch: 81/200, Batch: 3/29, Batch_Loss_Train: 0.927
2024-06-21 19:15:21,180 - INFO: Epoch: 81/200, Batch: 4/29, Batch_Loss_Train: 0.823
2024-06-21 19:15:21,603 - INFO: Epoch: 81/200, Batch: 5/29, Batch_Loss_Train: 0.759
2024-06-21 19:15:21,902 - INFO: Epoch: 81/200, Batch: 6/29, Batch_Loss_Train: 0.723
2024-06-21 19:15:22,287 - INFO: Epoch: 81/200, Batch: 7/29, Batch_Loss_Train: 0.805
2024-06-21 19:15:22,599 - INFO: Epoch: 81/200, Batch: 8/29, Batch_Loss_Train: 1.013
2024-06-21 19:15:23,042 - INFO: Epoch: 81/200, Batch: 9/29, Batch_Loss_Train: 0.669
2024-06-21 19:15:23,334 - INFO: Epoch: 81/200, Batch: 10/29, Batch_Loss_Train: 1.059
2024-06-21 19:15:23,709 - INFO: Epoch: 81/200, Batch: 11/29, Batch_Loss_Train: 0.757
2024-06-21 19:15:24,024 - INFO: Epoch: 81/200, Batch: 12/29, Batch_Loss_Train: 0.970
2024-06-21 19:15:24,443 - INFO: Epoch: 81/200, Batch: 13/29, Batch_Loss_Train: 0.953
2024-06-21 19:15:24,745 - INFO: Epoch: 81/200, Batch: 14/29, Batch_Loss_Train: 0.791
2024-06-21 19:15:25,152 - INFO: Epoch: 81/200, Batch: 15/29, Batch_Loss_Train: 0.784
2024-06-21 19:15:25,464 - INFO: Epoch: 81/200, Batch: 16/29, Batch_Loss_Train: 1.047
2024-06-21 19:15:25,880 - INFO: Epoch: 81/200, Batch: 17/29, Batch_Loss_Train: 0.789
2024-06-21 19:15:26,178 - INFO: Epoch: 81/200, Batch: 18/29, Batch_Loss_Train: 0.833
2024-06-21 19:15:26,574 - INFO: Epoch: 81/200, Batch: 19/29, Batch_Loss_Train: 0.886
2024-06-21 19:15:26,880 - INFO: Epoch: 81/200, Batch: 20/29, Batch_Loss_Train: 0.822
2024-06-21 19:15:27,285 - INFO: Epoch: 81/200, Batch: 21/29, Batch_Loss_Train: 0.949
2024-06-21 19:15:27,585 - INFO: Epoch: 81/200, Batch: 22/29, Batch_Loss_Train: 1.065
2024-06-21 19:15:27,969 - INFO: Epoch: 81/200, Batch: 23/29, Batch_Loss_Train: 0.796
2024-06-21 19:15:28,280 - INFO: Epoch: 81/200, Batch: 24/29, Batch_Loss_Train: 0.920
2024-06-21 19:15:28,682 - INFO: Epoch: 81/200, Batch: 25/29, Batch_Loss_Train: 0.692
2024-06-21 19:15:28,977 - INFO: Epoch: 81/200, Batch: 26/29, Batch_Loss_Train: 0.790
2024-06-21 19:15:29,365 - INFO: Epoch: 81/200, Batch: 27/29, Batch_Loss_Train: 0.928
2024-06-21 19:15:29,673 - INFO: Epoch: 81/200, Batch: 28/29, Batch_Loss_Train: 0.884
2024-06-21 19:15:29,881 - INFO: Epoch: 81/200, Batch: 29/29, Batch_Loss_Train: 0.890
2024-06-21 19:15:41,041 - INFO: 81/200 final results:
2024-06-21 19:15:41,041 - INFO: Training loss: 0.871.
2024-06-21 19:15:41,041 - INFO: Training MAE: 0.871.
2024-06-21 19:15:41,041 - INFO: Training MSE: 1.702.
2024-06-21 19:16:01,523 - INFO: Epoch: 81/200, Loss_train: 0.8713276694560873, Loss_val: 1.951540202930056
2024-06-21 19:16:01,524 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:16:01,524 - INFO: Epoch 82/200...
2024-06-21 19:16:01,524 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:16:01,524 - INFO: Batch size: 32.
2024-06-21 19:16:01,528 - INFO: Dataset:
2024-06-21 19:16:01,528 - INFO: Batch size:
2024-06-21 19:16:01,528 - INFO: Number of workers:
2024-06-21 19:16:02,701 - INFO: Epoch: 82/200, Batch: 1/29, Batch_Loss_Train: 0.926
2024-06-21 19:16:03,004 - INFO: Epoch: 82/200, Batch: 2/29, Batch_Loss_Train: 0.873
2024-06-21 19:16:03,414 - INFO: Epoch: 82/200, Batch: 3/29, Batch_Loss_Train: 0.792
2024-06-21 19:16:03,731 - INFO: Epoch: 82/200, Batch: 4/29, Batch_Loss_Train: 0.729
2024-06-21 19:16:04,137 - INFO: Epoch: 82/200, Batch: 5/29, Batch_Loss_Train: 0.881
2024-06-21 19:16:04,435 - INFO: Epoch: 82/200, Batch: 6/29, Batch_Loss_Train: 0.821
2024-06-21 19:16:04,835 - INFO: Epoch: 82/200, Batch: 7/29, Batch_Loss_Train: 0.834
2024-06-21 19:16:05,148 - INFO: Epoch: 82/200, Batch: 8/29, Batch_Loss_Train: 1.029
2024-06-21 19:16:05,549 - INFO: Epoch: 82/200, Batch: 9/29, Batch_Loss_Train: 0.753
2024-06-21 19:16:05,841 - INFO: Epoch: 82/200, Batch: 10/29, Batch_Loss_Train: 0.802
2024-06-21 19:16:06,230 - INFO: Epoch: 82/200, Batch: 11/29, Batch_Loss_Train: 0.973
2024-06-21 19:16:06,544 - INFO: Epoch: 82/200, Batch: 12/29, Batch_Loss_Train: 0.970
2024-06-21 19:16:06,961 - INFO: Epoch: 82/200, Batch: 13/29, Batch_Loss_Train: 0.921
2024-06-21 19:16:07,261 - INFO: Epoch: 82/200, Batch: 14/29, Batch_Loss_Train: 0.827
2024-06-21 19:16:07,668 - INFO: Epoch: 82/200, Batch: 15/29, Batch_Loss_Train: 0.836
2024-06-21 19:16:07,978 - INFO: Epoch: 82/200, Batch: 16/29, Batch_Loss_Train: 0.986
2024-06-21 19:16:08,393 - INFO: Epoch: 82/200, Batch: 17/29, Batch_Loss_Train: 0.864
2024-06-21 19:16:08,691 - INFO: Epoch: 82/200, Batch: 18/29, Batch_Loss_Train: 0.710
2024-06-21 19:16:09,088 - INFO: Epoch: 82/200, Batch: 19/29, Batch_Loss_Train: 0.866
2024-06-21 19:16:09,394 - INFO: Epoch: 82/200, Batch: 20/29, Batch_Loss_Train: 0.827
2024-06-21 19:16:09,798 - INFO: Epoch: 82/200, Batch: 21/29, Batch_Loss_Train: 1.084
2024-06-21 19:16:10,099 - INFO: Epoch: 82/200, Batch: 22/29, Batch_Loss_Train: 0.720
2024-06-21 19:16:10,497 - INFO: Epoch: 82/200, Batch: 23/29, Batch_Loss_Train: 0.869
2024-06-21 19:16:10,810 - INFO: Epoch: 82/200, Batch: 24/29, Batch_Loss_Train: 0.897
2024-06-21 19:16:11,215 - INFO: Epoch: 82/200, Batch: 25/29, Batch_Loss_Train: 0.914
2024-06-21 19:16:11,512 - INFO: Epoch: 82/200, Batch: 26/29, Batch_Loss_Train: 0.804
2024-06-21 19:16:11,907 - INFO: Epoch: 82/200, Batch: 27/29, Batch_Loss_Train: 0.655
2024-06-21 19:16:12,215 - INFO: Epoch: 82/200, Batch: 28/29, Batch_Loss_Train: 0.793
2024-06-21 19:16:12,434 - INFO: Epoch: 82/200, Batch: 29/29, Batch_Loss_Train: 0.789
2024-06-21 19:16:23,243 - INFO: 82/200 final results:
2024-06-21 19:16:23,244 - INFO: Training loss: 0.853.
2024-06-21 19:16:23,244 - INFO: Training MAE: 0.855.
2024-06-21 19:16:23,244 - INFO: Training MSE: 1.676.
2024-06-21 19:16:43,884 - INFO: Epoch: 82/200, Loss_train: 0.8532733896683002, Loss_val: 2.0880755021654327
2024-06-21 19:16:43,885 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:16:43,885 - INFO: Epoch 83/200...
2024-06-21 19:16:43,885 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:16:43,885 - INFO: Batch size: 32.
2024-06-21 19:16:43,889 - INFO: Dataset:
2024-06-21 19:16:43,889 - INFO: Batch size:
2024-06-21 19:16:43,889 - INFO: Number of workers:
2024-06-21 19:16:45,043 - INFO: Epoch: 83/200, Batch: 1/29, Batch_Loss_Train: 0.841
2024-06-21 19:16:45,375 - INFO: Epoch: 83/200, Batch: 2/29, Batch_Loss_Train: 1.032
2024-06-21 19:16:45,770 - INFO: Epoch: 83/200, Batch: 3/29, Batch_Loss_Train: 0.675
2024-06-21 19:16:46,087 - INFO: Epoch: 83/200, Batch: 4/29, Batch_Loss_Train: 0.907
2024-06-21 19:16:46,499 - INFO: Epoch: 83/200, Batch: 5/29, Batch_Loss_Train: 0.786
2024-06-21 19:16:46,812 - INFO: Epoch: 83/200, Batch: 6/29, Batch_Loss_Train: 0.836
2024-06-21 19:16:47,198 - INFO: Epoch: 83/200, Batch: 7/29, Batch_Loss_Train: 0.700
2024-06-21 19:16:47,511 - INFO: Epoch: 83/200, Batch: 8/29, Batch_Loss_Train: 0.810
2024-06-21 19:16:47,920 - INFO: Epoch: 83/200, Batch: 9/29, Batch_Loss_Train: 0.955
2024-06-21 19:16:48,224 - INFO: Epoch: 83/200, Batch: 10/29, Batch_Loss_Train: 0.884
2024-06-21 19:16:48,599 - INFO: Epoch: 83/200, Batch: 11/29, Batch_Loss_Train: 0.848
2024-06-21 19:16:48,913 - INFO: Epoch: 83/200, Batch: 12/29, Batch_Loss_Train: 0.632
2024-06-21 19:16:49,331 - INFO: Epoch: 83/200, Batch: 13/29, Batch_Loss_Train: 0.935
2024-06-21 19:16:49,647 - INFO: Epoch: 83/200, Batch: 14/29, Batch_Loss_Train: 0.885
2024-06-21 19:16:50,041 - INFO: Epoch: 83/200, Batch: 15/29, Batch_Loss_Train: 0.757
2024-06-21 19:16:50,352 - INFO: Epoch: 83/200, Batch: 16/29, Batch_Loss_Train: 0.697
2024-06-21 19:16:50,765 - INFO: Epoch: 83/200, Batch: 17/29, Batch_Loss_Train: 0.804
2024-06-21 19:16:51,076 - INFO: Epoch: 83/200, Batch: 18/29, Batch_Loss_Train: 0.972
2024-06-21 19:16:51,458 - INFO: Epoch: 83/200, Batch: 19/29, Batch_Loss_Train: 0.908
2024-06-21 19:16:51,764 - INFO: Epoch: 83/200, Batch: 20/29, Batch_Loss_Train: 0.796
2024-06-21 19:16:52,165 - INFO: Epoch: 83/200, Batch: 21/29, Batch_Loss_Train: 0.954
2024-06-21 19:16:52,477 - INFO: Epoch: 83/200, Batch: 22/29, Batch_Loss_Train: 0.635
2024-06-21 19:16:52,847 - INFO: Epoch: 83/200, Batch: 23/29, Batch_Loss_Train: 0.878
2024-06-21 19:16:53,158 - INFO: Epoch: 83/200, Batch: 24/29, Batch_Loss_Train: 0.724
2024-06-21 19:16:53,553 - INFO: Epoch: 83/200, Batch: 25/29, Batch_Loss_Train: 1.160
2024-06-21 19:16:53,862 - INFO: Epoch: 83/200, Batch: 26/29, Batch_Loss_Train: 0.699
2024-06-21 19:16:54,227 - INFO: Epoch: 83/200, Batch: 27/29, Batch_Loss_Train: 0.855
2024-06-21 19:16:54,535 - INFO: Epoch: 83/200, Batch: 28/29, Batch_Loss_Train: 0.859
2024-06-21 19:16:54,742 - INFO: Epoch: 83/200, Batch: 29/29, Batch_Loss_Train: 0.809
2024-06-21 19:17:05,890 - INFO: 83/200 final results:
2024-06-21 19:17:05,891 - INFO: Training loss: 0.836.
2024-06-21 19:17:05,891 - INFO: Training MAE: 0.836.
2024-06-21 19:17:05,891 - INFO: Training MSE: 1.544.
2024-06-21 19:17:26,385 - INFO: Epoch: 83/200, Loss_train: 0.8356393267368448, Loss_val: 1.889975432691903
2024-06-21 19:17:26,385 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:17:26,385 - INFO: Epoch 84/200...
2024-06-21 19:17:26,385 - INFO: Learning rate: 5.456658335201633e-05.
2024-06-21 19:17:26,385 - INFO: Batch size: 32.
2024-06-21 19:17:26,389 - INFO: Dataset:
2024-06-21 19:17:26,389 - INFO: Batch size:
2024-06-21 19:17:26,389 - INFO: Number of workers:
2024-06-21 19:17:27,559 - INFO: Epoch: 84/200, Batch: 1/29, Batch_Loss_Train: 0.711
2024-06-21 19:17:27,863 - INFO: Epoch: 84/200, Batch: 2/29, Batch_Loss_Train: 0.849
2024-06-21 19:17:28,258 - INFO: Epoch: 84/200, Batch: 3/29, Batch_Loss_Train: 0.629
2024-06-21 19:17:28,574 - INFO: Epoch: 84/200, Batch: 4/29, Batch_Loss_Train: 0.920
2024-06-21 19:17:29,003 - INFO: Epoch: 84/200, Batch: 5/29, Batch_Loss_Train: 0.769
2024-06-21 19:17:29,305 - INFO: Epoch: 84/200, Batch: 6/29, Batch_Loss_Train: 0.611
2024-06-21 19:17:29,695 - INFO: Epoch: 84/200, Batch: 7/29, Batch_Loss_Train: 0.761
2024-06-21 19:17:30,010 - INFO: Epoch: 84/200, Batch: 8/29, Batch_Loss_Train: 0.995
2024-06-21 19:17:30,433 - INFO: Epoch: 84/200, Batch: 9/29, Batch_Loss_Train: 0.923
2024-06-21 19:17:30,727 - INFO: Epoch: 84/200, Batch: 10/29, Batch_Loss_Train: 0.897
2024-06-21 19:17:31,104 - INFO: Epoch: 84/200, Batch: 11/29, Batch_Loss_Train: 0.723
2024-06-21 19:17:31,421 - INFO: Epoch: 84/200, Batch: 12/29, Batch_Loss_Train: 0.707
2024-06-21 19:17:31,855 - INFO: Epoch: 84/200, Batch: 13/29, Batch_Loss_Train: 0.792
2024-06-21 19:17:32,160 - INFO: Epoch: 84/200, Batch: 14/29, Batch_Loss_Train: 0.841
2024-06-21 19:17:32,559 - INFO: Epoch: 84/200, Batch: 15/29, Batch_Loss_Train: 0.777
2024-06-21 19:17:32,873 - INFO: Epoch: 84/200, Batch: 16/29, Batch_Loss_Train: 0.836
2024-06-21 19:17:33,301 - INFO: Epoch: 84/200, Batch: 17/29, Batch_Loss_Train: 0.758
2024-06-21 19:17:33,602 - INFO: Epoch: 84/200, Batch: 18/29, Batch_Loss_Train: 0.755
2024-06-21 19:17:33,988 - INFO: Epoch: 84/200, Batch: 19/29, Batch_Loss_Train: 0.629
2024-06-21 19:17:34,296 - INFO: Epoch: 84/200, Batch: 20/29, Batch_Loss_Train: 1.043
2024-06-21 19:17:34,713 - INFO: Epoch: 84/200, Batch: 21/29, Batch_Loss_Train: 0.795
2024-06-21 19:17:35,013 - INFO: Epoch: 84/200, Batch: 22/29, Batch_Loss_Train: 0.788
2024-06-21 19:17:35,388 - INFO: Epoch: 84/200, Batch: 23/29, Batch_Loss_Train: 1.132
2024-06-21 19:17:35,701 - INFO: Epoch: 84/200, Batch: 24/29, Batch_Loss_Train: 0.897
2024-06-21 19:17:36,118 - INFO: Epoch: 84/200, Batch: 25/29, Batch_Loss_Train: 0.802
2024-06-21 19:17:36,416 - INFO: Epoch: 84/200, Batch: 26/29, Batch_Loss_Train: 0.786
2024-06-21 19:17:36,796 - INFO: Epoch: 84/200, Batch: 27/29, Batch_Loss_Train: 0.797
2024-06-21 19:17:37,107 - INFO: Epoch: 84/200, Batch: 28/29, Batch_Loss_Train: 0.893
2024-06-21 19:17:37,321 - INFO: Epoch: 84/200, Batch: 29/29, Batch_Loss_Train: 0.569
2024-06-21 19:17:48,436 - INFO: 84/200 final results:
2024-06-21 19:17:48,437 - INFO: Training loss: 0.806.
2024-06-21 19:17:48,437 - INFO: Training MAE: 0.811.
2024-06-21 19:17:48,437 - INFO: Training MSE: 1.540.
2024-06-21 19:18:08,505 - INFO: Epoch: 84/200, Loss_train: 0.8064707969797069, Loss_val: 1.8849803127091507
2024-06-21 19:18:08,505 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:18:08,505 - INFO: Epoch 85/200...
2024-06-21 19:18:08,505 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:18:08,505 - INFO: Batch size: 32.
2024-06-21 19:18:08,509 - INFO: Dataset:
2024-06-21 19:18:08,509 - INFO: Batch size:
2024-06-21 19:18:08,509 - INFO: Number of workers:
2024-06-21 19:18:09,708 - INFO: Epoch: 85/200, Batch: 1/29, Batch_Loss_Train: 0.636
2024-06-21 19:18:10,016 - INFO: Epoch: 85/200, Batch: 2/29, Batch_Loss_Train: 0.781
2024-06-21 19:18:10,414 - INFO: Epoch: 85/200, Batch: 3/29, Batch_Loss_Train: 0.678
2024-06-21 19:18:10,734 - INFO: Epoch: 85/200, Batch: 4/29, Batch_Loss_Train: 0.854
2024-06-21 19:18:11,163 - INFO: Epoch: 85/200, Batch: 5/29, Batch_Loss_Train: 0.719
2024-06-21 19:18:11,465 - INFO: Epoch: 85/200, Batch: 6/29, Batch_Loss_Train: 0.824
2024-06-21 19:18:11,855 - INFO: Epoch: 85/200, Batch: 7/29, Batch_Loss_Train: 0.664
2024-06-21 19:18:12,172 - INFO: Epoch: 85/200, Batch: 8/29, Batch_Loss_Train: 0.804
2024-06-21 19:18:12,595 - INFO: Epoch: 85/200, Batch: 9/29, Batch_Loss_Train: 0.677
2024-06-21 19:18:12,886 - INFO: Epoch: 85/200, Batch: 10/29, Batch_Loss_Train: 0.780
2024-06-21 19:18:13,257 - INFO: Epoch: 85/200, Batch: 11/29, Batch_Loss_Train: 0.807
2024-06-21 19:18:13,572 - INFO: Epoch: 85/200, Batch: 12/29, Batch_Loss_Train: 0.879
2024-06-21 19:18:13,999 - INFO: Epoch: 85/200, Batch: 13/29, Batch_Loss_Train: 0.731
2024-06-21 19:18:14,301 - INFO: Epoch: 85/200, Batch: 14/29, Batch_Loss_Train: 0.625
2024-06-21 19:18:14,693 - INFO: Epoch: 85/200, Batch: 15/29, Batch_Loss_Train: 0.780
2024-06-21 19:18:15,004 - INFO: Epoch: 85/200, Batch: 16/29, Batch_Loss_Train: 0.725
2024-06-21 19:18:15,427 - INFO: Epoch: 85/200, Batch: 17/29, Batch_Loss_Train: 0.914
2024-06-21 19:18:15,725 - INFO: Epoch: 85/200, Batch: 18/29, Batch_Loss_Train: 0.635
2024-06-21 19:18:16,105 - INFO: Epoch: 85/200, Batch: 19/29, Batch_Loss_Train: 0.742
2024-06-21 19:18:16,409 - INFO: Epoch: 85/200, Batch: 20/29, Batch_Loss_Train: 0.641
2024-06-21 19:18:16,823 - INFO: Epoch: 85/200, Batch: 21/29, Batch_Loss_Train: 0.767
2024-06-21 19:18:17,123 - INFO: Epoch: 85/200, Batch: 22/29, Batch_Loss_Train: 0.686
2024-06-21 19:18:17,493 - INFO: Epoch: 85/200, Batch: 23/29, Batch_Loss_Train: 0.842
2024-06-21 19:18:17,805 - INFO: Epoch: 85/200, Batch: 24/29, Batch_Loss_Train: 0.747
2024-06-21 19:18:18,211 - INFO: Epoch: 85/200, Batch: 25/29, Batch_Loss_Train: 0.871
2024-06-21 19:18:18,505 - INFO: Epoch: 85/200, Batch: 26/29, Batch_Loss_Train: 0.650
2024-06-21 19:18:18,870 - INFO: Epoch: 85/200, Batch: 27/29, Batch_Loss_Train: 0.690
2024-06-21 19:18:19,176 - INFO: Epoch: 85/200, Batch: 28/29, Batch_Loss_Train: 0.736
2024-06-21 19:18:19,382 - INFO: Epoch: 85/200, Batch: 29/29, Batch_Loss_Train: 0.699
2024-06-21 19:18:30,471 - INFO: 85/200 final results:
2024-06-21 19:18:30,471 - INFO: Training loss: 0.744.
2024-06-21 19:18:30,471 - INFO: Training MAE: 0.745.
2024-06-21 19:18:30,471 - INFO: Training MSE: 1.340.
2024-06-21 19:18:51,138 - INFO: Epoch: 85/200, Loss_train: 0.7442357087957447, Loss_val: 1.9218278876666366
2024-06-21 19:18:51,138 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:18:51,138 - INFO: Epoch 86/200...
2024-06-21 19:18:51,138 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:18:51,138 - INFO: Batch size: 32.
2024-06-21 19:18:51,142 - INFO: Dataset:
2024-06-21 19:18:51,142 - INFO: Batch size:
2024-06-21 19:18:51,142 - INFO: Number of workers:
2024-06-21 19:18:52,312 - INFO: Epoch: 86/200, Batch: 1/29, Batch_Loss_Train: 0.714
2024-06-21 19:18:52,620 - INFO: Epoch: 86/200, Batch: 2/29, Batch_Loss_Train: 0.821
2024-06-21 19:18:53,018 - INFO: Epoch: 86/200, Batch: 3/29, Batch_Loss_Train: 0.779
2024-06-21 19:18:53,338 - INFO: Epoch: 86/200, Batch: 4/29, Batch_Loss_Train: 0.656
2024-06-21 19:18:53,768 - INFO: Epoch: 86/200, Batch: 5/29, Batch_Loss_Train: 0.617
2024-06-21 19:18:54,070 - INFO: Epoch: 86/200, Batch: 6/29, Batch_Loss_Train: 0.743
2024-06-21 19:18:54,459 - INFO: Epoch: 86/200, Batch: 7/29, Batch_Loss_Train: 0.705
2024-06-21 19:18:54,774 - INFO: Epoch: 86/200, Batch: 8/29, Batch_Loss_Train: 0.786
2024-06-21 19:18:55,208 - INFO: Epoch: 86/200, Batch: 9/29, Batch_Loss_Train: 0.777
2024-06-21 19:18:55,501 - INFO: Epoch: 86/200, Batch: 10/29, Batch_Loss_Train: 0.618
2024-06-21 19:18:55,877 - INFO: Epoch: 86/200, Batch: 11/29, Batch_Loss_Train: 0.620
2024-06-21 19:18:56,195 - INFO: Epoch: 86/200, Batch: 12/29, Batch_Loss_Train: 0.718
2024-06-21 19:18:56,630 - INFO: Epoch: 86/200, Batch: 13/29, Batch_Loss_Train: 0.568
2024-06-21 19:18:56,934 - INFO: Epoch: 86/200, Batch: 14/29, Batch_Loss_Train: 0.711
2024-06-21 19:18:57,332 - INFO: Epoch: 86/200, Batch: 15/29, Batch_Loss_Train: 0.773
2024-06-21 19:18:57,647 - INFO: Epoch: 86/200, Batch: 16/29, Batch_Loss_Train: 0.655
2024-06-21 19:18:58,075 - INFO: Epoch: 86/200, Batch: 17/29, Batch_Loss_Train: 0.911
2024-06-21 19:18:58,373 - INFO: Epoch: 86/200, Batch: 18/29, Batch_Loss_Train: 0.718
2024-06-21 19:18:58,759 - INFO: Epoch: 86/200, Batch: 19/29, Batch_Loss_Train: 0.731
2024-06-21 19:18:59,065 - INFO: Epoch: 86/200, Batch: 20/29, Batch_Loss_Train: 0.701
2024-06-21 19:18:59,485 - INFO: Epoch: 86/200, Batch: 21/29, Batch_Loss_Train: 0.770
2024-06-21 19:18:59,788 - INFO: Epoch: 86/200, Batch: 22/29, Batch_Loss_Train: 0.570
2024-06-21 19:19:00,170 - INFO: Epoch: 86/200, Batch: 23/29, Batch_Loss_Train: 0.783
2024-06-21 19:19:00,486 - INFO: Epoch: 86/200, Batch: 24/29, Batch_Loss_Train: 0.869
2024-06-21 19:19:00,905 - INFO: Epoch: 86/200, Batch: 25/29, Batch_Loss_Train: 0.766
2024-06-21 19:19:01,203 - INFO: Epoch: 86/200, Batch: 26/29, Batch_Loss_Train: 0.648
2024-06-21 19:19:01,582 - INFO: Epoch: 86/200, Batch: 27/29, Batch_Loss_Train: 0.661
2024-06-21 19:19:01,893 - INFO: Epoch: 86/200, Batch: 28/29, Batch_Loss_Train: 0.806
2024-06-21 19:19:02,109 - INFO: Epoch: 86/200, Batch: 29/29, Batch_Loss_Train: 0.806
2024-06-21 19:19:13,327 - INFO: 86/200 final results:
2024-06-21 19:19:13,327 - INFO: Training loss: 0.724.
2024-06-21 19:19:13,327 - INFO: Training MAE: 0.723.
2024-06-21 19:19:13,327 - INFO: Training MSE: 1.263.
2024-06-21 19:19:33,586 - INFO: Epoch: 86/200, Loss_train: 0.7241781312843849, Loss_val: 1.898134342555342
2024-06-21 19:19:33,586 - INFO: Best internal validation val_loss: 1.885 at epoch: 77.
2024-06-21 19:19:33,586 - INFO: Epoch 87/200...
2024-06-21 19:19:33,586 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:19:33,586 - INFO: Batch size: 32.
2024-06-21 19:19:33,589 - INFO: Dataset:
2024-06-21 19:19:33,590 - INFO: Batch size:
2024-06-21 19:19:33,590 - INFO: Number of workers:
2024-06-21 19:19:34,754 - INFO: Epoch: 87/200, Batch: 1/29, Batch_Loss_Train: 0.528
2024-06-21 19:19:35,058 - INFO: Epoch: 87/200, Batch: 2/29, Batch_Loss_Train: 0.703
2024-06-21 19:19:35,471 - INFO: Epoch: 87/200, Batch: 3/29, Batch_Loss_Train: 0.628
2024-06-21 19:19:35,788 - INFO: Epoch: 87/200, Batch: 4/29, Batch_Loss_Train: 0.730
2024-06-21 19:19:36,208 - INFO: Epoch: 87/200, Batch: 5/29, Batch_Loss_Train: 0.750
2024-06-21 19:19:36,506 - INFO: Epoch: 87/200, Batch: 6/29, Batch_Loss_Train: 0.653
2024-06-21 19:19:36,891 - INFO: Epoch: 87/200, Batch: 7/29, Batch_Loss_Train: 0.640
2024-06-21 19:19:37,204 - INFO: Epoch: 87/200, Batch: 8/29, Batch_Loss_Train: 0.725
2024-06-21 19:19:37,627 - INFO: Epoch: 87/200, Batch: 9/29, Batch_Loss_Train: 0.820
2024-06-21 19:19:37,919 - INFO: Epoch: 87/200, Batch: 10/29, Batch_Loss_Train: 0.735
2024-06-21 19:19:38,296 - INFO: Epoch: 87/200, Batch: 11/29, Batch_Loss_Train: 0.547
2024-06-21 19:19:38,611 - INFO: Epoch: 87/200, Batch: 12/29, Batch_Loss_Train: 0.638
2024-06-21 19:19:39,037 - INFO: Epoch: 87/200, Batch: 13/29, Batch_Loss_Train: 0.803
2024-06-21 19:19:39,339 - INFO: Epoch: 87/200, Batch: 14/29, Batch_Loss_Train: 0.953
2024-06-21 19:19:39,732 - INFO: Epoch: 87/200, Batch: 15/29, Batch_Loss_Train: 0.691
2024-06-21 19:19:40,044 - INFO: Epoch: 87/200, Batch: 16/29, Batch_Loss_Train: 0.881
2024-06-21 19:19:40,464 - INFO: Epoch: 87/200, Batch: 17/29, Batch_Loss_Train: 0.736
2024-06-21 19:19:40,763 - INFO: Epoch: 87/200, Batch: 18/29, Batch_Loss_Train: 0.618
2024-06-21 19:19:41,146 - INFO: Epoch: 87/200, Batch: 19/29, Batch_Loss_Train: 0.777
2024-06-21 19:19:41,453 - INFO: Epoch: 87/200, Batch: 20/29, Batch_Loss_Train: 0.898
2024-06-21 19:19:41,861 - INFO: Epoch: 87/200, Batch: 21/29, Batch_Loss_Train: 0.702
2024-06-21 19:19:42,162 - INFO: Epoch: 87/200, Batch: 22/29, Batch_Loss_Train: 0.850
2024-06-21 19:19:42,539 - INFO: Epoch: 87/200, Batch: 23/29, Batch_Loss_Train: 0.637
2024-06-21 19:19:42,853 - INFO: Epoch: 87/200, Batch: 24/29, Batch_Loss_Train: 0.713
2024-06-21 19:19:43,262 - INFO: Epoch: 87/200, Batch: 25/29, Batch_Loss_Train: 0.771
2024-06-21 19:19:43,559 - INFO: Epoch: 87/200, Batch: 26/29, Batch_Loss_Train: 0.662
2024-06-21 19:19:43,929 - INFO: Epoch: 87/200, Batch: 27/29, Batch_Loss_Train: 0.740
2024-06-21 19:19:44,239 - INFO: Epoch: 87/200, Batch: 28/29, Batch_Loss_Train: 0.766
2024-06-21 19:19:44,446 - INFO: Epoch: 87/200, Batch: 29/29, Batch_Loss_Train: 0.816
2024-06-21 19:19:55,583 - INFO: 87/200 final results:
2024-06-21 19:19:55,583 - INFO: Training loss: 0.728.
2024-06-21 19:19:55,584 - INFO: Training MAE: 0.726.
2024-06-21 19:19:55,584 - INFO: Training MSE: 1.257.
2024-06-21 19:20:16,386 - INFO: Epoch: 87/200, Loss_train: 0.727961842356057, Loss_val: 1.864918799235903
2024-06-21 19:20:16,405 - INFO: Saved new best metric model for epoch 87.
2024-06-21 19:20:16,405 - INFO: Best internal validation val_loss: 1.865 at epoch: 87.
2024-06-21 19:20:16,405 - INFO: Epoch 88/200...
2024-06-21 19:20:16,405 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:20:16,406 - INFO: Batch size: 32.
2024-06-21 19:20:16,410 - INFO: Dataset:
2024-06-21 19:20:16,410 - INFO: Batch size:
2024-06-21 19:20:16,410 - INFO: Number of workers:
2024-06-21 19:20:17,557 - INFO: Epoch: 88/200, Batch: 1/29, Batch_Loss_Train: 0.607
2024-06-21 19:20:17,893 - INFO: Epoch: 88/200, Batch: 2/29, Batch_Loss_Train: 0.701
2024-06-21 19:20:18,284 - INFO: Epoch: 88/200, Batch: 3/29, Batch_Loss_Train: 0.757
2024-06-21 19:20:18,600 - INFO: Epoch: 88/200, Batch: 4/29, Batch_Loss_Train: 0.779
2024-06-21 19:20:19,006 - INFO: Epoch: 88/200, Batch: 5/29, Batch_Loss_Train: 0.729
2024-06-21 19:20:19,329 - INFO: Epoch: 88/200, Batch: 6/29, Batch_Loss_Train: 0.612
2024-06-21 19:20:19,716 - INFO: Epoch: 88/200, Batch: 7/29, Batch_Loss_Train: 0.740
2024-06-21 19:20:20,019 - INFO: Epoch: 88/200, Batch: 8/29, Batch_Loss_Train: 0.592
2024-06-21 19:20:20,463 - INFO: Epoch: 88/200, Batch: 9/29, Batch_Loss_Train: 0.702
2024-06-21 19:20:20,757 - INFO: Epoch: 88/200, Batch: 10/29, Batch_Loss_Train: 0.741
2024-06-21 19:20:21,134 - INFO: Epoch: 88/200, Batch: 11/29, Batch_Loss_Train: 0.749
2024-06-21 19:20:21,439 - INFO: Epoch: 88/200, Batch: 12/29, Batch_Loss_Train: 0.561
2024-06-21 19:20:21,885 - INFO: Epoch: 88/200, Batch: 13/29, Batch_Loss_Train: 0.588
2024-06-21 19:20:22,190 - INFO: Epoch: 88/200, Batch: 14/29, Batch_Loss_Train: 0.768
2024-06-21 19:20:22,586 - INFO: Epoch: 88/200, Batch: 15/29, Batch_Loss_Train: 0.689
2024-06-21 19:20:22,888 - INFO: Epoch: 88/200, Batch: 16/29, Batch_Loss_Train: 0.796
2024-06-21 19:20:23,333 - INFO: Epoch: 88/200, Batch: 17/29, Batch_Loss_Train: 0.704
2024-06-21 19:20:23,635 - INFO: Epoch: 88/200, Batch: 18/29, Batch_Loss_Train: 0.650
2024-06-21 19:20:24,023 - INFO: Epoch: 88/200, Batch: 19/29, Batch_Loss_Train: 0.771
2024-06-21 19:20:24,318 - INFO: Epoch: 88/200, Batch: 20/29, Batch_Loss_Train: 0.697
2024-06-21 19:20:24,754 - INFO: Epoch: 88/200, Batch: 21/29, Batch_Loss_Train: 0.695
2024-06-21 19:20:25,057 - INFO: Epoch: 88/200, Batch: 22/29, Batch_Loss_Train: 0.760
2024-06-21 19:20:25,438 - INFO: Epoch: 88/200, Batch: 23/29, Batch_Loss_Train: 0.914
2024-06-21 19:20:25,741 - INFO: Epoch: 88/200, Batch: 24/29, Batch_Loss_Train: 0.916
2024-06-21 19:20:26,167 - INFO: Epoch: 88/200, Batch: 25/29, Batch_Loss_Train: 0.771
2024-06-21 19:20:26,465 - INFO: Epoch: 88/200, Batch: 26/29, Batch_Loss_Train: 0.577
2024-06-21 19:20:26,835 - INFO: Epoch: 88/200, Batch: 27/29, Batch_Loss_Train: 0.800
2024-06-21 19:20:27,133 - INFO: Epoch: 88/200, Batch: 28/29, Batch_Loss_Train: 0.771
2024-06-21 19:20:27,348 - INFO: Epoch: 88/200, Batch: 29/29, Batch_Loss_Train: 0.755
2024-06-21 19:20:38,377 - INFO: 88/200 final results:
2024-06-21 19:20:38,377 - INFO: Training loss: 0.720.
2024-06-21 19:20:38,377 - INFO: Training MAE: 0.720.
2024-06-21 19:20:38,377 - INFO: Training MSE: 1.267.
2024-06-21 19:20:58,677 - INFO: Epoch: 88/200, Loss_train: 0.7204946279525757, Loss_val: 1.8661031270849293
2024-06-21 19:20:58,677 - INFO: Best internal validation val_loss: 1.865 at epoch: 87.
2024-06-21 19:20:58,677 - INFO: Epoch 89/200...
2024-06-21 19:20:58,677 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:20:58,677 - INFO: Batch size: 32.
2024-06-21 19:20:58,682 - INFO: Dataset:
2024-06-21 19:20:58,682 - INFO: Batch size:
2024-06-21 19:20:58,682 - INFO: Number of workers:
2024-06-21 19:20:59,830 - INFO: Epoch: 89/200, Batch: 1/29, Batch_Loss_Train: 0.655
2024-06-21 19:21:00,148 - INFO: Epoch: 89/200, Batch: 2/29, Batch_Loss_Train: 0.752
2024-06-21 19:21:00,557 - INFO: Epoch: 89/200, Batch: 3/29, Batch_Loss_Train: 0.827
2024-06-21 19:21:00,874 - INFO: Epoch: 89/200, Batch: 4/29, Batch_Loss_Train: 0.866
2024-06-21 19:21:01,287 - INFO: Epoch: 89/200, Batch: 5/29, Batch_Loss_Train: 0.828
2024-06-21 19:21:01,587 - INFO: Epoch: 89/200, Batch: 6/29, Batch_Loss_Train: 0.745
2024-06-21 19:21:01,988 - INFO: Epoch: 89/200, Batch: 7/29, Batch_Loss_Train: 0.614
2024-06-21 19:21:02,303 - INFO: Epoch: 89/200, Batch: 8/29, Batch_Loss_Train: 0.731
2024-06-21 19:21:02,715 - INFO: Epoch: 89/200, Batch: 9/29, Batch_Loss_Train: 0.860
2024-06-21 19:21:03,009 - INFO: Epoch: 89/200, Batch: 10/29, Batch_Loss_Train: 0.657
2024-06-21 19:21:03,402 - INFO: Epoch: 89/200, Batch: 11/29, Batch_Loss_Train: 0.717
2024-06-21 19:21:03,720 - INFO: Epoch: 89/200, Batch: 12/29, Batch_Loss_Train: 0.659
2024-06-21 19:21:04,136 - INFO: Epoch: 89/200, Batch: 13/29, Batch_Loss_Train: 0.736
2024-06-21 19:21:04,440 - INFO: Epoch: 89/200, Batch: 14/29, Batch_Loss_Train: 0.729
2024-06-21 19:21:04,851 - INFO: Epoch: 89/200, Batch: 15/29, Batch_Loss_Train: 0.765
2024-06-21 19:21:05,165 - INFO: Epoch: 89/200, Batch: 16/29, Batch_Loss_Train: 0.646
2024-06-21 19:21:05,574 - INFO: Epoch: 89/200, Batch: 17/29, Batch_Loss_Train: 0.592
2024-06-21 19:21:05,872 - INFO: Epoch: 89/200, Batch: 18/29, Batch_Loss_Train: 0.671
2024-06-21 19:21:06,268 - INFO: Epoch: 89/200, Batch: 19/29, Batch_Loss_Train: 0.657
2024-06-21 19:21:06,574 - INFO: Epoch: 89/200, Batch: 20/29, Batch_Loss_Train: 0.706
2024-06-21 19:21:06,975 - INFO: Epoch: 89/200, Batch: 21/29, Batch_Loss_Train: 0.735
2024-06-21 19:21:07,277 - INFO: Epoch: 89/200, Batch: 22/29, Batch_Loss_Train: 0.717
2024-06-21 19:21:07,673 - INFO: Epoch: 89/200, Batch: 23/29, Batch_Loss_Train: 0.679
2024-06-21 19:21:07,987 - INFO: Epoch: 89/200, Batch: 24/29, Batch_Loss_Train: 0.763
2024-06-21 19:21:08,390 - INFO: Epoch: 89/200, Batch: 25/29, Batch_Loss_Train: 0.624
2024-06-21 19:21:08,686 - INFO: Epoch: 89/200, Batch: 26/29, Batch_Loss_Train: 0.856
2024-06-21 19:21:09,069 - INFO: Epoch: 89/200, Batch: 27/29, Batch_Loss_Train: 0.790
2024-06-21 19:21:09,377 - INFO: Epoch: 89/200, Batch: 28/29, Batch_Loss_Train: 0.714
2024-06-21 19:21:09,593 - INFO: Epoch: 89/200, Batch: 29/29, Batch_Loss_Train: 0.584
2024-06-21 19:21:20,669 - INFO: 89/200 final results:
2024-06-21 19:21:20,669 - INFO: Training loss: 0.720.
2024-06-21 19:21:20,669 - INFO: Training MAE: 0.723.
2024-06-21 19:21:20,669 - INFO: Training MSE: 1.237.
2024-06-21 19:21:41,139 - INFO: Epoch: 89/200, Loss_train: 0.7198291975876381, Loss_val: 1.845251358788589
2024-06-21 19:21:41,158 - INFO: Saved new best metric model for epoch 89.
2024-06-21 19:21:41,158 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:21:41,158 - INFO: Epoch 90/200...
2024-06-21 19:21:41,158 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:21:41,158 - INFO: Batch size: 32.
2024-06-21 19:21:41,162 - INFO: Dataset:
2024-06-21 19:21:41,162 - INFO: Batch size:
2024-06-21 19:21:41,163 - INFO: Number of workers:
2024-06-21 19:21:42,342 - INFO: Epoch: 90/200, Batch: 1/29, Batch_Loss_Train: 0.601
2024-06-21 19:21:42,663 - INFO: Epoch: 90/200, Batch: 2/29, Batch_Loss_Train: 0.691
2024-06-21 19:21:43,047 - INFO: Epoch: 90/200, Batch: 3/29, Batch_Loss_Train: 0.737
2024-06-21 19:21:43,365 - INFO: Epoch: 90/200, Batch: 4/29, Batch_Loss_Train: 0.785
2024-06-21 19:21:43,775 - INFO: Epoch: 90/200, Batch: 5/29, Batch_Loss_Train: 0.707
2024-06-21 19:21:44,072 - INFO: Epoch: 90/200, Batch: 6/29, Batch_Loss_Train: 0.973
2024-06-21 19:21:44,447 - INFO: Epoch: 90/200, Batch: 7/29, Batch_Loss_Train: 0.941
2024-06-21 19:21:44,763 - INFO: Epoch: 90/200, Batch: 8/29, Batch_Loss_Train: 0.673
2024-06-21 19:21:45,184 - INFO: Epoch: 90/200, Batch: 9/29, Batch_Loss_Train: 0.781
2024-06-21 19:21:45,476 - INFO: Epoch: 90/200, Batch: 10/29, Batch_Loss_Train: 0.715
2024-06-21 19:21:45,850 - INFO: Epoch: 90/200, Batch: 11/29, Batch_Loss_Train: 0.757
2024-06-21 19:21:46,164 - INFO: Epoch: 90/200, Batch: 12/29, Batch_Loss_Train: 0.624
2024-06-21 19:21:46,592 - INFO: Epoch: 90/200, Batch: 13/29, Batch_Loss_Train: 0.723
2024-06-21 19:21:46,893 - INFO: Epoch: 90/200, Batch: 14/29, Batch_Loss_Train: 0.776
2024-06-21 19:21:47,289 - INFO: Epoch: 90/200, Batch: 15/29, Batch_Loss_Train: 0.813
2024-06-21 19:21:47,600 - INFO: Epoch: 90/200, Batch: 16/29, Batch_Loss_Train: 0.677
2024-06-21 19:21:48,028 - INFO: Epoch: 90/200, Batch: 17/29, Batch_Loss_Train: 0.738
2024-06-21 19:21:48,326 - INFO: Epoch: 90/200, Batch: 18/29, Batch_Loss_Train: 0.540
2024-06-21 19:21:48,706 - INFO: Epoch: 90/200, Batch: 19/29, Batch_Loss_Train: 0.924
2024-06-21 19:21:49,013 - INFO: Epoch: 90/200, Batch: 20/29, Batch_Loss_Train: 0.868
2024-06-21 19:21:49,428 - INFO: Epoch: 90/200, Batch: 21/29, Batch_Loss_Train: 0.687
2024-06-21 19:21:49,729 - INFO: Epoch: 90/200, Batch: 22/29, Batch_Loss_Train: 0.603
2024-06-21 19:21:50,110 - INFO: Epoch: 90/200, Batch: 23/29, Batch_Loss_Train: 0.688
2024-06-21 19:21:50,423 - INFO: Epoch: 90/200, Batch: 24/29, Batch_Loss_Train: 0.572
2024-06-21 19:21:50,834 - INFO: Epoch: 90/200, Batch: 25/29, Batch_Loss_Train: 0.754
2024-06-21 19:21:51,129 - INFO: Epoch: 90/200, Batch: 26/29, Batch_Loss_Train: 0.732
2024-06-21 19:21:51,502 - INFO: Epoch: 90/200, Batch: 27/29, Batch_Loss_Train: 0.710
2024-06-21 19:21:51,812 - INFO: Epoch: 90/200, Batch: 28/29, Batch_Loss_Train: 0.584
2024-06-21 19:21:52,029 - INFO: Epoch: 90/200, Batch: 29/29, Batch_Loss_Train: 0.668
2024-06-21 19:22:03,183 - INFO: 90/200 final results:
2024-06-21 19:22:03,183 - INFO: Training loss: 0.726.
2024-06-21 19:22:03,183 - INFO: Training MAE: 0.727.
2024-06-21 19:22:03,183 - INFO: Training MSE: 1.289.
2024-06-21 19:22:23,671 - INFO: Epoch: 90/200, Loss_train: 0.7255506474396278, Loss_val: 1.884819117085687
2024-06-21 19:22:23,671 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:22:23,671 - INFO: Epoch 91/200...
2024-06-21 19:22:23,671 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:22:23,671 - INFO: Batch size: 32.
2024-06-21 19:22:23,676 - INFO: Dataset:
2024-06-21 19:22:23,676 - INFO: Batch size:
2024-06-21 19:22:23,676 - INFO: Number of workers:
2024-06-21 19:22:24,864 - INFO: Epoch: 91/200, Batch: 1/29, Batch_Loss_Train: 0.673
2024-06-21 19:22:25,170 - INFO: Epoch: 91/200, Batch: 2/29, Batch_Loss_Train: 0.650
2024-06-21 19:22:25,567 - INFO: Epoch: 91/200, Batch: 3/29, Batch_Loss_Train: 0.688
2024-06-21 19:22:25,885 - INFO: Epoch: 91/200, Batch: 4/29, Batch_Loss_Train: 0.742
2024-06-21 19:22:26,309 - INFO: Epoch: 91/200, Batch: 5/29, Batch_Loss_Train: 0.615
2024-06-21 19:22:26,610 - INFO: Epoch: 91/200, Batch: 6/29, Batch_Loss_Train: 0.628
2024-06-21 19:22:26,999 - INFO: Epoch: 91/200, Batch: 7/29, Batch_Loss_Train: 0.636
2024-06-21 19:22:27,312 - INFO: Epoch: 91/200, Batch: 8/29, Batch_Loss_Train: 0.556
2024-06-21 19:22:27,745 - INFO: Epoch: 91/200, Batch: 9/29, Batch_Loss_Train: 0.802
2024-06-21 19:22:28,037 - INFO: Epoch: 91/200, Batch: 10/29, Batch_Loss_Train: 0.952
2024-06-21 19:22:28,416 - INFO: Epoch: 91/200, Batch: 11/29, Batch_Loss_Train: 0.647
2024-06-21 19:22:28,731 - INFO: Epoch: 91/200, Batch: 12/29, Batch_Loss_Train: 0.662
2024-06-21 19:22:29,161 - INFO: Epoch: 91/200, Batch: 13/29, Batch_Loss_Train: 0.744
2024-06-21 19:22:29,464 - INFO: Epoch: 91/200, Batch: 14/29, Batch_Loss_Train: 0.903
2024-06-21 19:22:29,859 - INFO: Epoch: 91/200, Batch: 15/29, Batch_Loss_Train: 0.650
2024-06-21 19:22:30,172 - INFO: Epoch: 91/200, Batch: 16/29, Batch_Loss_Train: 0.711
2024-06-21 19:22:30,601 - INFO: Epoch: 91/200, Batch: 17/29, Batch_Loss_Train: 0.673
2024-06-21 19:22:30,901 - INFO: Epoch: 91/200, Batch: 18/29, Batch_Loss_Train: 0.802
2024-06-21 19:22:31,289 - INFO: Epoch: 91/200, Batch: 19/29, Batch_Loss_Train: 0.729
2024-06-21 19:22:31,595 - INFO: Epoch: 91/200, Batch: 20/29, Batch_Loss_Train: 0.592
2024-06-21 19:22:32,019 - INFO: Epoch: 91/200, Batch: 21/29, Batch_Loss_Train: 0.732
2024-06-21 19:22:32,321 - INFO: Epoch: 91/200, Batch: 22/29, Batch_Loss_Train: 0.632
2024-06-21 19:22:32,703 - INFO: Epoch: 91/200, Batch: 23/29, Batch_Loss_Train: 0.677
2024-06-21 19:22:33,018 - INFO: Epoch: 91/200, Batch: 24/29, Batch_Loss_Train: 0.642
2024-06-21 19:22:33,434 - INFO: Epoch: 91/200, Batch: 25/29, Batch_Loss_Train: 0.586
2024-06-21 19:22:33,731 - INFO: Epoch: 91/200, Batch: 26/29, Batch_Loss_Train: 0.779
2024-06-21 19:22:34,102 - INFO: Epoch: 91/200, Batch: 27/29, Batch_Loss_Train: 0.831
2024-06-21 19:22:34,412 - INFO: Epoch: 91/200, Batch: 28/29, Batch_Loss_Train: 0.728
2024-06-21 19:22:34,625 - INFO: Epoch: 91/200, Batch: 29/29, Batch_Loss_Train: 0.654
2024-06-21 19:22:45,774 - INFO: 91/200 final results:
2024-06-21 19:22:45,774 - INFO: Training loss: 0.700.
2024-06-21 19:22:45,774 - INFO: Training MAE: 0.701.
2024-06-21 19:22:45,774 - INFO: Training MSE: 1.213.
2024-06-21 19:23:06,396 - INFO: Epoch: 91/200, Loss_train: 0.7004734257171894, Loss_val: 1.878600996116112
2024-06-21 19:23:06,396 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:23:06,396 - INFO: Epoch 92/200...
2024-06-21 19:23:06,396 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:23:06,396 - INFO: Batch size: 32.
2024-06-21 19:23:06,400 - INFO: Dataset:
2024-06-21 19:23:06,400 - INFO: Batch size:
2024-06-21 19:23:06,400 - INFO: Number of workers:
2024-06-21 19:23:07,548 - INFO: Epoch: 92/200, Batch: 1/29, Batch_Loss_Train: 0.659
2024-06-21 19:23:07,854 - INFO: Epoch: 92/200, Batch: 2/29, Batch_Loss_Train: 0.628
2024-06-21 19:23:08,264 - INFO: Epoch: 92/200, Batch: 3/29, Batch_Loss_Train: 0.753
2024-06-21 19:23:08,581 - INFO: Epoch: 92/200, Batch: 4/29, Batch_Loss_Train: 0.574
2024-06-21 19:23:08,997 - INFO: Epoch: 92/200, Batch: 5/29, Batch_Loss_Train: 0.710
2024-06-21 19:23:09,299 - INFO: Epoch: 92/200, Batch: 6/29, Batch_Loss_Train: 0.625
2024-06-21 19:23:09,702 - INFO: Epoch: 92/200, Batch: 7/29, Batch_Loss_Train: 0.741
2024-06-21 19:23:10,017 - INFO: Epoch: 92/200, Batch: 8/29, Batch_Loss_Train: 0.657
2024-06-21 19:23:10,417 - INFO: Epoch: 92/200, Batch: 9/29, Batch_Loss_Train: 0.748
2024-06-21 19:23:10,710 - INFO: Epoch: 92/200, Batch: 10/29, Batch_Loss_Train: 0.850
2024-06-21 19:23:11,101 - INFO: Epoch: 92/200, Batch: 11/29, Batch_Loss_Train: 0.694
2024-06-21 19:23:11,418 - INFO: Epoch: 92/200, Batch: 12/29, Batch_Loss_Train: 0.881
2024-06-21 19:23:11,829 - INFO: Epoch: 92/200, Batch: 13/29, Batch_Loss_Train: 0.689
2024-06-21 19:23:12,133 - INFO: Epoch: 92/200, Batch: 14/29, Batch_Loss_Train: 0.679
2024-06-21 19:23:12,531 - INFO: Epoch: 92/200, Batch: 15/29, Batch_Loss_Train: 0.666
2024-06-21 19:23:12,845 - INFO: Epoch: 92/200, Batch: 16/29, Batch_Loss_Train: 0.761
2024-06-21 19:23:13,253 - INFO: Epoch: 92/200, Batch: 17/29, Batch_Loss_Train: 0.612
2024-06-21 19:23:13,554 - INFO: Epoch: 92/200, Batch: 18/29, Batch_Loss_Train: 0.625
2024-06-21 19:23:13,956 - INFO: Epoch: 92/200, Batch: 19/29, Batch_Loss_Train: 0.642
2024-06-21 19:23:14,264 - INFO: Epoch: 92/200, Batch: 20/29, Batch_Loss_Train: 0.693
2024-06-21 19:23:14,668 - INFO: Epoch: 92/200, Batch: 21/29, Batch_Loss_Train: 0.608
2024-06-21 19:23:14,970 - INFO: Epoch: 92/200, Batch: 22/29, Batch_Loss_Train: 0.930
2024-06-21 19:23:15,366 - INFO: Epoch: 92/200, Batch: 23/29, Batch_Loss_Train: 0.627
2024-06-21 19:23:15,681 - INFO: Epoch: 92/200, Batch: 24/29, Batch_Loss_Train: 0.516
2024-06-21 19:23:16,090 - INFO: Epoch: 92/200, Batch: 25/29, Batch_Loss_Train: 0.665
2024-06-21 19:23:16,389 - INFO: Epoch: 92/200, Batch: 26/29, Batch_Loss_Train: 0.773
2024-06-21 19:23:16,779 - INFO: Epoch: 92/200, Batch: 27/29, Batch_Loss_Train: 0.816
2024-06-21 19:23:17,090 - INFO: Epoch: 92/200, Batch: 28/29, Batch_Loss_Train: 0.666
2024-06-21 19:23:17,301 - INFO: Epoch: 92/200, Batch: 29/29, Batch_Loss_Train: 0.545
2024-06-21 19:23:28,473 - INFO: 92/200 final results:
2024-06-21 19:23:28,473 - INFO: Training loss: 0.691.
2024-06-21 19:23:28,474 - INFO: Training MAE: 0.694.
2024-06-21 19:23:28,474 - INFO: Training MSE: 1.190.
2024-06-21 19:23:49,006 - INFO: Epoch: 92/200, Loss_train: 0.6907756410796067, Loss_val: 1.8701120779432099
2024-06-21 19:23:49,006 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:23:49,006 - INFO: Epoch 93/200...
2024-06-21 19:23:49,006 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:23:49,006 - INFO: Batch size: 32.
2024-06-21 19:23:49,010 - INFO: Dataset:
2024-06-21 19:23:49,010 - INFO: Batch size:
2024-06-21 19:23:49,010 - INFO: Number of workers:
2024-06-21 19:23:50,190 - INFO: Epoch: 93/200, Batch: 1/29, Batch_Loss_Train: 0.688
2024-06-21 19:23:50,508 - INFO: Epoch: 93/200, Batch: 2/29, Batch_Loss_Train: 0.556
2024-06-21 19:23:50,904 - INFO: Epoch: 93/200, Batch: 3/29, Batch_Loss_Train: 0.595
2024-06-21 19:23:51,221 - INFO: Epoch: 93/200, Batch: 4/29, Batch_Loss_Train: 0.516
2024-06-21 19:23:51,646 - INFO: Epoch: 93/200, Batch: 5/29, Batch_Loss_Train: 0.660
2024-06-21 19:23:51,945 - INFO: Epoch: 93/200, Batch: 6/29, Batch_Loss_Train: 0.822
2024-06-21 19:23:52,334 - INFO: Epoch: 93/200, Batch: 7/29, Batch_Loss_Train: 0.854
2024-06-21 19:23:52,646 - INFO: Epoch: 93/200, Batch: 8/29, Batch_Loss_Train: 0.620
2024-06-21 19:23:53,071 - INFO: Epoch: 93/200, Batch: 9/29, Batch_Loss_Train: 0.828
2024-06-21 19:23:53,362 - INFO: Epoch: 93/200, Batch: 10/29, Batch_Loss_Train: 0.694
2024-06-21 19:23:53,737 - INFO: Epoch: 93/200, Batch: 11/29, Batch_Loss_Train: 0.606
2024-06-21 19:23:54,052 - INFO: Epoch: 93/200, Batch: 12/29, Batch_Loss_Train: 0.691
2024-06-21 19:23:54,485 - INFO: Epoch: 93/200, Batch: 13/29, Batch_Loss_Train: 0.768
2024-06-21 19:23:54,787 - INFO: Epoch: 93/200, Batch: 14/29, Batch_Loss_Train: 0.765
2024-06-21 19:23:55,182 - INFO: Epoch: 93/200, Batch: 15/29, Batch_Loss_Train: 0.622
2024-06-21 19:23:55,494 - INFO: Epoch: 93/200, Batch: 16/29, Batch_Loss_Train: 0.554
2024-06-21 19:23:55,922 - INFO: Epoch: 93/200, Batch: 17/29, Batch_Loss_Train: 0.582
2024-06-21 19:23:56,221 - INFO: Epoch: 93/200, Batch: 18/29, Batch_Loss_Train: 0.585
2024-06-21 19:23:56,607 - INFO: Epoch: 93/200, Batch: 19/29, Batch_Loss_Train: 0.563
2024-06-21 19:23:56,912 - INFO: Epoch: 93/200, Batch: 20/29, Batch_Loss_Train: 0.903
2024-06-21 19:23:57,333 - INFO: Epoch: 93/200, Batch: 21/29, Batch_Loss_Train: 0.720
2024-06-21 19:23:57,633 - INFO: Epoch: 93/200, Batch: 22/29, Batch_Loss_Train: 0.622
2024-06-21 19:23:58,018 - INFO: Epoch: 93/200, Batch: 23/29, Batch_Loss_Train: 0.771
2024-06-21 19:23:58,332 - INFO: Epoch: 93/200, Batch: 24/29, Batch_Loss_Train: 0.799
2024-06-21 19:23:58,753 - INFO: Epoch: 93/200, Batch: 25/29, Batch_Loss_Train: 0.599
2024-06-21 19:23:59,052 - INFO: Epoch: 93/200, Batch: 26/29, Batch_Loss_Train: 0.816
2024-06-21 19:23:59,437 - INFO: Epoch: 93/200, Batch: 27/29, Batch_Loss_Train: 0.596
2024-06-21 19:23:59,750 - INFO: Epoch: 93/200, Batch: 28/29, Batch_Loss_Train: 0.769
2024-06-21 19:23:59,973 - INFO: Epoch: 93/200, Batch: 29/29, Batch_Loss_Train: 0.748
2024-06-21 19:24:11,198 - INFO: 93/200 final results:
2024-06-21 19:24:11,198 - INFO: Training loss: 0.687.
2024-06-21 19:24:11,198 - INFO: Training MAE: 0.685.
2024-06-21 19:24:11,198 - INFO: Training MSE: 1.146.
2024-06-21 19:24:31,591 - INFO: Epoch: 93/200, Loss_train: 0.6866278874463049, Loss_val: 1.8628716756557595
2024-06-21 19:24:31,591 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:24:31,591 - INFO: Epoch 94/200...
2024-06-21 19:24:31,591 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:24:31,591 - INFO: Batch size: 32.
2024-06-21 19:24:31,595 - INFO: Dataset:
2024-06-21 19:24:31,595 - INFO: Batch size:
2024-06-21 19:24:31,595 - INFO: Number of workers:
2024-06-21 19:24:32,764 - INFO: Epoch: 94/200, Batch: 1/29, Batch_Loss_Train: 0.711
2024-06-21 19:24:33,069 - INFO: Epoch: 94/200, Batch: 2/29, Batch_Loss_Train: 0.647
2024-06-21 19:24:33,472 - INFO: Epoch: 94/200, Batch: 3/29, Batch_Loss_Train: 0.697
2024-06-21 19:24:33,789 - INFO: Epoch: 94/200, Batch: 4/29, Batch_Loss_Train: 0.711
2024-06-21 19:24:34,185 - INFO: Epoch: 94/200, Batch: 5/29, Batch_Loss_Train: 0.767
2024-06-21 19:24:34,496 - INFO: Epoch: 94/200, Batch: 6/29, Batch_Loss_Train: 0.648
2024-06-21 19:24:34,885 - INFO: Epoch: 94/200, Batch: 7/29, Batch_Loss_Train: 0.661
2024-06-21 19:24:35,197 - INFO: Epoch: 94/200, Batch: 8/29, Batch_Loss_Train: 0.811
2024-06-21 19:24:35,580 - INFO: Epoch: 94/200, Batch: 9/29, Batch_Loss_Train: 0.611
2024-06-21 19:24:35,884 - INFO: Epoch: 94/200, Batch: 10/29, Batch_Loss_Train: 0.687
2024-06-21 19:24:36,264 - INFO: Epoch: 94/200, Batch: 11/29, Batch_Loss_Train: 0.894
2024-06-21 19:24:36,578 - INFO: Epoch: 94/200, Batch: 12/29, Batch_Loss_Train: 0.747
2024-06-21 19:24:36,978 - INFO: Epoch: 94/200, Batch: 13/29, Batch_Loss_Train: 0.619
2024-06-21 19:24:37,291 - INFO: Epoch: 94/200, Batch: 14/29, Batch_Loss_Train: 0.544
2024-06-21 19:24:37,687 - INFO: Epoch: 94/200, Batch: 15/29, Batch_Loss_Train: 0.670
2024-06-21 19:24:37,997 - INFO: Epoch: 94/200, Batch: 16/29, Batch_Loss_Train: 0.872
2024-06-21 19:24:38,402 - INFO: Epoch: 94/200, Batch: 17/29, Batch_Loss_Train: 0.819
2024-06-21 19:24:38,716 - INFO: Epoch: 94/200, Batch: 18/29, Batch_Loss_Train: 0.563
2024-06-21 19:24:39,115 - INFO: Epoch: 94/200, Batch: 19/29, Batch_Loss_Train: 0.677
2024-06-21 19:24:39,424 - INFO: Epoch: 94/200, Batch: 20/29, Batch_Loss_Train: 0.685
2024-06-21 19:24:39,817 - INFO: Epoch: 94/200, Batch: 21/29, Batch_Loss_Train: 0.598
2024-06-21 19:24:40,133 - INFO: Epoch: 94/200, Batch: 22/29, Batch_Loss_Train: 0.714
2024-06-21 19:24:40,531 - INFO: Epoch: 94/200, Batch: 23/29, Batch_Loss_Train: 0.692
2024-06-21 19:24:40,847 - INFO: Epoch: 94/200, Batch: 24/29, Batch_Loss_Train: 0.701
2024-06-21 19:24:41,239 - INFO: Epoch: 94/200, Batch: 25/29, Batch_Loss_Train: 0.919
2024-06-21 19:24:41,550 - INFO: Epoch: 94/200, Batch: 26/29, Batch_Loss_Train: 0.624
2024-06-21 19:24:41,939 - INFO: Epoch: 94/200, Batch: 27/29, Batch_Loss_Train: 0.734
2024-06-21 19:24:42,250 - INFO: Epoch: 94/200, Batch: 28/29, Batch_Loss_Train: 0.699
2024-06-21 19:24:42,465 - INFO: Epoch: 94/200, Batch: 29/29, Batch_Loss_Train: 1.087
2024-06-21 19:24:53,737 - INFO: 94/200 final results:
2024-06-21 19:24:53,737 - INFO: Training loss: 0.718.
2024-06-21 19:24:53,737 - INFO: Training MAE: 0.710.
2024-06-21 19:24:53,737 - INFO: Training MSE: 1.245.
2024-06-21 19:25:13,946 - INFO: Epoch: 94/200, Loss_train: 0.717508063234132, Loss_val: 1.8659273550428193
2024-06-21 19:25:13,946 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:25:13,946 - INFO: Epoch 95/200...
2024-06-21 19:25:13,946 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:25:13,946 - INFO: Batch size: 32.
2024-06-21 19:25:13,950 - INFO: Dataset:
2024-06-21 19:25:13,950 - INFO: Batch size:
2024-06-21 19:25:13,950 - INFO: Number of workers:
2024-06-21 19:25:15,095 - INFO: Epoch: 95/200, Batch: 1/29, Batch_Loss_Train: 0.647
2024-06-21 19:25:15,414 - INFO: Epoch: 95/200, Batch: 2/29, Batch_Loss_Train: 0.812
2024-06-21 19:25:15,823 - INFO: Epoch: 95/200, Batch: 3/29, Batch_Loss_Train: 0.745
2024-06-21 19:25:16,139 - INFO: Epoch: 95/200, Batch: 4/29, Batch_Loss_Train: 0.659
2024-06-21 19:25:16,544 - INFO: Epoch: 95/200, Batch: 5/29, Batch_Loss_Train: 0.614
2024-06-21 19:25:16,843 - INFO: Epoch: 95/200, Batch: 6/29, Batch_Loss_Train: 0.696
2024-06-21 19:25:17,235 - INFO: Epoch: 95/200, Batch: 7/29, Batch_Loss_Train: 0.660
2024-06-21 19:25:17,547 - INFO: Epoch: 95/200, Batch: 8/29, Batch_Loss_Train: 0.555
2024-06-21 19:25:17,960 - INFO: Epoch: 95/200, Batch: 9/29, Batch_Loss_Train: 0.741
2024-06-21 19:25:18,252 - INFO: Epoch: 95/200, Batch: 10/29, Batch_Loss_Train: 0.781
2024-06-21 19:25:18,635 - INFO: Epoch: 95/200, Batch: 11/29, Batch_Loss_Train: 0.709
2024-06-21 19:25:18,950 - INFO: Epoch: 95/200, Batch: 12/29, Batch_Loss_Train: 0.555
2024-06-21 19:25:19,358 - INFO: Epoch: 95/200, Batch: 13/29, Batch_Loss_Train: 0.737
2024-06-21 19:25:19,660 - INFO: Epoch: 95/200, Batch: 14/29, Batch_Loss_Train: 0.529
2024-06-21 19:25:20,064 - INFO: Epoch: 95/200, Batch: 15/29, Batch_Loss_Train: 0.624
2024-06-21 19:25:20,374 - INFO: Epoch: 95/200, Batch: 16/29, Batch_Loss_Train: 0.727
2024-06-21 19:25:20,780 - INFO: Epoch: 95/200, Batch: 17/29, Batch_Loss_Train: 0.626
2024-06-21 19:25:21,078 - INFO: Epoch: 95/200, Batch: 18/29, Batch_Loss_Train: 0.755
2024-06-21 19:25:21,473 - INFO: Epoch: 95/200, Batch: 19/29, Batch_Loss_Train: 0.688
2024-06-21 19:25:21,780 - INFO: Epoch: 95/200, Batch: 20/29, Batch_Loss_Train: 0.843
2024-06-21 19:25:22,178 - INFO: Epoch: 95/200, Batch: 21/29, Batch_Loss_Train: 0.924
2024-06-21 19:25:22,477 - INFO: Epoch: 95/200, Batch: 22/29, Batch_Loss_Train: 0.870
2024-06-21 19:25:22,874 - INFO: Epoch: 95/200, Batch: 23/29, Batch_Loss_Train: 0.725
2024-06-21 19:25:23,186 - INFO: Epoch: 95/200, Batch: 24/29, Batch_Loss_Train: 0.589
2024-06-21 19:25:23,591 - INFO: Epoch: 95/200, Batch: 25/29, Batch_Loss_Train: 0.652
2024-06-21 19:25:23,886 - INFO: Epoch: 95/200, Batch: 26/29, Batch_Loss_Train: 0.609
2024-06-21 19:25:24,281 - INFO: Epoch: 95/200, Batch: 27/29, Batch_Loss_Train: 0.666
2024-06-21 19:25:24,589 - INFO: Epoch: 95/200, Batch: 28/29, Batch_Loss_Train: 0.579
2024-06-21 19:25:24,806 - INFO: Epoch: 95/200, Batch: 29/29, Batch_Loss_Train: 0.668
2024-06-21 19:25:35,938 - INFO: 95/200 final results:
2024-06-21 19:25:35,939 - INFO: Training loss: 0.689.
2024-06-21 19:25:35,939 - INFO: Training MAE: 0.690.
2024-06-21 19:25:35,939 - INFO: Training MSE: 1.170.
2024-06-21 19:25:56,231 - INFO: Epoch: 95/200, Loss_train: 0.6891371217267267, Loss_val: 1.8861537102995247
2024-06-21 19:25:56,231 - INFO: Best internal validation val_loss: 1.845 at epoch: 89.
2024-06-21 19:25:56,231 - INFO: Epoch 96/200...
2024-06-21 19:25:56,231 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:25:56,231 - INFO: Batch size: 32.
2024-06-21 19:25:56,235 - INFO: Dataset:
2024-06-21 19:25:56,235 - INFO: Batch size:
2024-06-21 19:25:56,235 - INFO: Number of workers:
2024-06-21 19:25:57,400 - INFO: Epoch: 96/200, Batch: 1/29, Batch_Loss_Train: 0.771
2024-06-21 19:25:57,706 - INFO: Epoch: 96/200, Batch: 2/29, Batch_Loss_Train: 0.679
2024-06-21 19:25:58,113 - INFO: Epoch: 96/200, Batch: 3/29, Batch_Loss_Train: 0.572
2024-06-21 19:25:58,415 - INFO: Epoch: 96/200, Batch: 4/29, Batch_Loss_Train: 0.838
2024-06-21 19:25:58,819 - INFO: Epoch: 96/200, Batch: 5/29, Batch_Loss_Train: 0.623
2024-06-21 19:25:59,117 - INFO: Epoch: 96/200, Batch: 6/29, Batch_Loss_Train: 0.645
2024-06-21 19:25:59,517 - INFO: Epoch: 96/200, Batch: 7/29, Batch_Loss_Train: 0.844
2024-06-21 19:25:59,821 - INFO: Epoch: 96/200, Batch: 8/29, Batch_Loss_Train: 0.623
2024-06-21 19:26:00,229 - INFO: Epoch: 96/200, Batch: 9/29, Batch_Loss_Train: 0.710
2024-06-21 19:26:00,524 - INFO: Epoch: 96/200, Batch: 10/29, Batch_Loss_Train: 0.700
2024-06-21 19:26:00,931 - INFO: Epoch: 96/200, Batch: 11/29, Batch_Loss_Train: 0.738
2024-06-21 19:26:01,236 - INFO: Epoch: 96/200, Batch: 12/29, Batch_Loss_Train: 0.621
2024-06-21 19:26:01,656 - INFO: Epoch: 96/200, Batch: 13/29, Batch_Loss_Train: 0.618
2024-06-21 19:26:01,958 - INFO: Epoch: 96/200, Batch: 14/29, Batch_Loss_Train: 0.727
2024-06-21 19:26:02,386 - INFO: Epoch: 96/200, Batch: 15/29, Batch_Loss_Train: 0.557
2024-06-21 19:26:02,684 - INFO: Epoch: 96/200, Batch: 16/29, Batch_Loss_Train: 0.789
2024-06-21 19:26:03,087 - INFO: Epoch: 96/200, Batch: 17/29, Batch_Loss_Train: 0.928
2024-06-21 19:26:03,386 - INFO: Epoch: 96/200, Batch: 18/29, Batch_Loss_Train: 0.834
2024-06-21 19:26:03,808 - INFO: Epoch: 96/200, Batch: 19/29, Batch_Loss_Train: 0.630
2024-06-21 19:26:04,104 - INFO: Epoch: 96/200, Batch: 20/29, Batch_Loss_Train: 0.749
2024-06-21 19:26:04,505 - INFO: Epoch: 96/200, Batch: 21/29, Batch_Loss_Train: 0.725
2024-06-21 19:26:04,809 - INFO: Epoch: 96/200, Batch: 22/29, Batch_Loss_Train: 0.712
2024-06-21 19:26:05,235 - INFO: Epoch: 96/200, Batch: 23/29, Batch_Loss_Train: 0.665
2024-06-21 19:26:05,538 - INFO: Epoch: 96/200, Batch: 24/29, Batch_Loss_Train: 0.586
2024-06-21 19:26:05,938 - INFO: Epoch: 96/200, Batch: 25/29, Batch_Loss_Train: 0.801
2024-06-21 19:26:06,238 - INFO: Epoch: 96/200, Batch: 26/29, Batch_Loss_Train: 0.811
2024-06-21 19:26:06,655 - INFO: Epoch: 96/200, Batch: 27/29, Batch_Loss_Train: 0.654
2024-06-21 19:26:06,954 - INFO: Epoch: 96/200, Batch: 28/29, Batch_Loss_Train: 0.571
2024-06-21 19:26:07,168 - INFO: Epoch: 96/200, Batch: 29/29, Batch_Loss_Train: 0.628
2024-06-21 19:26:18,277 - INFO: 96/200 final results:
2024-06-21 19:26:18,277 - INFO: Training loss: 0.702.
2024-06-21 19:26:18,277 - INFO: Training MAE: 0.703.
2024-06-21 19:26:18,277 - INFO: Training MSE: 1.193.
2024-06-21 19:26:38,480 - INFO: Epoch: 96/200, Loss_train: 0.7016025863844773, Loss_val: 1.8346187122936906
2024-06-21 19:26:38,498 - INFO: Saved new best metric model for epoch 96.
2024-06-21 19:26:38,499 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:26:38,499 - INFO: Epoch 97/200...
2024-06-21 19:26:38,499 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:26:38,499 - INFO: Batch size: 32.
2024-06-21 19:26:38,503 - INFO: Dataset:
2024-06-21 19:26:38,503 - INFO: Batch size:
2024-06-21 19:26:38,503 - INFO: Number of workers:
2024-06-21 19:26:39,663 - INFO: Epoch: 97/200, Batch: 1/29, Batch_Loss_Train: 0.520
2024-06-21 19:26:39,985 - INFO: Epoch: 97/200, Batch: 2/29, Batch_Loss_Train: 0.763
2024-06-21 19:26:40,394 - INFO: Epoch: 97/200, Batch: 3/29, Batch_Loss_Train: 0.565
2024-06-21 19:26:40,714 - INFO: Epoch: 97/200, Batch: 4/29, Batch_Loss_Train: 0.718
2024-06-21 19:26:41,119 - INFO: Epoch: 97/200, Batch: 5/29, Batch_Loss_Train: 0.525
2024-06-21 19:26:41,421 - INFO: Epoch: 97/200, Batch: 6/29, Batch_Loss_Train: 0.575
2024-06-21 19:26:41,818 - INFO: Epoch: 97/200, Batch: 7/29, Batch_Loss_Train: 0.586
2024-06-21 19:26:42,135 - INFO: Epoch: 97/200, Batch: 8/29, Batch_Loss_Train: 0.723
2024-06-21 19:26:42,550 - INFO: Epoch: 97/200, Batch: 9/29, Batch_Loss_Train: 0.690
2024-06-21 19:26:42,842 - INFO: Epoch: 97/200, Batch: 10/29, Batch_Loss_Train: 0.632
2024-06-21 19:26:43,223 - INFO: Epoch: 97/200, Batch: 11/29, Batch_Loss_Train: 0.740
2024-06-21 19:26:43,540 - INFO: Epoch: 97/200, Batch: 12/29, Batch_Loss_Train: 0.553
2024-06-21 19:26:43,962 - INFO: Epoch: 97/200, Batch: 13/29, Batch_Loss_Train: 0.614
2024-06-21 19:26:44,267 - INFO: Epoch: 97/200, Batch: 14/29, Batch_Loss_Train: 0.594
2024-06-21 19:26:44,669 - INFO: Epoch: 97/200, Batch: 15/29, Batch_Loss_Train: 0.630
2024-06-21 19:26:44,981 - INFO: Epoch: 97/200, Batch: 16/29, Batch_Loss_Train: 0.698
2024-06-21 19:26:45,384 - INFO: Epoch: 97/200, Batch: 17/29, Batch_Loss_Train: 0.521
2024-06-21 19:26:45,682 - INFO: Epoch: 97/200, Batch: 18/29, Batch_Loss_Train: 0.535
2024-06-21 19:26:46,078 - INFO: Epoch: 97/200, Batch: 19/29, Batch_Loss_Train: 0.553
2024-06-21 19:26:46,383 - INFO: Epoch: 97/200, Batch: 20/29, Batch_Loss_Train: 0.705
2024-06-21 19:26:46,782 - INFO: Epoch: 97/200, Batch: 21/29, Batch_Loss_Train: 0.970
2024-06-21 19:26:47,082 - INFO: Epoch: 97/200, Batch: 22/29, Batch_Loss_Train: 0.755
2024-06-21 19:26:47,477 - INFO: Epoch: 97/200, Batch: 23/29, Batch_Loss_Train: 0.927
2024-06-21 19:26:47,790 - INFO: Epoch: 97/200, Batch: 24/29, Batch_Loss_Train: 0.831
2024-06-21 19:26:48,193 - INFO: Epoch: 97/200, Batch: 25/29, Batch_Loss_Train: 0.843
2024-06-21 19:26:48,489 - INFO: Epoch: 97/200, Batch: 26/29, Batch_Loss_Train: 0.749
2024-06-21 19:26:48,879 - INFO: Epoch: 97/200, Batch: 27/29, Batch_Loss_Train: 0.680
2024-06-21 19:26:49,188 - INFO: Epoch: 97/200, Batch: 28/29, Batch_Loss_Train: 0.882
2024-06-21 19:26:49,405 - INFO: Epoch: 97/200, Batch: 29/29, Batch_Loss_Train: 0.582
2024-06-21 19:27:00,445 - INFO: 97/200 final results:
2024-06-21 19:27:00,445 - INFO: Training loss: 0.678.
2024-06-21 19:27:00,445 - INFO: Training MAE: 0.680.
2024-06-21 19:27:00,445 - INFO: Training MSE: 1.121.
2024-06-21 19:27:20,854 - INFO: Epoch: 97/200, Loss_train: 0.6779770707261974, Loss_val: 1.8842551584901481
2024-06-21 19:27:20,854 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:27:20,854 - INFO: Epoch 98/200...
2024-06-21 19:27:20,854 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:27:20,854 - INFO: Batch size: 32.
2024-06-21 19:27:20,858 - INFO: Dataset:
2024-06-21 19:27:20,858 - INFO: Batch size:
2024-06-21 19:27:20,858 - INFO: Number of workers:
2024-06-21 19:27:22,027 - INFO: Epoch: 98/200, Batch: 1/29, Batch_Loss_Train: 0.579
2024-06-21 19:27:22,346 - INFO: Epoch: 98/200, Batch: 2/29, Batch_Loss_Train: 0.766
2024-06-21 19:27:22,737 - INFO: Epoch: 98/200, Batch: 3/29, Batch_Loss_Train: 0.789
2024-06-21 19:27:23,055 - INFO: Epoch: 98/200, Batch: 4/29, Batch_Loss_Train: 0.727
2024-06-21 19:27:23,464 - INFO: Epoch: 98/200, Batch: 5/29, Batch_Loss_Train: 0.646
2024-06-21 19:27:23,777 - INFO: Epoch: 98/200, Batch: 6/29, Batch_Loss_Train: 0.784
2024-06-21 19:27:24,163 - INFO: Epoch: 98/200, Batch: 7/29, Batch_Loss_Train: 0.735
2024-06-21 19:27:24,477 - INFO: Epoch: 98/200, Batch: 8/29, Batch_Loss_Train: 0.694
2024-06-21 19:27:24,882 - INFO: Epoch: 98/200, Batch: 9/29, Batch_Loss_Train: 0.632
2024-06-21 19:27:25,190 - INFO: Epoch: 98/200, Batch: 10/29, Batch_Loss_Train: 0.799
2024-06-21 19:27:25,564 - INFO: Epoch: 98/200, Batch: 11/29, Batch_Loss_Train: 0.618
2024-06-21 19:27:25,881 - INFO: Epoch: 98/200, Batch: 12/29, Batch_Loss_Train: 0.608
2024-06-21 19:27:26,301 - INFO: Epoch: 98/200, Batch: 13/29, Batch_Loss_Train: 0.591
2024-06-21 19:27:26,618 - INFO: Epoch: 98/200, Batch: 14/29, Batch_Loss_Train: 0.798
2024-06-21 19:27:27,005 - INFO: Epoch: 98/200, Batch: 15/29, Batch_Loss_Train: 0.620
2024-06-21 19:27:27,318 - INFO: Epoch: 98/200, Batch: 16/29, Batch_Loss_Train: 0.613
2024-06-21 19:27:27,735 - INFO: Epoch: 98/200, Batch: 17/29, Batch_Loss_Train: 0.637
2024-06-21 19:27:28,049 - INFO: Epoch: 98/200, Batch: 18/29, Batch_Loss_Train: 0.672
2024-06-21 19:27:28,421 - INFO: Epoch: 98/200, Batch: 19/29, Batch_Loss_Train: 0.569
2024-06-21 19:27:28,729 - INFO: Epoch: 98/200, Batch: 20/29, Batch_Loss_Train: 0.745
2024-06-21 19:27:29,128 - INFO: Epoch: 98/200, Batch: 21/29, Batch_Loss_Train: 0.766
2024-06-21 19:27:29,443 - INFO: Epoch: 98/200, Batch: 22/29, Batch_Loss_Train: 0.674
2024-06-21 19:27:29,818 - INFO: Epoch: 98/200, Batch: 23/29, Batch_Loss_Train: 0.524
2024-06-21 19:27:30,132 - INFO: Epoch: 98/200, Batch: 24/29, Batch_Loss_Train: 0.519
2024-06-21 19:27:30,534 - INFO: Epoch: 98/200, Batch: 25/29, Batch_Loss_Train: 0.664
2024-06-21 19:27:30,842 - INFO: Epoch: 98/200, Batch: 26/29, Batch_Loss_Train: 0.935
2024-06-21 19:27:31,208 - INFO: Epoch: 98/200, Batch: 27/29, Batch_Loss_Train: 0.713
2024-06-21 19:27:31,516 - INFO: Epoch: 98/200, Batch: 28/29, Batch_Loss_Train: 0.698
2024-06-21 19:27:31,724 - INFO: Epoch: 98/200, Batch: 29/29, Batch_Loss_Train: 1.081
2024-06-21 19:27:42,878 - INFO: 98/200 final results:
2024-06-21 19:27:42,879 - INFO: Training loss: 0.696.
2024-06-21 19:27:42,879 - INFO: Training MAE: 0.689.
2024-06-21 19:27:42,879 - INFO: Training MSE: 1.121.
2024-06-21 19:28:03,422 - INFO: Epoch: 98/200, Loss_train: 0.6963620473598612, Loss_val: 1.8365278572871768
2024-06-21 19:28:03,422 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:28:03,422 - INFO: Epoch 99/200...
2024-06-21 19:28:03,422 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:28:03,422 - INFO: Batch size: 32.
2024-06-21 19:28:03,426 - INFO: Dataset:
2024-06-21 19:28:03,427 - INFO: Batch size:
2024-06-21 19:28:03,427 - INFO: Number of workers:
2024-06-21 19:28:04,596 - INFO: Epoch: 99/200, Batch: 1/29, Batch_Loss_Train: 0.625
2024-06-21 19:28:04,913 - INFO: Epoch: 99/200, Batch: 2/29, Batch_Loss_Train: 0.550
2024-06-21 19:28:05,319 - INFO: Epoch: 99/200, Batch: 3/29, Batch_Loss_Train: 0.563
2024-06-21 19:28:05,635 - INFO: Epoch: 99/200, Batch: 4/29, Batch_Loss_Train: 0.481
2024-06-21 19:28:06,039 - INFO: Epoch: 99/200, Batch: 5/29, Batch_Loss_Train: 0.677
2024-06-21 19:28:06,337 - INFO: Epoch: 99/200, Batch: 6/29, Batch_Loss_Train: 0.726
2024-06-21 19:28:06,730 - INFO: Epoch: 99/200, Batch: 7/29, Batch_Loss_Train: 0.491
2024-06-21 19:28:07,043 - INFO: Epoch: 99/200, Batch: 8/29, Batch_Loss_Train: 0.653
2024-06-21 19:28:07,438 - INFO: Epoch: 99/200, Batch: 9/29, Batch_Loss_Train: 0.724
2024-06-21 19:28:07,729 - INFO: Epoch: 99/200, Batch: 10/29, Batch_Loss_Train: 0.694
2024-06-21 19:28:08,117 - INFO: Epoch: 99/200, Batch: 11/29, Batch_Loss_Train: 0.728
2024-06-21 19:28:08,431 - INFO: Epoch: 99/200, Batch: 12/29, Batch_Loss_Train: 0.593
2024-06-21 19:28:08,843 - INFO: Epoch: 99/200, Batch: 13/29, Batch_Loss_Train: 0.868
2024-06-21 19:28:09,144 - INFO: Epoch: 99/200, Batch: 14/29, Batch_Loss_Train: 0.703
2024-06-21 19:28:09,545 - INFO: Epoch: 99/200, Batch: 15/29, Batch_Loss_Train: 0.693
2024-06-21 19:28:09,855 - INFO: Epoch: 99/200, Batch: 16/29, Batch_Loss_Train: 0.642
2024-06-21 19:28:10,270 - INFO: Epoch: 99/200, Batch: 17/29, Batch_Loss_Train: 0.651
2024-06-21 19:28:10,567 - INFO: Epoch: 99/200, Batch: 18/29, Batch_Loss_Train: 0.600
2024-06-21 19:28:10,961 - INFO: Epoch: 99/200, Batch: 19/29, Batch_Loss_Train: 0.586
2024-06-21 19:28:11,266 - INFO: Epoch: 99/200, Batch: 20/29, Batch_Loss_Train: 0.557
2024-06-21 19:28:11,666 - INFO: Epoch: 99/200, Batch: 21/29, Batch_Loss_Train: 0.961
2024-06-21 19:28:11,967 - INFO: Epoch: 99/200, Batch: 22/29, Batch_Loss_Train: 0.700
2024-06-21 19:28:12,364 - INFO: Epoch: 99/200, Batch: 23/29, Batch_Loss_Train: 0.747
2024-06-21 19:28:12,679 - INFO: Epoch: 99/200, Batch: 24/29, Batch_Loss_Train: 0.682
2024-06-21 19:28:13,079 - INFO: Epoch: 99/200, Batch: 25/29, Batch_Loss_Train: 0.742
2024-06-21 19:28:13,375 - INFO: Epoch: 99/200, Batch: 26/29, Batch_Loss_Train: 0.606
2024-06-21 19:28:13,768 - INFO: Epoch: 99/200, Batch: 27/29, Batch_Loss_Train: 0.729
2024-06-21 19:28:14,076 - INFO: Epoch: 99/200, Batch: 28/29, Batch_Loss_Train: 0.772
2024-06-21 19:28:14,294 - INFO: Epoch: 99/200, Batch: 29/29, Batch_Loss_Train: 1.031
2024-06-21 19:28:25,353 - INFO: 99/200 final results:
2024-06-21 19:28:25,354 - INFO: Training loss: 0.682.
2024-06-21 19:28:25,354 - INFO: Training MAE: 0.675.
2024-06-21 19:28:25,354 - INFO: Training MSE: 1.097.
2024-06-21 19:28:45,866 - INFO: Epoch: 99/200, Loss_train: 0.6818633603638616, Loss_val: 1.87242447096726
2024-06-21 19:28:45,866 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:28:45,866 - INFO: Epoch 100/200...
2024-06-21 19:28:45,866 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:28:45,866 - INFO: Batch size: 32.
2024-06-21 19:28:45,870 - INFO: Dataset:
2024-06-21 19:28:45,871 - INFO: Batch size:
2024-06-21 19:28:45,871 - INFO: Number of workers:
2024-06-21 19:28:47,050 - INFO: Epoch: 100/200, Batch: 1/29, Batch_Loss_Train: 0.727
2024-06-21 19:28:47,356 - INFO: Epoch: 100/200, Batch: 2/29, Batch_Loss_Train: 0.665
2024-06-21 19:28:47,760 - INFO: Epoch: 100/200, Batch: 3/29, Batch_Loss_Train: 0.619
2024-06-21 19:28:48,077 - INFO: Epoch: 100/200, Batch: 4/29, Batch_Loss_Train: 0.681
2024-06-21 19:28:48,490 - INFO: Epoch: 100/200, Batch: 5/29, Batch_Loss_Train: 0.583
2024-06-21 19:28:48,790 - INFO: Epoch: 100/200, Batch: 6/29, Batch_Loss_Train: 0.549
2024-06-21 19:28:49,180 - INFO: Epoch: 100/200, Batch: 7/29, Batch_Loss_Train: 0.963
2024-06-21 19:28:49,493 - INFO: Epoch: 100/200, Batch: 8/29, Batch_Loss_Train: 0.638
2024-06-21 19:28:49,901 - INFO: Epoch: 100/200, Batch: 9/29, Batch_Loss_Train: 0.669
2024-06-21 19:28:50,193 - INFO: Epoch: 100/200, Batch: 10/29, Batch_Loss_Train: 0.693
2024-06-21 19:28:50,569 - INFO: Epoch: 100/200, Batch: 11/29, Batch_Loss_Train: 0.640
2024-06-21 19:28:50,884 - INFO: Epoch: 100/200, Batch: 12/29, Batch_Loss_Train: 0.686
2024-06-21 19:28:51,304 - INFO: Epoch: 100/200, Batch: 13/29, Batch_Loss_Train: 0.771
2024-06-21 19:28:51,606 - INFO: Epoch: 100/200, Batch: 14/29, Batch_Loss_Train: 0.651
2024-06-21 19:28:52,007 - INFO: Epoch: 100/200, Batch: 15/29, Batch_Loss_Train: 0.649
2024-06-21 19:28:52,317 - INFO: Epoch: 100/200, Batch: 16/29, Batch_Loss_Train: 0.572
2024-06-21 19:28:52,734 - INFO: Epoch: 100/200, Batch: 17/29, Batch_Loss_Train: 0.783
2024-06-21 19:28:53,032 - INFO: Epoch: 100/200, Batch: 18/29, Batch_Loss_Train: 0.630
2024-06-21 19:28:53,418 - INFO: Epoch: 100/200, Batch: 19/29, Batch_Loss_Train: 0.694
2024-06-21 19:28:53,724 - INFO: Epoch: 100/200, Batch: 20/29, Batch_Loss_Train: 0.931
2024-06-21 19:28:54,130 - INFO: Epoch: 100/200, Batch: 21/29, Batch_Loss_Train: 0.757
2024-06-21 19:28:54,430 - INFO: Epoch: 100/200, Batch: 22/29, Batch_Loss_Train: 0.804
2024-06-21 19:28:54,819 - INFO: Epoch: 100/200, Batch: 23/29, Batch_Loss_Train: 0.623
2024-06-21 19:28:55,130 - INFO: Epoch: 100/200, Batch: 24/29, Batch_Loss_Train: 0.635
2024-06-21 19:28:55,529 - INFO: Epoch: 100/200, Batch: 25/29, Batch_Loss_Train: 0.595
2024-06-21 19:28:55,825 - INFO: Epoch: 100/200, Batch: 26/29, Batch_Loss_Train: 0.700
2024-06-21 19:28:56,216 - INFO: Epoch: 100/200, Batch: 27/29, Batch_Loss_Train: 0.679
2024-06-21 19:28:56,524 - INFO: Epoch: 100/200, Batch: 28/29, Batch_Loss_Train: 0.575
2024-06-21 19:28:56,736 - INFO: Epoch: 100/200, Batch: 29/29, Batch_Loss_Train: 0.784
2024-06-21 19:29:07,834 - INFO: 100/200 final results:
2024-06-21 19:29:07,834 - INFO: Training loss: 0.688.
2024-06-21 19:29:07,834 - INFO: Training MAE: 0.686.
2024-06-21 19:29:07,834 - INFO: Training MSE: 1.124.
2024-06-21 19:29:28,437 - INFO: Epoch: 100/200, Loss_train: 0.6878259284742947, Loss_val: 1.8700631569171775
2024-06-21 19:29:28,437 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:29:28,437 - INFO: Epoch 101/200...
2024-06-21 19:29:28,437 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:29:28,437 - INFO: Batch size: 32.
2024-06-21 19:29:28,441 - INFO: Dataset:
2024-06-21 19:29:28,442 - INFO: Batch size:
2024-06-21 19:29:28,442 - INFO: Number of workers:
2024-06-21 19:29:29,600 - INFO: Epoch: 101/200, Batch: 1/29, Batch_Loss_Train: 0.783
2024-06-21 19:29:29,917 - INFO: Epoch: 101/200, Batch: 2/29, Batch_Loss_Train: 0.779
2024-06-21 19:29:30,314 - INFO: Epoch: 101/200, Batch: 3/29, Batch_Loss_Train: 0.680
2024-06-21 19:29:30,629 - INFO: Epoch: 101/200, Batch: 4/29, Batch_Loss_Train: 0.545
2024-06-21 19:29:31,039 - INFO: Epoch: 101/200, Batch: 5/29, Batch_Loss_Train: 0.672
2024-06-21 19:29:31,339 - INFO: Epoch: 101/200, Batch: 6/29, Batch_Loss_Train: 0.626
2024-06-21 19:29:31,738 - INFO: Epoch: 101/200, Batch: 7/29, Batch_Loss_Train: 0.540
2024-06-21 19:29:32,055 - INFO: Epoch: 101/200, Batch: 8/29, Batch_Loss_Train: 0.689
2024-06-21 19:29:32,449 - INFO: Epoch: 101/200, Batch: 9/29, Batch_Loss_Train: 0.689
2024-06-21 19:29:32,743 - INFO: Epoch: 101/200, Batch: 10/29, Batch_Loss_Train: 0.691
2024-06-21 19:29:33,132 - INFO: Epoch: 101/200, Batch: 11/29, Batch_Loss_Train: 0.747
2024-06-21 19:29:33,449 - INFO: Epoch: 101/200, Batch: 12/29, Batch_Loss_Train: 0.565
2024-06-21 19:29:33,854 - INFO: Epoch: 101/200, Batch: 13/29, Batch_Loss_Train: 0.626
2024-06-21 19:29:34,159 - INFO: Epoch: 101/200, Batch: 14/29, Batch_Loss_Train: 0.672
2024-06-21 19:29:34,566 - INFO: Epoch: 101/200, Batch: 15/29, Batch_Loss_Train: 0.862
2024-06-21 19:29:34,880 - INFO: Epoch: 101/200, Batch: 16/29, Batch_Loss_Train: 0.581
2024-06-21 19:29:35,285 - INFO: Epoch: 101/200, Batch: 17/29, Batch_Loss_Train: 0.675
2024-06-21 19:29:35,586 - INFO: Epoch: 101/200, Batch: 18/29, Batch_Loss_Train: 0.789
2024-06-21 19:29:35,983 - INFO: Epoch: 101/200, Batch: 19/29, Batch_Loss_Train: 0.838
2024-06-21 19:29:36,290 - INFO: Epoch: 101/200, Batch: 20/29, Batch_Loss_Train: 0.743
2024-06-21 19:29:36,701 - INFO: Epoch: 101/200, Batch: 21/29, Batch_Loss_Train: 0.566
2024-06-21 19:29:37,011 - INFO: Epoch: 101/200, Batch: 22/29, Batch_Loss_Train: 0.677
2024-06-21 19:29:37,411 - INFO: Epoch: 101/200, Batch: 23/29, Batch_Loss_Train: 0.649
2024-06-21 19:29:37,724 - INFO: Epoch: 101/200, Batch: 24/29, Batch_Loss_Train: 0.608
2024-06-21 19:29:38,122 - INFO: Epoch: 101/200, Batch: 25/29, Batch_Loss_Train: 0.742
2024-06-21 19:29:38,417 - INFO: Epoch: 101/200, Batch: 26/29, Batch_Loss_Train: 0.617
2024-06-21 19:29:38,804 - INFO: Epoch: 101/200, Batch: 27/29, Batch_Loss_Train: 0.675
2024-06-21 19:29:39,111 - INFO: Epoch: 101/200, Batch: 28/29, Batch_Loss_Train: 0.685
2024-06-21 19:29:39,328 - INFO: Epoch: 101/200, Batch: 29/29, Batch_Loss_Train: 0.542
2024-06-21 19:29:50,460 - INFO: 101/200 final results:
2024-06-21 19:29:50,460 - INFO: Training loss: 0.674.
2024-06-21 19:29:50,460 - INFO: Training MAE: 0.677.
2024-06-21 19:29:50,460 - INFO: Training MSE: 1.066.
2024-06-21 19:30:10,659 - INFO: Epoch: 101/200, Loss_train: 0.6742337263863662, Loss_val: 1.8870575469115685
2024-06-21 19:30:10,659 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:30:10,659 - INFO: Epoch 102/200...
2024-06-21 19:30:10,659 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:30:10,659 - INFO: Batch size: 32.
2024-06-21 19:30:10,663 - INFO: Dataset:
2024-06-21 19:30:10,663 - INFO: Batch size:
2024-06-21 19:30:10,663 - INFO: Number of workers:
2024-06-21 19:30:11,829 - INFO: Epoch: 102/200, Batch: 1/29, Batch_Loss_Train: 0.611
2024-06-21 19:30:12,133 - INFO: Epoch: 102/200, Batch: 2/29, Batch_Loss_Train: 0.642
2024-06-21 19:30:12,530 - INFO: Epoch: 102/200, Batch: 3/29, Batch_Loss_Train: 0.620
2024-06-21 19:30:12,845 - INFO: Epoch: 102/200, Batch: 4/29, Batch_Loss_Train: 0.634
2024-06-21 19:30:13,243 - INFO: Epoch: 102/200, Batch: 5/29, Batch_Loss_Train: 0.639
2024-06-21 19:30:13,540 - INFO: Epoch: 102/200, Batch: 6/29, Batch_Loss_Train: 0.682
2024-06-21 19:30:13,933 - INFO: Epoch: 102/200, Batch: 7/29, Batch_Loss_Train: 0.710
2024-06-21 19:30:14,244 - INFO: Epoch: 102/200, Batch: 8/29, Batch_Loss_Train: 0.603
2024-06-21 19:30:14,640 - INFO: Epoch: 102/200, Batch: 9/29, Batch_Loss_Train: 0.514
2024-06-21 19:30:14,930 - INFO: Epoch: 102/200, Batch: 10/29, Batch_Loss_Train: 0.648
2024-06-21 19:30:15,310 - INFO: Epoch: 102/200, Batch: 11/29, Batch_Loss_Train: 0.650
2024-06-21 19:30:15,622 - INFO: Epoch: 102/200, Batch: 12/29, Batch_Loss_Train: 0.900
2024-06-21 19:30:16,036 - INFO: Epoch: 102/200, Batch: 13/29, Batch_Loss_Train: 0.621
2024-06-21 19:30:16,336 - INFO: Epoch: 102/200, Batch: 14/29, Batch_Loss_Train: 0.711
2024-06-21 19:30:16,732 - INFO: Epoch: 102/200, Batch: 15/29, Batch_Loss_Train: 0.528
2024-06-21 19:30:17,041 - INFO: Epoch: 102/200, Batch: 16/29, Batch_Loss_Train: 0.560
2024-06-21 19:30:17,454 - INFO: Epoch: 102/200, Batch: 17/29, Batch_Loss_Train: 0.716
2024-06-21 19:30:17,751 - INFO: Epoch: 102/200, Batch: 18/29, Batch_Loss_Train: 0.763
2024-06-21 19:30:18,143 - INFO: Epoch: 102/200, Batch: 19/29, Batch_Loss_Train: 0.577
2024-06-21 19:30:18,447 - INFO: Epoch: 102/200, Batch: 20/29, Batch_Loss_Train: 0.622
2024-06-21 19:30:18,848 - INFO: Epoch: 102/200, Batch: 21/29, Batch_Loss_Train: 0.565
2024-06-21 19:30:19,148 - INFO: Epoch: 102/200, Batch: 22/29, Batch_Loss_Train: 0.595
2024-06-21 19:30:19,543 - INFO: Epoch: 102/200, Batch: 23/29, Batch_Loss_Train: 0.632
2024-06-21 19:30:19,854 - INFO: Epoch: 102/200, Batch: 24/29, Batch_Loss_Train: 0.699
2024-06-21 19:30:20,258 - INFO: Epoch: 102/200, Batch: 25/29, Batch_Loss_Train: 0.648
2024-06-21 19:30:20,553 - INFO: Epoch: 102/200, Batch: 26/29, Batch_Loss_Train: 0.690
2024-06-21 19:30:20,947 - INFO: Epoch: 102/200, Batch: 27/29, Batch_Loss_Train: 0.552
2024-06-21 19:30:21,254 - INFO: Epoch: 102/200, Batch: 28/29, Batch_Loss_Train: 0.774
2024-06-21 19:30:21,473 - INFO: Epoch: 102/200, Batch: 29/29, Batch_Loss_Train: 0.590
2024-06-21 19:30:32,642 - INFO: 102/200 final results:
2024-06-21 19:30:32,642 - INFO: Training loss: 0.645.
2024-06-21 19:30:32,642 - INFO: Training MAE: 0.646.
2024-06-21 19:30:32,642 - INFO: Training MSE: 1.017.
2024-06-21 19:30:52,885 - INFO: Epoch: 102/200, Loss_train: 0.6446715716657967, Loss_val: 1.8669275168714852
2024-06-21 19:30:52,885 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:30:52,886 - INFO: Epoch 103/200...
2024-06-21 19:30:52,886 - INFO: Learning rate: 2.7283291676008165e-05.
2024-06-21 19:30:52,886 - INFO: Batch size: 32.
2024-06-21 19:30:52,890 - INFO: Dataset:
2024-06-21 19:30:52,890 - INFO: Batch size:
2024-06-21 19:30:52,890 - INFO: Number of workers:
2024-06-21 19:30:54,051 - INFO: Epoch: 103/200, Batch: 1/29, Batch_Loss_Train: 0.627
2024-06-21 19:30:54,371 - INFO: Epoch: 103/200, Batch: 2/29, Batch_Loss_Train: 0.627
2024-06-21 19:30:54,781 - INFO: Epoch: 103/200, Batch: 3/29, Batch_Loss_Train: 0.601
2024-06-21 19:30:55,101 - INFO: Epoch: 103/200, Batch: 4/29, Batch_Loss_Train: 0.647
2024-06-21 19:30:55,507 - INFO: Epoch: 103/200, Batch: 5/29, Batch_Loss_Train: 0.891
2024-06-21 19:30:55,822 - INFO: Epoch: 103/200, Batch: 6/29, Batch_Loss_Train: 0.473
2024-06-21 19:30:56,224 - INFO: Epoch: 103/200, Batch: 7/29, Batch_Loss_Train: 0.579
2024-06-21 19:30:56,540 - INFO: Epoch: 103/200, Batch: 8/29, Batch_Loss_Train: 0.620
2024-06-21 19:30:56,922 - INFO: Epoch: 103/200, Batch: 9/29, Batch_Loss_Train: 0.621
2024-06-21 19:30:57,230 - INFO: Epoch: 103/200, Batch: 10/29, Batch_Loss_Train: 0.627
2024-06-21 19:30:57,616 - INFO: Epoch: 103/200, Batch: 11/29, Batch_Loss_Train: 0.751
2024-06-21 19:30:57,934 - INFO: Epoch: 103/200, Batch: 12/29, Batch_Loss_Train: 0.531
2024-06-21 19:30:58,328 - INFO: Epoch: 103/200, Batch: 13/29, Batch_Loss_Train: 0.612
2024-06-21 19:30:58,644 - INFO: Epoch: 103/200, Batch: 14/29, Batch_Loss_Train: 0.692
2024-06-21 19:30:59,046 - INFO: Epoch: 103/200, Batch: 15/29, Batch_Loss_Train: 0.595
2024-06-21 19:30:59,360 - INFO: Epoch: 103/200, Batch: 16/29, Batch_Loss_Train: 0.672
2024-06-21 19:30:59,749 - INFO: Epoch: 103/200, Batch: 17/29, Batch_Loss_Train: 0.777
2024-06-21 19:31:00,063 - INFO: Epoch: 103/200, Batch: 18/29, Batch_Loss_Train: 0.660
2024-06-21 19:31:00,458 - INFO: Epoch: 103/200, Batch: 19/29, Batch_Loss_Train: 0.709
2024-06-21 19:31:00,766 - INFO: Epoch: 103/200, Batch: 20/29, Batch_Loss_Train: 0.869
2024-06-21 19:31:01,154 - INFO: Epoch: 103/200, Batch: 21/29, Batch_Loss_Train: 0.737
2024-06-21 19:31:01,470 - INFO: Epoch: 103/200, Batch: 22/29, Batch_Loss_Train: 0.621
2024-06-21 19:31:01,857 - INFO: Epoch: 103/200, Batch: 23/29, Batch_Loss_Train: 0.739
2024-06-21 19:31:02,172 - INFO: Epoch: 103/200, Batch: 24/29, Batch_Loss_Train: 0.584
2024-06-21 19:31:02,563 - INFO: Epoch: 103/200, Batch: 25/29, Batch_Loss_Train: 0.686
2024-06-21 19:31:02,874 - INFO: Epoch: 103/200, Batch: 26/29, Batch_Loss_Train: 0.709
2024-06-21 19:31:03,262 - INFO: Epoch: 103/200, Batch: 27/29, Batch_Loss_Train: 0.531
2024-06-21 19:31:03,574 - INFO: Epoch: 103/200, Batch: 28/29, Batch_Loss_Train: 0.643
2024-06-21 19:31:03,789 - INFO: Epoch: 103/200, Batch: 29/29, Batch_Loss_Train: 0.529
2024-06-21 19:31:14,967 - INFO: 103/200 final results:
2024-06-21 19:31:14,967 - INFO: Training loss: 0.654.
2024-06-21 19:31:14,967 - INFO: Training MAE: 0.656.
2024-06-21 19:31:14,967 - INFO: Training MSE: 1.030.
2024-06-21 19:31:35,328 - INFO: Epoch: 103/200, Loss_train: 0.6537975083137381, Loss_val: 1.8579916378547405
2024-06-21 19:31:35,328 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:31:35,328 - INFO: Epoch 104/200...
2024-06-21 19:31:35,328 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:31:35,328 - INFO: Batch size: 32.
2024-06-21 19:31:35,332 - INFO: Dataset:
2024-06-21 19:31:35,332 - INFO: Batch size:
2024-06-21 19:31:35,332 - INFO: Number of workers:
2024-06-21 19:31:36,509 - INFO: Epoch: 104/200, Batch: 1/29, Batch_Loss_Train: 0.684
2024-06-21 19:31:36,827 - INFO: Epoch: 104/200, Batch: 2/29, Batch_Loss_Train: 0.588
2024-06-21 19:31:37,209 - INFO: Epoch: 104/200, Batch: 3/29, Batch_Loss_Train: 0.684
2024-06-21 19:31:37,523 - INFO: Epoch: 104/200, Batch: 4/29, Batch_Loss_Train: 0.573
2024-06-21 19:31:37,937 - INFO: Epoch: 104/200, Batch: 5/29, Batch_Loss_Train: 0.515
2024-06-21 19:31:38,249 - INFO: Epoch: 104/200, Batch: 6/29, Batch_Loss_Train: 0.537
2024-06-21 19:31:38,624 - INFO: Epoch: 104/200, Batch: 7/29, Batch_Loss_Train: 0.497
2024-06-21 19:31:38,936 - INFO: Epoch: 104/200, Batch: 8/29, Batch_Loss_Train: 0.867
2024-06-21 19:31:39,339 - INFO: Epoch: 104/200, Batch: 9/29, Batch_Loss_Train: 0.704
2024-06-21 19:31:39,644 - INFO: Epoch: 104/200, Batch: 10/29, Batch_Loss_Train: 0.561
2024-06-21 19:31:40,012 - INFO: Epoch: 104/200, Batch: 11/29, Batch_Loss_Train: 0.593
2024-06-21 19:31:40,326 - INFO: Epoch: 104/200, Batch: 12/29, Batch_Loss_Train: 0.691
2024-06-21 19:31:40,733 - INFO: Epoch: 104/200, Batch: 13/29, Batch_Loss_Train: 0.540
2024-06-21 19:31:41,048 - INFO: Epoch: 104/200, Batch: 14/29, Batch_Loss_Train: 0.536
2024-06-21 19:31:41,433 - INFO: Epoch: 104/200, Batch: 15/29, Batch_Loss_Train: 0.492
2024-06-21 19:31:41,743 - INFO: Epoch: 104/200, Batch: 16/29, Batch_Loss_Train: 0.547
2024-06-21 19:31:42,150 - INFO: Epoch: 104/200, Batch: 17/29, Batch_Loss_Train: 0.560
2024-06-21 19:31:42,461 - INFO: Epoch: 104/200, Batch: 18/29, Batch_Loss_Train: 0.517
2024-06-21 19:31:42,837 - INFO: Epoch: 104/200, Batch: 19/29, Batch_Loss_Train: 0.621
2024-06-21 19:31:43,143 - INFO: Epoch: 104/200, Batch: 20/29, Batch_Loss_Train: 0.614
2024-06-21 19:31:43,535 - INFO: Epoch: 104/200, Batch: 21/29, Batch_Loss_Train: 0.566
2024-06-21 19:31:43,848 - INFO: Epoch: 104/200, Batch: 22/29, Batch_Loss_Train: 0.646
2024-06-21 19:31:44,228 - INFO: Epoch: 104/200, Batch: 23/29, Batch_Loss_Train: 0.648
2024-06-21 19:31:44,540 - INFO: Epoch: 104/200, Batch: 24/29, Batch_Loss_Train: 0.757
2024-06-21 19:31:44,943 - INFO: Epoch: 104/200, Batch: 25/29, Batch_Loss_Train: 0.519
2024-06-21 19:31:45,251 - INFO: Epoch: 104/200, Batch: 26/29, Batch_Loss_Train: 0.583
2024-06-21 19:31:45,626 - INFO: Epoch: 104/200, Batch: 27/29, Batch_Loss_Train: 0.730
2024-06-21 19:31:45,934 - INFO: Epoch: 104/200, Batch: 28/29, Batch_Loss_Train: 0.793
2024-06-21 19:31:46,142 - INFO: Epoch: 104/200, Batch: 29/29, Batch_Loss_Train: 0.759
2024-06-21 19:31:57,308 - INFO: 104/200 final results:
2024-06-21 19:31:57,308 - INFO: Training loss: 0.618.
2024-06-21 19:31:57,308 - INFO: Training MAE: 0.615.
2024-06-21 19:31:57,308 - INFO: Training MSE: 0.975.
2024-06-21 19:32:17,981 - INFO: Epoch: 104/200, Loss_train: 0.6179976648297804, Loss_val: 1.8465876250431454
2024-06-21 19:32:17,981 - INFO: Best internal validation val_loss: 1.835 at epoch: 96.
2024-06-21 19:32:17,981 - INFO: Epoch 105/200...
2024-06-21 19:32:17,981 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:32:17,981 - INFO: Batch size: 32.
2024-06-21 19:32:17,986 - INFO: Dataset:
2024-06-21 19:32:17,986 - INFO: Batch size:
2024-06-21 19:32:17,986 - INFO: Number of workers:
2024-06-21 19:32:19,192 - INFO: Epoch: 105/200, Batch: 1/29, Batch_Loss_Train: 0.544
2024-06-21 19:32:19,524 - INFO: Epoch: 105/200, Batch: 2/29, Batch_Loss_Train: 0.641
2024-06-21 19:32:19,909 - INFO: Epoch: 105/200, Batch: 3/29, Batch_Loss_Train: 0.617
2024-06-21 19:32:20,226 - INFO: Epoch: 105/200, Batch: 4/29, Batch_Loss_Train: 0.478
2024-06-21 19:32:20,619 - INFO: Epoch: 105/200, Batch: 5/29, Batch_Loss_Train: 0.552
2024-06-21 19:32:20,943 - INFO: Epoch: 105/200, Batch: 6/29, Batch_Loss_Train: 0.618
2024-06-21 19:32:21,328 - INFO: Epoch: 105/200, Batch: 7/29, Batch_Loss_Train: 0.530
2024-06-21 19:32:21,641 - INFO: Epoch: 105/200, Batch: 8/29, Batch_Loss_Train: 0.738
2024-06-21 19:32:22,050 - INFO: Epoch: 105/200, Batch: 9/29, Batch_Loss_Train: 0.647
2024-06-21 19:32:22,340 - INFO: Epoch: 105/200, Batch: 10/29, Batch_Loss_Train: 0.531
2024-06-21 19:32:22,705 - INFO: Epoch: 105/200, Batch: 11/29, Batch_Loss_Train: 0.658
2024-06-21 19:32:23,017 - INFO: Epoch: 105/200, Batch: 12/29, Batch_Loss_Train: 0.689
2024-06-21 19:32:23,453 - INFO: Epoch: 105/200, Batch: 13/29, Batch_Loss_Train: 0.615
2024-06-21 19:32:23,758 - INFO: Epoch: 105/200, Batch: 14/29, Batch_Loss_Train: 0.548
2024-06-21 19:32:24,156 - INFO: Epoch: 105/200, Batch: 15/29, Batch_Loss_Train: 0.667
2024-06-21 19:32:24,469 - INFO: Epoch: 105/200, Batch: 16/29, Batch_Loss_Train: 0.640
2024-06-21 19:32:24,899 - INFO: Epoch: 105/200, Batch: 17/29, Batch_Loss_Train: 0.654
2024-06-21 19:32:25,200 - INFO: Epoch: 105/200, Batch: 18/29, Batch_Loss_Train: 0.568
2024-06-21 19:32:25,589 - INFO: Epoch: 105/200, Batch: 19/29, Batch_Loss_Train: 0.543
2024-06-21 19:32:25,896 - INFO: Epoch: 105/200, Batch: 20/29, Batch_Loss_Train: 0.571
2024-06-21 19:32:26,319 - INFO: Epoch: 105/200, Batch: 21/29, Batch_Loss_Train: 0.730
2024-06-21 19:32:26,621 - INFO: Epoch: 105/200, Batch: 22/29, Batch_Loss_Train: 0.662
2024-06-21 19:32:26,995 - INFO: Epoch: 105/200, Batch: 23/29, Batch_Loss_Train: 0.701
2024-06-21 19:32:27,310 - INFO: Epoch: 105/200, Batch: 24/29, Batch_Loss_Train: 0.723
2024-06-21 19:32:27,728 - INFO: Epoch: 105/200, Batch: 25/29, Batch_Loss_Train: 0.531
2024-06-21 19:32:28,026 - INFO: Epoch: 105/200, Batch: 26/29, Batch_Loss_Train: 0.559
2024-06-21 19:32:28,404 - INFO: Epoch: 105/200, Batch: 27/29, Batch_Loss_Train: 0.597
2024-06-21 19:32:28,716 - INFO: Epoch: 105/200, Batch: 28/29, Batch_Loss_Train: 0.636
2024-06-21 19:32:28,935 - INFO: Epoch: 105/200, Batch: 29/29, Batch_Loss_Train: 0.528
2024-06-21 19:32:39,858 - INFO: 105/200 final results:
2024-06-21 19:32:39,859 - INFO: Training loss: 0.611.
2024-06-21 19:32:39,859 - INFO: Training MAE: 0.612.
2024-06-21 19:32:39,859 - INFO: Training MSE: 0.931.
2024-06-21 19:33:00,497 - INFO: Epoch: 105/200, Loss_train: 0.6108196501074166, Loss_val: 1.8199725479915225
2024-06-21 19:33:00,516 - INFO: Saved new best metric model for epoch 105.
2024-06-21 19:33:00,517 - INFO: Best internal validation val_loss: 1.820 at epoch: 105.
2024-06-21 19:33:00,517 - INFO: Epoch 106/200...
2024-06-21 19:33:00,517 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:33:00,517 - INFO: Batch size: 32.
2024-06-21 19:33:00,521 - INFO: Dataset:
2024-06-21 19:33:00,521 - INFO: Batch size:
2024-06-21 19:33:00,521 - INFO: Number of workers:
2024-06-21 19:33:01,690 - INFO: Epoch: 106/200, Batch: 1/29, Batch_Loss_Train: 0.683
2024-06-21 19:33:02,009 - INFO: Epoch: 106/200, Batch: 2/29, Batch_Loss_Train: 0.609
2024-06-21 19:33:02,399 - INFO: Epoch: 106/200, Batch: 3/29, Batch_Loss_Train: 0.555
2024-06-21 19:33:02,716 - INFO: Epoch: 106/200, Batch: 4/29, Batch_Loss_Train: 0.713
2024-06-21 19:33:03,117 - INFO: Epoch: 106/200, Batch: 5/29, Batch_Loss_Train: 0.711
2024-06-21 19:33:03,429 - INFO: Epoch: 106/200, Batch: 6/29, Batch_Loss_Train: 0.582
2024-06-21 19:33:03,801 - INFO: Epoch: 106/200, Batch: 7/29, Batch_Loss_Train: 0.783
2024-06-21 19:33:04,113 - INFO: Epoch: 106/200, Batch: 8/29, Batch_Loss_Train: 0.727
2024-06-21 19:33:04,513 - INFO: Epoch: 106/200, Batch: 9/29, Batch_Loss_Train: 0.628
2024-06-21 19:33:04,818 - INFO: Epoch: 106/200, Batch: 10/29, Batch_Loss_Train: 0.567
2024-06-21 19:33:05,178 - INFO: Epoch: 106/200, Batch: 11/29, Batch_Loss_Train: 0.608
2024-06-21 19:33:05,491 - INFO: Epoch: 106/200, Batch: 12/29, Batch_Loss_Train: 0.643
2024-06-21 19:33:05,894 - INFO: Epoch: 106/200, Batch: 13/29, Batch_Loss_Train: 0.616
2024-06-21 19:33:06,208 - INFO: Epoch: 106/200, Batch: 14/29, Batch_Loss_Train: 0.696
2024-06-21 19:33:06,606 - INFO: Epoch: 106/200, Batch: 15/29, Batch_Loss_Train: 0.605
2024-06-21 19:33:06,919 - INFO: Epoch: 106/200, Batch: 16/29, Batch_Loss_Train: 0.517
2024-06-21 19:33:07,339 - INFO: Epoch: 106/200, Batch: 17/29, Batch_Loss_Train: 0.649
2024-06-21 19:33:07,653 - INFO: Epoch: 106/200, Batch: 18/29, Batch_Loss_Train: 0.484
2024-06-21 19:33:08,041 - INFO: Epoch: 106/200, Batch: 19/29, Batch_Loss_Train: 0.631
2024-06-21 19:33:08,349 - INFO: Epoch: 106/200, Batch: 20/29, Batch_Loss_Train: 0.560
2024-06-21 19:33:08,756 - INFO: Epoch: 106/200, Batch: 21/29, Batch_Loss_Train: 0.574
2024-06-21 19:33:09,071 - INFO: Epoch: 106/200, Batch: 22/29, Batch_Loss_Train: 0.603
2024-06-21 19:33:09,453 - INFO: Epoch: 106/200, Batch: 23/29, Batch_Loss_Train: 0.637
2024-06-21 19:33:09,769 - INFO: Epoch: 106/200, Batch: 24/29, Batch_Loss_Train: 0.552
2024-06-21 19:33:10,175 - INFO: Epoch: 106/200, Batch: 25/29, Batch_Loss_Train: 0.612
2024-06-21 19:33:10,483 - INFO: Epoch: 106/200, Batch: 26/29, Batch_Loss_Train: 0.643
2024-06-21 19:33:10,867 - INFO: Epoch: 106/200, Batch: 27/29, Batch_Loss_Train: 0.632
2024-06-21 19:33:11,176 - INFO: Epoch: 106/200, Batch: 28/29, Batch_Loss_Train: 0.601
2024-06-21 19:33:11,396 - INFO: Epoch: 106/200, Batch: 29/29, Batch_Loss_Train: 0.648
2024-06-21 19:33:22,636 - INFO: 106/200 final results:
2024-06-21 19:33:22,637 - INFO: Training loss: 0.623.
2024-06-21 19:33:22,637 - INFO: Training MAE: 0.623.
2024-06-21 19:33:22,637 - INFO: Training MSE: 0.953.
2024-06-21 19:33:42,907 - INFO: Epoch: 106/200, Loss_train: 0.6230061650276184, Loss_val: 1.8087931008174503
2024-06-21 19:33:42,926 - INFO: Saved new best metric model for epoch 106.
2024-06-21 19:33:42,926 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:33:42,926 - INFO: Epoch 107/200...
2024-06-21 19:33:42,927 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:33:42,927 - INFO: Batch size: 32.
2024-06-21 19:33:42,931 - INFO: Dataset:
2024-06-21 19:33:42,931 - INFO: Batch size:
2024-06-21 19:33:42,931 - INFO: Number of workers:
2024-06-21 19:33:44,105 - INFO: Epoch: 107/200, Batch: 1/29, Batch_Loss_Train: 0.632
2024-06-21 19:33:44,411 - INFO: Epoch: 107/200, Batch: 2/29, Batch_Loss_Train: 0.554
2024-06-21 19:33:44,823 - INFO: Epoch: 107/200, Batch: 3/29, Batch_Loss_Train: 0.677
2024-06-21 19:33:45,141 - INFO: Epoch: 107/200, Batch: 4/29, Batch_Loss_Train: 0.702
2024-06-21 19:33:45,556 - INFO: Epoch: 107/200, Batch: 5/29, Batch_Loss_Train: 0.562
2024-06-21 19:33:45,856 - INFO: Epoch: 107/200, Batch: 6/29, Batch_Loss_Train: 0.498
2024-06-21 19:33:46,258 - INFO: Epoch: 107/200, Batch: 7/29, Batch_Loss_Train: 0.610
2024-06-21 19:33:46,573 - INFO: Epoch: 107/200, Batch: 8/29, Batch_Loss_Train: 0.591
2024-06-21 19:33:46,981 - INFO: Epoch: 107/200, Batch: 9/29, Batch_Loss_Train: 0.557
2024-06-21 19:33:47,274 - INFO: Epoch: 107/200, Batch: 10/29, Batch_Loss_Train: 0.493
2024-06-21 19:33:47,665 - INFO: Epoch: 107/200, Batch: 11/29, Batch_Loss_Train: 0.570
2024-06-21 19:33:47,980 - INFO: Epoch: 107/200, Batch: 12/29, Batch_Loss_Train: 0.563
2024-06-21 19:33:48,400 - INFO: Epoch: 107/200, Batch: 13/29, Batch_Loss_Train: 0.520
2024-06-21 19:33:48,704 - INFO: Epoch: 107/200, Batch: 14/29, Batch_Loss_Train: 0.573
2024-06-21 19:33:49,114 - INFO: Epoch: 107/200, Batch: 15/29, Batch_Loss_Train: 0.587
2024-06-21 19:33:49,426 - INFO: Epoch: 107/200, Batch: 16/29, Batch_Loss_Train: 0.864
2024-06-21 19:33:49,841 - INFO: Epoch: 107/200, Batch: 17/29, Batch_Loss_Train: 0.842
2024-06-21 19:33:50,140 - INFO: Epoch: 107/200, Batch: 18/29, Batch_Loss_Train: 0.565
2024-06-21 19:33:50,541 - INFO: Epoch: 107/200, Batch: 19/29, Batch_Loss_Train: 0.447
2024-06-21 19:33:50,848 - INFO: Epoch: 107/200, Batch: 20/29, Batch_Loss_Train: 0.671
2024-06-21 19:33:51,254 - INFO: Epoch: 107/200, Batch: 21/29, Batch_Loss_Train: 0.697
2024-06-21 19:33:51,556 - INFO: Epoch: 107/200, Batch: 22/29, Batch_Loss_Train: 0.516
2024-06-21 19:33:51,956 - INFO: Epoch: 107/200, Batch: 23/29, Batch_Loss_Train: 0.599
2024-06-21 19:33:52,271 - INFO: Epoch: 107/200, Batch: 24/29, Batch_Loss_Train: 0.596
2024-06-21 19:33:52,674 - INFO: Epoch: 107/200, Batch: 25/29, Batch_Loss_Train: 0.543
2024-06-21 19:33:52,971 - INFO: Epoch: 107/200, Batch: 26/29, Batch_Loss_Train: 0.690
2024-06-21 19:33:53,368 - INFO: Epoch: 107/200, Batch: 27/29, Batch_Loss_Train: 0.556
2024-06-21 19:33:53,678 - INFO: Epoch: 107/200, Batch: 28/29, Batch_Loss_Train: 0.668
2024-06-21 19:33:53,898 - INFO: Epoch: 107/200, Batch: 29/29, Batch_Loss_Train: 0.631
2024-06-21 19:34:05,084 - INFO: 107/200 final results:
2024-06-21 19:34:05,084 - INFO: Training loss: 0.606.
2024-06-21 19:34:05,084 - INFO: Training MAE: 0.605.
2024-06-21 19:34:05,084 - INFO: Training MSE: 0.907.
2024-06-21 19:34:25,310 - INFO: Epoch: 107/200, Loss_train: 0.6059367482004494, Loss_val: 1.8106248419860314
2024-06-21 19:34:25,310 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:34:25,310 - INFO: Epoch 108/200...
2024-06-21 19:34:25,310 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:34:25,310 - INFO: Batch size: 32.
2024-06-21 19:34:25,315 - INFO: Dataset:
2024-06-21 19:34:25,315 - INFO: Batch size:
2024-06-21 19:34:25,315 - INFO: Number of workers:
2024-06-21 19:34:26,459 - INFO: Epoch: 108/200, Batch: 1/29, Batch_Loss_Train: 0.477
2024-06-21 19:34:26,791 - INFO: Epoch: 108/200, Batch: 2/29, Batch_Loss_Train: 0.739
2024-06-21 19:34:27,187 - INFO: Epoch: 108/200, Batch: 3/29, Batch_Loss_Train: 0.505
2024-06-21 19:34:27,505 - INFO: Epoch: 108/200, Batch: 4/29, Batch_Loss_Train: 0.629
2024-06-21 19:34:27,914 - INFO: Epoch: 108/200, Batch: 5/29, Batch_Loss_Train: 0.502
2024-06-21 19:34:28,227 - INFO: Epoch: 108/200, Batch: 6/29, Batch_Loss_Train: 0.677
2024-06-21 19:34:28,603 - INFO: Epoch: 108/200, Batch: 7/29, Batch_Loss_Train: 0.452
2024-06-21 19:34:28,917 - INFO: Epoch: 108/200, Batch: 8/29, Batch_Loss_Train: 0.814
2024-06-21 19:34:29,322 - INFO: Epoch: 108/200, Batch: 9/29, Batch_Loss_Train: 0.686
2024-06-21 19:34:29,628 - INFO: Epoch: 108/200, Batch: 10/29, Batch_Loss_Train: 0.762
2024-06-21 19:34:29,999 - INFO: Epoch: 108/200, Batch: 11/29, Batch_Loss_Train: 0.536
2024-06-21 19:34:30,315 - INFO: Epoch: 108/200, Batch: 12/29, Batch_Loss_Train: 0.492
2024-06-21 19:34:30,732 - INFO: Epoch: 108/200, Batch: 13/29, Batch_Loss_Train: 0.559
2024-06-21 19:34:31,049 - INFO: Epoch: 108/200, Batch: 14/29, Batch_Loss_Train: 0.638
2024-06-21 19:34:31,443 - INFO: Epoch: 108/200, Batch: 15/29, Batch_Loss_Train: 0.859
2024-06-21 19:34:31,755 - INFO: Epoch: 108/200, Batch: 16/29, Batch_Loss_Train: 0.499
2024-06-21 19:34:32,172 - INFO: Epoch: 108/200, Batch: 17/29, Batch_Loss_Train: 0.521
2024-06-21 19:34:32,484 - INFO: Epoch: 108/200, Batch: 18/29, Batch_Loss_Train: 0.680
2024-06-21 19:34:32,865 - INFO: Epoch: 108/200, Batch: 19/29, Batch_Loss_Train: 0.595
2024-06-21 19:34:33,174 - INFO: Epoch: 108/200, Batch: 20/29, Batch_Loss_Train: 0.527
2024-06-21 19:34:33,582 - INFO: Epoch: 108/200, Batch: 21/29, Batch_Loss_Train: 0.554
2024-06-21 19:34:33,898 - INFO: Epoch: 108/200, Batch: 22/29, Batch_Loss_Train: 0.708
2024-06-21 19:34:34,287 - INFO: Epoch: 108/200, Batch: 23/29, Batch_Loss_Train: 0.640
2024-06-21 19:34:34,603 - INFO: Epoch: 108/200, Batch: 24/29, Batch_Loss_Train: 0.682
2024-06-21 19:34:35,012 - INFO: Epoch: 108/200, Batch: 25/29, Batch_Loss_Train: 0.960
2024-06-21 19:34:35,324 - INFO: Epoch: 108/200, Batch: 26/29, Batch_Loss_Train: 0.490
2024-06-21 19:34:35,709 - INFO: Epoch: 108/200, Batch: 27/29, Batch_Loss_Train: 0.556
2024-06-21 19:34:36,021 - INFO: Epoch: 108/200, Batch: 28/29, Batch_Loss_Train: 0.519
2024-06-21 19:34:36,242 - INFO: Epoch: 108/200, Batch: 29/29, Batch_Loss_Train: 0.875
2024-06-21 19:34:47,253 - INFO: 108/200 final results:
2024-06-21 19:34:47,253 - INFO: Training loss: 0.625.
2024-06-21 19:34:47,253 - INFO: Training MAE: 0.620.
2024-06-21 19:34:47,253 - INFO: Training MSE: 0.934.
2024-06-21 19:35:07,638 - INFO: Epoch: 108/200, Loss_train: 0.6253028810024261, Loss_val: 1.8314498457415351
2024-06-21 19:35:07,639 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:35:07,639 - INFO: Epoch 109/200...
2024-06-21 19:35:07,639 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:35:07,639 - INFO: Batch size: 32.
2024-06-21 19:35:07,643 - INFO: Dataset:
2024-06-21 19:35:07,643 - INFO: Batch size:
2024-06-21 19:35:07,643 - INFO: Number of workers:
2024-06-21 19:35:08,816 - INFO: Epoch: 109/200, Batch: 1/29, Batch_Loss_Train: 0.617
2024-06-21 19:35:09,136 - INFO: Epoch: 109/200, Batch: 2/29, Batch_Loss_Train: 0.809
2024-06-21 19:35:09,533 - INFO: Epoch: 109/200, Batch: 3/29, Batch_Loss_Train: 0.693
2024-06-21 19:35:09,852 - INFO: Epoch: 109/200, Batch: 4/29, Batch_Loss_Train: 0.577
2024-06-21 19:35:10,269 - INFO: Epoch: 109/200, Batch: 5/29, Batch_Loss_Train: 0.595
2024-06-21 19:35:10,584 - INFO: Epoch: 109/200, Batch: 6/29, Batch_Loss_Train: 0.487
2024-06-21 19:35:10,973 - INFO: Epoch: 109/200, Batch: 7/29, Batch_Loss_Train: 0.584
2024-06-21 19:35:11,289 - INFO: Epoch: 109/200, Batch: 8/29, Batch_Loss_Train: 0.617
2024-06-21 19:35:11,697 - INFO: Epoch: 109/200, Batch: 9/29, Batch_Loss_Train: 0.572
2024-06-21 19:35:12,004 - INFO: Epoch: 109/200, Batch: 10/29, Batch_Loss_Train: 0.529
2024-06-21 19:35:12,382 - INFO: Epoch: 109/200, Batch: 11/29, Batch_Loss_Train: 0.577
2024-06-21 19:35:12,700 - INFO: Epoch: 109/200, Batch: 12/29, Batch_Loss_Train: 0.726
2024-06-21 19:35:13,120 - INFO: Epoch: 109/200, Batch: 13/29, Batch_Loss_Train: 0.566
2024-06-21 19:35:13,438 - INFO: Epoch: 109/200, Batch: 14/29, Batch_Loss_Train: 0.485
2024-06-21 19:35:13,831 - INFO: Epoch: 109/200, Batch: 15/29, Batch_Loss_Train: 0.601
2024-06-21 19:35:14,145 - INFO: Epoch: 109/200, Batch: 16/29, Batch_Loss_Train: 0.621
2024-06-21 19:35:14,564 - INFO: Epoch: 109/200, Batch: 17/29, Batch_Loss_Train: 0.551
2024-06-21 19:35:14,878 - INFO: Epoch: 109/200, Batch: 18/29, Batch_Loss_Train: 0.697
2024-06-21 19:35:15,260 - INFO: Epoch: 109/200, Batch: 19/29, Batch_Loss_Train: 0.506
2024-06-21 19:35:15,569 - INFO: Epoch: 109/200, Batch: 20/29, Batch_Loss_Train: 0.533
2024-06-21 19:35:15,974 - INFO: Epoch: 109/200, Batch: 21/29, Batch_Loss_Train: 0.635
2024-06-21 19:35:16,290 - INFO: Epoch: 109/200, Batch: 22/29, Batch_Loss_Train: 0.699
2024-06-21 19:35:16,667 - INFO: Epoch: 109/200, Batch: 23/29, Batch_Loss_Train: 0.652
2024-06-21 19:35:16,983 - INFO: Epoch: 109/200, Batch: 24/29, Batch_Loss_Train: 0.512
2024-06-21 19:35:17,389 - INFO: Epoch: 109/200, Batch: 25/29, Batch_Loss_Train: 0.581
2024-06-21 19:35:17,701 - INFO: Epoch: 109/200, Batch: 26/29, Batch_Loss_Train: 0.681
2024-06-21 19:35:18,078 - INFO: Epoch: 109/200, Batch: 27/29, Batch_Loss_Train: 0.485
2024-06-21 19:35:18,390 - INFO: Epoch: 109/200, Batch: 28/29, Batch_Loss_Train: 0.641
2024-06-21 19:35:18,602 - INFO: Epoch: 109/200, Batch: 29/29, Batch_Loss_Train: 0.656
2024-06-21 19:35:29,668 - INFO: 109/200 final results:
2024-06-21 19:35:29,669 - INFO: Training loss: 0.603.
2024-06-21 19:35:29,669 - INFO: Training MAE: 0.602.
2024-06-21 19:35:29,669 - INFO: Training MSE: 0.910.
2024-06-21 19:35:50,214 - INFO: Epoch: 109/200, Loss_train: 0.6028870005032112, Loss_val: 1.819164567980273
2024-06-21 19:35:50,214 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:35:50,214 - INFO: Epoch 110/200...
2024-06-21 19:35:50,214 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:35:50,214 - INFO: Batch size: 32.
2024-06-21 19:35:50,218 - INFO: Dataset:
2024-06-21 19:35:50,219 - INFO: Batch size:
2024-06-21 19:35:50,219 - INFO: Number of workers:
2024-06-21 19:35:51,375 - INFO: Epoch: 110/200, Batch: 1/29, Batch_Loss_Train: 0.750
2024-06-21 19:35:51,693 - INFO: Epoch: 110/200, Batch: 2/29, Batch_Loss_Train: 0.591
2024-06-21 19:35:52,101 - INFO: Epoch: 110/200, Batch: 3/29, Batch_Loss_Train: 0.565
2024-06-21 19:35:52,417 - INFO: Epoch: 110/200, Batch: 4/29, Batch_Loss_Train: 0.518
2024-06-21 19:35:52,830 - INFO: Epoch: 110/200, Batch: 5/29, Batch_Loss_Train: 0.577
2024-06-21 19:35:53,128 - INFO: Epoch: 110/200, Batch: 6/29, Batch_Loss_Train: 0.557
2024-06-21 19:35:53,528 - INFO: Epoch: 110/200, Batch: 7/29, Batch_Loss_Train: 0.561
2024-06-21 19:35:53,839 - INFO: Epoch: 110/200, Batch: 8/29, Batch_Loss_Train: 0.787
2024-06-21 19:35:54,244 - INFO: Epoch: 110/200, Batch: 9/29, Batch_Loss_Train: 0.495
2024-06-21 19:35:54,535 - INFO: Epoch: 110/200, Batch: 10/29, Batch_Loss_Train: 0.535
2024-06-21 19:35:54,924 - INFO: Epoch: 110/200, Batch: 11/29, Batch_Loss_Train: 0.683
2024-06-21 19:35:55,239 - INFO: Epoch: 110/200, Batch: 12/29, Batch_Loss_Train: 0.521
2024-06-21 19:35:55,656 - INFO: Epoch: 110/200, Batch: 13/29, Batch_Loss_Train: 0.579
2024-06-21 19:35:55,956 - INFO: Epoch: 110/200, Batch: 14/29, Batch_Loss_Train: 0.562
2024-06-21 19:35:56,363 - INFO: Epoch: 110/200, Batch: 15/29, Batch_Loss_Train: 0.551
2024-06-21 19:35:56,673 - INFO: Epoch: 110/200, Batch: 16/29, Batch_Loss_Train: 0.590
2024-06-21 19:35:57,091 - INFO: Epoch: 110/200, Batch: 17/29, Batch_Loss_Train: 0.547
2024-06-21 19:35:57,387 - INFO: Epoch: 110/200, Batch: 18/29, Batch_Loss_Train: 0.670
2024-06-21 19:35:57,786 - INFO: Epoch: 110/200, Batch: 19/29, Batch_Loss_Train: 0.652
2024-06-21 19:35:58,092 - INFO: Epoch: 110/200, Batch: 20/29, Batch_Loss_Train: 0.836
2024-06-21 19:35:58,500 - INFO: Epoch: 110/200, Batch: 21/29, Batch_Loss_Train: 0.545
2024-06-21 19:35:58,801 - INFO: Epoch: 110/200, Batch: 22/29, Batch_Loss_Train: 0.460
2024-06-21 19:35:59,189 - INFO: Epoch: 110/200, Batch: 23/29, Batch_Loss_Train: 0.666
2024-06-21 19:35:59,503 - INFO: Epoch: 110/200, Batch: 24/29, Batch_Loss_Train: 0.657
2024-06-21 19:35:59,909 - INFO: Epoch: 110/200, Batch: 25/29, Batch_Loss_Train: 0.735
2024-06-21 19:36:00,206 - INFO: Epoch: 110/200, Batch: 26/29, Batch_Loss_Train: 0.609
2024-06-21 19:36:00,601 - INFO: Epoch: 110/200, Batch: 27/29, Batch_Loss_Train: 0.661
2024-06-21 19:36:00,910 - INFO: Epoch: 110/200, Batch: 28/29, Batch_Loss_Train: 0.541
2024-06-21 19:36:01,129 - INFO: Epoch: 110/200, Batch: 29/29, Batch_Loss_Train: 0.778
2024-06-21 19:36:12,201 - INFO: 110/200 final results:
2024-06-21 19:36:12,202 - INFO: Training loss: 0.613.
2024-06-21 19:36:12,202 - INFO: Training MAE: 0.610.
2024-06-21 19:36:12,202 - INFO: Training MSE: 0.939.
2024-06-21 19:36:32,828 - INFO: Epoch: 110/200, Loss_train: 0.6130621895707887, Loss_val: 1.8233925145247887
2024-06-21 19:36:32,829 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:36:32,829 - INFO: Epoch 111/200...
2024-06-21 19:36:32,829 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:36:32,829 - INFO: Batch size: 32.
2024-06-21 19:36:32,833 - INFO: Dataset:
2024-06-21 19:36:32,833 - INFO: Batch size:
2024-06-21 19:36:32,833 - INFO: Number of workers:
2024-06-21 19:36:33,983 - INFO: Epoch: 111/200, Batch: 1/29, Batch_Loss_Train: 0.621
2024-06-21 19:36:34,326 - INFO: Epoch: 111/200, Batch: 2/29, Batch_Loss_Train: 0.566
2024-06-21 19:36:34,719 - INFO: Epoch: 111/200, Batch: 3/29, Batch_Loss_Train: 0.646
2024-06-21 19:36:35,025 - INFO: Epoch: 111/200, Batch: 4/29, Batch_Loss_Train: 0.527
2024-06-21 19:36:35,431 - INFO: Epoch: 111/200, Batch: 5/29, Batch_Loss_Train: 0.578
2024-06-21 19:36:35,756 - INFO: Epoch: 111/200, Batch: 6/29, Batch_Loss_Train: 0.540
2024-06-21 19:36:36,142 - INFO: Epoch: 111/200, Batch: 7/29, Batch_Loss_Train: 0.760
2024-06-21 19:36:36,444 - INFO: Epoch: 111/200, Batch: 8/29, Batch_Loss_Train: 0.634
2024-06-21 19:36:36,854 - INFO: Epoch: 111/200, Batch: 9/29, Batch_Loss_Train: 0.566
2024-06-21 19:36:37,185 - INFO: Epoch: 111/200, Batch: 10/29, Batch_Loss_Train: 0.603
2024-06-21 19:36:37,559 - INFO: Epoch: 111/200, Batch: 11/29, Batch_Loss_Train: 0.531
2024-06-21 19:36:37,863 - INFO: Epoch: 111/200, Batch: 12/29, Batch_Loss_Train: 0.670
2024-06-21 19:36:38,269 - INFO: Epoch: 111/200, Batch: 13/29, Batch_Loss_Train: 0.669
2024-06-21 19:36:38,596 - INFO: Epoch: 111/200, Batch: 14/29, Batch_Loss_Train: 0.823
2024-06-21 19:36:38,988 - INFO: Epoch: 111/200, Batch: 15/29, Batch_Loss_Train: 0.599
2024-06-21 19:36:39,286 - INFO: Epoch: 111/200, Batch: 16/29, Batch_Loss_Train: 0.726
2024-06-21 19:36:39,698 - INFO: Epoch: 111/200, Batch: 17/29, Batch_Loss_Train: 0.608
2024-06-21 19:36:40,021 - INFO: Epoch: 111/200, Batch: 18/29, Batch_Loss_Train: 0.745
2024-06-21 19:36:40,402 - INFO: Epoch: 111/200, Batch: 19/29, Batch_Loss_Train: 0.600
2024-06-21 19:36:40,695 - INFO: Epoch: 111/200, Batch: 20/29, Batch_Loss_Train: 0.542
2024-06-21 19:36:41,093 - INFO: Epoch: 111/200, Batch: 21/29, Batch_Loss_Train: 0.472
2024-06-21 19:36:41,418 - INFO: Epoch: 111/200, Batch: 22/29, Batch_Loss_Train: 0.647
2024-06-21 19:36:41,793 - INFO: Epoch: 111/200, Batch: 23/29, Batch_Loss_Train: 0.519
2024-06-21 19:36:42,094 - INFO: Epoch: 111/200, Batch: 24/29, Batch_Loss_Train: 0.606
2024-06-21 19:36:42,497 - INFO: Epoch: 111/200, Batch: 25/29, Batch_Loss_Train: 0.556
2024-06-21 19:36:42,817 - INFO: Epoch: 111/200, Batch: 26/29, Batch_Loss_Train: 0.538
2024-06-21 19:36:43,196 - INFO: Epoch: 111/200, Batch: 27/29, Batch_Loss_Train: 0.466
2024-06-21 19:36:43,492 - INFO: Epoch: 111/200, Batch: 28/29, Batch_Loss_Train: 0.654
2024-06-21 19:36:43,702 - INFO: Epoch: 111/200, Batch: 29/29, Batch_Loss_Train: 0.912
2024-06-21 19:36:54,776 - INFO: 111/200 final results:
2024-06-21 19:36:54,776 - INFO: Training loss: 0.618.
2024-06-21 19:36:54,776 - INFO: Training MAE: 0.612.
2024-06-21 19:36:54,776 - INFO: Training MSE: 0.943.
2024-06-21 19:37:14,809 - INFO: Epoch: 111/200, Loss_train: 0.618121745257542, Loss_val: 1.8145493186753372
2024-06-21 19:37:14,809 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:37:14,810 - INFO: Epoch 112/200...
2024-06-21 19:37:14,810 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:37:14,810 - INFO: Batch size: 32.
2024-06-21 19:37:14,814 - INFO: Dataset:
2024-06-21 19:37:14,814 - INFO: Batch size:
2024-06-21 19:37:14,814 - INFO: Number of workers:
2024-06-21 19:37:15,979 - INFO: Epoch: 112/200, Batch: 1/29, Batch_Loss_Train: 0.572
2024-06-21 19:37:16,283 - INFO: Epoch: 112/200, Batch: 2/29, Batch_Loss_Train: 0.478
2024-06-21 19:37:16,703 - INFO: Epoch: 112/200, Batch: 3/29, Batch_Loss_Train: 0.555
2024-06-21 19:37:17,011 - INFO: Epoch: 112/200, Batch: 4/29, Batch_Loss_Train: 0.576
2024-06-21 19:37:17,427 - INFO: Epoch: 112/200, Batch: 5/29, Batch_Loss_Train: 0.501
2024-06-21 19:37:17,729 - INFO: Epoch: 112/200, Batch: 6/29, Batch_Loss_Train: 0.533
2024-06-21 19:37:18,137 - INFO: Epoch: 112/200, Batch: 7/29, Batch_Loss_Train: 0.598
2024-06-21 19:37:18,441 - INFO: Epoch: 112/200, Batch: 8/29, Batch_Loss_Train: 0.870
2024-06-21 19:37:18,836 - INFO: Epoch: 112/200, Batch: 9/29, Batch_Loss_Train: 0.695
2024-06-21 19:37:19,131 - INFO: Epoch: 112/200, Batch: 10/29, Batch_Loss_Train: 0.570
2024-06-21 19:37:19,536 - INFO: Epoch: 112/200, Batch: 11/29, Batch_Loss_Train: 0.730
2024-06-21 19:37:19,841 - INFO: Epoch: 112/200, Batch: 12/29, Batch_Loss_Train: 0.486
2024-06-21 19:37:20,254 - INFO: Epoch: 112/200, Batch: 13/29, Batch_Loss_Train: 0.618
2024-06-21 19:37:20,557 - INFO: Epoch: 112/200, Batch: 14/29, Batch_Loss_Train: 0.555
2024-06-21 19:37:20,988 - INFO: Epoch: 112/200, Batch: 15/29, Batch_Loss_Train: 0.685
2024-06-21 19:37:21,289 - INFO: Epoch: 112/200, Batch: 16/29, Batch_Loss_Train: 0.674
2024-06-21 19:37:21,679 - INFO: Epoch: 112/200, Batch: 17/29, Batch_Loss_Train: 0.563
2024-06-21 19:37:21,979 - INFO: Epoch: 112/200, Batch: 18/29, Batch_Loss_Train: 0.784
2024-06-21 19:37:22,399 - INFO: Epoch: 112/200, Batch: 19/29, Batch_Loss_Train: 0.632
2024-06-21 19:37:22,692 - INFO: Epoch: 112/200, Batch: 20/29, Batch_Loss_Train: 0.597
2024-06-21 19:37:23,081 - INFO: Epoch: 112/200, Batch: 21/29, Batch_Loss_Train: 0.592
2024-06-21 19:37:23,382 - INFO: Epoch: 112/200, Batch: 22/29, Batch_Loss_Train: 0.593
2024-06-21 19:37:23,802 - INFO: Epoch: 112/200, Batch: 23/29, Batch_Loss_Train: 0.692
2024-06-21 19:37:24,102 - INFO: Epoch: 112/200, Batch: 24/29, Batch_Loss_Train: 0.508
2024-06-21 19:37:24,494 - INFO: Epoch: 112/200, Batch: 25/29, Batch_Loss_Train: 0.531
2024-06-21 19:37:24,790 - INFO: Epoch: 112/200, Batch: 26/29, Batch_Loss_Train: 0.595
2024-06-21 19:37:25,204 - INFO: Epoch: 112/200, Batch: 27/29, Batch_Loss_Train: 0.463
2024-06-21 19:37:25,500 - INFO: Epoch: 112/200, Batch: 28/29, Batch_Loss_Train: 0.545
2024-06-21 19:37:25,711 - INFO: Epoch: 112/200, Batch: 29/29, Batch_Loss_Train: 0.518
2024-06-21 19:37:36,778 - INFO: 112/200 final results:
2024-06-21 19:37:36,778 - INFO: Training loss: 0.597.
2024-06-21 19:37:36,778 - INFO: Training MAE: 0.598.
2024-06-21 19:37:36,778 - INFO: Training MSE: 0.880.
2024-06-21 19:37:57,490 - INFO: Epoch: 112/200, Loss_train: 0.5968362956211485, Loss_val: 1.8523755320187272
2024-06-21 19:37:57,490 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:37:57,491 - INFO: Epoch 113/200...
2024-06-21 19:37:57,491 - INFO: Learning rate: 1.3641645838004082e-05.
2024-06-21 19:37:57,491 - INFO: Batch size: 32.
2024-06-21 19:37:57,495 - INFO: Dataset:
2024-06-21 19:37:57,495 - INFO: Batch size:
2024-06-21 19:37:57,495 - INFO: Number of workers:
2024-06-21 19:37:58,648 - INFO: Epoch: 113/200, Batch: 1/29, Batch_Loss_Train: 0.590
2024-06-21 19:37:58,967 - INFO: Epoch: 113/200, Batch: 2/29, Batch_Loss_Train: 0.621
2024-06-21 19:37:59,375 - INFO: Epoch: 113/200, Batch: 3/29, Batch_Loss_Train: 0.530
2024-06-21 19:37:59,691 - INFO: Epoch: 113/200, Batch: 4/29, Batch_Loss_Train: 0.627
2024-06-21 19:38:00,107 - INFO: Epoch: 113/200, Batch: 5/29, Batch_Loss_Train: 0.530
2024-06-21 19:38:00,409 - INFO: Epoch: 113/200, Batch: 6/29, Batch_Loss_Train: 0.829
2024-06-21 19:38:00,802 - INFO: Epoch: 113/200, Batch: 7/29, Batch_Loss_Train: 0.609
2024-06-21 19:38:01,116 - INFO: Epoch: 113/200, Batch: 8/29, Batch_Loss_Train: 0.560
2024-06-21 19:38:01,521 - INFO: Epoch: 113/200, Batch: 9/29, Batch_Loss_Train: 0.745
2024-06-21 19:38:01,813 - INFO: Epoch: 113/200, Batch: 10/29, Batch_Loss_Train: 0.647
2024-06-21 19:38:02,188 - INFO: Epoch: 113/200, Batch: 11/29, Batch_Loss_Train: 0.542
2024-06-21 19:38:02,503 - INFO: Epoch: 113/200, Batch: 12/29, Batch_Loss_Train: 0.510
2024-06-21 19:38:02,923 - INFO: Epoch: 113/200, Batch: 13/29, Batch_Loss_Train: 0.590
2024-06-21 19:38:03,226 - INFO: Epoch: 113/200, Batch: 14/29, Batch_Loss_Train: 0.631
2024-06-21 19:38:03,619 - INFO: Epoch: 113/200, Batch: 15/29, Batch_Loss_Train: 0.519
2024-06-21 19:38:03,930 - INFO: Epoch: 113/200, Batch: 16/29, Batch_Loss_Train: 0.619
2024-06-21 19:38:04,344 - INFO: Epoch: 113/200, Batch: 17/29, Batch_Loss_Train: 0.536
2024-06-21 19:38:04,643 - INFO: Epoch: 113/200, Batch: 18/29, Batch_Loss_Train: 0.656
2024-06-21 19:38:05,026 - INFO: Epoch: 113/200, Batch: 19/29, Batch_Loss_Train: 0.516
2024-06-21 19:38:05,335 - INFO: Epoch: 113/200, Batch: 20/29, Batch_Loss_Train: 0.660
2024-06-21 19:38:05,737 - INFO: Epoch: 113/200, Batch: 21/29, Batch_Loss_Train: 0.522
2024-06-21 19:38:06,037 - INFO: Epoch: 113/200, Batch: 22/29, Batch_Loss_Train: 0.724
2024-06-21 19:38:06,425 - INFO: Epoch: 113/200, Batch: 23/29, Batch_Loss_Train: 0.584
2024-06-21 19:38:06,737 - INFO: Epoch: 113/200, Batch: 24/29, Batch_Loss_Train: 0.541
2024-06-21 19:38:07,142 - INFO: Epoch: 113/200, Batch: 25/29, Batch_Loss_Train: 0.632
2024-06-21 19:38:07,438 - INFO: Epoch: 113/200, Batch: 26/29, Batch_Loss_Train: 0.632
2024-06-21 19:38:07,824 - INFO: Epoch: 113/200, Batch: 27/29, Batch_Loss_Train: 0.625
2024-06-21 19:38:08,132 - INFO: Epoch: 113/200, Batch: 28/29, Batch_Loss_Train: 0.538
2024-06-21 19:38:08,339 - INFO: Epoch: 113/200, Batch: 29/29, Batch_Loss_Train: 0.729
2024-06-21 19:38:19,441 - INFO: 113/200 final results:
2024-06-21 19:38:19,441 - INFO: Training loss: 0.607.
2024-06-21 19:38:19,441 - INFO: Training MAE: 0.604.
2024-06-21 19:38:19,441 - INFO: Training MSE: 0.911.
2024-06-21 19:38:39,794 - INFO: Epoch: 113/200, Loss_train: 0.6067015898638758, Loss_val: 1.8284712659901585
2024-06-21 19:38:39,794 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:38:39,794 - INFO: Epoch 114/200...
2024-06-21 19:38:39,794 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:38:39,794 - INFO: Batch size: 32.
2024-06-21 19:38:39,798 - INFO: Dataset:
2024-06-21 19:38:39,798 - INFO: Batch size:
2024-06-21 19:38:39,798 - INFO: Number of workers:
2024-06-21 19:38:40,974 - INFO: Epoch: 114/200, Batch: 1/29, Batch_Loss_Train: 0.539
2024-06-21 19:38:41,294 - INFO: Epoch: 114/200, Batch: 2/29, Batch_Loss_Train: 0.490
2024-06-21 19:38:41,685 - INFO: Epoch: 114/200, Batch: 3/29, Batch_Loss_Train: 0.544
2024-06-21 19:38:42,004 - INFO: Epoch: 114/200, Batch: 4/29, Batch_Loss_Train: 0.580
2024-06-21 19:38:42,421 - INFO: Epoch: 114/200, Batch: 5/29, Batch_Loss_Train: 0.650
2024-06-21 19:38:42,735 - INFO: Epoch: 114/200, Batch: 6/29, Batch_Loss_Train: 0.586
2024-06-21 19:38:43,117 - INFO: Epoch: 114/200, Batch: 7/29, Batch_Loss_Train: 0.492
2024-06-21 19:38:43,433 - INFO: Epoch: 114/200, Batch: 8/29, Batch_Loss_Train: 0.590
2024-06-21 19:38:43,860 - INFO: Epoch: 114/200, Batch: 9/29, Batch_Loss_Train: 0.475
2024-06-21 19:38:44,167 - INFO: Epoch: 114/200, Batch: 10/29, Batch_Loss_Train: 0.531
2024-06-21 19:38:44,536 - INFO: Epoch: 114/200, Batch: 11/29, Batch_Loss_Train: 0.497
2024-06-21 19:38:44,853 - INFO: Epoch: 114/200, Batch: 12/29, Batch_Loss_Train: 0.741
2024-06-21 19:38:45,272 - INFO: Epoch: 114/200, Batch: 13/29, Batch_Loss_Train: 0.804
2024-06-21 19:38:45,588 - INFO: Epoch: 114/200, Batch: 14/29, Batch_Loss_Train: 0.481
2024-06-21 19:38:45,972 - INFO: Epoch: 114/200, Batch: 15/29, Batch_Loss_Train: 0.618
2024-06-21 19:38:46,286 - INFO: Epoch: 114/200, Batch: 16/29, Batch_Loss_Train: 0.524
2024-06-21 19:38:46,706 - INFO: Epoch: 114/200, Batch: 17/29, Batch_Loss_Train: 0.590
2024-06-21 19:38:47,019 - INFO: Epoch: 114/200, Batch: 18/29, Batch_Loss_Train: 0.573
2024-06-21 19:38:47,391 - INFO: Epoch: 114/200, Batch: 19/29, Batch_Loss_Train: 0.732
2024-06-21 19:38:47,699 - INFO: Epoch: 114/200, Batch: 20/29, Batch_Loss_Train: 0.560
2024-06-21 19:38:48,107 - INFO: Epoch: 114/200, Batch: 21/29, Batch_Loss_Train: 0.527
2024-06-21 19:38:48,423 - INFO: Epoch: 114/200, Batch: 22/29, Batch_Loss_Train: 0.569
2024-06-21 19:38:48,801 - INFO: Epoch: 114/200, Batch: 23/29, Batch_Loss_Train: 0.551
2024-06-21 19:38:49,116 - INFO: Epoch: 114/200, Batch: 24/29, Batch_Loss_Train: 0.513
2024-06-21 19:38:49,513 - INFO: Epoch: 114/200, Batch: 25/29, Batch_Loss_Train: 0.436
2024-06-21 19:38:49,823 - INFO: Epoch: 114/200, Batch: 26/29, Batch_Loss_Train: 0.584
2024-06-21 19:38:50,191 - INFO: Epoch: 114/200, Batch: 27/29, Batch_Loss_Train: 0.562
2024-06-21 19:38:50,502 - INFO: Epoch: 114/200, Batch: 28/29, Batch_Loss_Train: 0.591
2024-06-21 19:38:50,718 - INFO: Epoch: 114/200, Batch: 29/29, Batch_Loss_Train: 0.594
2024-06-21 19:39:01,881 - INFO: 114/200 final results:
2024-06-21 19:39:01,881 - INFO: Training loss: 0.570.
2024-06-21 19:39:01,881 - INFO: Training MAE: 0.569.
2024-06-21 19:39:01,881 - INFO: Training MSE: 0.823.
2024-06-21 19:39:22,441 - INFO: Epoch: 114/200, Loss_train: 0.5698797291722791, Loss_val: 1.8179316849544132
2024-06-21 19:39:22,441 - INFO: Best internal validation val_loss: 1.809 at epoch: 106.
2024-06-21 19:39:22,441 - INFO: Epoch 115/200...
2024-06-21 19:39:22,441 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:39:22,441 - INFO: Batch size: 32.
2024-06-21 19:39:22,445 - INFO: Dataset:
2024-06-21 19:39:22,446 - INFO: Batch size:
2024-06-21 19:39:22,446 - INFO: Number of workers:
2024-06-21 19:39:23,622 - INFO: Epoch: 115/200, Batch: 1/29, Batch_Loss_Train: 0.488
2024-06-21 19:39:23,926 - INFO: Epoch: 115/200, Batch: 2/29, Batch_Loss_Train: 0.595
2024-06-21 19:39:24,319 - INFO: Epoch: 115/200, Batch: 3/29, Batch_Loss_Train: 0.509
2024-06-21 19:39:24,634 - INFO: Epoch: 115/200, Batch: 4/29, Batch_Loss_Train: 0.625
2024-06-21 19:39:25,059 - INFO: Epoch: 115/200, Batch: 5/29, Batch_Loss_Train: 0.469
2024-06-21 19:39:25,358 - INFO: Epoch: 115/200, Batch: 6/29, Batch_Loss_Train: 0.529
2024-06-21 19:39:25,745 - INFO: Epoch: 115/200, Batch: 7/29, Batch_Loss_Train: 0.504
2024-06-21 19:39:26,058 - INFO: Epoch: 115/200, Batch: 8/29, Batch_Loss_Train: 0.483
2024-06-21 19:39:26,474 - INFO: Epoch: 115/200, Batch: 9/29, Batch_Loss_Train: 0.669
2024-06-21 19:39:26,769 - INFO: Epoch: 115/200, Batch: 10/29, Batch_Loss_Train: 0.548
2024-06-21 19:39:27,148 - INFO: Epoch: 115/200, Batch: 11/29, Batch_Loss_Train: 0.585
2024-06-21 19:39:27,465 - INFO: Epoch: 115/200, Batch: 12/29, Batch_Loss_Train: 0.752
2024-06-21 19:39:27,891 - INFO: Epoch: 115/200, Batch: 13/29, Batch_Loss_Train: 0.515
2024-06-21 19:39:28,193 - INFO: Epoch: 115/200, Batch: 14/29, Batch_Loss_Train: 0.497
2024-06-21 19:39:28,579 - INFO: Epoch: 115/200, Batch: 15/29, Batch_Loss_Train: 0.666
2024-06-21 19:39:28,893 - INFO: Epoch: 115/200, Batch: 16/29, Batch_Loss_Train: 0.463
2024-06-21 19:39:29,308 - INFO: Epoch: 115/200, Batch: 17/29, Batch_Loss_Train: 0.596
2024-06-21 19:39:29,608 - INFO: Epoch: 115/200, Batch: 18/29, Batch_Loss_Train: 0.734
2024-06-21 19:39:29,982 - INFO: Epoch: 115/200, Batch: 19/29, Batch_Loss_Train: 0.640
2024-06-21 19:39:30,291 - INFO: Epoch: 115/200, Batch: 20/29, Batch_Loss_Train: 0.467
2024-06-21 19:39:30,705 - INFO: Epoch: 115/200, Batch: 21/29, Batch_Loss_Train: 0.536
2024-06-21 19:39:31,008 - INFO: Epoch: 115/200, Batch: 22/29, Batch_Loss_Train: 0.520
2024-06-21 19:39:31,390 - INFO: Epoch: 115/200, Batch: 23/29, Batch_Loss_Train: 0.713
2024-06-21 19:39:31,705 - INFO: Epoch: 115/200, Batch: 24/29, Batch_Loss_Train: 0.467
2024-06-21 19:39:32,119 - INFO: Epoch: 115/200, Batch: 25/29, Batch_Loss_Train: 0.491
2024-06-21 19:39:32,416 - INFO: Epoch: 115/200, Batch: 26/29, Batch_Loss_Train: 0.552
2024-06-21 19:39:32,796 - INFO: Epoch: 115/200, Batch: 27/29, Batch_Loss_Train: 0.496
2024-06-21 19:39:33,107 - INFO: Epoch: 115/200, Batch: 28/29, Batch_Loss_Train: 0.505
2024-06-21 19:39:33,321 - INFO: Epoch: 115/200, Batch: 29/29, Batch_Loss_Train: 0.857
2024-06-21 19:39:44,209 - INFO: 115/200 final results:
2024-06-21 19:39:44,209 - INFO: Training loss: 0.568.
2024-06-21 19:39:44,209 - INFO: Training MAE: 0.562.
2024-06-21 19:39:44,209 - INFO: Training MSE: 0.835.
2024-06-21 19:40:04,875 - INFO: Epoch: 115/200, Loss_train: 0.5679784704898966, Loss_val: 1.8047287998528316
2024-06-21 19:40:04,893 - INFO: Saved new best metric model for epoch 115.
2024-06-21 19:40:04,893 - INFO: Best internal validation val_loss: 1.805 at epoch: 115.
2024-06-21 19:40:04,893 - INFO: Epoch 116/200...
2024-06-21 19:40:04,893 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:40:04,893 - INFO: Batch size: 32.
2024-06-21 19:40:04,897 - INFO: Dataset:
2024-06-21 19:40:04,897 - INFO: Batch size:
2024-06-21 19:40:04,897 - INFO: Number of workers:
2024-06-21 19:40:06,063 - INFO: Epoch: 116/200, Batch: 1/29, Batch_Loss_Train: 0.547
2024-06-21 19:40:06,367 - INFO: Epoch: 116/200, Batch: 2/29, Batch_Loss_Train: 0.676
2024-06-21 19:40:06,762 - INFO: Epoch: 116/200, Batch: 3/29, Batch_Loss_Train: 0.542
2024-06-21 19:40:07,080 - INFO: Epoch: 116/200, Batch: 4/29, Batch_Loss_Train: 0.592
2024-06-21 19:40:07,495 - INFO: Epoch: 116/200, Batch: 5/29, Batch_Loss_Train: 0.513
2024-06-21 19:40:07,797 - INFO: Epoch: 116/200, Batch: 6/29, Batch_Loss_Train: 0.450
2024-06-21 19:40:08,185 - INFO: Epoch: 116/200, Batch: 7/29, Batch_Loss_Train: 0.453
2024-06-21 19:40:08,501 - INFO: Epoch: 116/200, Batch: 8/29, Batch_Loss_Train: 0.749
2024-06-21 19:40:08,924 - INFO: Epoch: 116/200, Batch: 9/29, Batch_Loss_Train: 0.729
2024-06-21 19:40:09,218 - INFO: Epoch: 116/200, Batch: 10/29, Batch_Loss_Train: 0.544
2024-06-21 19:40:09,595 - INFO: Epoch: 116/200, Batch: 11/29, Batch_Loss_Train: 0.590
2024-06-21 19:40:09,912 - INFO: Epoch: 116/200, Batch: 12/29, Batch_Loss_Train: 0.509
2024-06-21 19:40:10,340 - INFO: Epoch: 116/200, Batch: 13/29, Batch_Loss_Train: 0.478
2024-06-21 19:40:10,645 - INFO: Epoch: 116/200, Batch: 14/29, Batch_Loss_Train: 0.646
2024-06-21 19:40:11,041 - INFO: Epoch: 116/200, Batch: 15/29, Batch_Loss_Train: 0.486
2024-06-21 19:40:11,354 - INFO: Epoch: 116/200, Batch: 16/29, Batch_Loss_Train: 0.611
2024-06-21 19:40:11,784 - INFO: Epoch: 116/200, Batch: 17/29, Batch_Loss_Train: 0.511
2024-06-21 19:40:12,084 - INFO: Epoch: 116/200, Batch: 18/29, Batch_Loss_Train: 0.556
2024-06-21 19:40:12,469 - INFO: Epoch: 116/200, Batch: 19/29, Batch_Loss_Train: 0.664
2024-06-21 19:40:12,776 - INFO: Epoch: 116/200, Batch: 20/29, Batch_Loss_Train: 0.633
2024-06-21 19:40:13,186 - INFO: Epoch: 116/200, Batch: 21/29, Batch_Loss_Train: 0.543
2024-06-21 19:40:13,489 - INFO: Epoch: 116/200, Batch: 22/29, Batch_Loss_Train: 0.581
2024-06-21 19:40:13,877 - INFO: Epoch: 116/200, Batch: 23/29, Batch_Loss_Train: 0.471
2024-06-21 19:40:14,193 - INFO: Epoch: 116/200, Batch: 24/29, Batch_Loss_Train: 0.588
2024-06-21 19:40:14,612 - INFO: Epoch: 116/200, Batch: 25/29, Batch_Loss_Train: 0.681
2024-06-21 19:40:14,910 - INFO: Epoch: 116/200, Batch: 26/29, Batch_Loss_Train: 0.641
2024-06-21 19:40:15,293 - INFO: Epoch: 116/200, Batch: 27/29, Batch_Loss_Train: 0.642
2024-06-21 19:40:15,604 - INFO: Epoch: 116/200, Batch: 28/29, Batch_Loss_Train: 0.660
2024-06-21 19:40:15,821 - INFO: Epoch: 116/200, Batch: 29/29, Batch_Loss_Train: 0.586
2024-06-21 19:40:26,935 - INFO: 116/200 final results:
2024-06-21 19:40:26,935 - INFO: Training loss: 0.582.
2024-06-21 19:40:26,935 - INFO: Training MAE: 0.582.
2024-06-21 19:40:26,935 - INFO: Training MSE: 0.869.
2024-06-21 19:40:47,421 - INFO: Epoch: 116/200, Loss_train: 0.5818048088714994, Loss_val: 1.8059158695155177
2024-06-21 19:40:47,421 - INFO: Best internal validation val_loss: 1.805 at epoch: 115.
2024-06-21 19:40:47,421 - INFO: Epoch 117/200...
2024-06-21 19:40:47,421 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:40:47,421 - INFO: Batch size: 32.
2024-06-21 19:40:47,425 - INFO: Dataset:
2024-06-21 19:40:47,426 - INFO: Batch size:
2024-06-21 19:40:47,426 - INFO: Number of workers:
2024-06-21 19:40:48,576 - INFO: Epoch: 117/200, Batch: 1/29, Batch_Loss_Train: 0.585
2024-06-21 19:40:48,893 - INFO: Epoch: 117/200, Batch: 2/29, Batch_Loss_Train: 0.545
2024-06-21 19:40:49,297 - INFO: Epoch: 117/200, Batch: 3/29, Batch_Loss_Train: 0.765
2024-06-21 19:40:49,614 - INFO: Epoch: 117/200, Batch: 4/29, Batch_Loss_Train: 0.544
2024-06-21 19:40:50,007 - INFO: Epoch: 117/200, Batch: 5/29, Batch_Loss_Train: 0.493
2024-06-21 19:40:50,317 - INFO: Epoch: 117/200, Batch: 6/29, Batch_Loss_Train: 0.543
2024-06-21 19:40:50,711 - INFO: Epoch: 117/200, Batch: 7/29, Batch_Loss_Train: 0.553
2024-06-21 19:40:51,023 - INFO: Epoch: 117/200, Batch: 8/29, Batch_Loss_Train: 0.700
2024-06-21 19:40:51,403 - INFO: Epoch: 117/200, Batch: 9/29, Batch_Loss_Train: 0.507
2024-06-21 19:40:51,707 - INFO: Epoch: 117/200, Batch: 10/29, Batch_Loss_Train: 0.622
2024-06-21 19:40:52,092 - INFO: Epoch: 117/200, Batch: 11/29, Batch_Loss_Train: 0.508
2024-06-21 19:40:52,405 - INFO: Epoch: 117/200, Batch: 12/29, Batch_Loss_Train: 0.563
2024-06-21 19:40:52,794 - INFO: Epoch: 117/200, Batch: 13/29, Batch_Loss_Train: 0.551
2024-06-21 19:40:53,108 - INFO: Epoch: 117/200, Batch: 14/29, Batch_Loss_Train: 0.522
2024-06-21 19:40:53,509 - INFO: Epoch: 117/200, Batch: 15/29, Batch_Loss_Train: 0.648
2024-06-21 19:40:53,822 - INFO: Epoch: 117/200, Batch: 16/29, Batch_Loss_Train: 0.615
2024-06-21 19:40:54,229 - INFO: Epoch: 117/200, Batch: 17/29, Batch_Loss_Train: 0.522
2024-06-21 19:40:54,543 - INFO: Epoch: 117/200, Batch: 18/29, Batch_Loss_Train: 0.635
2024-06-21 19:40:54,936 - INFO: Epoch: 117/200, Batch: 19/29, Batch_Loss_Train: 0.666
2024-06-21 19:40:55,244 - INFO: Epoch: 117/200, Batch: 20/29, Batch_Loss_Train: 0.606
2024-06-21 19:40:55,642 - INFO: Epoch: 117/200, Batch: 21/29, Batch_Loss_Train: 0.493
2024-06-21 19:40:55,958 - INFO: Epoch: 117/200, Batch: 22/29, Batch_Loss_Train: 0.691
2024-06-21 19:40:56,355 - INFO: Epoch: 117/200, Batch: 23/29, Batch_Loss_Train: 0.505
2024-06-21 19:40:56,670 - INFO: Epoch: 117/200, Batch: 24/29, Batch_Loss_Train: 0.629
2024-06-21 19:40:57,064 - INFO: Epoch: 117/200, Batch: 25/29, Batch_Loss_Train: 0.487
2024-06-21 19:40:57,375 - INFO: Epoch: 117/200, Batch: 26/29, Batch_Loss_Train: 0.656
2024-06-21 19:40:57,761 - INFO: Epoch: 117/200, Batch: 27/29, Batch_Loss_Train: 0.560
2024-06-21 19:40:58,072 - INFO: Epoch: 117/200, Batch: 28/29, Batch_Loss_Train: 0.540
2024-06-21 19:40:58,286 - INFO: Epoch: 117/200, Batch: 29/29, Batch_Loss_Train: 0.589
2024-06-21 19:41:09,481 - INFO: 117/200 final results:
2024-06-21 19:41:09,481 - INFO: Training loss: 0.581.
2024-06-21 19:41:09,481 - INFO: Training MAE: 0.581.
2024-06-21 19:41:09,481 - INFO: Training MSE: 0.863.
2024-06-21 19:41:30,040 - INFO: Epoch: 117/200, Loss_train: 0.580728235943564, Loss_val: 1.8102429611929531
2024-06-21 19:41:30,040 - INFO: Best internal validation val_loss: 1.805 at epoch: 115.
2024-06-21 19:41:30,040 - INFO: Epoch 118/200...
2024-06-21 19:41:30,040 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:41:30,041 - INFO: Batch size: 32.
2024-06-21 19:41:30,044 - INFO: Dataset:
2024-06-21 19:41:30,045 - INFO: Batch size:
2024-06-21 19:41:30,045 - INFO: Number of workers:
2024-06-21 19:41:31,200 - INFO: Epoch: 118/200, Batch: 1/29, Batch_Loss_Train: 0.589
2024-06-21 19:41:31,504 - INFO: Epoch: 118/200, Batch: 2/29, Batch_Loss_Train: 0.586
2024-06-21 19:41:31,898 - INFO: Epoch: 118/200, Batch: 3/29, Batch_Loss_Train: 0.518
2024-06-21 19:41:32,213 - INFO: Epoch: 118/200, Batch: 4/29, Batch_Loss_Train: 0.472
2024-06-21 19:41:32,634 - INFO: Epoch: 118/200, Batch: 5/29, Batch_Loss_Train: 0.643
2024-06-21 19:41:32,932 - INFO: Epoch: 118/200, Batch: 6/29, Batch_Loss_Train: 0.553
2024-06-21 19:41:33,313 - INFO: Epoch: 118/200, Batch: 7/29, Batch_Loss_Train: 0.487
2024-06-21 19:41:33,624 - INFO: Epoch: 118/200, Batch: 8/29, Batch_Loss_Train: 0.465
2024-06-21 19:41:34,036 - INFO: Epoch: 118/200, Batch: 9/29, Batch_Loss_Train: 0.757
2024-06-21 19:41:34,327 - INFO: Epoch: 118/200, Batch: 10/29, Batch_Loss_Train: 0.545
2024-06-21 19:41:34,699 - INFO: Epoch: 118/200, Batch: 11/29, Batch_Loss_Train: 0.644
2024-06-21 19:41:35,014 - INFO: Epoch: 118/200, Batch: 12/29, Batch_Loss_Train: 0.516
2024-06-21 19:41:35,438 - INFO: Epoch: 118/200, Batch: 13/29, Batch_Loss_Train: 0.571
2024-06-21 19:41:35,739 - INFO: Epoch: 118/200, Batch: 14/29, Batch_Loss_Train: 0.456
2024-06-21 19:41:36,128 - INFO: Epoch: 118/200, Batch: 15/29, Batch_Loss_Train: 0.526
2024-06-21 19:41:36,437 - INFO: Epoch: 118/200, Batch: 16/29, Batch_Loss_Train: 0.504
2024-06-21 19:41:36,863 - INFO: Epoch: 118/200, Batch: 17/29, Batch_Loss_Train: 0.502
2024-06-21 19:41:37,160 - INFO: Epoch: 118/200, Batch: 18/29, Batch_Loss_Train: 0.494
2024-06-21 19:41:37,539 - INFO: Epoch: 118/200, Batch: 19/29, Batch_Loss_Train: 0.536
2024-06-21 19:41:37,847 - INFO: Epoch: 118/200, Batch: 20/29, Batch_Loss_Train: 0.751
2024-06-21 19:41:38,258 - INFO: Epoch: 118/200, Batch: 21/29, Batch_Loss_Train: 0.657
2024-06-21 19:41:38,558 - INFO: Epoch: 118/200, Batch: 22/29, Batch_Loss_Train: 0.483
2024-06-21 19:41:38,932 - INFO: Epoch: 118/200, Batch: 23/29, Batch_Loss_Train: 0.473
2024-06-21 19:41:39,244 - INFO: Epoch: 118/200, Batch: 24/29, Batch_Loss_Train: 0.521
2024-06-21 19:41:39,656 - INFO: Epoch: 118/200, Batch: 25/29, Batch_Loss_Train: 0.593
2024-06-21 19:41:39,951 - INFO: Epoch: 118/200, Batch: 26/29, Batch_Loss_Train: 0.527
2024-06-21 19:41:40,319 - INFO: Epoch: 118/200, Batch: 27/29, Batch_Loss_Train: 0.524
2024-06-21 19:41:40,627 - INFO: Epoch: 118/200, Batch: 28/29, Batch_Loss_Train: 0.607
2024-06-21 19:41:40,833 - INFO: Epoch: 118/200, Batch: 29/29, Batch_Loss_Train: 0.624
2024-06-21 19:41:51,951 - INFO: 118/200 final results:
2024-06-21 19:41:51,951 - INFO: Training loss: 0.556.
2024-06-21 19:41:51,951 - INFO: Training MAE: 0.555.
2024-06-21 19:41:51,951 - INFO: Training MSE: 0.803.
2024-06-21 19:42:12,339 - INFO: Epoch: 118/200, Loss_train: 0.5558823285431698, Loss_val: 1.835825332279863
2024-06-21 19:42:12,339 - INFO: Best internal validation val_loss: 1.805 at epoch: 115.
2024-06-21 19:42:12,339 - INFO: Epoch 119/200...
2024-06-21 19:42:12,339 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:42:12,339 - INFO: Batch size: 32.
2024-06-21 19:42:12,343 - INFO: Dataset:
2024-06-21 19:42:12,344 - INFO: Batch size:
2024-06-21 19:42:12,344 - INFO: Number of workers:
2024-06-21 19:42:13,524 - INFO: Epoch: 119/200, Batch: 1/29, Batch_Loss_Train: 0.608
2024-06-21 19:42:13,829 - INFO: Epoch: 119/200, Batch: 2/29, Batch_Loss_Train: 0.526
2024-06-21 19:42:14,227 - INFO: Epoch: 119/200, Batch: 3/29, Batch_Loss_Train: 0.545
2024-06-21 19:42:14,545 - INFO: Epoch: 119/200, Batch: 4/29, Batch_Loss_Train: 0.494
2024-06-21 19:42:14,982 - INFO: Epoch: 119/200, Batch: 5/29, Batch_Loss_Train: 0.697
2024-06-21 19:42:15,281 - INFO: Epoch: 119/200, Batch: 6/29, Batch_Loss_Train: 0.462
2024-06-21 19:42:15,668 - INFO: Epoch: 119/200, Batch: 7/29, Batch_Loss_Train: 0.625
2024-06-21 19:42:15,968 - INFO: Epoch: 119/200, Batch: 8/29, Batch_Loss_Train: 0.522
2024-06-21 19:42:16,421 - INFO: Epoch: 119/200, Batch: 9/29, Batch_Loss_Train: 0.513
2024-06-21 19:42:16,713 - INFO: Epoch: 119/200, Batch: 10/29, Batch_Loss_Train: 0.597
2024-06-21 19:42:17,090 - INFO: Epoch: 119/200, Batch: 11/29, Batch_Loss_Train: 0.806
2024-06-21 19:42:17,394 - INFO: Epoch: 119/200, Batch: 12/29, Batch_Loss_Train: 0.590
2024-06-21 19:42:17,840 - INFO: Epoch: 119/200, Batch: 13/29, Batch_Loss_Train: 0.615
2024-06-21 19:42:18,142 - INFO: Epoch: 119/200, Batch: 14/29, Batch_Loss_Train: 0.530
2024-06-21 19:42:18,539 - INFO: Epoch: 119/200, Batch: 15/29, Batch_Loss_Train: 0.565
2024-06-21 19:42:18,838 - INFO: Epoch: 119/200, Batch: 16/29, Batch_Loss_Train: 0.534
2024-06-21 19:42:19,279 - INFO: Epoch: 119/200, Batch: 17/29, Batch_Loss_Train: 0.632
2024-06-21 19:42:19,578 - INFO: Epoch: 119/200, Batch: 18/29, Batch_Loss_Train: 0.464
2024-06-21 19:42:19,965 - INFO: Epoch: 119/200, Batch: 19/29, Batch_Loss_Train: 0.640
2024-06-21 19:42:20,259 - INFO: Epoch: 119/200, Batch: 20/29, Batch_Loss_Train: 0.548
2024-06-21 19:42:20,693 - INFO: Epoch: 119/200, Batch: 21/29, Batch_Loss_Train: 0.445
2024-06-21 19:42:20,994 - INFO: Epoch: 119/200, Batch: 22/29, Batch_Loss_Train: 0.738
2024-06-21 19:42:21,380 - INFO: Epoch: 119/200, Batch: 23/29, Batch_Loss_Train: 0.527
2024-06-21 19:42:21,682 - INFO: Epoch: 119/200, Batch: 24/29, Batch_Loss_Train: 0.586
2024-06-21 19:42:22,109 - INFO: Epoch: 119/200, Batch: 25/29, Batch_Loss_Train: 0.677
2024-06-21 19:42:22,406 - INFO: Epoch: 119/200, Batch: 26/29, Batch_Loss_Train: 0.567
2024-06-21 19:42:22,789 - INFO: Epoch: 119/200, Batch: 27/29, Batch_Loss_Train: 0.557
2024-06-21 19:42:23,086 - INFO: Epoch: 119/200, Batch: 28/29, Batch_Loss_Train: 0.560
2024-06-21 19:42:23,299 - INFO: Epoch: 119/200, Batch: 29/29, Batch_Loss_Train: 0.606
2024-06-21 19:42:34,437 - INFO: 119/200 final results:
2024-06-21 19:42:34,438 - INFO: Training loss: 0.578.
2024-06-21 19:42:34,438 - INFO: Training MAE: 0.578.
2024-06-21 19:42:34,438 - INFO: Training MSE: 0.850.
2024-06-21 19:42:54,439 - INFO: Epoch: 119/200, Loss_train: 0.5784053155060472, Loss_val: 1.793052599347871
2024-06-21 19:42:54,457 - INFO: Saved new best metric model for epoch 119.
2024-06-21 19:42:54,457 - INFO: Best internal validation val_loss: 1.793 at epoch: 119.
2024-06-21 19:42:54,458 - INFO: Epoch 120/200...
2024-06-21 19:42:54,458 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:42:54,458 - INFO: Batch size: 32.
2024-06-21 19:42:54,462 - INFO: Dataset:
2024-06-21 19:42:54,462 - INFO: Batch size:
2024-06-21 19:42:54,462 - INFO: Number of workers:
2024-06-21 19:42:55,642 - INFO: Epoch: 120/200, Batch: 1/29, Batch_Loss_Train: 0.517
2024-06-21 19:42:55,959 - INFO: Epoch: 120/200, Batch: 2/29, Batch_Loss_Train: 0.522
2024-06-21 19:42:56,354 - INFO: Epoch: 120/200, Batch: 3/29, Batch_Loss_Train: 0.570
2024-06-21 19:42:56,669 - INFO: Epoch: 120/200, Batch: 4/29, Batch_Loss_Train: 0.485
2024-06-21 19:42:57,093 - INFO: Epoch: 120/200, Batch: 5/29, Batch_Loss_Train: 0.744
2024-06-21 19:42:57,393 - INFO: Epoch: 120/200, Batch: 6/29, Batch_Loss_Train: 0.629
2024-06-21 19:42:57,781 - INFO: Epoch: 120/200, Batch: 7/29, Batch_Loss_Train: 0.635
2024-06-21 19:42:58,095 - INFO: Epoch: 120/200, Batch: 8/29, Batch_Loss_Train: 0.618
2024-06-21 19:42:58,513 - INFO: Epoch: 120/200, Batch: 9/29, Batch_Loss_Train: 0.618
2024-06-21 19:42:58,804 - INFO: Epoch: 120/200, Batch: 10/29, Batch_Loss_Train: 0.556
2024-06-21 19:42:59,169 - INFO: Epoch: 120/200, Batch: 11/29, Batch_Loss_Train: 0.493
2024-06-21 19:42:59,486 - INFO: Epoch: 120/200, Batch: 12/29, Batch_Loss_Train: 0.681
2024-06-21 19:42:59,919 - INFO: Epoch: 120/200, Batch: 13/29, Batch_Loss_Train: 0.459
2024-06-21 19:43:00,223 - INFO: Epoch: 120/200, Batch: 14/29, Batch_Loss_Train: 0.558
2024-06-21 19:43:00,608 - INFO: Epoch: 120/200, Batch: 15/29, Batch_Loss_Train: 0.575
2024-06-21 19:43:00,922 - INFO: Epoch: 120/200, Batch: 16/29, Batch_Loss_Train: 0.523
2024-06-21 19:43:01,350 - INFO: Epoch: 120/200, Batch: 17/29, Batch_Loss_Train: 0.462
2024-06-21 19:43:01,652 - INFO: Epoch: 120/200, Batch: 18/29, Batch_Loss_Train: 0.524
2024-06-21 19:43:02,026 - INFO: Epoch: 120/200, Batch: 19/29, Batch_Loss_Train: 0.651
2024-06-21 19:43:02,334 - INFO: Epoch: 120/200, Batch: 20/29, Batch_Loss_Train: 0.565
2024-06-21 19:43:02,750 - INFO: Epoch: 120/200, Batch: 21/29, Batch_Loss_Train: 0.519
2024-06-21 19:43:03,052 - INFO: Epoch: 120/200, Batch: 22/29, Batch_Loss_Train: 0.653
2024-06-21 19:43:03,426 - INFO: Epoch: 120/200, Batch: 23/29, Batch_Loss_Train: 0.570
2024-06-21 19:43:03,742 - INFO: Epoch: 120/200, Batch: 24/29, Batch_Loss_Train: 0.808
2024-06-21 19:43:04,155 - INFO: Epoch: 120/200, Batch: 25/29, Batch_Loss_Train: 0.506
2024-06-21 19:43:04,453 - INFO: Epoch: 120/200, Batch: 26/29, Batch_Loss_Train: 0.593
2024-06-21 19:43:04,820 - INFO: Epoch: 120/200, Batch: 27/29, Batch_Loss_Train: 0.536
2024-06-21 19:43:05,131 - INFO: Epoch: 120/200, Batch: 28/29, Batch_Loss_Train: 0.522
2024-06-21 19:43:05,343 - INFO: Epoch: 120/200, Batch: 29/29, Batch_Loss_Train: 0.516
2024-06-21 19:43:16,502 - INFO: 120/200 final results:
2024-06-21 19:43:16,502 - INFO: Training loss: 0.573.
2024-06-21 19:43:16,502 - INFO: Training MAE: 0.574.
2024-06-21 19:43:16,502 - INFO: Training MSE: 0.837.
2024-06-21 19:43:37,218 - INFO: Epoch: 120/200, Loss_train: 0.5725807167332748, Loss_val: 1.7963524440239216
2024-06-21 19:43:37,218 - INFO: Best internal validation val_loss: 1.793 at epoch: 119.
2024-06-21 19:43:37,218 - INFO: Epoch 121/200...
2024-06-21 19:43:37,218 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:43:37,218 - INFO: Batch size: 32.
2024-06-21 19:43:37,222 - INFO: Dataset:
2024-06-21 19:43:37,222 - INFO: Batch size:
2024-06-21 19:43:37,222 - INFO: Number of workers:
2024-06-21 19:43:38,376 - INFO: Epoch: 121/200, Batch: 1/29, Batch_Loss_Train: 0.568
2024-06-21 19:43:38,709 - INFO: Epoch: 121/200, Batch: 2/29, Batch_Loss_Train: 0.441
2024-06-21 19:43:39,106 - INFO: Epoch: 121/200, Batch: 3/29, Batch_Loss_Train: 0.518
2024-06-21 19:43:39,425 - INFO: Epoch: 121/200, Batch: 4/29, Batch_Loss_Train: 0.735
2024-06-21 19:43:39,814 - INFO: Epoch: 121/200, Batch: 5/29, Batch_Loss_Train: 0.604
2024-06-21 19:43:40,139 - INFO: Epoch: 121/200, Batch: 6/29, Batch_Loss_Train: 0.452
2024-06-21 19:43:40,527 - INFO: Epoch: 121/200, Batch: 7/29, Batch_Loss_Train: 0.768
2024-06-21 19:43:40,844 - INFO: Epoch: 121/200, Batch: 8/29, Batch_Loss_Train: 0.550
2024-06-21 19:43:41,222 - INFO: Epoch: 121/200, Batch: 9/29, Batch_Loss_Train: 0.693
2024-06-21 19:43:41,556 - INFO: Epoch: 121/200, Batch: 10/29, Batch_Loss_Train: 0.639
2024-06-21 19:43:41,933 - INFO: Epoch: 121/200, Batch: 11/29, Batch_Loss_Train: 0.534
2024-06-21 19:43:42,249 - INFO: Epoch: 121/200, Batch: 12/29, Batch_Loss_Train: 0.634
2024-06-21 19:43:42,654 - INFO: Epoch: 121/200, Batch: 13/29, Batch_Loss_Train: 0.622
2024-06-21 19:43:42,983 - INFO: Epoch: 121/200, Batch: 14/29, Batch_Loss_Train: 0.577
2024-06-21 19:43:43,377 - INFO: Epoch: 121/200, Batch: 15/29, Batch_Loss_Train: 0.596
2024-06-21 19:43:43,690 - INFO: Epoch: 121/200, Batch: 16/29, Batch_Loss_Train: 0.523
2024-06-21 19:43:44,089 - INFO: Epoch: 121/200, Batch: 17/29, Batch_Loss_Train: 0.483
2024-06-21 19:43:44,415 - INFO: Epoch: 121/200, Batch: 18/29, Batch_Loss_Train: 0.535
2024-06-21 19:43:44,800 - INFO: Epoch: 121/200, Batch: 19/29, Batch_Loss_Train: 0.554
2024-06-21 19:43:45,108 - INFO: Epoch: 121/200, Batch: 20/29, Batch_Loss_Train: 0.474
2024-06-21 19:43:45,502 - INFO: Epoch: 121/200, Batch: 21/29, Batch_Loss_Train: 0.511
2024-06-21 19:43:45,829 - INFO: Epoch: 121/200, Batch: 22/29, Batch_Loss_Train: 0.581
2024-06-21 19:43:46,210 - INFO: Epoch: 121/200, Batch: 23/29, Batch_Loss_Train: 0.476
2024-06-21 19:43:46,525 - INFO: Epoch: 121/200, Batch: 24/29, Batch_Loss_Train: 0.528
2024-06-21 19:43:46,916 - INFO: Epoch: 121/200, Batch: 25/29, Batch_Loss_Train: 0.594
2024-06-21 19:43:47,239 - INFO: Epoch: 121/200, Batch: 26/29, Batch_Loss_Train: 0.478
2024-06-21 19:43:47,617 - INFO: Epoch: 121/200, Batch: 27/29, Batch_Loss_Train: 0.518
2024-06-21 19:43:47,927 - INFO: Epoch: 121/200, Batch: 28/29, Batch_Loss_Train: 0.576
2024-06-21 19:43:48,147 - INFO: Epoch: 121/200, Batch: 29/29, Batch_Loss_Train: 0.630
2024-06-21 19:43:59,331 - INFO: 121/200 final results:
2024-06-21 19:43:59,332 - INFO: Training loss: 0.565.
2024-06-21 19:43:59,332 - INFO: Training MAE: 0.564.
2024-06-21 19:43:59,332 - INFO: Training MSE: 0.831.
2024-06-21 19:44:19,544 - INFO: Epoch: 121/200, Loss_train: 0.565230721029742, Loss_val: 1.8320686652742584
2024-06-21 19:44:19,544 - INFO: Best internal validation val_loss: 1.793 at epoch: 119.
2024-06-21 19:44:19,544 - INFO: Epoch 122/200...
2024-06-21 19:44:19,544 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:44:19,544 - INFO: Batch size: 32.
2024-06-21 19:44:19,548 - INFO: Dataset:
2024-06-21 19:44:19,548 - INFO: Batch size:
2024-06-21 19:44:19,548 - INFO: Number of workers:
2024-06-21 19:44:20,709 - INFO: Epoch: 122/200, Batch: 1/29, Batch_Loss_Train: 0.420
2024-06-21 19:44:21,041 - INFO: Epoch: 122/200, Batch: 2/29, Batch_Loss_Train: 0.447
2024-06-21 19:44:21,436 - INFO: Epoch: 122/200, Batch: 3/29, Batch_Loss_Train: 0.547
2024-06-21 19:44:21,753 - INFO: Epoch: 122/200, Batch: 4/29, Batch_Loss_Train: 0.416
2024-06-21 19:44:22,155 - INFO: Epoch: 122/200, Batch: 5/29, Batch_Loss_Train: 0.552
2024-06-21 19:44:22,480 - INFO: Epoch: 122/200, Batch: 6/29, Batch_Loss_Train: 0.592
2024-06-21 19:44:22,867 - INFO: Epoch: 122/200, Batch: 7/29, Batch_Loss_Train: 0.577
2024-06-21 19:44:23,180 - INFO: Epoch: 122/200, Batch: 8/29, Batch_Loss_Train: 0.581
2024-06-21 19:44:23,570 - INFO: Epoch: 122/200, Batch: 9/29, Batch_Loss_Train: 0.549
2024-06-21 19:44:23,899 - INFO: Epoch: 122/200, Batch: 10/29, Batch_Loss_Train: 0.532
2024-06-21 19:44:24,274 - INFO: Epoch: 122/200, Batch: 11/29, Batch_Loss_Train: 0.495
2024-06-21 19:44:24,590 - INFO: Epoch: 122/200, Batch: 12/29, Batch_Loss_Train: 0.451
2024-06-21 19:44:24,995 - INFO: Epoch: 122/200, Batch: 13/29, Batch_Loss_Train: 0.496
2024-06-21 19:44:25,323 - INFO: Epoch: 122/200, Batch: 14/29, Batch_Loss_Train: 0.503
2024-06-21 19:44:25,718 - INFO: Epoch: 122/200, Batch: 15/29, Batch_Loss_Train: 0.669
2024-06-21 19:44:26,030 - INFO: Epoch: 122/200, Batch: 16/29, Batch_Loss_Train: 0.659
2024-06-21 19:44:26,430 - INFO: Epoch: 122/200, Batch: 17/29, Batch_Loss_Train: 0.824
2024-06-21 19:44:26,753 - INFO: Epoch: 122/200, Batch: 18/29, Batch_Loss_Train: 0.532
2024-06-21 19:44:27,135 - INFO: Epoch: 122/200, Batch: 19/29, Batch_Loss_Train: 0.653
2024-06-21 19:44:27,440 - INFO: Epoch: 122/200, Batch: 20/29, Batch_Loss_Train: 0.503
2024-06-21 19:44:27,830 - INFO: Epoch: 122/200, Batch: 21/29, Batch_Loss_Train: 0.520
2024-06-21 19:44:28,156 - INFO: Epoch: 122/200, Batch: 22/29, Batch_Loss_Train: 0.533
2024-06-21 19:44:28,537 - INFO: Epoch: 122/200, Batch: 23/29, Batch_Loss_Train: 0.472
2024-06-21 19:44:28,848 - INFO: Epoch: 122/200, Batch: 24/29, Batch_Loss_Train: 0.526
2024-06-21 19:44:29,241 - INFO: Epoch: 122/200, Batch: 25/29, Batch_Loss_Train: 0.563
2024-06-21 19:44:29,560 - INFO: Epoch: 122/200, Batch: 26/29, Batch_Loss_Train: 0.464
2024-06-21 19:44:29,942 - INFO: Epoch: 122/200, Batch: 27/29, Batch_Loss_Train: 0.603
2024-06-21 19:44:30,249 - INFO: Epoch: 122/200, Batch: 28/29, Batch_Loss_Train: 0.596
2024-06-21 19:44:30,458 - INFO: Epoch: 122/200, Batch: 29/29, Batch_Loss_Train: 0.500
2024-06-21 19:44:41,485 - INFO: 122/200 final results:
2024-06-21 19:44:41,486 - INFO: Training loss: 0.544.
2024-06-21 19:44:41,486 - INFO: Training MAE: 0.545.
2024-06-21 19:44:41,486 - INFO: Training MSE: 0.774.
2024-06-21 19:45:01,940 - INFO: Epoch: 122/200, Loss_train: 0.5440553940575699, Loss_val: 1.8028968531509926
2024-06-21 19:45:01,940 - INFO: Best internal validation val_loss: 1.793 at epoch: 119.
2024-06-21 19:45:01,940 - INFO: Epoch 123/200...
2024-06-21 19:45:01,940 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:45:01,940 - INFO: Batch size: 32.
2024-06-21 19:45:01,944 - INFO: Dataset:
2024-06-21 19:45:01,944 - INFO: Batch size:
2024-06-21 19:45:01,945 - INFO: Number of workers:
2024-06-21 19:45:03,104 - INFO: Epoch: 123/200, Batch: 1/29, Batch_Loss_Train: 0.587
2024-06-21 19:45:03,411 - INFO: Epoch: 123/200, Batch: 2/29, Batch_Loss_Train: 0.653
2024-06-21 19:45:03,810 - INFO: Epoch: 123/200, Batch: 3/29, Batch_Loss_Train: 0.552
2024-06-21 19:45:04,129 - INFO: Epoch: 123/200, Batch: 4/29, Batch_Loss_Train: 0.453
2024-06-21 19:45:04,530 - INFO: Epoch: 123/200, Batch: 5/29, Batch_Loss_Train: 0.510
2024-06-21 19:45:04,829 - INFO: Epoch: 123/200, Batch: 6/29, Batch_Loss_Train: 0.590
2024-06-21 19:45:05,240 - INFO: Epoch: 123/200, Batch: 7/29, Batch_Loss_Train: 0.530
2024-06-21 19:45:05,540 - INFO: Epoch: 123/200, Batch: 8/29, Batch_Loss_Train: 0.424
2024-06-21 19:45:05,945 - INFO: Epoch: 123/200, Batch: 9/29, Batch_Loss_Train: 0.506
2024-06-21 19:45:06,238 - INFO: Epoch: 123/200, Batch: 10/29, Batch_Loss_Train: 0.480
2024-06-21 19:45:06,638 - INFO: Epoch: 123/200, Batch: 11/29, Batch_Loss_Train: 0.532
2024-06-21 19:45:06,940 - INFO: Epoch: 123/200, Batch: 12/29, Batch_Loss_Train: 0.524
2024-06-21 19:45:07,356 - INFO: Epoch: 123/200, Batch: 13/29, Batch_Loss_Train: 0.688
2024-06-21 19:45:07,658 - INFO: Epoch: 123/200, Batch: 14/29, Batch_Loss_Train: 0.620
2024-06-21 19:45:08,073 - INFO: Epoch: 123/200, Batch: 15/29, Batch_Loss_Train: 0.614
2024-06-21 19:45:08,372 - INFO: Epoch: 123/200, Batch: 16/29, Batch_Loss_Train: 0.527
2024-06-21 19:45:08,793 - INFO: Epoch: 123/200, Batch: 17/29, Batch_Loss_Train: 0.714
2024-06-21 19:45:09,094 - INFO: Epoch: 123/200, Batch: 18/29, Batch_Loss_Train: 0.646
2024-06-21 19:45:09,502 - INFO: Epoch: 123/200, Batch: 19/29, Batch_Loss_Train: 0.573
2024-06-21 19:45:09,795 - INFO: Epoch: 123/200, Batch: 20/29, Batch_Loss_Train: 0.549
2024-06-21 19:45:10,203 - INFO: Epoch: 123/200, Batch: 21/29, Batch_Loss_Train: 0.590
2024-06-21 19:45:10,504 - INFO: Epoch: 123/200, Batch: 22/29, Batch_Loss_Train: 0.649
2024-06-21 19:45:10,914 - INFO: Epoch: 123/200, Batch: 23/29, Batch_Loss_Train: 0.590
2024-06-21 19:45:11,215 - INFO: Epoch: 123/200, Batch: 24/29, Batch_Loss_Train: 0.504
2024-06-21 19:45:11,617 - INFO: Epoch: 123/200, Batch: 25/29, Batch_Loss_Train: 0.515
2024-06-21 19:45:11,914 - INFO: Epoch: 123/200, Batch: 26/29, Batch_Loss_Train: 0.580
2024-06-21 19:45:12,310 - INFO: Epoch: 123/200, Batch: 27/29, Batch_Loss_Train: 0.511
2024-06-21 19:45:12,606 - INFO: Epoch: 123/200, Batch: 28/29, Batch_Loss_Train: 0.476
2024-06-21 19:45:12,825 - INFO: Epoch: 123/200, Batch: 29/29, Batch_Loss_Train: 0.744
2024-06-21 19:45:23,919 - INFO: 123/200 final results:
2024-06-21 19:45:23,919 - INFO: Training loss: 0.567.
2024-06-21 19:45:23,919 - INFO: Training MAE: 0.563.
2024-06-21 19:45:23,919 - INFO: Training MSE: 0.825.
2024-06-21 19:45:44,282 - INFO: Epoch: 123/200, Loss_train: 0.5666493382947199, Loss_val: 1.788669076459161
2024-06-21 19:45:44,301 - INFO: Saved new best metric model for epoch 123.
2024-06-21 19:45:44,302 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:45:44,302 - INFO: Epoch 124/200...
2024-06-21 19:45:44,302 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:45:44,302 - INFO: Batch size: 32.
2024-06-21 19:45:44,306 - INFO: Dataset:
2024-06-21 19:45:44,306 - INFO: Batch size:
2024-06-21 19:45:44,306 - INFO: Number of workers:
2024-06-21 19:45:45,485 - INFO: Epoch: 124/200, Batch: 1/29, Batch_Loss_Train: 0.536
2024-06-21 19:45:45,803 - INFO: Epoch: 124/200, Batch: 2/29, Batch_Loss_Train: 0.752
2024-06-21 19:45:46,199 - INFO: Epoch: 124/200, Batch: 3/29, Batch_Loss_Train: 0.540
2024-06-21 19:45:46,515 - INFO: Epoch: 124/200, Batch: 4/29, Batch_Loss_Train: 0.553
2024-06-21 19:45:46,934 - INFO: Epoch: 124/200, Batch: 5/29, Batch_Loss_Train: 0.662
2024-06-21 19:45:47,234 - INFO: Epoch: 124/200, Batch: 6/29, Batch_Loss_Train: 0.505
2024-06-21 19:45:47,608 - INFO: Epoch: 124/200, Batch: 7/29, Batch_Loss_Train: 0.592
2024-06-21 19:45:47,921 - INFO: Epoch: 124/200, Batch: 8/29, Batch_Loss_Train: 0.517
2024-06-21 19:45:48,350 - INFO: Epoch: 124/200, Batch: 9/29, Batch_Loss_Train: 0.545
2024-06-21 19:45:48,643 - INFO: Epoch: 124/200, Batch: 10/29, Batch_Loss_Train: 0.497
2024-06-21 19:45:49,013 - INFO: Epoch: 124/200, Batch: 11/29, Batch_Loss_Train: 0.500
2024-06-21 19:45:49,330 - INFO: Epoch: 124/200, Batch: 12/29, Batch_Loss_Train: 0.624
2024-06-21 19:45:49,763 - INFO: Epoch: 124/200, Batch: 13/29, Batch_Loss_Train: 0.503
2024-06-21 19:45:50,068 - INFO: Epoch: 124/200, Batch: 14/29, Batch_Loss_Train: 0.453
2024-06-21 19:45:50,458 - INFO: Epoch: 124/200, Batch: 15/29, Batch_Loss_Train: 0.441
2024-06-21 19:45:50,772 - INFO: Epoch: 124/200, Batch: 16/29, Batch_Loss_Train: 0.594
2024-06-21 19:45:51,199 - INFO: Epoch: 124/200, Batch: 17/29, Batch_Loss_Train: 0.770
2024-06-21 19:45:51,500 - INFO: Epoch: 124/200, Batch: 18/29, Batch_Loss_Train: 0.559
2024-06-21 19:45:51,875 - INFO: Epoch: 124/200, Batch: 19/29, Batch_Loss_Train: 0.523
2024-06-21 19:45:52,185 - INFO: Epoch: 124/200, Batch: 20/29, Batch_Loss_Train: 0.439
2024-06-21 19:45:52,603 - INFO: Epoch: 124/200, Batch: 21/29, Batch_Loss_Train: 0.683
2024-06-21 19:45:52,905 - INFO: Epoch: 124/200, Batch: 22/29, Batch_Loss_Train: 0.687
2024-06-21 19:45:53,292 - INFO: Epoch: 124/200, Batch: 23/29, Batch_Loss_Train: 0.442
2024-06-21 19:45:53,608 - INFO: Epoch: 124/200, Batch: 24/29, Batch_Loss_Train: 0.495
2024-06-21 19:45:54,030 - INFO: Epoch: 124/200, Batch: 25/29, Batch_Loss_Train: 0.570
2024-06-21 19:45:54,329 - INFO: Epoch: 124/200, Batch: 26/29, Batch_Loss_Train: 0.528
2024-06-21 19:45:54,715 - INFO: Epoch: 124/200, Batch: 27/29, Batch_Loss_Train: 0.590
2024-06-21 19:45:55,027 - INFO: Epoch: 124/200, Batch: 28/29, Batch_Loss_Train: 0.575
2024-06-21 19:45:55,249 - INFO: Epoch: 124/200, Batch: 29/29, Batch_Loss_Train: 0.657
2024-06-21 19:46:06,317 - INFO: 124/200 final results:
2024-06-21 19:46:06,317 - INFO: Training loss: 0.563.
2024-06-21 19:46:06,317 - INFO: Training MAE: 0.561.
2024-06-21 19:46:06,317 - INFO: Training MSE: 0.816.
2024-06-21 19:46:26,659 - INFO: Epoch: 124/200, Loss_train: 0.563128504259833, Loss_val: 1.7896762469719196
2024-06-21 19:46:26,659 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:46:26,659 - INFO: Epoch 125/200...
2024-06-21 19:46:26,659 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:46:26,659 - INFO: Batch size: 32.
2024-06-21 19:46:26,663 - INFO: Dataset:
2024-06-21 19:46:26,663 - INFO: Batch size:
2024-06-21 19:46:26,664 - INFO: Number of workers:
2024-06-21 19:46:27,830 - INFO: Epoch: 125/200, Batch: 1/29, Batch_Loss_Train: 0.469
2024-06-21 19:46:28,148 - INFO: Epoch: 125/200, Batch: 2/29, Batch_Loss_Train: 0.521
2024-06-21 19:46:28,554 - INFO: Epoch: 125/200, Batch: 3/29, Batch_Loss_Train: 0.493
2024-06-21 19:46:28,872 - INFO: Epoch: 125/200, Batch: 4/29, Batch_Loss_Train: 0.554
2024-06-21 19:46:29,280 - INFO: Epoch: 125/200, Batch: 5/29, Batch_Loss_Train: 0.455
2024-06-21 19:46:29,580 - INFO: Epoch: 125/200, Batch: 6/29, Batch_Loss_Train: 0.559
2024-06-21 19:46:29,978 - INFO: Epoch: 125/200, Batch: 7/29, Batch_Loss_Train: 0.625
2024-06-21 19:46:30,292 - INFO: Epoch: 125/200, Batch: 8/29, Batch_Loss_Train: 0.511
2024-06-21 19:46:30,693 - INFO: Epoch: 125/200, Batch: 9/29, Batch_Loss_Train: 0.499
2024-06-21 19:46:30,986 - INFO: Epoch: 125/200, Batch: 10/29, Batch_Loss_Train: 0.581
2024-06-21 19:46:31,373 - INFO: Epoch: 125/200, Batch: 11/29, Batch_Loss_Train: 0.463
2024-06-21 19:46:31,689 - INFO: Epoch: 125/200, Batch: 12/29, Batch_Loss_Train: 0.622
2024-06-21 19:46:32,108 - INFO: Epoch: 125/200, Batch: 13/29, Batch_Loss_Train: 0.567
2024-06-21 19:46:32,409 - INFO: Epoch: 125/200, Batch: 14/29, Batch_Loss_Train: 0.567
2024-06-21 19:46:32,817 - INFO: Epoch: 125/200, Batch: 15/29, Batch_Loss_Train: 0.540
2024-06-21 19:46:33,128 - INFO: Epoch: 125/200, Batch: 16/29, Batch_Loss_Train: 0.751
2024-06-21 19:46:33,536 - INFO: Epoch: 125/200, Batch: 17/29, Batch_Loss_Train: 0.539
2024-06-21 19:46:33,835 - INFO: Epoch: 125/200, Batch: 18/29, Batch_Loss_Train: 0.481
2024-06-21 19:46:34,231 - INFO: Epoch: 125/200, Batch: 19/29, Batch_Loss_Train: 0.609
2024-06-21 19:46:34,537 - INFO: Epoch: 125/200, Batch: 20/29, Batch_Loss_Train: 0.449
2024-06-21 19:46:34,931 - INFO: Epoch: 125/200, Batch: 21/29, Batch_Loss_Train: 0.522
2024-06-21 19:46:35,233 - INFO: Epoch: 125/200, Batch: 22/29, Batch_Loss_Train: 0.621
2024-06-21 19:46:35,625 - INFO: Epoch: 125/200, Batch: 23/29, Batch_Loss_Train: 0.471
2024-06-21 19:46:35,938 - INFO: Epoch: 125/200, Batch: 24/29, Batch_Loss_Train: 0.468
2024-06-21 19:46:36,344 - INFO: Epoch: 125/200, Batch: 25/29, Batch_Loss_Train: 0.677
2024-06-21 19:46:36,643 - INFO: Epoch: 125/200, Batch: 26/29, Batch_Loss_Train: 0.522
2024-06-21 19:46:37,035 - INFO: Epoch: 125/200, Batch: 27/29, Batch_Loss_Train: 0.639
2024-06-21 19:46:37,345 - INFO: Epoch: 125/200, Batch: 28/29, Batch_Loss_Train: 0.549
2024-06-21 19:46:37,556 - INFO: Epoch: 125/200, Batch: 29/29, Batch_Loss_Train: 0.721
2024-06-21 19:46:48,704 - INFO: 125/200 final results:
2024-06-21 19:46:48,704 - INFO: Training loss: 0.553.
2024-06-21 19:46:48,704 - INFO: Training MAE: 0.550.
2024-06-21 19:46:48,704 - INFO: Training MSE: 0.794.
2024-06-21 19:47:09,163 - INFO: Epoch: 125/200, Loss_train: 0.5533718137905516, Loss_val: 1.8414130498623025
2024-06-21 19:47:09,163 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:47:09,163 - INFO: Epoch 126/200...
2024-06-21 19:47:09,163 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:47:09,163 - INFO: Batch size: 32.
2024-06-21 19:47:09,167 - INFO: Dataset:
2024-06-21 19:47:09,167 - INFO: Batch size:
2024-06-21 19:47:09,167 - INFO: Number of workers:
2024-06-21 19:47:10,319 - INFO: Epoch: 126/200, Batch: 1/29, Batch_Loss_Train: 0.464
2024-06-21 19:47:10,650 - INFO: Epoch: 126/200, Batch: 2/29, Batch_Loss_Train: 0.511
2024-06-21 19:47:11,042 - INFO: Epoch: 126/200, Batch: 3/29, Batch_Loss_Train: 0.509
2024-06-21 19:47:11,357 - INFO: Epoch: 126/200, Batch: 4/29, Batch_Loss_Train: 0.557
2024-06-21 19:47:11,762 - INFO: Epoch: 126/200, Batch: 5/29, Batch_Loss_Train: 0.535
2024-06-21 19:47:12,072 - INFO: Epoch: 126/200, Batch: 6/29, Batch_Loss_Train: 0.619
2024-06-21 19:47:12,453 - INFO: Epoch: 126/200, Batch: 7/29, Batch_Loss_Train: 0.497
2024-06-21 19:47:12,764 - INFO: Epoch: 126/200, Batch: 8/29, Batch_Loss_Train: 0.641
2024-06-21 19:47:13,163 - INFO: Epoch: 126/200, Batch: 9/29, Batch_Loss_Train: 0.489
2024-06-21 19:47:13,467 - INFO: Epoch: 126/200, Batch: 10/29, Batch_Loss_Train: 0.601
2024-06-21 19:47:13,836 - INFO: Epoch: 126/200, Batch: 11/29, Batch_Loss_Train: 0.597
2024-06-21 19:47:14,154 - INFO: Epoch: 126/200, Batch: 12/29, Batch_Loss_Train: 0.492
2024-06-21 19:47:14,576 - INFO: Epoch: 126/200, Batch: 13/29, Batch_Loss_Train: 0.618
2024-06-21 19:47:14,892 - INFO: Epoch: 126/200, Batch: 14/29, Batch_Loss_Train: 0.554
2024-06-21 19:47:15,284 - INFO: Epoch: 126/200, Batch: 15/29, Batch_Loss_Train: 0.567
2024-06-21 19:47:15,597 - INFO: Epoch: 126/200, Batch: 16/29, Batch_Loss_Train: 0.763
2024-06-21 19:47:16,011 - INFO: Epoch: 126/200, Batch: 17/29, Batch_Loss_Train: 0.549
2024-06-21 19:47:16,325 - INFO: Epoch: 126/200, Batch: 18/29, Batch_Loss_Train: 0.658
2024-06-21 19:47:16,712 - INFO: Epoch: 126/200, Batch: 19/29, Batch_Loss_Train: 0.542
2024-06-21 19:47:17,021 - INFO: Epoch: 126/200, Batch: 20/29, Batch_Loss_Train: 0.627
2024-06-21 19:47:17,427 - INFO: Epoch: 126/200, Batch: 21/29, Batch_Loss_Train: 0.383
2024-06-21 19:47:17,743 - INFO: Epoch: 126/200, Batch: 22/29, Batch_Loss_Train: 0.569
2024-06-21 19:47:18,120 - INFO: Epoch: 126/200, Batch: 23/29, Batch_Loss_Train: 0.398
2024-06-21 19:47:18,436 - INFO: Epoch: 126/200, Batch: 24/29, Batch_Loss_Train: 0.536
2024-06-21 19:47:18,840 - INFO: Epoch: 126/200, Batch: 25/29, Batch_Loss_Train: 0.582
2024-06-21 19:47:19,151 - INFO: Epoch: 126/200, Batch: 26/29, Batch_Loss_Train: 0.543
2024-06-21 19:47:19,530 - INFO: Epoch: 126/200, Batch: 27/29, Batch_Loss_Train: 0.679
2024-06-21 19:47:19,842 - INFO: Epoch: 126/200, Batch: 28/29, Batch_Loss_Train: 0.464
2024-06-21 19:47:20,050 - INFO: Epoch: 126/200, Batch: 29/29, Batch_Loss_Train: 0.515
2024-06-21 19:47:31,166 - INFO: 126/200 final results:
2024-06-21 19:47:31,167 - INFO: Training loss: 0.554.
2024-06-21 19:47:31,167 - INFO: Training MAE: 0.554.
2024-06-21 19:47:31,167 - INFO: Training MSE: 0.809.
2024-06-21 19:47:51,347 - INFO: Epoch: 126/200, Loss_train: 0.5537131274568623, Loss_val: 1.8053302888212532
2024-06-21 19:47:51,348 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:47:51,348 - INFO: Epoch 127/200...
2024-06-21 19:47:51,348 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:47:51,348 - INFO: Batch size: 32.
2024-06-21 19:47:51,352 - INFO: Dataset:
2024-06-21 19:47:51,352 - INFO: Batch size:
2024-06-21 19:47:51,352 - INFO: Number of workers:
2024-06-21 19:47:52,535 - INFO: Epoch: 127/200, Batch: 1/29, Batch_Loss_Train: 0.549
2024-06-21 19:47:52,839 - INFO: Epoch: 127/200, Batch: 2/29, Batch_Loss_Train: 0.541
2024-06-21 19:47:53,238 - INFO: Epoch: 127/200, Batch: 3/29, Batch_Loss_Train: 0.691
2024-06-21 19:47:53,558 - INFO: Epoch: 127/200, Batch: 4/29, Batch_Loss_Train: 0.477
2024-06-21 19:47:53,983 - INFO: Epoch: 127/200, Batch: 5/29, Batch_Loss_Train: 0.635
2024-06-21 19:47:54,282 - INFO: Epoch: 127/200, Batch: 6/29, Batch_Loss_Train: 0.576
2024-06-21 19:47:54,667 - INFO: Epoch: 127/200, Batch: 7/29, Batch_Loss_Train: 0.515
2024-06-21 19:47:54,980 - INFO: Epoch: 127/200, Batch: 8/29, Batch_Loss_Train: 0.513
2024-06-21 19:47:55,408 - INFO: Epoch: 127/200, Batch: 9/29, Batch_Loss_Train: 0.520
2024-06-21 19:47:55,699 - INFO: Epoch: 127/200, Batch: 10/29, Batch_Loss_Train: 0.547
2024-06-21 19:47:56,068 - INFO: Epoch: 127/200, Batch: 11/29, Batch_Loss_Train: 0.542
2024-06-21 19:47:56,383 - INFO: Epoch: 127/200, Batch: 12/29, Batch_Loss_Train: 0.731
2024-06-21 19:47:56,816 - INFO: Epoch: 127/200, Batch: 13/29, Batch_Loss_Train: 0.423
2024-06-21 19:47:57,121 - INFO: Epoch: 127/200, Batch: 14/29, Batch_Loss_Train: 0.558
2024-06-21 19:47:57,507 - INFO: Epoch: 127/200, Batch: 15/29, Batch_Loss_Train: 0.504
2024-06-21 19:47:57,820 - INFO: Epoch: 127/200, Batch: 16/29, Batch_Loss_Train: 0.544
2024-06-21 19:47:58,244 - INFO: Epoch: 127/200, Batch: 17/29, Batch_Loss_Train: 0.586
2024-06-21 19:47:58,543 - INFO: Epoch: 127/200, Batch: 18/29, Batch_Loss_Train: 0.505
2024-06-21 19:47:58,914 - INFO: Epoch: 127/200, Batch: 19/29, Batch_Loss_Train: 0.836
2024-06-21 19:47:59,224 - INFO: Epoch: 127/200, Batch: 20/29, Batch_Loss_Train: 0.640
2024-06-21 19:47:59,645 - INFO: Epoch: 127/200, Batch: 21/29, Batch_Loss_Train: 0.640
2024-06-21 19:47:59,948 - INFO: Epoch: 127/200, Batch: 22/29, Batch_Loss_Train: 0.504
2024-06-21 19:48:00,326 - INFO: Epoch: 127/200, Batch: 23/29, Batch_Loss_Train: 0.504
2024-06-21 19:48:00,641 - INFO: Epoch: 127/200, Batch: 24/29, Batch_Loss_Train: 0.461
2024-06-21 19:48:01,363 - INFO: Epoch: 127/200, Batch: 25/29, Batch_Loss_Train: 0.663
2024-06-21 19:48:01,662 - INFO: Epoch: 127/200, Batch: 26/29, Batch_Loss_Train: 0.515
2024-06-21 19:48:02,040 - INFO: Epoch: 127/200, Batch: 27/29, Batch_Loss_Train: 0.540
2024-06-21 19:48:02,352 - INFO: Epoch: 127/200, Batch: 28/29, Batch_Loss_Train: 0.595
2024-06-21 19:48:02,568 - INFO: Epoch: 127/200, Batch: 29/29, Batch_Loss_Train: 0.531
2024-06-21 19:48:13,468 - INFO: 127/200 final results:
2024-06-21 19:48:13,468 - INFO: Training loss: 0.565.
2024-06-21 19:48:13,468 - INFO: Training MAE: 0.566.
2024-06-21 19:48:13,468 - INFO: Training MSE: 0.818.
2024-06-21 19:48:33,910 - INFO: Epoch: 127/200, Loss_train: 0.5650854686210895, Loss_val: 1.8047575745089302
2024-06-21 19:48:33,911 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:48:33,911 - INFO: Epoch 128/200...
2024-06-21 19:48:33,911 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:48:33,911 - INFO: Batch size: 32.
2024-06-21 19:48:33,915 - INFO: Dataset:
2024-06-21 19:48:33,915 - INFO: Batch size:
2024-06-21 19:48:33,915 - INFO: Number of workers:
2024-06-21 19:48:35,099 - INFO: Epoch: 128/200, Batch: 1/29, Batch_Loss_Train: 0.517
2024-06-21 19:48:35,407 - INFO: Epoch: 128/200, Batch: 2/29, Batch_Loss_Train: 0.531
2024-06-21 19:48:35,818 - INFO: Epoch: 128/200, Batch: 3/29, Batch_Loss_Train: 0.564
2024-06-21 19:48:36,139 - INFO: Epoch: 128/200, Batch: 4/29, Batch_Loss_Train: 0.641
2024-06-21 19:48:36,569 - INFO: Epoch: 128/200, Batch: 5/29, Batch_Loss_Train: 0.450
2024-06-21 19:48:36,871 - INFO: Epoch: 128/200, Batch: 6/29, Batch_Loss_Train: 0.563
2024-06-21 19:48:37,261 - INFO: Epoch: 128/200, Batch: 7/29, Batch_Loss_Train: 0.598
2024-06-21 19:48:37,577 - INFO: Epoch: 128/200, Batch: 8/29, Batch_Loss_Train: 0.795
2024-06-21 19:48:38,003 - INFO: Epoch: 128/200, Batch: 9/29, Batch_Loss_Train: 0.478
2024-06-21 19:48:38,297 - INFO: Epoch: 128/200, Batch: 10/29, Batch_Loss_Train: 0.597
2024-06-21 19:48:38,671 - INFO: Epoch: 128/200, Batch: 11/29, Batch_Loss_Train: 0.539
2024-06-21 19:48:38,990 - INFO: Epoch: 128/200, Batch: 12/29, Batch_Loss_Train: 0.444
2024-06-21 19:48:39,416 - INFO: Epoch: 128/200, Batch: 13/29, Batch_Loss_Train: 0.994
2024-06-21 19:48:39,718 - INFO: Epoch: 128/200, Batch: 14/29, Batch_Loss_Train: 0.590
2024-06-21 19:48:40,105 - INFO: Epoch: 128/200, Batch: 15/29, Batch_Loss_Train: 0.555
2024-06-21 19:48:40,417 - INFO: Epoch: 128/200, Batch: 16/29, Batch_Loss_Train: 0.669
2024-06-21 19:48:40,836 - INFO: Epoch: 128/200, Batch: 17/29, Batch_Loss_Train: 0.524
2024-06-21 19:48:41,134 - INFO: Epoch: 128/200, Batch: 18/29, Batch_Loss_Train: 0.600
2024-06-21 19:48:41,515 - INFO: Epoch: 128/200, Batch: 19/29, Batch_Loss_Train: 0.463
2024-06-21 19:48:41,821 - INFO: Epoch: 128/200, Batch: 20/29, Batch_Loss_Train: 0.437
2024-06-21 19:48:42,230 - INFO: Epoch: 128/200, Batch: 21/29, Batch_Loss_Train: 0.450
2024-06-21 19:48:42,531 - INFO: Epoch: 128/200, Batch: 22/29, Batch_Loss_Train: 0.611
2024-06-21 19:48:42,913 - INFO: Epoch: 128/200, Batch: 23/29, Batch_Loss_Train: 0.521
2024-06-21 19:48:43,226 - INFO: Epoch: 128/200, Batch: 24/29, Batch_Loss_Train: 0.473
2024-06-21 19:48:43,648 - INFO: Epoch: 128/200, Batch: 25/29, Batch_Loss_Train: 0.533
2024-06-21 19:48:43,946 - INFO: Epoch: 128/200, Batch: 26/29, Batch_Loss_Train: 0.568
2024-06-21 19:48:44,319 - INFO: Epoch: 128/200, Batch: 27/29, Batch_Loss_Train: 0.591
2024-06-21 19:48:44,630 - INFO: Epoch: 128/200, Batch: 28/29, Batch_Loss_Train: 0.516
2024-06-21 19:48:44,845 - INFO: Epoch: 128/200, Batch: 29/29, Batch_Loss_Train: 0.508
2024-06-21 19:48:56,026 - INFO: 128/200 final results:
2024-06-21 19:48:56,026 - INFO: Training loss: 0.563.
2024-06-21 19:48:56,026 - INFO: Training MAE: 0.564.
2024-06-21 19:48:56,026 - INFO: Training MSE: 0.826.
2024-06-21 19:49:16,815 - INFO: Epoch: 128/200, Loss_train: 0.5626678096837011, Loss_val: 1.797793281489405
2024-06-21 19:49:16,815 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:49:16,815 - INFO: Epoch 129/200...
2024-06-21 19:49:16,815 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:49:16,815 - INFO: Batch size: 32.
2024-06-21 19:49:16,819 - INFO: Dataset:
2024-06-21 19:49:16,819 - INFO: Batch size:
2024-06-21 19:49:16,819 - INFO: Number of workers:
2024-06-21 19:49:17,997 - INFO: Epoch: 129/200, Batch: 1/29, Batch_Loss_Train: 0.719
2024-06-21 19:49:18,317 - INFO: Epoch: 129/200, Batch: 2/29, Batch_Loss_Train: 0.535
2024-06-21 19:49:18,707 - INFO: Epoch: 129/200, Batch: 3/29, Batch_Loss_Train: 0.522
2024-06-21 19:49:19,025 - INFO: Epoch: 129/200, Batch: 4/29, Batch_Loss_Train: 0.597
2024-06-21 19:49:19,427 - INFO: Epoch: 129/200, Batch: 5/29, Batch_Loss_Train: 0.422
2024-06-21 19:49:19,752 - INFO: Epoch: 129/200, Batch: 6/29, Batch_Loss_Train: 0.496
2024-06-21 19:49:20,128 - INFO: Epoch: 129/200, Batch: 7/29, Batch_Loss_Train: 0.527
2024-06-21 19:49:20,443 - INFO: Epoch: 129/200, Batch: 8/29, Batch_Loss_Train: 0.551
2024-06-21 19:49:20,836 - INFO: Epoch: 129/200, Batch: 9/29, Batch_Loss_Train: 0.473
2024-06-21 19:49:21,173 - INFO: Epoch: 129/200, Batch: 10/29, Batch_Loss_Train: 0.478
2024-06-21 19:49:21,535 - INFO: Epoch: 129/200, Batch: 11/29, Batch_Loss_Train: 0.530
2024-06-21 19:49:21,852 - INFO: Epoch: 129/200, Batch: 12/29, Batch_Loss_Train: 0.571
2024-06-21 19:49:22,259 - INFO: Epoch: 129/200, Batch: 13/29, Batch_Loss_Train: 0.579
2024-06-21 19:49:22,587 - INFO: Epoch: 129/200, Batch: 14/29, Batch_Loss_Train: 0.569
2024-06-21 19:49:22,970 - INFO: Epoch: 129/200, Batch: 15/29, Batch_Loss_Train: 0.518
2024-06-21 19:49:23,282 - INFO: Epoch: 129/200, Batch: 16/29, Batch_Loss_Train: 0.620
2024-06-21 19:49:23,685 - INFO: Epoch: 129/200, Batch: 17/29, Batch_Loss_Train: 0.595
2024-06-21 19:49:24,010 - INFO: Epoch: 129/200, Batch: 18/29, Batch_Loss_Train: 0.474
2024-06-21 19:49:24,380 - INFO: Epoch: 129/200, Batch: 19/29, Batch_Loss_Train: 0.643
2024-06-21 19:49:24,688 - INFO: Epoch: 129/200, Batch: 20/29, Batch_Loss_Train: 0.669
2024-06-21 19:49:25,078 - INFO: Epoch: 129/200, Batch: 21/29, Batch_Loss_Train: 0.682
2024-06-21 19:49:25,405 - INFO: Epoch: 129/200, Batch: 22/29, Batch_Loss_Train: 0.544
2024-06-21 19:49:25,778 - INFO: Epoch: 129/200, Batch: 23/29, Batch_Loss_Train: 0.598
2024-06-21 19:49:26,091 - INFO: Epoch: 129/200, Batch: 24/29, Batch_Loss_Train: 0.610
2024-06-21 19:49:26,477 - INFO: Epoch: 129/200, Batch: 25/29, Batch_Loss_Train: 0.703
2024-06-21 19:49:26,799 - INFO: Epoch: 129/200, Batch: 26/29, Batch_Loss_Train: 0.531
2024-06-21 19:49:27,167 - INFO: Epoch: 129/200, Batch: 27/29, Batch_Loss_Train: 0.503
2024-06-21 19:49:27,478 - INFO: Epoch: 129/200, Batch: 28/29, Batch_Loss_Train: 0.569
2024-06-21 19:49:27,699 - INFO: Epoch: 129/200, Batch: 29/29, Batch_Loss_Train: 0.446
2024-06-21 19:49:38,875 - INFO: 129/200 final results:
2024-06-21 19:49:38,875 - INFO: Training loss: 0.561.
2024-06-21 19:49:38,875 - INFO: Training MAE: 0.563.
2024-06-21 19:49:38,875 - INFO: Training MSE: 0.808.
2024-06-21 19:49:59,248 - INFO: Epoch: 129/200, Loss_train: 0.5611656719240649, Loss_val: 1.8041020755110115
2024-06-21 19:49:59,248 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:49:59,249 - INFO: Epoch 130/200...
2024-06-21 19:49:59,249 - INFO: Learning rate: 6.820822919002041e-06.
2024-06-21 19:49:59,249 - INFO: Batch size: 32.
2024-06-21 19:49:59,253 - INFO: Dataset:
2024-06-21 19:49:59,253 - INFO: Batch size:
2024-06-21 19:49:59,253 - INFO: Number of workers:
2024-06-21 19:50:00,404 - INFO: Epoch: 130/200, Batch: 1/29, Batch_Loss_Train: 0.535
2024-06-21 19:50:00,723 - INFO: Epoch: 130/200, Batch: 2/29, Batch_Loss_Train: 0.785
2024-06-21 19:50:01,119 - INFO: Epoch: 130/200, Batch: 3/29, Batch_Loss_Train: 0.522
2024-06-21 19:50:01,434 - INFO: Epoch: 130/200, Batch: 4/29, Batch_Loss_Train: 0.503
2024-06-21 19:50:01,849 - INFO: Epoch: 130/200, Batch: 5/29, Batch_Loss_Train: 0.518
2024-06-21 19:50:02,149 - INFO: Epoch: 130/200, Batch: 6/29, Batch_Loss_Train: 0.660
2024-06-21 19:50:02,539 - INFO: Epoch: 130/200, Batch: 7/29, Batch_Loss_Train: 0.455
2024-06-21 19:50:02,854 - INFO: Epoch: 130/200, Batch: 8/29, Batch_Loss_Train: 0.603
2024-06-21 19:50:03,269 - INFO: Epoch: 130/200, Batch: 9/29, Batch_Loss_Train: 0.561
2024-06-21 19:50:03,564 - INFO: Epoch: 130/200, Batch: 10/29, Batch_Loss_Train: 0.607
2024-06-21 19:50:03,957 - INFO: Epoch: 130/200, Batch: 11/29, Batch_Loss_Train: 0.464
2024-06-21 19:50:04,274 - INFO: Epoch: 130/200, Batch: 12/29, Batch_Loss_Train: 0.478
2024-06-21 19:50:04,687 - INFO: Epoch: 130/200, Batch: 13/29, Batch_Loss_Train: 0.608
2024-06-21 19:50:04,991 - INFO: Epoch: 130/200, Batch: 14/29, Batch_Loss_Train: 0.599
2024-06-21 19:50:05,402 - INFO: Epoch: 130/200, Batch: 15/29, Batch_Loss_Train: 0.528
2024-06-21 19:50:05,716 - INFO: Epoch: 130/200, Batch: 16/29, Batch_Loss_Train: 0.505
2024-06-21 19:50:06,130 - INFO: Epoch: 130/200, Batch: 17/29, Batch_Loss_Train: 0.580
2024-06-21 19:50:06,428 - INFO: Epoch: 130/200, Batch: 18/29, Batch_Loss_Train: 0.542
2024-06-21 19:50:06,824 - INFO: Epoch: 130/200, Batch: 19/29, Batch_Loss_Train: 0.398
2024-06-21 19:50:07,130 - INFO: Epoch: 130/200, Batch: 20/29, Batch_Loss_Train: 0.525
2024-06-21 19:50:07,532 - INFO: Epoch: 130/200, Batch: 21/29, Batch_Loss_Train: 0.695
2024-06-21 19:50:07,831 - INFO: Epoch: 130/200, Batch: 22/29, Batch_Loss_Train: 0.447
2024-06-21 19:50:08,225 - INFO: Epoch: 130/200, Batch: 23/29, Batch_Loss_Train: 0.547
2024-06-21 19:50:08,537 - INFO: Epoch: 130/200, Batch: 24/29, Batch_Loss_Train: 0.576
2024-06-21 19:50:08,941 - INFO: Epoch: 130/200, Batch: 25/29, Batch_Loss_Train: 0.625
2024-06-21 19:50:09,236 - INFO: Epoch: 130/200, Batch: 26/29, Batch_Loss_Train: 0.496
2024-06-21 19:50:09,631 - INFO: Epoch: 130/200, Batch: 27/29, Batch_Loss_Train: 0.556
2024-06-21 19:50:09,939 - INFO: Epoch: 130/200, Batch: 28/29, Batch_Loss_Train: 0.504
2024-06-21 19:50:10,159 - INFO: Epoch: 130/200, Batch: 29/29, Batch_Loss_Train: 0.622
2024-06-21 19:50:21,195 - INFO: 130/200 final results:
2024-06-21 19:50:21,195 - INFO: Training loss: 0.553.
2024-06-21 19:50:21,195 - INFO: Training MAE: 0.552.
2024-06-21 19:50:21,195 - INFO: Training MSE: 0.805.
2024-06-21 19:50:41,558 - INFO: Epoch: 130/200, Loss_train: 0.5532087056801237, Loss_val: 1.7977512417168453
2024-06-21 19:50:41,558 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:50:41,558 - INFO: Epoch 131/200...
2024-06-21 19:50:41,558 - INFO: Learning rate: 3.4104114595010206e-06.
2024-06-21 19:50:41,558 - INFO: Batch size: 32.
2024-06-21 19:50:41,561 - INFO: Dataset:
2024-06-21 19:50:41,562 - INFO: Batch size:
2024-06-21 19:50:41,562 - INFO: Number of workers:
2024-06-21 19:50:42,733 - INFO: Epoch: 131/200, Batch: 1/29, Batch_Loss_Train: 0.510
2024-06-21 19:50:43,039 - INFO: Epoch: 131/200, Batch: 2/29, Batch_Loss_Train: 0.502
2024-06-21 19:50:43,437 - INFO: Epoch: 131/200, Batch: 3/29, Batch_Loss_Train: 0.423
2024-06-21 19:50:43,755 - INFO: Epoch: 131/200, Batch: 4/29, Batch_Loss_Train: 0.730
2024-06-21 19:50:44,186 - INFO: Epoch: 131/200, Batch: 5/29, Batch_Loss_Train: 0.468
2024-06-21 19:50:44,488 - INFO: Epoch: 131/200, Batch: 6/29, Batch_Loss_Train: 0.526
2024-06-21 19:50:44,875 - INFO: Epoch: 131/200, Batch: 7/29, Batch_Loss_Train: 0.526
2024-06-21 19:50:45,177 - INFO: Epoch: 131/200, Batch: 8/29, Batch_Loss_Train: 0.594
2024-06-21 19:50:45,608 - INFO: Epoch: 131/200, Batch: 9/29, Batch_Loss_Train: 0.624
2024-06-21 19:50:45,901 - INFO: Epoch: 131/200, Batch: 10/29, Batch_Loss_Train: 0.601
2024-06-21 19:50:46,274 - INFO: Epoch: 131/200, Batch: 11/29, Batch_Loss_Train: 0.576
2024-06-21 19:50:46,577 - INFO: Epoch: 131/200, Batch: 12/29, Batch_Loss_Train: 0.507
2024-06-21 19:50:47,015 - INFO: Epoch: 131/200, Batch: 13/29, Batch_Loss_Train: 0.455
2024-06-21 19:50:47,320 - INFO: Epoch: 131/200, Batch: 14/29, Batch_Loss_Train: 0.526
2024-06-21 19:50:47,717 - INFO: Epoch: 131/200, Batch: 15/29, Batch_Loss_Train: 0.622
2024-06-21 19:50:48,017 - INFO: Epoch: 131/200, Batch: 16/29, Batch_Loss_Train: 0.686
2024-06-21 19:50:48,442 - INFO: Epoch: 131/200, Batch: 17/29, Batch_Loss_Train: 0.482
2024-06-21 19:50:48,742 - INFO: Epoch: 131/200, Batch: 18/29, Batch_Loss_Train: 0.562
2024-06-21 19:50:49,126 - INFO: Epoch: 131/200, Batch: 19/29, Batch_Loss_Train: 0.439
2024-06-21 19:50:49,421 - INFO: Epoch: 131/200, Batch: 20/29, Batch_Loss_Train: 0.511
2024-06-21 19:50:49,853 - INFO: Epoch: 131/200, Batch: 21/29, Batch_Loss_Train: 0.604
2024-06-21 19:50:50,153 - INFO: Epoch: 131/200, Batch: 22/29, Batch_Loss_Train: 0.702
2024-06-21 19:50:50,535 - INFO: Epoch: 131/200, Batch: 23/29, Batch_Loss_Train: 0.497
2024-06-21 19:50:50,835 - INFO: Epoch: 131/200, Batch: 24/29, Batch_Loss_Train: 0.537
2024-06-21 19:50:51,265 - INFO: Epoch: 131/200, Batch: 25/29, Batch_Loss_Train: 0.505
2024-06-21 19:50:51,562 - INFO: Epoch: 131/200, Batch: 26/29, Batch_Loss_Train: 0.477
2024-06-21 19:50:51,944 - INFO: Epoch: 131/200, Batch: 27/29, Batch_Loss_Train: 0.456
2024-06-21 19:50:52,240 - INFO: Epoch: 131/200, Batch: 28/29, Batch_Loss_Train: 0.509
2024-06-21 19:50:52,457 - INFO: Epoch: 131/200, Batch: 29/29, Batch_Loss_Train: 0.727
2024-06-21 19:51:03,545 - INFO: 131/200 final results:
2024-06-21 19:51:03,545 - INFO: Training loss: 0.548.
2024-06-21 19:51:03,545 - INFO: Training MAE: 0.544.
2024-06-21 19:51:03,545 - INFO: Training MSE: 0.778.
2024-06-21 19:51:23,825 - INFO: Epoch: 131/200, Loss_train: 0.5477513167364844, Loss_val: 1.8048125341020782
2024-06-21 19:51:23,826 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:51:23,826 - INFO: Epoch 132/200...
2024-06-21 19:51:23,826 - INFO: Learning rate: 3.4104114595010206e-06.
2024-06-21 19:51:23,826 - INFO: Batch size: 32.
2024-06-21 19:51:23,830 - INFO: Dataset:
2024-06-21 19:51:23,830 - INFO: Batch size:
2024-06-21 19:51:23,830 - INFO: Number of workers:
2024-06-21 19:51:25,004 - INFO: Epoch: 132/200, Batch: 1/29, Batch_Loss_Train: 0.458
2024-06-21 19:51:25,344 - INFO: Epoch: 132/200, Batch: 2/29, Batch_Loss_Train: 0.517
2024-06-21 19:51:25,737 - INFO: Epoch: 132/200, Batch: 3/29, Batch_Loss_Train: 0.571
2024-06-21 19:51:26,053 - INFO: Epoch: 132/200, Batch: 4/29, Batch_Loss_Train: 0.675
2024-06-21 19:51:26,456 - INFO: Epoch: 132/200, Batch: 5/29, Batch_Loss_Train: 0.396
2024-06-21 19:51:26,796 - INFO: Epoch: 132/200, Batch: 6/29, Batch_Loss_Train: 0.473
2024-06-21 19:51:27,184 - INFO: Epoch: 132/200, Batch: 7/29, Batch_Loss_Train: 0.525
2024-06-21 19:51:27,487 - INFO: Epoch: 132/200, Batch: 8/29, Batch_Loss_Train: 0.532
2024-06-21 19:51:27,874 - INFO: Epoch: 132/200, Batch: 9/29, Batch_Loss_Train: 0.458
2024-06-21 19:51:28,217 - INFO: Epoch: 132/200, Batch: 10/29, Batch_Loss_Train: 0.447
2024-06-21 19:51:28,594 - INFO: Epoch: 132/200, Batch: 11/29, Batch_Loss_Train: 0.409
2024-06-21 19:51:28,897 - INFO: Epoch: 132/200, Batch: 12/29, Batch_Loss_Train: 0.565
2024-06-21 19:51:29,304 - INFO: Epoch: 132/200, Batch: 13/29, Batch_Loss_Train: 0.499
2024-06-21 19:51:29,644 - INFO: Epoch: 132/200, Batch: 14/29, Batch_Loss_Train: 0.842
2024-06-21 19:51:30,040 - INFO: Epoch: 132/200, Batch: 15/29, Batch_Loss_Train: 0.588
2024-06-21 19:51:30,339 - INFO: Epoch: 132/200, Batch: 16/29, Batch_Loss_Train: 0.680
2024-06-21 19:51:30,743 - INFO: Epoch: 132/200, Batch: 17/29, Batch_Loss_Train: 0.484
2024-06-21 19:51:31,079 - INFO: Epoch: 132/200, Batch: 18/29, Batch_Loss_Train: 0.535
2024-06-21 19:51:31,466 - INFO: Epoch: 132/200, Batch: 19/29, Batch_Loss_Train: 0.543
2024-06-21 19:51:31,760 - INFO: Epoch: 132/200, Batch: 20/29, Batch_Loss_Train: 0.495
2024-06-21 19:51:32,157 - INFO: Epoch: 132/200, Batch: 21/29, Batch_Loss_Train: 0.648
2024-06-21 19:51:32,495 - INFO: Epoch: 132/200, Batch: 22/29, Batch_Loss_Train: 0.580
2024-06-21 19:51:32,870 - INFO: Epoch: 132/200, Batch: 23/29, Batch_Loss_Train: 0.639
2024-06-21 19:51:33,171 - INFO: Epoch: 132/200, Batch: 24/29, Batch_Loss_Train: 0.546
2024-06-21 19:51:33,564 - INFO: Epoch: 132/200, Batch: 25/29, Batch_Loss_Train: 0.695
2024-06-21 19:51:33,896 - INFO: Epoch: 132/200, Batch: 26/29, Batch_Loss_Train: 0.459
2024-06-21 19:51:34,279 - INFO: Epoch: 132/200, Batch: 27/29, Batch_Loss_Train: 0.508
2024-06-21 19:51:34,574 - INFO: Epoch: 132/200, Batch: 28/29, Batch_Loss_Train: 0.446
2024-06-21 19:51:34,784 - INFO: Epoch: 132/200, Batch: 29/29, Batch_Loss_Train: 0.521
2024-06-21 19:51:45,911 - INFO: 132/200 final results:
2024-06-21 19:51:45,911 - INFO: Training loss: 0.543.
2024-06-21 19:51:45,911 - INFO: Training MAE: 0.543.
2024-06-21 19:51:45,911 - INFO: Training MSE: 0.784.
2024-06-21 19:52:06,316 - INFO: Epoch: 132/200, Loss_train: 0.5425897939451809, Loss_val: 1.7979092762388031
2024-06-21 19:52:06,316 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:52:06,316 - INFO: Epoch 133/200...
2024-06-21 19:52:06,316 - INFO: Learning rate: 3.4104114595010206e-06.
2024-06-21 19:52:06,316 - INFO: Batch size: 32.
2024-06-21 19:52:06,320 - INFO: Dataset:
2024-06-21 19:52:06,320 - INFO: Batch size:
2024-06-21 19:52:06,321 - INFO: Number of workers:
2024-06-21 19:52:07,511 - INFO: Epoch: 133/200, Batch: 1/29, Batch_Loss_Train: 0.527
2024-06-21 19:52:07,816 - INFO: Epoch: 133/200, Batch: 2/29, Batch_Loss_Train: 0.573
2024-06-21 19:52:08,210 - INFO: Epoch: 133/200, Batch: 3/29, Batch_Loss_Train: 0.551
2024-06-21 19:52:08,527 - INFO: Epoch: 133/200, Batch: 4/29, Batch_Loss_Train: 0.435
2024-06-21 19:52:08,960 - INFO: Epoch: 133/200, Batch: 5/29, Batch_Loss_Train: 0.477
2024-06-21 19:52:09,259 - INFO: Epoch: 133/200, Batch: 6/29, Batch_Loss_Train: 0.519
2024-06-21 19:52:09,647 - INFO: Epoch: 133/200, Batch: 7/29, Batch_Loss_Train: 0.641
2024-06-21 19:52:09,950 - INFO: Epoch: 133/200, Batch: 8/29, Batch_Loss_Train: 0.500
2024-06-21 19:52:10,388 - INFO: Epoch: 133/200, Batch: 9/29, Batch_Loss_Train: 0.475
2024-06-21 19:52:10,681 - INFO: Epoch: 133/200, Batch: 10/29, Batch_Loss_Train: 0.539
2024-06-21 19:52:11,056 - INFO: Epoch: 133/200, Batch: 11/29, Batch_Loss_Train: 0.633
2024-06-21 19:52:11,361 - INFO: Epoch: 133/200, Batch: 12/29, Batch_Loss_Train: 0.501
2024-06-21 19:52:11,805 - INFO: Epoch: 133/200, Batch: 13/29, Batch_Loss_Train: 0.641
2024-06-21 19:52:12,110 - INFO: Epoch: 133/200, Batch: 14/29, Batch_Loss_Train: 0.525
2024-06-21 19:52:12,506 - INFO: Epoch: 133/200, Batch: 15/29, Batch_Loss_Train: 0.496
2024-06-21 19:52:12,807 - INFO: Epoch: 133/200, Batch: 16/29, Batch_Loss_Train: 0.483
2024-06-21 19:52:13,552 - INFO: Epoch: 133/200, Batch: 17/29, Batch_Loss_Train: 0.566
2024-06-21 19:52:13,853 - INFO: Epoch: 133/200, Batch: 18/29, Batch_Loss_Train: 0.579
2024-06-21 19:52:14,236 - INFO: Epoch: 133/200, Batch: 19/29, Batch_Loss_Train: 0.629
2024-06-21 19:52:14,532 - INFO: Epoch: 133/200, Batch: 20/29, Batch_Loss_Train: 0.400
2024-06-21 19:52:14,961 - INFO: Epoch: 133/200, Batch: 21/29, Batch_Loss_Train: 0.380
2024-06-21 19:52:15,263 - INFO: Epoch: 133/200, Batch: 22/29, Batch_Loss_Train: 0.695
2024-06-21 19:52:15,636 - INFO: Epoch: 133/200, Batch: 23/29, Batch_Loss_Train: 0.511
2024-06-21 19:52:15,938 - INFO: Epoch: 133/200, Batch: 24/29, Batch_Loss_Train: 0.450
2024-06-21 19:52:16,360 - INFO: Epoch: 133/200, Batch: 25/29, Batch_Loss_Train: 0.535
2024-06-21 19:52:16,659 - INFO: Epoch: 133/200, Batch: 26/29, Batch_Loss_Train: 0.676
2024-06-21 19:52:17,026 - INFO: Epoch: 133/200, Batch: 27/29, Batch_Loss_Train: 0.783
2024-06-21 19:52:17,323 - INFO: Epoch: 133/200, Batch: 28/29, Batch_Loss_Train: 0.623
2024-06-21 19:52:17,533 - INFO: Epoch: 133/200, Batch: 29/29, Batch_Loss_Train: 0.515
2024-06-21 19:52:28,466 - INFO: 133/200 final results:
2024-06-21 19:52:28,467 - INFO: Training loss: 0.547.
2024-06-21 19:52:28,467 - INFO: Training MAE: 0.547.
2024-06-21 19:52:28,467 - INFO: Training MSE: 0.792.
2024-06-21 19:52:49,206 - INFO: Epoch: 133/200, Loss_train: 0.5468390368182083, Loss_val: 1.8064214731084889
2024-06-21 19:52:49,206 - INFO: Best internal validation val_loss: 1.789 at epoch: 123.
2024-06-21 19:52:49,206 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
