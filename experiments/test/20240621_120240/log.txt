2024-06-21 12:02:40,244 - INFO: Device: cuda.
2024-06-21 12:02:40,244 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 12:02:40,244 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 12:02:40,244 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 12:02:40,244 - INFO: Seed: 4
2024-06-21 12:02:40,244 - INFO: 42 patients have been found in the data directory.
2024-06-21 12:02:40,284 - INFO: Train set contains 32 patients.
2024-06-21 12:02:40,284 - INFO: Val set contains 5 patients.
2024-06-21 12:02:40,284 - INFO: Test set contains 5 patients.
2024-06-21 12:02:40,284 - INFO: Fold: 0
2024-06-21 12:02:40,285 - INFO: Performing 2-fold Cross Validation.
2024-06-21 12:02:40,285 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 12:02:40,285 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 12:02:40,285 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 12:02:40,415 - INFO: To_device: False.
2024-06-21 12:02:40,417 - INFO: Transformers have been made successfully.
2024-06-21 12:02:40,417 - INFO: Dataset type: cache.
2024-06-21 12:02:40,417 - INFO: Dataloader type: standard.
2024-06-21 12:04:31,385 - INFO: Train dataloader arguments.
2024-06-21 12:04:31,385 - INFO: 	Batch_size: 32.
2024-06-21 12:04:31,385 - INFO: 	Shuffle: True.
2024-06-21 12:04:31,385 - INFO: 	Sampler: None.
2024-06-21 12:04:31,385 - INFO: 	Num_workers: 4.
2024-06-21 12:04:31,385 - INFO: 	Drop_last: False.
2024-06-21 12:04:31,557 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 7, 7), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 7, 7), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 7, 7), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 7, 7), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 4, 4), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=1048576, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 12:04:32,438 - INFO: Weight init name: kaiming_uniform.
2024-06-21 12:04:35,435 - INFO: Number of training iterations per epoch: 29.
2024-06-21 12:04:35,435 - INFO: Epoch 1/10...
2024-06-21 12:04:35,435 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:04:35,435 - INFO: Batch size: 32.
2024-06-21 12:04:35,436 - INFO: Dataset:
2024-06-21 12:04:35,436 - INFO: Batch size:
2024-06-21 12:04:35,436 - INFO: Number of workers:
2024-06-21 12:04:39,029 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 78.765
2024-06-21 12:04:39,597 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 62.098
2024-06-21 12:04:40,235 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 70.039
2024-06-21 12:04:40,789 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 74.273
2024-06-21 12:04:41,434 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 65.409
2024-06-21 12:04:42,019 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 79.374
2024-06-21 12:04:42,651 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 659.936
2024-06-21 12:04:43,203 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 24710180.000
2024-06-21 12:04:44,034 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 2228797304897770510985723904.000
2024-06-21 12:04:44,491 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:04:44,992 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:04:45,421 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:04:45,956 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:04:46,429 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:04:46,946 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:04:47,372 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:04:47,902 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:04:48,363 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:04:48,869 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:04:49,289 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:04:49,804 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:04:50,265 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:04:50,761 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:04:51,185 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:04:51,697 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:04:52,154 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:04:52,643 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:04:53,063 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:04:54,902 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:05:05,925 - INFO: 1/10 final results:
2024-06-21 12:05:05,925 - INFO: Training loss: nan.
2024-06-21 12:05:05,925 - INFO: Training MAE: nan.
2024-06-21 12:05:05,925 - INFO: Training MSE: nan.
2024-06-21 12:05:26,806 - INFO: Epoch: 1/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:05:26,806 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:05:26,806 - INFO: Epoch 2/10...
2024-06-21 12:05:26,806 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:05:26,806 - INFO: Batch size: 32.
2024-06-21 12:05:26,808 - INFO: Dataset:
2024-06-21 12:05:26,809 - INFO: Batch size:
2024-06-21 12:05:26,809 - INFO: Number of workers:
2024-06-21 12:05:28,084 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:05:28,519 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:05:29,034 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:05:29,481 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:05:30,031 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:05:30,460 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:05:30,971 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:05:31,414 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:05:31,967 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:05:32,390 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:05:32,895 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:05:33,340 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:05:33,894 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:05:34,324 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:05:34,846 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:05:35,287 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:05:35,842 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:05:36,268 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:05:36,774 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:05:37,205 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:05:37,751 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:05:38,176 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:05:38,672 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:05:39,108 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:05:39,638 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:05:40,060 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:05:40,554 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:05:40,989 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:05:41,298 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:05:52,388 - INFO: 2/10 final results:
2024-06-21 12:05:52,388 - INFO: Training loss: nan.
2024-06-21 12:05:52,388 - INFO: Training MAE: nan.
2024-06-21 12:05:52,388 - INFO: Training MSE: nan.
2024-06-21 12:06:13,080 - INFO: Epoch: 2/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:06:13,080 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:06:13,080 - INFO: Epoch 3/10...
2024-06-21 12:06:13,080 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:06:13,080 - INFO: Batch size: 32.
2024-06-21 12:06:13,083 - INFO: Dataset:
2024-06-21 12:06:13,083 - INFO: Batch size:
2024-06-21 12:06:13,083 - INFO: Number of workers:
2024-06-21 12:06:14,347 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:06:14,778 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:06:15,296 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:06:15,740 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:06:16,282 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:06:16,708 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:06:17,220 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:06:17,660 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:06:18,215 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:06:18,637 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:06:19,144 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:06:19,588 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:06:20,179 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:06:20,610 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:06:21,131 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:06:21,561 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:06:22,128 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:06:22,554 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:06:23,063 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:06:23,483 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:06:24,041 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:06:24,471 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:06:24,989 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:06:25,419 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:06:25,980 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:06:26,409 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:06:26,925 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:06:27,354 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:06:27,681 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:06:38,854 - INFO: 3/10 final results:
2024-06-21 12:06:38,854 - INFO: Training loss: nan.
2024-06-21 12:06:38,855 - INFO: Training MAE: nan.
2024-06-21 12:06:38,855 - INFO: Training MSE: nan.
2024-06-21 12:06:59,532 - INFO: Epoch: 3/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:06:59,532 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:06:59,532 - INFO: Epoch 4/10...
2024-06-21 12:06:59,532 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:06:59,532 - INFO: Batch size: 32.
2024-06-21 12:06:59,535 - INFO: Dataset:
2024-06-21 12:06:59,535 - INFO: Batch size:
2024-06-21 12:06:59,535 - INFO: Number of workers:
2024-06-21 12:07:00,811 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:07:01,246 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:07:01,763 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:07:02,209 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:07:02,773 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:07:03,202 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:07:03,715 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:07:04,146 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:07:04,718 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:07:05,141 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:07:05,644 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:07:06,076 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:07:06,642 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:07:07,069 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:07:07,584 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:07:08,011 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:07:08,568 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:07:08,994 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:07:09,495 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:07:09,915 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:07:10,468 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:07:10,894 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:07:11,404 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:07:11,830 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:07:12,380 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:07:12,803 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:07:13,301 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:07:13,725 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:07:14,041 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:07:25,085 - INFO: 4/10 final results:
2024-06-21 12:07:25,085 - INFO: Training loss: nan.
2024-06-21 12:07:25,085 - INFO: Training MAE: nan.
2024-06-21 12:07:25,085 - INFO: Training MSE: nan.
2024-06-21 12:07:45,823 - INFO: Epoch: 4/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:07:45,824 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:07:45,824 - INFO: Epoch 5/10...
2024-06-21 12:07:45,824 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:07:45,824 - INFO: Batch size: 32.
2024-06-21 12:07:45,826 - INFO: Dataset:
2024-06-21 12:07:45,826 - INFO: Batch size:
2024-06-21 12:07:45,826 - INFO: Number of workers:
2024-06-21 12:07:47,066 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:07:47,523 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:07:48,033 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:07:48,478 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:07:49,015 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:07:49,459 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:07:49,975 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:07:50,418 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:07:50,953 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:07:51,387 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:07:51,897 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:07:52,341 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:07:52,887 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:07:53,331 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:07:53,855 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:07:54,297 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:07:54,845 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:07:55,287 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:07:55,802 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:07:56,239 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:07:56,774 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:07:57,216 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:07:57,734 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:07:58,176 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:07:58,709 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:07:59,144 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:07:59,655 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:08:00,092 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:08:00,415 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:08:11,458 - INFO: 5/10 final results:
2024-06-21 12:08:11,458 - INFO: Training loss: nan.
2024-06-21 12:08:11,458 - INFO: Training MAE: nan.
2024-06-21 12:08:11,459 - INFO: Training MSE: nan.
2024-06-21 12:08:32,169 - INFO: Epoch: 5/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:08:32,169 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:08:32,169 - INFO: Epoch 6/10...
2024-06-21 12:08:32,169 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:08:32,169 - INFO: Batch size: 32.
2024-06-21 12:08:32,171 - INFO: Dataset:
2024-06-21 12:08:32,171 - INFO: Batch size:
2024-06-21 12:08:32,171 - INFO: Number of workers:
2024-06-21 12:08:33,447 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:08:33,877 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:08:34,381 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:08:34,823 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:08:35,383 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:08:35,810 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:08:36,320 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:08:36,748 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:08:37,332 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:08:37,755 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:08:38,250 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:08:38,681 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:08:39,249 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:08:39,679 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:08:40,193 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:08:40,625 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:08:41,189 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:08:41,618 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:08:42,119 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:08:42,542 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:08:43,099 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:08:43,526 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:08:44,036 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:08:44,463 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:08:45,020 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:08:45,444 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:08:45,955 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:08:46,381 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:08:46,703 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:08:57,825 - INFO: 6/10 final results:
2024-06-21 12:08:57,825 - INFO: Training loss: nan.
2024-06-21 12:08:57,825 - INFO: Training MAE: nan.
2024-06-21 12:08:57,825 - INFO: Training MSE: nan.
2024-06-21 12:09:18,622 - INFO: Epoch: 6/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:09:18,623 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:09:18,623 - INFO: Epoch 7/10...
2024-06-21 12:09:18,623 - INFO: Learning rate: 0.002542355689225227.
2024-06-21 12:09:18,623 - INFO: Batch size: 32.
2024-06-21 12:09:18,625 - INFO: Dataset:
2024-06-21 12:09:18,625 - INFO: Batch size:
2024-06-21 12:09:18,625 - INFO: Number of workers:
2024-06-21 12:09:19,902 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:09:20,334 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:09:20,854 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:09:21,298 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:09:21,858 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:09:22,284 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:09:22,794 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:09:23,222 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:09:23,788 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:09:24,207 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:09:24,710 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:09:25,137 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:09:25,701 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:09:26,126 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:09:26,641 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:09:27,065 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:09:27,638 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:09:28,066 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:09:28,578 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:09:28,999 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:09:29,554 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:09:29,978 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:09:30,488 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:09:30,913 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:09:31,468 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:09:31,889 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:09:32,393 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:09:32,815 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:09:33,135 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:09:44,247 - INFO: 7/10 final results:
2024-06-21 12:09:44,248 - INFO: Training loss: nan.
2024-06-21 12:09:44,248 - INFO: Training MAE: nan.
2024-06-21 12:09:44,248 - INFO: Training MSE: nan.
2024-06-21 12:10:04,745 - INFO: Epoch: 7/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:10:04,745 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:10:04,745 - INFO: Epoch 8/10...
2024-06-21 12:10:04,745 - INFO: Learning rate: 0.0012711778446126134.
2024-06-21 12:10:04,745 - INFO: Batch size: 32.
2024-06-21 12:10:04,748 - INFO: Dataset:
2024-06-21 12:10:04,748 - INFO: Batch size:
2024-06-21 12:10:04,748 - INFO: Number of workers:
2024-06-21 12:10:06,009 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:10:06,438 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:10:06,953 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:10:07,395 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:10:07,951 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:10:08,375 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:10:08,883 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:10:09,309 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:10:09,875 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:10:10,292 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:10:10,793 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:10:11,220 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:10:11,789 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:10:12,217 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:10:12,735 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:10:13,161 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:10:13,727 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:10:14,151 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:10:14,656 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:10:15,074 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:10:15,625 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:10:16,049 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:10:16,560 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:10:16,985 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:10:17,541 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:10:17,965 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:10:18,478 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:10:18,904 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:10:19,228 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:10:30,374 - INFO: 8/10 final results:
2024-06-21 12:10:30,374 - INFO: Training loss: nan.
2024-06-21 12:10:30,374 - INFO: Training MAE: nan.
2024-06-21 12:10:30,374 - INFO: Training MSE: nan.
2024-06-21 12:10:50,760 - INFO: Epoch: 8/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:10:50,760 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:10:50,760 - INFO: Epoch 9/10...
2024-06-21 12:10:50,760 - INFO: Learning rate: 0.0012711778446126134.
2024-06-21 12:10:50,760 - INFO: Batch size: 32.
2024-06-21 12:10:50,763 - INFO: Dataset:
2024-06-21 12:10:50,763 - INFO: Batch size:
2024-06-21 12:10:50,763 - INFO: Number of workers:
2024-06-21 12:10:51,998 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:10:52,446 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:10:52,976 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:10:53,423 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:10:53,960 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:10:54,390 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:10:54,914 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:10:55,357 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:10:55,894 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:10:56,316 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:10:56,833 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:10:57,277 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:10:57,818 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:10:58,249 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:10:58,778 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:10:59,215 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:10:59,749 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:11:00,174 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:11:00,689 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:11:01,121 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:11:01,648 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:11:02,074 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:11:02,583 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:11:03,020 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:11:03,543 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:11:03,965 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:11:04,469 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:11:04,903 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:11:05,212 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:11:16,309 - INFO: 9/10 final results:
2024-06-21 12:11:16,309 - INFO: Training loss: nan.
2024-06-21 12:11:16,309 - INFO: Training MAE: nan.
2024-06-21 12:11:16,309 - INFO: Training MSE: nan.
2024-06-21 12:11:37,103 - INFO: Epoch: 9/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:11:37,104 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:11:37,104 - INFO: Epoch 10/10...
2024-06-21 12:11:37,104 - INFO: Learning rate: 0.0012711778446126134.
2024-06-21 12:11:37,104 - INFO: Batch size: 32.
2024-06-21 12:11:37,106 - INFO: Dataset:
2024-06-21 12:11:37,106 - INFO: Batch size:
2024-06-21 12:11:37,106 - INFO: Number of workers:
2024-06-21 12:11:38,375 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: nan
2024-06-21 12:11:38,809 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: nan
2024-06-21 12:11:39,343 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: nan
2024-06-21 12:11:39,790 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: nan
2024-06-21 12:11:40,332 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: nan
2024-06-21 12:11:40,762 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: nan
2024-06-21 12:11:41,273 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: nan
2024-06-21 12:11:41,716 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: nan
2024-06-21 12:11:42,261 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: nan
2024-06-21 12:11:42,684 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: nan
2024-06-21 12:11:43,188 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: nan
2024-06-21 12:11:43,630 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: nan
2024-06-21 12:11:44,187 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: nan
2024-06-21 12:11:44,614 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: nan
2024-06-21 12:11:45,129 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: nan
2024-06-21 12:11:45,568 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: nan
2024-06-21 12:11:46,120 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: nan
2024-06-21 12:11:46,549 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: nan
2024-06-21 12:11:47,055 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: nan
2024-06-21 12:11:47,492 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: nan
2024-06-21 12:11:48,031 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: nan
2024-06-21 12:11:48,458 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: nan
2024-06-21 12:11:48,961 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: nan
2024-06-21 12:11:49,402 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: nan
2024-06-21 12:11:49,945 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: nan
2024-06-21 12:11:50,372 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: nan
2024-06-21 12:11:50,875 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: nan
2024-06-21 12:11:51,316 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: nan
2024-06-21 12:11:51,638 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: nan
2024-06-21 12:12:02,938 - INFO: 10/10 final results:
2024-06-21 12:12:02,938 - INFO: Training loss: nan.
2024-06-21 12:12:02,939 - INFO: Training MAE: nan.
2024-06-21 12:12:02,939 - INFO: Training MSE: nan.
2024-06-21 12:12:23,497 - INFO: Epoch: 10/10, Loss_train: nan, Loss_val: nan
2024-06-21 12:12:23,498 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 12:12:23,498 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
