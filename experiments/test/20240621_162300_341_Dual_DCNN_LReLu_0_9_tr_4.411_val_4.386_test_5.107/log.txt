2024-06-21 16:13:15,408 - INFO: Device: cuda.
2024-06-21 16:13:15,408 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 16:13:15,408 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 16:13:15,408 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 16:13:15,408 - INFO: Seed: 4
2024-06-21 16:13:15,408 - INFO: 42 patients have been found in the data directory.
2024-06-21 16:13:15,447 - INFO: Train set contains 32 patients.
2024-06-21 16:13:15,447 - INFO: Val set contains 5 patients.
2024-06-21 16:13:15,447 - INFO: Test set contains 5 patients.
2024-06-21 16:13:15,447 - INFO: Fold: 0
2024-06-21 16:13:15,448 - INFO: Performing 2-fold Cross Validation.
2024-06-21 16:13:15,448 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 16:13:15,448 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 16:13:15,448 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 16:13:15,578 - INFO: To_device: False.
2024-06-21 16:13:15,580 - INFO: Transformers have been made successfully.
2024-06-21 16:13:15,580 - INFO: Dataset type: cache.
2024-06-21 16:13:15,580 - INFO: Dataloader type: standard.
2024-06-21 16:15:07,224 - INFO: Train dataloader arguments.
2024-06-21 16:15:07,224 - INFO: 	Batch_size: 32.
2024-06-21 16:15:07,224 - INFO: 	Shuffle: True.
2024-06-21 16:15:07,224 - INFO: 	Sampler: None.
2024-06-21 16:15:07,224 - INFO: 	Num_workers: 4.
2024-06-21 16:15:07,224 - INFO: 	Drop_last: False.
2024-06-21 16:15:07,273 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 16:15:08,140 - INFO: Weight init name: kaiming_uniform.
2024-06-21 16:15:10,572 - INFO: Number of training iterations per epoch: 29.
2024-06-21 16:15:10,572 - INFO: Epoch 1/10...
2024-06-21 16:15:10,572 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:15:10,572 - INFO: Batch size: 32.
2024-06-21 16:15:10,572 - INFO: Dataset:
2024-06-21 16:15:10,572 - INFO: Batch size:
2024-06-21 16:15:10,572 - INFO: Number of workers:
2024-06-21 16:15:13,532 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 7.229
2024-06-21 16:15:13,831 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.810
2024-06-21 16:15:14,205 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 7.015
2024-06-21 16:15:14,506 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 12.883
2024-06-21 16:15:14,915 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 42.780
2024-06-21 16:15:15,213 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 24.597
2024-06-21 16:15:15,588 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 13.667
2024-06-21 16:15:15,901 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 11.263
2024-06-21 16:15:16,303 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 26.127
2024-06-21 16:15:16,599 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 8.977
2024-06-21 16:15:16,972 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 10.009
2024-06-21 16:15:17,287 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 7.528
2024-06-21 16:15:17,717 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 7.150
2024-06-21 16:15:18,019 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 6.755
2024-06-21 16:15:18,408 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 7.574
2024-06-21 16:15:18,723 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 7.173
2024-06-21 16:15:19,146 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.798
2024-06-21 16:15:19,450 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.385
2024-06-21 16:15:19,830 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 7.713
2024-06-21 16:15:20,141 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 6.960
2024-06-21 16:15:20,556 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 7.609
2024-06-21 16:15:20,855 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 7.825
2024-06-21 16:15:21,223 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 7.104
2024-06-21 16:15:21,534 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 7.229
2024-06-21 16:15:21,937 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 6.837
2024-06-21 16:15:22,233 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 7.199
2024-06-21 16:15:22,596 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 7.617
2024-06-21 16:15:22,905 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 7.453
2024-06-21 16:15:24,179 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 6.640
2024-06-21 16:15:35,148 - INFO: 1/10 final results:
2024-06-21 16:15:35,148 - INFO: Training loss: 10.411.
2024-06-21 16:15:35,148 - INFO: Training MAE: 10.485.
2024-06-21 16:15:35,149 - INFO: Training MSE: 221.202.
2024-06-21 16:15:55,314 - INFO: Epoch: 1/10, Loss_train: 10.410537867710508, Loss_val: 7.031782364023143
2024-06-21 16:15:55,314 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 16:15:55,315 - INFO: Epoch 2/10...
2024-06-21 16:15:55,315 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:15:55,315 - INFO: Batch size: 32.
2024-06-21 16:15:55,317 - INFO: Dataset:
2024-06-21 16:15:55,318 - INFO: Batch size:
2024-06-21 16:15:55,318 - INFO: Number of workers:
2024-06-21 16:15:56,457 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 6.542
2024-06-21 16:15:56,765 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 6.902
2024-06-21 16:15:57,162 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 6.019
2024-06-21 16:15:57,482 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 7.551
2024-06-21 16:15:57,909 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 6.484
2024-06-21 16:15:58,211 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 6.533
2024-06-21 16:15:58,599 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 6.539
2024-06-21 16:15:58,914 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 6.645
2024-06-21 16:15:59,340 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 7.726
2024-06-21 16:15:59,636 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 6.145
2024-06-21 16:16:00,017 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 6.473
2024-06-21 16:16:00,334 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 7.161
2024-06-21 16:16:00,771 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 6.254
2024-06-21 16:16:01,075 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 6.297
2024-06-21 16:16:01,475 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 6.597
2024-06-21 16:16:01,791 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 7.475
2024-06-21 16:16:02,520 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 6.390
2024-06-21 16:16:02,824 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 7.330
2024-06-21 16:16:03,213 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 6.602
2024-06-21 16:16:03,523 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 6.857
2024-06-21 16:16:03,948 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 6.756
2024-06-21 16:16:04,249 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 7.020
2024-06-21 16:16:04,637 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 6.421
2024-06-21 16:16:04,953 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 7.264
2024-06-21 16:16:05,374 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 5.801
2024-06-21 16:16:05,673 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 6.369
2024-06-21 16:16:06,061 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 6.496
2024-06-21 16:16:06,375 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 6.606
2024-06-21 16:16:06,597 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 7.138
2024-06-21 16:16:17,045 - INFO: 2/10 final results:
2024-06-21 16:16:17,045 - INFO: Training loss: 6.703.
2024-06-21 16:16:17,045 - INFO: Training MAE: 6.695.
2024-06-21 16:16:17,045 - INFO: Training MSE: 62.974.
2024-06-21 16:16:37,381 - INFO: Epoch: 2/10, Loss_train: 6.703199846991177, Loss_val: 6.208545569715829
2024-06-21 16:16:37,428 - INFO: Saved new best metric model for epoch 2.
2024-06-21 16:16:37,428 - INFO: Best internal validation val_loss: 6.209 at epoch: 2.
2024-06-21 16:16:37,428 - INFO: Epoch 3/10...
2024-06-21 16:16:37,428 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:16:37,428 - INFO: Batch size: 32.
2024-06-21 16:16:37,431 - INFO: Dataset:
2024-06-21 16:16:37,431 - INFO: Batch size:
2024-06-21 16:16:37,431 - INFO: Number of workers:
2024-06-21 16:16:38,542 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 6.510
2024-06-21 16:16:38,885 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 6.127
2024-06-21 16:16:39,277 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 5.717
2024-06-21 16:16:39,596 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 5.567
2024-06-21 16:16:39,995 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 5.524
2024-06-21 16:16:40,320 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 4.672
2024-06-21 16:16:40,706 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 6.116
2024-06-21 16:16:41,008 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 6.529
2024-06-21 16:16:41,420 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 7.501
2024-06-21 16:16:41,752 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 6.069
2024-06-21 16:16:42,131 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 6.494
2024-06-21 16:16:42,434 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 6.308
2024-06-21 16:16:42,837 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 5.240
2024-06-21 16:16:43,164 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 5.927
2024-06-21 16:16:43,557 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 6.049
2024-06-21 16:16:43,858 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 4.941
2024-06-21 16:16:44,268 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 5.258
2024-06-21 16:16:44,596 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 5.649
2024-06-21 16:16:44,982 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 6.194
2024-06-21 16:16:45,282 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 5.933
2024-06-21 16:16:45,695 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 5.195
2024-06-21 16:16:46,025 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 5.558
2024-06-21 16:16:46,408 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 5.741
2024-06-21 16:16:46,713 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 5.822
2024-06-21 16:16:47,116 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 6.899
2024-06-21 16:16:47,443 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 5.645
2024-06-21 16:16:47,828 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 5.720
2024-06-21 16:16:48,132 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 5.619
2024-06-21 16:16:48,351 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 6.538
2024-06-21 16:16:59,422 - INFO: 3/10 final results:
2024-06-21 16:16:59,422 - INFO: Training loss: 5.899.
2024-06-21 16:16:59,422 - INFO: Training MAE: 5.886.
2024-06-21 16:16:59,422 - INFO: Training MSE: 51.467.
2024-06-21 16:17:19,807 - INFO: Epoch: 3/10, Loss_train: 5.898688332787875, Loss_val: 5.481679390216696
2024-06-21 16:17:19,863 - INFO: Saved new best metric model for epoch 3.
2024-06-21 16:17:19,863 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:17:19,863 - INFO: Epoch 4/10...
2024-06-21 16:17:19,863 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:17:19,863 - INFO: Batch size: 32.
2024-06-21 16:17:19,866 - INFO: Dataset:
2024-06-21 16:17:19,867 - INFO: Batch size:
2024-06-21 16:17:19,867 - INFO: Number of workers:
2024-06-21 16:17:20,965 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 4.809
2024-06-21 16:17:21,274 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 5.364
2024-06-21 16:17:21,672 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 5.296
2024-06-21 16:17:21,993 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 5.418
2024-06-21 16:17:22,399 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 5.304
2024-06-21 16:17:22,705 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 4.647
2024-06-21 16:17:23,108 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 6.451
2024-06-21 16:17:23,426 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 5.603
2024-06-21 16:17:23,830 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 5.441
2024-06-21 16:17:24,129 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 5.125
2024-06-21 16:17:24,524 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 4.468
2024-06-21 16:17:24,844 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 7.389
2024-06-21 16:17:25,266 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 5.968
2024-06-21 16:17:25,574 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 6.222
2024-06-21 16:17:25,985 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 5.526
2024-06-21 16:17:26,303 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 5.903
2024-06-21 16:17:26,721 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 5.441
2024-06-21 16:17:27,026 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 6.076
2024-06-21 16:17:27,426 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 4.935
2024-06-21 16:17:27,738 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 5.120
2024-06-21 16:17:28,145 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 5.020
2024-06-21 16:17:28,450 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 5.091
2024-06-21 16:17:28,840 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 5.912
2024-06-21 16:17:29,157 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 6.746
2024-06-21 16:17:29,557 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 5.343
2024-06-21 16:17:29,859 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 5.561
2024-06-21 16:17:30,249 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 4.990
2024-06-21 16:17:30,564 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 6.375
2024-06-21 16:17:30,781 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 5.929
2024-06-21 16:17:41,759 - INFO: 4/10 final results:
2024-06-21 16:17:41,759 - INFO: Training loss: 5.568.
2024-06-21 16:17:41,759 - INFO: Training MAE: 5.561.
2024-06-21 16:17:41,759 - INFO: Training MSE: 47.218.
2024-06-21 16:18:01,894 - INFO: Epoch: 4/10, Loss_train: 5.568026065826416, Loss_val: 6.020817625111547
2024-06-21 16:18:01,894 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:18:01,894 - INFO: Epoch 5/10...
2024-06-21 16:18:01,894 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:18:01,894 - INFO: Batch size: 32.
2024-06-21 16:18:01,898 - INFO: Dataset:
2024-06-21 16:18:01,898 - INFO: Batch size:
2024-06-21 16:18:01,898 - INFO: Number of workers:
2024-06-21 16:18:02,992 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 5.913
2024-06-21 16:18:03,316 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 4.615
2024-06-21 16:18:03,713 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 4.764
2024-06-21 16:18:04,036 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 5.386
2024-06-21 16:18:04,450 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 5.120
2024-06-21 16:18:04,757 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 5.672
2024-06-21 16:18:05,158 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 4.911
2024-06-21 16:18:05,477 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 4.480
2024-06-21 16:18:05,884 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 4.733
2024-06-21 16:18:06,179 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 5.748
2024-06-21 16:18:06,567 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 6.264
2024-06-21 16:18:06,884 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 5.713
2024-06-21 16:18:07,304 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 4.669
2024-06-21 16:18:07,609 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 4.788
2024-06-21 16:18:08,015 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 4.852
2024-06-21 16:18:08,330 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 5.257
2024-06-21 16:18:08,744 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 4.992
2024-06-21 16:18:09,046 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 4.775
2024-06-21 16:18:09,441 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 4.678
2024-06-21 16:18:09,754 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 4.940
2024-06-21 16:18:10,167 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 5.165
2024-06-21 16:18:10,473 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 5.522
2024-06-21 16:18:10,867 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 4.887
2024-06-21 16:18:11,194 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 5.884
2024-06-21 16:18:11,628 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 5.165
2024-06-21 16:18:11,936 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 4.248
2024-06-21 16:18:12,350 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 5.144
2024-06-21 16:18:12,668 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 5.149
2024-06-21 16:18:12,881 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 4.755
2024-06-21 16:18:23,958 - INFO: 5/10 final results:
2024-06-21 16:18:23,958 - INFO: Training loss: 5.110.
2024-06-21 16:18:23,958 - INFO: Training MAE: 5.117.
2024-06-21 16:18:23,958 - INFO: Training MSE: 41.169.
2024-06-21 16:18:44,631 - INFO: Epoch: 5/10, Loss_train: 5.109810319440118, Loss_val: 5.825266805188409
2024-06-21 16:18:44,632 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:18:44,632 - INFO: Epoch 6/10...
2024-06-21 16:18:44,632 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:18:44,632 - INFO: Batch size: 32.
2024-06-21 16:18:44,635 - INFO: Dataset:
2024-06-21 16:18:44,635 - INFO: Batch size:
2024-06-21 16:18:44,635 - INFO: Number of workers:
2024-06-21 16:18:45,725 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 4.856
2024-06-21 16:18:46,051 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 5.745
2024-06-21 16:18:46,465 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 4.404
2024-06-21 16:18:46,790 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 4.755
2024-06-21 16:18:47,208 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 4.267
2024-06-21 16:18:47,515 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 5.473
2024-06-21 16:18:47,919 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 5.950
2024-06-21 16:18:48,239 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 5.295
2024-06-21 16:18:48,647 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 4.282
2024-06-21 16:18:48,946 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 5.138
2024-06-21 16:18:49,342 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 3.950
2024-06-21 16:18:49,663 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 4.114
2024-06-21 16:18:50,089 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 4.724
2024-06-21 16:18:50,398 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 5.413
2024-06-21 16:18:50,810 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 4.465
2024-06-21 16:18:51,128 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 5.221
2024-06-21 16:18:51,548 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 4.606
2024-06-21 16:18:51,854 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 4.503
2024-06-21 16:18:52,259 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 3.648
2024-06-21 16:18:52,572 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 5.391
2024-06-21 16:18:52,982 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 4.347
2024-06-21 16:18:53,289 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 4.514
2024-06-21 16:18:53,691 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 4.686
2024-06-21 16:18:54,011 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 5.574
2024-06-21 16:18:54,412 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 6.042
2024-06-21 16:18:54,715 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 4.558
2024-06-21 16:18:55,109 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 4.442
2024-06-21 16:18:55,425 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 4.501
2024-06-21 16:18:55,646 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 4.241
2024-06-21 16:19:06,474 - INFO: 6/10 final results:
2024-06-21 16:19:06,474 - INFO: Training loss: 4.797.
2024-06-21 16:19:06,474 - INFO: Training MAE: 4.808.
2024-06-21 16:19:06,474 - INFO: Training MSE: 37.841.
2024-06-21 16:19:26,641 - INFO: Epoch: 6/10, Loss_train: 4.7967655412082015, Loss_val: 6.854615951406545
2024-06-21 16:19:26,641 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:19:26,641 - INFO: Epoch 7/10...
2024-06-21 16:19:26,641 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:19:26,641 - INFO: Batch size: 32.
2024-06-21 16:19:26,644 - INFO: Dataset:
2024-06-21 16:19:26,644 - INFO: Batch size:
2024-06-21 16:19:26,644 - INFO: Number of workers:
2024-06-21 16:19:27,753 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 4.674
2024-06-21 16:19:28,078 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 5.263
2024-06-21 16:19:28,481 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 4.855
2024-06-21 16:19:28,805 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 4.586
2024-06-21 16:19:29,210 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 4.395
2024-06-21 16:19:29,530 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 4.749
2024-06-21 16:19:29,937 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 4.356
2024-06-21 16:19:30,256 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 4.848
2024-06-21 16:19:30,649 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 4.850
2024-06-21 16:19:30,958 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 4.164
2024-06-21 16:19:31,352 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 4.523
2024-06-21 16:19:31,672 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 5.649
2024-06-21 16:19:32,080 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 5.714
2024-06-21 16:19:32,399 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 4.143
2024-06-21 16:19:32,809 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 3.955
2024-06-21 16:19:33,125 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 4.509
2024-06-21 16:19:33,533 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 4.668
2024-06-21 16:19:33,849 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 4.135
2024-06-21 16:19:34,252 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 4.233
2024-06-21 16:19:34,562 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 4.183
2024-06-21 16:19:34,959 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 4.341
2024-06-21 16:19:35,277 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 3.281
2024-06-21 16:19:35,679 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 4.316
2024-06-21 16:19:35,997 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 4.826
2024-06-21 16:19:36,396 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 4.684
2024-06-21 16:19:36,710 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 4.537
2024-06-21 16:19:37,110 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 4.815
2024-06-21 16:19:37,424 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 4.382
2024-06-21 16:19:37,648 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 4.267
2024-06-21 16:19:48,616 - INFO: 7/10 final results:
2024-06-21 16:19:48,616 - INFO: Training loss: 4.548.
2024-06-21 16:19:48,616 - INFO: Training MAE: 4.554.
2024-06-21 16:19:48,616 - INFO: Training MSE: 34.874.
2024-06-21 16:20:09,123 - INFO: Epoch: 7/10, Loss_train: 4.548353310289054, Loss_val: 4.569577841923155
2024-06-21 16:20:09,180 - INFO: Saved new best metric model for epoch 7.
2024-06-21 16:20:09,180 - INFO: Best internal validation val_loss: 4.570 at epoch: 7.
2024-06-21 16:20:09,180 - INFO: Epoch 8/10...
2024-06-21 16:20:09,180 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:20:09,180 - INFO: Batch size: 32.
2024-06-21 16:20:09,183 - INFO: Dataset:
2024-06-21 16:20:09,184 - INFO: Batch size:
2024-06-21 16:20:09,184 - INFO: Number of workers:
2024-06-21 16:20:10,303 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 4.445
2024-06-21 16:20:10,640 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 4.811
2024-06-21 16:20:11,040 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 4.096
2024-06-21 16:20:11,362 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 4.070
2024-06-21 16:20:11,770 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 4.139
2024-06-21 16:20:12,102 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 4.981
2024-06-21 16:20:12,481 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 3.988
2024-06-21 16:20:12,801 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 5.150
2024-06-21 16:20:13,192 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 4.262
2024-06-21 16:20:13,541 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 4.374
2024-06-21 16:20:13,907 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 5.037
2024-06-21 16:20:14,226 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 4.330
2024-06-21 16:20:14,637 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 4.543
2024-06-21 16:20:14,969 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 4.266
2024-06-21 16:20:15,353 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 4.710
2024-06-21 16:20:15,669 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 3.989
2024-06-21 16:20:16,073 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 4.780
2024-06-21 16:20:16,401 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 3.914
2024-06-21 16:20:16,774 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 4.601
2024-06-21 16:20:17,084 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 4.510
2024-06-21 16:20:17,481 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 3.771
2024-06-21 16:20:17,810 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 4.207
2024-06-21 16:20:18,198 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 4.434
2024-06-21 16:20:18,516 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 4.697
2024-06-21 16:20:18,914 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 4.838
2024-06-21 16:20:19,242 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 4.711
2024-06-21 16:20:19,630 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 4.244
2024-06-21 16:20:19,945 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 4.735
2024-06-21 16:20:20,167 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 3.614
2024-06-21 16:20:31,229 - INFO: 8/10 final results:
2024-06-21 16:20:31,229 - INFO: Training loss: 4.422.
2024-06-21 16:20:31,229 - INFO: Training MAE: 4.438.
2024-06-21 16:20:31,229 - INFO: Training MSE: 33.222.
2024-06-21 16:20:51,383 - INFO: Epoch: 8/10, Loss_train: 4.422263712718569, Loss_val: 4.821652741267763
2024-06-21 16:20:51,384 - INFO: Best internal validation val_loss: 4.570 at epoch: 7.
2024-06-21 16:20:51,384 - INFO: Epoch 9/10...
2024-06-21 16:20:51,384 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:20:51,384 - INFO: Batch size: 32.
2024-06-21 16:20:51,387 - INFO: Dataset:
2024-06-21 16:20:51,387 - INFO: Batch size:
2024-06-21 16:20:51,387 - INFO: Number of workers:
2024-06-21 16:20:52,496 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 4.608
2024-06-21 16:20:52,834 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 4.047
2024-06-21 16:20:53,236 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 4.470
2024-06-21 16:20:53,562 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 4.366
2024-06-21 16:20:53,985 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 4.966
2024-06-21 16:20:54,305 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 3.985
2024-06-21 16:20:54,700 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 4.463
2024-06-21 16:20:55,020 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 3.641
2024-06-21 16:20:55,435 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 4.204
2024-06-21 16:20:55,747 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 4.307
2024-06-21 16:20:56,131 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 4.512
2024-06-21 16:20:56,453 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 4.038
2024-06-21 16:20:56,878 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 4.662
2024-06-21 16:20:57,201 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 4.212
2024-06-21 16:20:57,601 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 4.076
2024-06-21 16:20:57,920 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 4.226
2024-06-21 16:20:58,344 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 4.654
2024-06-21 16:20:58,663 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 4.295
2024-06-21 16:20:59,056 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 4.047
2024-06-21 16:20:59,369 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 4.338
2024-06-21 16:20:59,781 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 3.868
2024-06-21 16:21:00,101 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 3.925
2024-06-21 16:21:00,483 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 4.062
2024-06-21 16:21:00,803 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 4.417
2024-06-21 16:21:01,209 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 3.943
2024-06-21 16:21:01,525 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 4.626
2024-06-21 16:21:01,902 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 3.821
2024-06-21 16:21:02,218 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 4.458
2024-06-21 16:21:02,440 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 4.832
2024-06-21 16:21:13,462 - INFO: 9/10 final results:
2024-06-21 16:21:13,463 - INFO: Training loss: 4.278.
2024-06-21 16:21:13,463 - INFO: Training MAE: 4.267.
2024-06-21 16:21:13,463 - INFO: Training MSE: 31.759.
2024-06-21 16:21:33,813 - INFO: Epoch: 9/10, Loss_train: 4.278311828087116, Loss_val: 4.386174645917169
2024-06-21 16:21:33,870 - INFO: Saved new best metric model for epoch 9.
2024-06-21 16:21:33,870 - INFO: Best internal validation val_loss: 4.386 at epoch: 9.
2024-06-21 16:21:33,870 - INFO: Epoch 10/10...
2024-06-21 16:21:33,870 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:21:33,870 - INFO: Batch size: 32.
2024-06-21 16:21:33,873 - INFO: Dataset:
2024-06-21 16:21:33,873 - INFO: Batch size:
2024-06-21 16:21:33,873 - INFO: Number of workers:
2024-06-21 16:21:34,980 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 4.883
2024-06-21 16:21:35,289 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 5.123
2024-06-21 16:21:35,687 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 4.306
2024-06-21 16:21:36,006 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 4.397
2024-06-21 16:21:36,420 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 3.445
2024-06-21 16:21:36,723 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 4.692
2024-06-21 16:21:37,115 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 4.132
2024-06-21 16:21:37,430 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 4.430
2024-06-21 16:21:37,837 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 3.893
2024-06-21 16:21:38,132 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 4.136
2024-06-21 16:21:38,516 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 4.799
2024-06-21 16:21:38,837 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 3.626
2024-06-21 16:21:39,263 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 5.897
2024-06-21 16:21:39,573 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 4.677
2024-06-21 16:21:39,980 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 5.032
2024-06-21 16:21:40,298 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 4.654
2024-06-21 16:21:40,713 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 4.793
2024-06-21 16:21:41,016 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 3.857
2024-06-21 16:21:41,415 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 3.755
2024-06-21 16:21:41,728 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 4.367
2024-06-21 16:21:42,143 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 4.390
2024-06-21 16:21:42,451 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 3.883
2024-06-21 16:21:42,852 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 3.769
2024-06-21 16:21:43,171 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 4.752
2024-06-21 16:21:43,584 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 3.972
2024-06-21 16:21:43,887 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 3.921
2024-06-21 16:21:44,284 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 3.972
2024-06-21 16:21:44,599 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 3.946
2024-06-21 16:21:44,821 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 3.954
2024-06-21 16:21:56,009 - INFO: 10/10 final results:
2024-06-21 16:21:56,009 - INFO: Training loss: 4.326.
2024-06-21 16:21:56,009 - INFO: Training MAE: 4.333.
2024-06-21 16:21:56,009 - INFO: Training MSE: 32.426.
2024-06-21 16:22:16,035 - INFO: Epoch: 10/10, Loss_train: 4.32594424280627, Loss_val: 4.851717685831004
2024-06-21 16:22:16,035 - INFO: Best internal validation val_loss: 4.386 at epoch: 9.
