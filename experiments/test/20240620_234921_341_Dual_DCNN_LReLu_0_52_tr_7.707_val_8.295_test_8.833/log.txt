2024-06-20 23:03:00,445 - INFO: Device: cuda.
2024-06-20 23:03:00,445 - INFO: Torch version: 2.0.1+cu117.
2024-06-20 23:03:00,445 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-20 23:03:00,445 - INFO: Model name: Dual_DCNN_LReLu
2024-06-20 23:03:00,445 - INFO: Seed: 4
2024-06-20 23:03:00,445 - INFO: 42 patients have been found in the data directory.
2024-06-20 23:03:00,485 - INFO: Train set contains 32 patients.
2024-06-20 23:03:00,485 - INFO: Val set contains 5 patients.
2024-06-20 23:03:00,485 - INFO: Test set contains 5 patients.
2024-06-20 23:03:00,485 - INFO: Fold: 0
2024-06-20 23:03:00,486 - INFO: Performing 2-fold Cross Validation.
2024-06-20 23:03:00,487 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-20 23:03:00,487 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-20 23:03:00,487 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-20 23:03:00,617 - INFO: To_device: False.
2024-06-20 23:03:00,619 - INFO: Transformers have been made successfully.
2024-06-20 23:03:00,619 - INFO: Dataset type: cache.
2024-06-20 23:03:00,619 - INFO: Dataloader type: standard.
2024-06-20 23:04:51,020 - INFO: Train dataloader arguments.
2024-06-20 23:04:51,020 - INFO: 	Batch_size: 32.
2024-06-20 23:04:51,020 - INFO: 	Shuffle: True.
2024-06-20 23:04:51,020 - INFO: 	Sampler: None.
2024-06-20 23:04:51,020 - INFO: 	Num_workers: 4.
2024-06-20 23:04:51,020 - INFO: 	Drop_last: False.
2024-06-20 23:04:51,070 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-20 23:04:51,915 - INFO: Weight init name: kaiming_uniform.
2024-06-20 23:04:54,331 - INFO: Number of training iterations per epoch: 29.
2024-06-20 23:04:54,332 - INFO: Epoch 1/200...
2024-06-20 23:04:54,332 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:04:54,332 - INFO: Batch size: 32.
2024-06-20 23:04:54,332 - INFO: Dataset:
2024-06-20 23:04:54,332 - INFO: Batch size:
2024-06-20 23:04:54,332 - INFO: Number of workers:
2024-06-20 23:04:57,249 - INFO: Epoch: 1/200, Batch: 1/29, Batch_Loss_Train: 70.996
2024-06-20 23:04:57,554 - INFO: Epoch: 1/200, Batch: 2/29, Batch_Loss_Train: 83.105
2024-06-20 23:04:57,944 - INFO: Epoch: 1/200, Batch: 3/29, Batch_Loss_Train: 19153.533
2024-06-20 23:04:58,250 - INFO: Epoch: 1/200, Batch: 4/29, Batch_Loss_Train: 5204.818
2024-06-20 23:04:58,675 - INFO: Epoch: 1/200, Batch: 5/29, Batch_Loss_Train: 79394.266
2024-06-20 23:04:58,981 - INFO: Epoch: 1/200, Batch: 6/29, Batch_Loss_Train: 110632.680
2024-06-20 23:04:59,369 - INFO: Epoch: 1/200, Batch: 7/29, Batch_Loss_Train: 76685.234
2024-06-20 23:04:59,687 - INFO: Epoch: 1/200, Batch: 8/29, Batch_Loss_Train: 128506.250
2024-06-20 23:05:00,108 - INFO: Epoch: 1/200, Batch: 9/29, Batch_Loss_Train: 115896.461
2024-06-20 23:05:00,407 - INFO: Epoch: 1/200, Batch: 10/29, Batch_Loss_Train: 14401.586
2024-06-20 23:05:00,795 - INFO: Epoch: 1/200, Batch: 11/29, Batch_Loss_Train: 3606.475
2024-06-20 23:05:01,113 - INFO: Epoch: 1/200, Batch: 12/29, Batch_Loss_Train: 3007.651
2024-06-20 23:05:01,557 - INFO: Epoch: 1/200, Batch: 13/29, Batch_Loss_Train: 1151.817
2024-06-20 23:05:01,861 - INFO: Epoch: 1/200, Batch: 14/29, Batch_Loss_Train: 433.926
2024-06-20 23:05:02,260 - INFO: Epoch: 1/200, Batch: 15/29, Batch_Loss_Train: 271.152
2024-06-20 23:05:02,576 - INFO: Epoch: 1/200, Batch: 16/29, Batch_Loss_Train: 202.061
2024-06-20 23:05:03,034 - INFO: Epoch: 1/200, Batch: 17/29, Batch_Loss_Train: 171.597
2024-06-20 23:05:03,337 - INFO: Epoch: 1/200, Batch: 18/29, Batch_Loss_Train: 151.734
2024-06-20 23:05:03,724 - INFO: Epoch: 1/200, Batch: 19/29, Batch_Loss_Train: 203.019
2024-06-20 23:05:04,024 - INFO: Epoch: 1/200, Batch: 20/29, Batch_Loss_Train: 185.843
2024-06-20 23:05:04,461 - INFO: Epoch: 1/200, Batch: 21/29, Batch_Loss_Train: 144.080
2024-06-20 23:05:04,763 - INFO: Epoch: 1/200, Batch: 22/29, Batch_Loss_Train: 116.611
2024-06-20 23:05:05,151 - INFO: Epoch: 1/200, Batch: 23/29, Batch_Loss_Train: 150.586
2024-06-20 23:05:05,453 - INFO: Epoch: 1/200, Batch: 24/29, Batch_Loss_Train: 98.869
2024-06-20 23:05:05,881 - INFO: Epoch: 1/200, Batch: 25/29, Batch_Loss_Train: 92.806
2024-06-20 23:05:06,182 - INFO: Epoch: 1/200, Batch: 26/29, Batch_Loss_Train: 161.764
2024-06-20 23:05:06,568 - INFO: Epoch: 1/200, Batch: 27/29, Batch_Loss_Train: 130.107
2024-06-20 23:05:06,868 - INFO: Epoch: 1/200, Batch: 28/29, Batch_Loss_Train: 103.861
2024-06-20 23:05:08,154 - INFO: Epoch: 1/200, Batch: 29/29, Batch_Loss_Train: 90.211
2024-06-20 23:05:19,117 - INFO: 1/200 final results:
2024-06-20 23:05:19,117 - INFO: Training loss: 19327.693.
2024-06-20 23:05:19,117 - INFO: Training MAE: 65.303.
2024-06-20 23:05:19,117 - INFO: Training MSE: 19708.217.
2024-06-20 23:05:39,209 - INFO: Epoch: 1/200, Loss_train: 19327.693098660173, Loss_val: 1530.5950927734375
2024-06-20 23:05:39,209 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-20 23:05:39,209 - INFO: Epoch 2/200...
2024-06-20 23:05:39,209 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:05:39,209 - INFO: Batch size: 32.
2024-06-20 23:05:39,212 - INFO: Dataset:
2024-06-20 23:05:39,213 - INFO: Batch size:
2024-06-20 23:05:39,213 - INFO: Number of workers:
2024-06-20 23:05:40,329 - INFO: Epoch: 2/200, Batch: 1/29, Batch_Loss_Train: 104.739
2024-06-20 23:05:40,651 - INFO: Epoch: 2/200, Batch: 2/29, Batch_Loss_Train: 72.476
2024-06-20 23:05:41,053 - INFO: Epoch: 2/200, Batch: 3/29, Batch_Loss_Train: 98.087
2024-06-20 23:05:41,377 - INFO: Epoch: 2/200, Batch: 4/29, Batch_Loss_Train: 87.648
2024-06-20 23:05:41,782 - INFO: Epoch: 2/200, Batch: 5/29, Batch_Loss_Train: 69.490
2024-06-20 23:05:42,102 - INFO: Epoch: 2/200, Batch: 6/29, Batch_Loss_Train: 80.562
2024-06-20 23:05:42,507 - INFO: Epoch: 2/200, Batch: 7/29, Batch_Loss_Train: 88.348
2024-06-20 23:05:42,835 - INFO: Epoch: 2/200, Batch: 8/29, Batch_Loss_Train: 86.107
2024-06-20 23:05:43,248 - INFO: Epoch: 2/200, Batch: 9/29, Batch_Loss_Train: 106.974
2024-06-20 23:05:43,571 - INFO: Epoch: 2/200, Batch: 10/29, Batch_Loss_Train: 92.037
2024-06-20 23:05:43,989 - INFO: Epoch: 2/200, Batch: 11/29, Batch_Loss_Train: 89.442
2024-06-20 23:05:44,316 - INFO: Epoch: 2/200, Batch: 12/29, Batch_Loss_Train: 85.692
2024-06-20 23:05:44,724 - INFO: Epoch: 2/200, Batch: 13/29, Batch_Loss_Train: 56.235
2024-06-20 23:05:45,044 - INFO: Epoch: 2/200, Batch: 14/29, Batch_Loss_Train: 109.072
2024-06-20 23:05:45,451 - INFO: Epoch: 2/200, Batch: 15/29, Batch_Loss_Train: 102.888
2024-06-20 23:05:45,770 - INFO: Epoch: 2/200, Batch: 16/29, Batch_Loss_Train: 91.379
2024-06-20 23:05:46,177 - INFO: Epoch: 2/200, Batch: 17/29, Batch_Loss_Train: 103.724
2024-06-20 23:05:46,495 - INFO: Epoch: 2/200, Batch: 18/29, Batch_Loss_Train: 99.727
2024-06-20 23:05:46,901 - INFO: Epoch: 2/200, Batch: 19/29, Batch_Loss_Train: 106.476
2024-06-20 23:05:47,217 - INFO: Epoch: 2/200, Batch: 20/29, Batch_Loss_Train: 86.424
2024-06-20 23:05:47,622 - INFO: Epoch: 2/200, Batch: 21/29, Batch_Loss_Train: 76.446
2024-06-20 23:05:47,940 - INFO: Epoch: 2/200, Batch: 22/29, Batch_Loss_Train: 76.435
2024-06-20 23:05:48,344 - INFO: Epoch: 2/200, Batch: 23/29, Batch_Loss_Train: 82.072
2024-06-20 23:05:48,661 - INFO: Epoch: 2/200, Batch: 24/29, Batch_Loss_Train: 68.569
2024-06-20 23:05:49,059 - INFO: Epoch: 2/200, Batch: 25/29, Batch_Loss_Train: 50.086
2024-06-20 23:05:49,373 - INFO: Epoch: 2/200, Batch: 26/29, Batch_Loss_Train: 54.824
2024-06-20 23:05:49,774 - INFO: Epoch: 2/200, Batch: 27/29, Batch_Loss_Train: 76.213
2024-06-20 23:05:50,087 - INFO: Epoch: 2/200, Batch: 28/29, Batch_Loss_Train: 60.363
2024-06-20 23:05:50,304 - INFO: Epoch: 2/200, Batch: 29/29, Batch_Loss_Train: 106.006
2024-06-20 23:06:01,277 - INFO: 2/200 final results:
2024-06-20 23:06:01,277 - INFO: Training loss: 85.122.
2024-06-20 23:06:01,277 - INFO: Training MAE: 7.416.
2024-06-20 23:06:01,277 - INFO: Training MSE: 84.709.
2024-06-20 23:06:21,612 - INFO: Epoch: 2/200, Loss_train: 85.12214489640861, Loss_val: 104.74891767830684
2024-06-20 23:06:21,659 - INFO: Saved new best metric model for epoch 2.
2024-06-20 23:06:21,659 - INFO: Best internal validation val_loss: 104.749 at epoch: 2.
2024-06-20 23:06:21,659 - INFO: Epoch 3/200...
2024-06-20 23:06:21,659 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:06:21,659 - INFO: Batch size: 32.
2024-06-20 23:06:21,663 - INFO: Dataset:
2024-06-20 23:06:21,663 - INFO: Batch size:
2024-06-20 23:06:21,663 - INFO: Number of workers:
2024-06-20 23:06:22,796 - INFO: Epoch: 3/200, Batch: 1/29, Batch_Loss_Train: 129.180
2024-06-20 23:06:23,121 - INFO: Epoch: 3/200, Batch: 2/29, Batch_Loss_Train: 181.691
2024-06-20 23:06:23,525 - INFO: Epoch: 3/200, Batch: 3/29, Batch_Loss_Train: 143.345
2024-06-20 23:06:23,847 - INFO: Epoch: 3/200, Batch: 4/29, Batch_Loss_Train: 70.204
2024-06-20 23:06:24,266 - INFO: Epoch: 3/200, Batch: 5/29, Batch_Loss_Train: 59.534
2024-06-20 23:06:24,584 - INFO: Epoch: 3/200, Batch: 6/29, Batch_Loss_Train: 102.840
2024-06-20 23:06:24,979 - INFO: Epoch: 3/200, Batch: 7/29, Batch_Loss_Train: 141.871
2024-06-20 23:06:25,296 - INFO: Epoch: 3/200, Batch: 8/29, Batch_Loss_Train: 79.664
2024-06-20 23:06:25,710 - INFO: Epoch: 3/200, Batch: 9/29, Batch_Loss_Train: 77.909
2024-06-20 23:06:26,024 - INFO: Epoch: 3/200, Batch: 10/29, Batch_Loss_Train: 55.044
2024-06-20 23:06:26,414 - INFO: Epoch: 3/200, Batch: 11/29, Batch_Loss_Train: 64.340
2024-06-20 23:06:26,732 - INFO: Epoch: 3/200, Batch: 12/29, Batch_Loss_Train: 61.795
2024-06-20 23:06:27,158 - INFO: Epoch: 3/200, Batch: 13/29, Batch_Loss_Train: 67.071
2024-06-20 23:06:27,479 - INFO: Epoch: 3/200, Batch: 14/29, Batch_Loss_Train: 71.108
2024-06-20 23:06:27,882 - INFO: Epoch: 3/200, Batch: 15/29, Batch_Loss_Train: 119.151
2024-06-20 23:06:28,200 - INFO: Epoch: 3/200, Batch: 16/29, Batch_Loss_Train: 64.102
2024-06-20 23:06:28,623 - INFO: Epoch: 3/200, Batch: 17/29, Batch_Loss_Train: 89.773
2024-06-20 23:06:28,938 - INFO: Epoch: 3/200, Batch: 18/29, Batch_Loss_Train: 89.995
2024-06-20 23:06:29,332 - INFO: Epoch: 3/200, Batch: 19/29, Batch_Loss_Train: 59.389
2024-06-20 23:06:29,644 - INFO: Epoch: 3/200, Batch: 20/29, Batch_Loss_Train: 58.210
2024-06-20 23:06:30,059 - INFO: Epoch: 3/200, Batch: 21/29, Batch_Loss_Train: 70.279
2024-06-20 23:06:30,375 - INFO: Epoch: 3/200, Batch: 22/29, Batch_Loss_Train: 76.979
2024-06-20 23:06:30,765 - INFO: Epoch: 3/200, Batch: 23/29, Batch_Loss_Train: 92.817
2024-06-20 23:06:31,081 - INFO: Epoch: 3/200, Batch: 24/29, Batch_Loss_Train: 97.521
2024-06-20 23:06:31,491 - INFO: Epoch: 3/200, Batch: 25/29, Batch_Loss_Train: 144.417
2024-06-20 23:06:31,805 - INFO: Epoch: 3/200, Batch: 26/29, Batch_Loss_Train: 171.656
2024-06-20 23:06:32,193 - INFO: Epoch: 3/200, Batch: 27/29, Batch_Loss_Train: 272.813
2024-06-20 23:06:32,506 - INFO: Epoch: 3/200, Batch: 28/29, Batch_Loss_Train: 294.505
2024-06-20 23:06:32,727 - INFO: Epoch: 3/200, Batch: 29/29, Batch_Loss_Train: 180.627
2024-06-20 23:06:43,632 - INFO: 3/200 final results:
2024-06-20 23:06:43,633 - INFO: Training loss: 109.925.
2024-06-20 23:06:43,633 - INFO: Training MAE: 8.396.
2024-06-20 23:06:43,633 - INFO: Training MSE: 108.527.
2024-06-20 23:07:03,720 - INFO: Epoch: 3/200, Loss_train: 109.92511959733635, Loss_val: 72.44183152297447
2024-06-20 23:07:03,777 - INFO: Saved new best metric model for epoch 3.
2024-06-20 23:07:03,777 - INFO: Best internal validation val_loss: 72.442 at epoch: 3.
2024-06-20 23:07:03,777 - INFO: Epoch 4/200...
2024-06-20 23:07:03,777 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:07:03,777 - INFO: Batch size: 32.
2024-06-20 23:07:03,781 - INFO: Dataset:
2024-06-20 23:07:03,781 - INFO: Batch size:
2024-06-20 23:07:03,781 - INFO: Number of workers:
2024-06-20 23:07:04,922 - INFO: Epoch: 4/200, Batch: 1/29, Batch_Loss_Train: 60.634
2024-06-20 23:07:05,233 - INFO: Epoch: 4/200, Batch: 2/29, Batch_Loss_Train: 54.296
2024-06-20 23:07:05,619 - INFO: Epoch: 4/200, Batch: 3/29, Batch_Loss_Train: 58.487
2024-06-20 23:07:05,940 - INFO: Epoch: 4/200, Batch: 4/29, Batch_Loss_Train: 68.218
2024-06-20 23:07:06,361 - INFO: Epoch: 4/200, Batch: 5/29, Batch_Loss_Train: 52.934
2024-06-20 23:07:06,667 - INFO: Epoch: 4/200, Batch: 6/29, Batch_Loss_Train: 49.609
2024-06-20 23:07:07,063 - INFO: Epoch: 4/200, Batch: 7/29, Batch_Loss_Train: 76.566
2024-06-20 23:07:07,384 - INFO: Epoch: 4/200, Batch: 8/29, Batch_Loss_Train: 47.620
2024-06-20 23:07:07,807 - INFO: Epoch: 4/200, Batch: 9/29, Batch_Loss_Train: 63.688
2024-06-20 23:07:08,108 - INFO: Epoch: 4/200, Batch: 10/29, Batch_Loss_Train: 61.874
2024-06-20 23:07:08,492 - INFO: Epoch: 4/200, Batch: 11/29, Batch_Loss_Train: 50.623
2024-06-20 23:07:08,814 - INFO: Epoch: 4/200, Batch: 12/29, Batch_Loss_Train: 39.984
2024-06-20 23:07:09,240 - INFO: Epoch: 4/200, Batch: 13/29, Batch_Loss_Train: 64.004
2024-06-20 23:07:09,548 - INFO: Epoch: 4/200, Batch: 14/29, Batch_Loss_Train: 92.042
2024-06-20 23:07:09,950 - INFO: Epoch: 4/200, Batch: 15/29, Batch_Loss_Train: 65.663
2024-06-20 23:07:10,268 - INFO: Epoch: 4/200, Batch: 16/29, Batch_Loss_Train: 53.482
2024-06-20 23:07:10,705 - INFO: Epoch: 4/200, Batch: 17/29, Batch_Loss_Train: 52.637
2024-06-20 23:07:11,011 - INFO: Epoch: 4/200, Batch: 18/29, Batch_Loss_Train: 126.367
2024-06-20 23:07:11,403 - INFO: Epoch: 4/200, Batch: 19/29, Batch_Loss_Train: 334.435
2024-06-20 23:07:11,718 - INFO: Epoch: 4/200, Batch: 20/29, Batch_Loss_Train: 551.892
2024-06-20 23:07:12,146 - INFO: Epoch: 4/200, Batch: 21/29, Batch_Loss_Train: 602.020
2024-06-20 23:07:12,450 - INFO: Epoch: 4/200, Batch: 22/29, Batch_Loss_Train: 413.993
2024-06-20 23:07:12,835 - INFO: Epoch: 4/200, Batch: 23/29, Batch_Loss_Train: 202.750
2024-06-20 23:07:13,152 - INFO: Epoch: 4/200, Batch: 24/29, Batch_Loss_Train: 84.773
2024-06-20 23:07:13,575 - INFO: Epoch: 4/200, Batch: 25/29, Batch_Loss_Train: 50.034
2024-06-20 23:07:13,879 - INFO: Epoch: 4/200, Batch: 26/29, Batch_Loss_Train: 33.161
2024-06-20 23:07:14,257 - INFO: Epoch: 4/200, Batch: 27/29, Batch_Loss_Train: 39.162
2024-06-20 23:07:14,573 - INFO: Epoch: 4/200, Batch: 28/29, Batch_Loss_Train: 70.937
2024-06-20 23:07:14,793 - INFO: Epoch: 4/200, Batch: 29/29, Batch_Loss_Train: 41.839
2024-06-20 23:07:25,885 - INFO: 4/200 final results:
2024-06-20 23:07:25,885 - INFO: Training loss: 122.887.
2024-06-20 23:07:25,885 - INFO: Training MAE: 8.355.
2024-06-20 23:07:25,885 - INFO: Training MSE: 124.490.
2024-06-20 23:07:46,188 - INFO: Epoch: 4/200, Loss_train: 122.8870832509008, Loss_val: 67.3720580791605
2024-06-20 23:07:46,244 - INFO: Saved new best metric model for epoch 4.
2024-06-20 23:07:46,244 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:07:46,244 - INFO: Epoch 5/200...
2024-06-20 23:07:46,244 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:07:46,244 - INFO: Batch size: 32.
2024-06-20 23:07:46,247 - INFO: Dataset:
2024-06-20 23:07:46,247 - INFO: Batch size:
2024-06-20 23:07:46,247 - INFO: Number of workers:
2024-06-20 23:07:47,359 - INFO: Epoch: 5/200, Batch: 1/29, Batch_Loss_Train: 61.211
2024-06-20 23:07:47,681 - INFO: Epoch: 5/200, Batch: 2/29, Batch_Loss_Train: 33.497
2024-06-20 23:07:48,090 - INFO: Epoch: 5/200, Batch: 3/29, Batch_Loss_Train: 31.429
2024-06-20 23:07:48,410 - INFO: Epoch: 5/200, Batch: 4/29, Batch_Loss_Train: 43.574
2024-06-20 23:07:48,828 - INFO: Epoch: 5/200, Batch: 5/29, Batch_Loss_Train: 52.831
2024-06-20 23:07:49,135 - INFO: Epoch: 5/200, Batch: 6/29, Batch_Loss_Train: 40.210
2024-06-20 23:07:49,544 - INFO: Epoch: 5/200, Batch: 7/29, Batch_Loss_Train: 49.144
2024-06-20 23:07:49,863 - INFO: Epoch: 5/200, Batch: 8/29, Batch_Loss_Train: 59.583
2024-06-20 23:07:50,285 - INFO: Epoch: 5/200, Batch: 9/29, Batch_Loss_Train: 51.965
2024-06-20 23:07:50,589 - INFO: Epoch: 5/200, Batch: 10/29, Batch_Loss_Train: 45.468
2024-06-20 23:07:50,983 - INFO: Epoch: 5/200, Batch: 11/29, Batch_Loss_Train: 73.701
2024-06-20 23:07:51,314 - INFO: Epoch: 5/200, Batch: 12/29, Batch_Loss_Train: 182.938
2024-06-20 23:07:51,738 - INFO: Epoch: 5/200, Batch: 13/29, Batch_Loss_Train: 289.596
2024-06-20 23:07:52,046 - INFO: Epoch: 5/200, Batch: 14/29, Batch_Loss_Train: 319.821
2024-06-20 23:07:52,449 - INFO: Epoch: 5/200, Batch: 15/29, Batch_Loss_Train: 255.893
2024-06-20 23:07:52,765 - INFO: Epoch: 5/200, Batch: 16/29, Batch_Loss_Train: 152.984
2024-06-20 23:07:53,177 - INFO: Epoch: 5/200, Batch: 17/29, Batch_Loss_Train: 98.767
2024-06-20 23:07:53,479 - INFO: Epoch: 5/200, Batch: 18/29, Batch_Loss_Train: 73.725
2024-06-20 23:07:53,882 - INFO: Epoch: 5/200, Batch: 19/29, Batch_Loss_Train: 65.327
2024-06-20 23:07:54,195 - INFO: Epoch: 5/200, Batch: 20/29, Batch_Loss_Train: 40.094
2024-06-20 23:07:54,612 - INFO: Epoch: 5/200, Batch: 21/29, Batch_Loss_Train: 63.092
2024-06-20 23:07:54,915 - INFO: Epoch: 5/200, Batch: 22/29, Batch_Loss_Train: 134.167
2024-06-20 23:07:55,307 - INFO: Epoch: 5/200, Batch: 23/29, Batch_Loss_Train: 167.988
2024-06-20 23:07:55,623 - INFO: Epoch: 5/200, Batch: 24/29, Batch_Loss_Train: 173.980
2024-06-20 23:07:56,026 - INFO: Epoch: 5/200, Batch: 25/29, Batch_Loss_Train: 118.959
2024-06-20 23:07:56,327 - INFO: Epoch: 5/200, Batch: 26/29, Batch_Loss_Train: 76.536
2024-06-20 23:07:56,719 - INFO: Epoch: 5/200, Batch: 27/29, Batch_Loss_Train: 66.305
2024-06-20 23:07:57,032 - INFO: Epoch: 5/200, Batch: 28/29, Batch_Loss_Train: 49.253
2024-06-20 23:07:57,243 - INFO: Epoch: 5/200, Batch: 29/29, Batch_Loss_Train: 87.671
2024-06-20 23:08:08,112 - INFO: 5/200 final results:
2024-06-20 23:08:08,112 - INFO: Training loss: 102.059.
2024-06-20 23:08:08,112 - INFO: Training MAE: 8.087.
2024-06-20 23:08:08,112 - INFO: Training MSE: 102.344.
2024-06-20 23:08:28,652 - INFO: Epoch: 5/200, Loss_train: 102.0589494376347, Loss_val: 109.55139186464507
2024-06-20 23:08:28,652 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:08:28,652 - INFO: Epoch 6/200...
2024-06-20 23:08:28,652 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:08:28,652 - INFO: Batch size: 32.
2024-06-20 23:08:28,655 - INFO: Dataset:
2024-06-20 23:08:28,655 - INFO: Batch size:
2024-06-20 23:08:28,655 - INFO: Number of workers:
2024-06-20 23:08:29,790 - INFO: Epoch: 6/200, Batch: 1/29, Batch_Loss_Train: 107.160
2024-06-20 23:08:30,098 - INFO: Epoch: 6/200, Batch: 2/29, Batch_Loss_Train: 125.583
2024-06-20 23:08:30,507 - INFO: Epoch: 6/200, Batch: 3/29, Batch_Loss_Train: 105.163
2024-06-20 23:08:30,827 - INFO: Epoch: 6/200, Batch: 4/29, Batch_Loss_Train: 88.020
2024-06-20 23:08:31,230 - INFO: Epoch: 6/200, Batch: 5/29, Batch_Loss_Train: 62.965
2024-06-20 23:08:31,534 - INFO: Epoch: 6/200, Batch: 6/29, Batch_Loss_Train: 45.076
2024-06-20 23:08:31,941 - INFO: Epoch: 6/200, Batch: 7/29, Batch_Loss_Train: 40.189
2024-06-20 23:08:32,263 - INFO: Epoch: 6/200, Batch: 8/29, Batch_Loss_Train: 37.097
2024-06-20 23:08:32,679 - INFO: Epoch: 6/200, Batch: 9/29, Batch_Loss_Train: 39.969
2024-06-20 23:08:32,979 - INFO: Epoch: 6/200, Batch: 10/29, Batch_Loss_Train: 46.029
2024-06-20 23:08:33,384 - INFO: Epoch: 6/200, Batch: 11/29, Batch_Loss_Train: 33.118
2024-06-20 23:08:33,702 - INFO: Epoch: 6/200, Batch: 12/29, Batch_Loss_Train: 28.073
2024-06-20 23:08:34,127 - INFO: Epoch: 6/200, Batch: 13/29, Batch_Loss_Train: 34.452
2024-06-20 23:08:34,433 - INFO: Epoch: 6/200, Batch: 14/29, Batch_Loss_Train: 45.620
2024-06-20 23:08:34,839 - INFO: Epoch: 6/200, Batch: 15/29, Batch_Loss_Train: 47.121
2024-06-20 23:08:35,155 - INFO: Epoch: 6/200, Batch: 16/29, Batch_Loss_Train: 50.627
2024-06-20 23:08:35,581 - INFO: Epoch: 6/200, Batch: 17/29, Batch_Loss_Train: 41.605
2024-06-20 23:08:35,887 - INFO: Epoch: 6/200, Batch: 18/29, Batch_Loss_Train: 49.508
2024-06-20 23:08:36,285 - INFO: Epoch: 6/200, Batch: 19/29, Batch_Loss_Train: 40.142
2024-06-20 23:08:36,600 - INFO: Epoch: 6/200, Batch: 20/29, Batch_Loss_Train: 56.520
2024-06-20 23:08:37,021 - INFO: Epoch: 6/200, Batch: 21/29, Batch_Loss_Train: 101.369
2024-06-20 23:08:37,328 - INFO: Epoch: 6/200, Batch: 22/29, Batch_Loss_Train: 106.110
2024-06-20 23:08:37,734 - INFO: Epoch: 6/200, Batch: 23/29, Batch_Loss_Train: 117.269
2024-06-20 23:08:38,053 - INFO: Epoch: 6/200, Batch: 24/29, Batch_Loss_Train: 177.081
2024-06-20 23:08:38,463 - INFO: Epoch: 6/200, Batch: 25/29, Batch_Loss_Train: 277.116
2024-06-20 23:08:38,765 - INFO: Epoch: 6/200, Batch: 26/29, Batch_Loss_Train: 421.661
2024-06-20 23:08:39,156 - INFO: Epoch: 6/200, Batch: 27/29, Batch_Loss_Train: 618.077
2024-06-20 23:08:39,470 - INFO: Epoch: 6/200, Batch: 28/29, Batch_Loss_Train: 848.410
2024-06-20 23:08:39,685 - INFO: Epoch: 6/200, Batch: 29/29, Batch_Loss_Train: 860.086
2024-06-20 23:08:50,364 - INFO: 6/200 final results:
2024-06-20 23:08:50,364 - INFO: Training loss: 160.387.
2024-06-20 23:08:50,364 - INFO: Training MAE: 8.947.
2024-06-20 23:08:50,364 - INFO: Training MSE: 146.547.
2024-06-20 23:09:10,691 - INFO: Epoch: 6/200, Loss_train: 160.38682339109224, Loss_val: 549.2428283691406
2024-06-20 23:09:10,691 - INFO: Best internal validation val_loss: 67.372 at epoch: 4.
2024-06-20 23:09:10,691 - INFO: Epoch 7/200...
2024-06-20 23:09:10,691 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:09:10,691 - INFO: Batch size: 32.
2024-06-20 23:09:10,694 - INFO: Dataset:
2024-06-20 23:09:10,694 - INFO: Batch size:
2024-06-20 23:09:10,694 - INFO: Number of workers:
2024-06-20 23:09:11,803 - INFO: Epoch: 7/200, Batch: 1/29, Batch_Loss_Train: 617.163
2024-06-20 23:09:12,130 - INFO: Epoch: 7/200, Batch: 2/29, Batch_Loss_Train: 323.068
2024-06-20 23:09:12,556 - INFO: Epoch: 7/200, Batch: 3/29, Batch_Loss_Train: 130.033
2024-06-20 23:09:12,867 - INFO: Epoch: 7/200, Batch: 4/29, Batch_Loss_Train: 57.774
2024-06-20 23:09:13,285 - INFO: Epoch: 7/200, Batch: 5/29, Batch_Loss_Train: 25.053
2024-06-20 23:09:13,593 - INFO: Epoch: 7/200, Batch: 6/29, Batch_Loss_Train: 57.819
2024-06-20 23:09:14,011 - INFO: Epoch: 7/200, Batch: 7/29, Batch_Loss_Train: 63.080
2024-06-20 23:09:14,320 - INFO: Epoch: 7/200, Batch: 8/29, Batch_Loss_Train: 35.081
2024-06-20 23:09:14,735 - INFO: Epoch: 7/200, Batch: 9/29, Batch_Loss_Train: 37.618
2024-06-20 23:09:15,039 - INFO: Epoch: 7/200, Batch: 10/29, Batch_Loss_Train: 24.346
2024-06-20 23:09:15,456 - INFO: Epoch: 7/200, Batch: 11/29, Batch_Loss_Train: 30.194
2024-06-20 23:09:15,765 - INFO: Epoch: 7/200, Batch: 12/29, Batch_Loss_Train: 43.391
2024-06-20 23:09:16,186 - INFO: Epoch: 7/200, Batch: 13/29, Batch_Loss_Train: 42.568
2024-06-20 23:09:16,494 - INFO: Epoch: 7/200, Batch: 14/29, Batch_Loss_Train: 35.328
2024-06-20 23:09:16,928 - INFO: Epoch: 7/200, Batch: 15/29, Batch_Loss_Train: 27.573
2024-06-20 23:09:17,233 - INFO: Epoch: 7/200, Batch: 16/29, Batch_Loss_Train: 34.408
2024-06-20 23:09:17,643 - INFO: Epoch: 7/200, Batch: 17/29, Batch_Loss_Train: 37.775
2024-06-20 23:09:17,949 - INFO: Epoch: 7/200, Batch: 18/29, Batch_Loss_Train: 31.227
2024-06-20 23:09:18,374 - INFO: Epoch: 7/200, Batch: 19/29, Batch_Loss_Train: 35.474
2024-06-20 23:09:18,674 - INFO: Epoch: 7/200, Batch: 20/29, Batch_Loss_Train: 37.219
2024-06-20 23:09:19,078 - INFO: Epoch: 7/200, Batch: 21/29, Batch_Loss_Train: 37.015
2024-06-20 23:09:19,381 - INFO: Epoch: 7/200, Batch: 22/29, Batch_Loss_Train: 51.489
2024-06-20 23:09:19,793 - INFO: Epoch: 7/200, Batch: 23/29, Batch_Loss_Train: 69.332
2024-06-20 23:09:20,096 - INFO: Epoch: 7/200, Batch: 24/29, Batch_Loss_Train: 49.845
2024-06-20 23:09:20,488 - INFO: Epoch: 7/200, Batch: 25/29, Batch_Loss_Train: 41.205
2024-06-20 23:09:20,788 - INFO: Epoch: 7/200, Batch: 26/29, Batch_Loss_Train: 36.287
2024-06-20 23:09:21,188 - INFO: Epoch: 7/200, Batch: 27/29, Batch_Loss_Train: 35.544
2024-06-20 23:09:21,487 - INFO: Epoch: 7/200, Batch: 28/29, Batch_Loss_Train: 46.571
2024-06-20 23:09:21,690 - INFO: Epoch: 7/200, Batch: 29/29, Batch_Loss_Train: 51.781
2024-06-20 23:09:32,482 - INFO: 7/200 final results:
2024-06-20 23:09:32,482 - INFO: Training loss: 73.974.
2024-06-20 23:09:32,482 - INFO: Training MAE: 6.362.
2024-06-20 23:09:32,482 - INFO: Training MSE: 74.413.
2024-06-20 23:09:52,644 - INFO: Epoch: 7/200, Loss_train: 73.97448769931135, Loss_val: 41.468528484476025
2024-06-20 23:09:52,701 - INFO: Saved new best metric model for epoch 7.
2024-06-20 23:09:52,701 - INFO: Best internal validation val_loss: 41.469 at epoch: 7.
2024-06-20 23:09:52,701 - INFO: Epoch 8/200...
2024-06-20 23:09:52,701 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:09:52,701 - INFO: Batch size: 32.
2024-06-20 23:09:52,705 - INFO: Dataset:
2024-06-20 23:09:52,705 - INFO: Batch size:
2024-06-20 23:09:52,705 - INFO: Number of workers:
2024-06-20 23:09:53,823 - INFO: Epoch: 8/200, Batch: 1/29, Batch_Loss_Train: 38.618
2024-06-20 23:09:54,148 - INFO: Epoch: 8/200, Batch: 2/29, Batch_Loss_Train: 41.944
2024-06-20 23:09:54,565 - INFO: Epoch: 8/200, Batch: 3/29, Batch_Loss_Train: 48.771
2024-06-20 23:09:54,872 - INFO: Epoch: 8/200, Batch: 4/29, Batch_Loss_Train: 52.428
2024-06-20 23:09:55,277 - INFO: Epoch: 8/200, Batch: 5/29, Batch_Loss_Train: 45.116
2024-06-20 23:09:55,581 - INFO: Epoch: 8/200, Batch: 6/29, Batch_Loss_Train: 33.103
2024-06-20 23:09:55,986 - INFO: Epoch: 8/200, Batch: 7/29, Batch_Loss_Train: 31.079
2024-06-20 23:09:56,290 - INFO: Epoch: 8/200, Batch: 8/29, Batch_Loss_Train: 24.984
2024-06-20 23:09:56,695 - INFO: Epoch: 8/200, Batch: 9/29, Batch_Loss_Train: 20.688
2024-06-20 23:09:56,994 - INFO: Epoch: 8/200, Batch: 10/29, Batch_Loss_Train: 28.207
2024-06-20 23:09:57,411 - INFO: Epoch: 8/200, Batch: 11/29, Batch_Loss_Train: 36.365
2024-06-20 23:09:57,716 - INFO: Epoch: 8/200, Batch: 12/29, Batch_Loss_Train: 56.361
2024-06-20 23:09:58,124 - INFO: Epoch: 8/200, Batch: 13/29, Batch_Loss_Train: 68.938
2024-06-20 23:09:58,429 - INFO: Epoch: 8/200, Batch: 14/29, Batch_Loss_Train: 61.529
2024-06-20 23:09:58,864 - INFO: Epoch: 8/200, Batch: 15/29, Batch_Loss_Train: 59.960
2024-06-20 23:09:59,167 - INFO: Epoch: 8/200, Batch: 16/29, Batch_Loss_Train: 51.444
2024-06-20 23:09:59,590 - INFO: Epoch: 8/200, Batch: 17/29, Batch_Loss_Train: 49.460
2024-06-20 23:09:59,902 - INFO: Epoch: 8/200, Batch: 18/29, Batch_Loss_Train: 35.192
2024-06-20 23:10:00,324 - INFO: Epoch: 8/200, Batch: 19/29, Batch_Loss_Train: 33.559
2024-06-20 23:10:00,627 - INFO: Epoch: 8/200, Batch: 20/29, Batch_Loss_Train: 37.368
2024-06-20 23:10:01,030 - INFO: Epoch: 8/200, Batch: 21/29, Batch_Loss_Train: 54.750
2024-06-20 23:10:01,337 - INFO: Epoch: 8/200, Batch: 22/29, Batch_Loss_Train: 69.632
2024-06-20 23:10:01,761 - INFO: Epoch: 8/200, Batch: 23/29, Batch_Loss_Train: 57.687
2024-06-20 23:10:02,065 - INFO: Epoch: 8/200, Batch: 24/29, Batch_Loss_Train: 65.025
2024-06-20 23:10:02,455 - INFO: Epoch: 8/200, Batch: 25/29, Batch_Loss_Train: 71.458
2024-06-20 23:10:02,754 - INFO: Epoch: 8/200, Batch: 26/29, Batch_Loss_Train: 65.939
2024-06-20 23:10:03,164 - INFO: Epoch: 8/200, Batch: 27/29, Batch_Loss_Train: 60.644
2024-06-20 23:10:03,463 - INFO: Epoch: 8/200, Batch: 28/29, Batch_Loss_Train: 53.098
2024-06-20 23:10:03,671 - INFO: Epoch: 8/200, Batch: 29/29, Batch_Loss_Train: 62.079
2024-06-20 23:10:14,555 - INFO: 8/200 final results:
2024-06-20 23:10:14,555 - INFO: Training loss: 48.808.
2024-06-20 23:10:14,555 - INFO: Training MAE: 5.694.
2024-06-20 23:10:14,555 - INFO: Training MSE: 48.545.
2024-06-20 23:10:34,860 - INFO: Epoch: 8/200, Loss_train: 48.807859618088294, Loss_val: 79.64902312180092
2024-06-20 23:10:34,860 - INFO: Best internal validation val_loss: 41.469 at epoch: 7.
2024-06-20 23:10:34,860 - INFO: Epoch 9/200...
2024-06-20 23:10:34,860 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:10:34,860 - INFO: Batch size: 32.
2024-06-20 23:10:34,864 - INFO: Dataset:
2024-06-20 23:10:34,864 - INFO: Batch size:
2024-06-20 23:10:34,864 - INFO: Number of workers:
2024-06-20 23:10:35,995 - INFO: Epoch: 9/200, Batch: 1/29, Batch_Loss_Train: 73.719
2024-06-20 23:10:36,305 - INFO: Epoch: 9/200, Batch: 2/29, Batch_Loss_Train: 69.151
2024-06-20 23:10:36,729 - INFO: Epoch: 9/200, Batch: 3/29, Batch_Loss_Train: 56.008
2024-06-20 23:10:37,039 - INFO: Epoch: 9/200, Batch: 4/29, Batch_Loss_Train: 53.107
2024-06-20 23:10:37,458 - INFO: Epoch: 9/200, Batch: 5/29, Batch_Loss_Train: 52.475
2024-06-20 23:10:37,766 - INFO: Epoch: 9/200, Batch: 6/29, Batch_Loss_Train: 42.693
2024-06-20 23:10:38,183 - INFO: Epoch: 9/200, Batch: 7/29, Batch_Loss_Train: 36.157
2024-06-20 23:10:38,492 - INFO: Epoch: 9/200, Batch: 8/29, Batch_Loss_Train: 36.875
2024-06-20 23:10:38,911 - INFO: Epoch: 9/200, Batch: 9/29, Batch_Loss_Train: 54.286
2024-06-20 23:10:39,215 - INFO: Epoch: 9/200, Batch: 10/29, Batch_Loss_Train: 80.958
2024-06-20 23:10:39,633 - INFO: Epoch: 9/200, Batch: 11/29, Batch_Loss_Train: 98.612
2024-06-20 23:10:39,942 - INFO: Epoch: 9/200, Batch: 12/29, Batch_Loss_Train: 122.208
2024-06-20 23:10:40,380 - INFO: Epoch: 9/200, Batch: 13/29, Batch_Loss_Train: 160.685
2024-06-20 23:10:40,688 - INFO: Epoch: 9/200, Batch: 14/29, Batch_Loss_Train: 219.933
2024-06-20 23:10:41,108 - INFO: Epoch: 9/200, Batch: 15/29, Batch_Loss_Train: 351.693
2024-06-20 23:10:41,414 - INFO: Epoch: 9/200, Batch: 16/29, Batch_Loss_Train: 524.953
2024-06-20 23:10:41,836 - INFO: Epoch: 9/200, Batch: 17/29, Batch_Loss_Train: 552.777
2024-06-20 23:10:42,142 - INFO: Epoch: 9/200, Batch: 18/29, Batch_Loss_Train: 426.985
2024-06-20 23:10:42,558 - INFO: Epoch: 9/200, Batch: 19/29, Batch_Loss_Train: 221.434
2024-06-20 23:10:42,861 - INFO: Epoch: 9/200, Batch: 20/29, Batch_Loss_Train: 76.240
2024-06-20 23:10:43,277 - INFO: Epoch: 9/200, Batch: 21/29, Batch_Loss_Train: 24.555
2024-06-20 23:10:43,583 - INFO: Epoch: 9/200, Batch: 22/29, Batch_Loss_Train: 40.331
2024-06-20 23:10:43,992 - INFO: Epoch: 9/200, Batch: 23/29, Batch_Loss_Train: 77.682
2024-06-20 23:10:44,299 - INFO: Epoch: 9/200, Batch: 24/29, Batch_Loss_Train: 51.390
2024-06-20 23:10:44,703 - INFO: Epoch: 9/200, Batch: 25/29, Batch_Loss_Train: 28.633
2024-06-20 23:10:45,005 - INFO: Epoch: 9/200, Batch: 26/29, Batch_Loss_Train: 30.197
2024-06-20 23:10:45,398 - INFO: Epoch: 9/200, Batch: 27/29, Batch_Loss_Train: 30.749
2024-06-20 23:10:45,699 - INFO: Epoch: 9/200, Batch: 28/29, Batch_Loss_Train: 31.665
2024-06-20 23:10:45,911 - INFO: Epoch: 9/200, Batch: 29/29, Batch_Loss_Train: 32.776
2024-06-20 23:10:56,882 - INFO: 9/200 final results:
2024-06-20 23:10:56,882 - INFO: Training loss: 126.170.
2024-06-20 23:10:56,883 - INFO: Training MAE: 8.446.
2024-06-20 23:10:56,883 - INFO: Training MSE: 128.017.
2024-06-20 23:11:17,133 - INFO: Epoch: 9/200, Loss_train: 126.16986761421992, Loss_val: 24.343069339620655
2024-06-20 23:11:17,188 - INFO: Saved new best metric model for epoch 9.
2024-06-20 23:11:17,189 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:11:17,189 - INFO: Epoch 10/200...
2024-06-20 23:11:17,189 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:11:17,189 - INFO: Batch size: 32.
2024-06-20 23:11:17,192 - INFO: Dataset:
2024-06-20 23:11:17,192 - INFO: Batch size:
2024-06-20 23:11:17,192 - INFO: Number of workers:
2024-06-20 23:11:18,407 - INFO: Epoch: 10/200, Batch: 1/29, Batch_Loss_Train: 29.661
2024-06-20 23:11:18,719 - INFO: Epoch: 10/200, Batch: 2/29, Batch_Loss_Train: 31.081
2024-06-20 23:11:19,119 - INFO: Epoch: 10/200, Batch: 3/29, Batch_Loss_Train: 27.003
2024-06-20 23:11:19,442 - INFO: Epoch: 10/200, Batch: 4/29, Batch_Loss_Train: 35.249
2024-06-20 23:11:19,865 - INFO: Epoch: 10/200, Batch: 5/29, Batch_Loss_Train: 27.812
2024-06-20 23:11:20,173 - INFO: Epoch: 10/200, Batch: 6/29, Batch_Loss_Train: 32.614
2024-06-20 23:11:20,574 - INFO: Epoch: 10/200, Batch: 7/29, Batch_Loss_Train: 27.478
2024-06-20 23:11:20,894 - INFO: Epoch: 10/200, Batch: 8/29, Batch_Loss_Train: 36.958
2024-06-20 23:11:21,313 - INFO: Epoch: 10/200, Batch: 9/29, Batch_Loss_Train: 41.150
2024-06-20 23:11:21,616 - INFO: Epoch: 10/200, Batch: 10/29, Batch_Loss_Train: 35.155
2024-06-20 23:11:22,021 - INFO: Epoch: 10/200, Batch: 11/29, Batch_Loss_Train: 41.154
2024-06-20 23:11:22,343 - INFO: Epoch: 10/200, Batch: 12/29, Batch_Loss_Train: 29.056
2024-06-20 23:11:22,773 - INFO: Epoch: 10/200, Batch: 13/29, Batch_Loss_Train: 37.993
2024-06-20 23:11:23,082 - INFO: Epoch: 10/200, Batch: 14/29, Batch_Loss_Train: 56.992
2024-06-20 23:11:23,494 - INFO: Epoch: 10/200, Batch: 15/29, Batch_Loss_Train: 59.902
2024-06-20 23:11:23,813 - INFO: Epoch: 10/200, Batch: 16/29, Batch_Loss_Train: 45.533
2024-06-20 23:11:24,240 - INFO: Epoch: 10/200, Batch: 17/29, Batch_Loss_Train: 83.853
2024-06-20 23:11:24,546 - INFO: Epoch: 10/200, Batch: 18/29, Batch_Loss_Train: 131.683
2024-06-20 23:11:24,948 - INFO: Epoch: 10/200, Batch: 19/29, Batch_Loss_Train: 177.404
2024-06-20 23:11:25,264 - INFO: Epoch: 10/200, Batch: 20/29, Batch_Loss_Train: 225.948
2024-06-20 23:11:25,682 - INFO: Epoch: 10/200, Batch: 21/29, Batch_Loss_Train: 217.905
2024-06-20 23:11:25,987 - INFO: Epoch: 10/200, Batch: 22/29, Batch_Loss_Train: 187.028
2024-06-20 23:11:26,392 - INFO: Epoch: 10/200, Batch: 23/29, Batch_Loss_Train: 133.533
2024-06-20 23:11:26,709 - INFO: Epoch: 10/200, Batch: 24/29, Batch_Loss_Train: 72.163
2024-06-20 23:11:27,120 - INFO: Epoch: 10/200, Batch: 25/29, Batch_Loss_Train: 50.667
2024-06-20 23:11:27,421 - INFO: Epoch: 10/200, Batch: 26/29, Batch_Loss_Train: 53.457
2024-06-20 23:11:27,822 - INFO: Epoch: 10/200, Batch: 27/29, Batch_Loss_Train: 69.968
2024-06-20 23:11:28,136 - INFO: Epoch: 10/200, Batch: 28/29, Batch_Loss_Train: 95.731
2024-06-20 23:11:28,354 - INFO: Epoch: 10/200, Batch: 29/29, Batch_Loss_Train: 116.054
2024-06-20 23:11:39,456 - INFO: 10/200 final results:
2024-06-20 23:11:39,456 - INFO: Training loss: 76.213.
2024-06-20 23:11:39,456 - INFO: Training MAE: 6.767.
2024-06-20 23:11:39,456 - INFO: Training MSE: 75.425.
2024-06-20 23:11:59,855 - INFO: Epoch: 10/200, Loss_train: 76.21326637268066, Loss_val: 99.97080257021148
2024-06-20 23:11:59,855 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:11:59,855 - INFO: Epoch 11/200...
2024-06-20 23:11:59,855 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:11:59,855 - INFO: Batch size: 32.
2024-06-20 23:11:59,858 - INFO: Dataset:
2024-06-20 23:11:59,858 - INFO: Batch size:
2024-06-20 23:11:59,858 - INFO: Number of workers:
2024-06-20 23:12:00,972 - INFO: Epoch: 11/200, Batch: 1/29, Batch_Loss_Train: 100.535
2024-06-20 23:12:01,281 - INFO: Epoch: 11/200, Batch: 2/29, Batch_Loss_Train: 54.682
2024-06-20 23:12:01,682 - INFO: Epoch: 11/200, Batch: 3/29, Batch_Loss_Train: 25.830
2024-06-20 23:12:02,004 - INFO: Epoch: 11/200, Batch: 4/29, Batch_Loss_Train: 30.036
2024-06-20 23:12:02,416 - INFO: Epoch: 11/200, Batch: 5/29, Batch_Loss_Train: 27.969
2024-06-20 23:12:02,736 - INFO: Epoch: 11/200, Batch: 6/29, Batch_Loss_Train: 21.081
2024-06-20 23:12:03,127 - INFO: Epoch: 11/200, Batch: 7/29, Batch_Loss_Train: 25.342
2024-06-20 23:12:03,448 - INFO: Epoch: 11/200, Batch: 8/29, Batch_Loss_Train: 26.106
2024-06-20 23:12:03,873 - INFO: Epoch: 11/200, Batch: 9/29, Batch_Loss_Train: 26.148
2024-06-20 23:12:04,188 - INFO: Epoch: 11/200, Batch: 10/29, Batch_Loss_Train: 27.979
2024-06-20 23:12:04,572 - INFO: Epoch: 11/200, Batch: 11/29, Batch_Loss_Train: 26.880
2024-06-20 23:12:04,893 - INFO: Epoch: 11/200, Batch: 12/29, Batch_Loss_Train: 18.753
2024-06-20 23:12:05,334 - INFO: Epoch: 11/200, Batch: 13/29, Batch_Loss_Train: 24.605
2024-06-20 23:12:05,643 - INFO: Epoch: 11/200, Batch: 14/29, Batch_Loss_Train: 24.467
2024-06-20 23:12:06,042 - INFO: Epoch: 11/200, Batch: 15/29, Batch_Loss_Train: 24.148
2024-06-20 23:12:06,361 - INFO: Epoch: 11/200, Batch: 16/29, Batch_Loss_Train: 31.790
2024-06-20 23:12:06,796 - INFO: Epoch: 11/200, Batch: 17/29, Batch_Loss_Train: 50.622
2024-06-20 23:12:07,102 - INFO: Epoch: 11/200, Batch: 18/29, Batch_Loss_Train: 68.481
2024-06-20 23:12:07,503 - INFO: Epoch: 11/200, Batch: 19/29, Batch_Loss_Train: 83.143
2024-06-20 23:12:07,819 - INFO: Epoch: 11/200, Batch: 20/29, Batch_Loss_Train: 70.438
2024-06-20 23:12:08,268 - INFO: Epoch: 11/200, Batch: 21/29, Batch_Loss_Train: 55.579
2024-06-20 23:12:08,581 - INFO: Epoch: 11/200, Batch: 22/29, Batch_Loss_Train: 55.768
2024-06-20 23:12:08,975 - INFO: Epoch: 11/200, Batch: 23/29, Batch_Loss_Train: 60.572
2024-06-20 23:12:09,294 - INFO: Epoch: 11/200, Batch: 24/29, Batch_Loss_Train: 38.614
2024-06-20 23:12:09,723 - INFO: Epoch: 11/200, Batch: 25/29, Batch_Loss_Train: 19.814
2024-06-20 23:12:10,026 - INFO: Epoch: 11/200, Batch: 26/29, Batch_Loss_Train: 26.140
2024-06-20 23:12:10,418 - INFO: Epoch: 11/200, Batch: 27/29, Batch_Loss_Train: 23.016
2024-06-20 23:12:10,734 - INFO: Epoch: 11/200, Batch: 28/29, Batch_Loss_Train: 43.121
2024-06-20 23:12:10,955 - INFO: Epoch: 11/200, Batch: 29/29, Batch_Loss_Train: 79.542
2024-06-20 23:12:21,880 - INFO: 11/200 final results:
2024-06-20 23:12:21,881 - INFO: Training loss: 41.076.
2024-06-20 23:12:21,881 - INFO: Training MAE: 5.028.
2024-06-20 23:12:21,881 - INFO: Training MSE: 40.315.
2024-06-20 23:12:42,262 - INFO: Epoch: 11/200, Loss_train: 41.07588787736564, Loss_val: 146.44099478886045
2024-06-20 23:12:42,262 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:12:42,262 - INFO: Epoch 12/200...
2024-06-20 23:12:42,263 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:12:42,263 - INFO: Batch size: 32.
2024-06-20 23:12:42,266 - INFO: Dataset:
2024-06-20 23:12:42,266 - INFO: Batch size:
2024-06-20 23:12:42,266 - INFO: Number of workers:
2024-06-20 23:12:43,390 - INFO: Epoch: 12/200, Batch: 1/29, Batch_Loss_Train: 140.114
2024-06-20 23:12:43,732 - INFO: Epoch: 12/200, Batch: 2/29, Batch_Loss_Train: 198.890
2024-06-20 23:12:44,123 - INFO: Epoch: 12/200, Batch: 3/29, Batch_Loss_Train: 279.892
2024-06-20 23:12:44,447 - INFO: Epoch: 12/200, Batch: 4/29, Batch_Loss_Train: 350.675
2024-06-20 23:12:44,856 - INFO: Epoch: 12/200, Batch: 5/29, Batch_Loss_Train: 397.431
2024-06-20 23:12:45,200 - INFO: Epoch: 12/200, Batch: 6/29, Batch_Loss_Train: 350.895
2024-06-20 23:12:45,597 - INFO: Epoch: 12/200, Batch: 7/29, Batch_Loss_Train: 286.751
2024-06-20 23:12:45,905 - INFO: Epoch: 12/200, Batch: 8/29, Batch_Loss_Train: 175.741
2024-06-20 23:12:46,308 - INFO: Epoch: 12/200, Batch: 9/29, Batch_Loss_Train: 84.924
2024-06-20 23:12:46,659 - INFO: Epoch: 12/200, Batch: 10/29, Batch_Loss_Train: 50.916
2024-06-20 23:12:47,055 - INFO: Epoch: 12/200, Batch: 11/29, Batch_Loss_Train: 47.968
2024-06-20 23:12:47,364 - INFO: Epoch: 12/200, Batch: 12/29, Batch_Loss_Train: 33.014
2024-06-20 23:12:47,782 - INFO: Epoch: 12/200, Batch: 13/29, Batch_Loss_Train: 21.566
2024-06-20 23:12:48,125 - INFO: Epoch: 12/200, Batch: 14/29, Batch_Loss_Train: 24.048
2024-06-20 23:12:48,525 - INFO: Epoch: 12/200, Batch: 15/29, Batch_Loss_Train: 30.409
2024-06-20 23:12:48,829 - INFO: Epoch: 12/200, Batch: 16/29, Batch_Loss_Train: 46.675
2024-06-20 23:12:49,242 - INFO: Epoch: 12/200, Batch: 17/29, Batch_Loss_Train: 54.669
2024-06-20 23:12:49,584 - INFO: Epoch: 12/200, Batch: 18/29, Batch_Loss_Train: 46.861
2024-06-20 23:12:49,964 - INFO: Epoch: 12/200, Batch: 19/29, Batch_Loss_Train: 41.088
2024-06-20 23:12:50,267 - INFO: Epoch: 12/200, Batch: 20/29, Batch_Loss_Train: 44.490
2024-06-20 23:12:50,678 - INFO: Epoch: 12/200, Batch: 21/29, Batch_Loss_Train: 40.899
2024-06-20 23:12:51,018 - INFO: Epoch: 12/200, Batch: 22/29, Batch_Loss_Train: 36.319
2024-06-20 23:12:51,412 - INFO: Epoch: 12/200, Batch: 23/29, Batch_Loss_Train: 27.510
2024-06-20 23:12:51,717 - INFO: Epoch: 12/200, Batch: 24/29, Batch_Loss_Train: 28.273
2024-06-20 23:12:52,119 - INFO: Epoch: 12/200, Batch: 25/29, Batch_Loss_Train: 26.110
2024-06-20 23:12:52,456 - INFO: Epoch: 12/200, Batch: 26/29, Batch_Loss_Train: 22.174
2024-06-20 23:12:52,845 - INFO: Epoch: 12/200, Batch: 27/29, Batch_Loss_Train: 25.362
2024-06-20 23:12:53,147 - INFO: Epoch: 12/200, Batch: 28/29, Batch_Loss_Train: 55.661
2024-06-20 23:12:53,364 - INFO: Epoch: 12/200, Batch: 29/29, Batch_Loss_Train: 81.382
2024-06-20 23:13:04,274 - INFO: 12/200 final results:
2024-06-20 23:13:04,274 - INFO: Training loss: 105.197.
2024-06-20 23:13:04,274 - INFO: Training MAE: 7.592.
2024-06-20 23:13:04,274 - INFO: Training MSE: 105.668.
2024-06-20 23:13:24,520 - INFO: Epoch: 12/200, Loss_train: 105.19685475579624, Loss_val: 82.82678735667261
2024-06-20 23:13:24,520 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:13:24,520 - INFO: Epoch 13/200...
2024-06-20 23:13:24,520 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:13:24,520 - INFO: Batch size: 32.
2024-06-20 23:13:24,523 - INFO: Dataset:
2024-06-20 23:13:24,523 - INFO: Batch size:
2024-06-20 23:13:24,523 - INFO: Number of workers:
2024-06-20 23:13:25,634 - INFO: Epoch: 13/200, Batch: 1/29, Batch_Loss_Train: 69.781
2024-06-20 23:13:25,958 - INFO: Epoch: 13/200, Batch: 2/29, Batch_Loss_Train: 55.916
2024-06-20 23:13:26,364 - INFO: Epoch: 13/200, Batch: 3/29, Batch_Loss_Train: 36.694
2024-06-20 23:13:26,685 - INFO: Epoch: 13/200, Batch: 4/29, Batch_Loss_Train: 23.297
2024-06-20 23:13:27,096 - INFO: Epoch: 13/200, Batch: 5/29, Batch_Loss_Train: 19.395
2024-06-20 23:13:27,404 - INFO: Epoch: 13/200, Batch: 6/29, Batch_Loss_Train: 33.141
2024-06-20 23:13:27,804 - INFO: Epoch: 13/200, Batch: 7/29, Batch_Loss_Train: 60.019
2024-06-20 23:13:28,124 - INFO: Epoch: 13/200, Batch: 8/29, Batch_Loss_Train: 68.998
2024-06-20 23:13:28,544 - INFO: Epoch: 13/200, Batch: 9/29, Batch_Loss_Train: 52.007
2024-06-20 23:13:28,848 - INFO: Epoch: 13/200, Batch: 10/29, Batch_Loss_Train: 38.150
2024-06-20 23:13:29,255 - INFO: Epoch: 13/200, Batch: 11/29, Batch_Loss_Train: 37.897
2024-06-20 23:13:29,577 - INFO: Epoch: 13/200, Batch: 12/29, Batch_Loss_Train: 28.000
2024-06-20 23:13:30,000 - INFO: Epoch: 13/200, Batch: 13/29, Batch_Loss_Train: 25.609
2024-06-20 23:13:30,310 - INFO: Epoch: 13/200, Batch: 14/29, Batch_Loss_Train: 29.595
2024-06-20 23:13:30,718 - INFO: Epoch: 13/200, Batch: 15/29, Batch_Loss_Train: 29.657
2024-06-20 23:13:31,037 - INFO: Epoch: 13/200, Batch: 16/29, Batch_Loss_Train: 32.155
2024-06-20 23:13:31,453 - INFO: Epoch: 13/200, Batch: 17/29, Batch_Loss_Train: 45.428
2024-06-20 23:13:31,759 - INFO: Epoch: 13/200, Batch: 18/29, Batch_Loss_Train: 69.320
2024-06-20 23:13:32,163 - INFO: Epoch: 13/200, Batch: 19/29, Batch_Loss_Train: 89.773
2024-06-20 23:13:32,479 - INFO: Epoch: 13/200, Batch: 20/29, Batch_Loss_Train: 137.054
2024-06-20 23:13:32,892 - INFO: Epoch: 13/200, Batch: 21/29, Batch_Loss_Train: 197.708
2024-06-20 23:13:33,197 - INFO: Epoch: 13/200, Batch: 22/29, Batch_Loss_Train: 201.986
2024-06-20 23:13:33,604 - INFO: Epoch: 13/200, Batch: 23/29, Batch_Loss_Train: 156.316
2024-06-20 23:13:33,921 - INFO: Epoch: 13/200, Batch: 24/29, Batch_Loss_Train: 72.018
2024-06-20 23:13:34,326 - INFO: Epoch: 13/200, Batch: 25/29, Batch_Loss_Train: 30.718
2024-06-20 23:13:34,627 - INFO: Epoch: 13/200, Batch: 26/29, Batch_Loss_Train: 24.526
2024-06-20 23:13:35,028 - INFO: Epoch: 13/200, Batch: 27/29, Batch_Loss_Train: 23.853
2024-06-20 23:13:35,341 - INFO: Epoch: 13/200, Batch: 28/29, Batch_Loss_Train: 27.193
2024-06-20 23:13:35,564 - INFO: Epoch: 13/200, Batch: 29/29, Batch_Loss_Train: 35.755
2024-06-20 23:13:46,453 - INFO: 13/200 final results:
2024-06-20 23:13:46,453 - INFO: Training loss: 60.412.
2024-06-20 23:13:46,453 - INFO: Training MAE: 6.007.
2024-06-20 23:13:46,453 - INFO: Training MSE: 60.900.
2024-06-20 23:14:06,588 - INFO: Epoch: 13/200, Loss_train: 60.41231405323949, Loss_val: 36.43836777785729
2024-06-20 23:14:06,588 - INFO: Best internal validation val_loss: 24.343 at epoch: 9.
2024-06-20 23:14:06,588 - INFO: Epoch 14/200...
2024-06-20 23:14:06,588 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:14:06,588 - INFO: Batch size: 32.
2024-06-20 23:14:06,591 - INFO: Dataset:
2024-06-20 23:14:06,591 - INFO: Batch size:
2024-06-20 23:14:06,591 - INFO: Number of workers:
2024-06-20 23:14:07,708 - INFO: Epoch: 14/200, Batch: 1/29, Batch_Loss_Train: 36.976
2024-06-20 23:14:08,031 - INFO: Epoch: 14/200, Batch: 2/29, Batch_Loss_Train: 40.106
2024-06-20 23:14:08,447 - INFO: Epoch: 14/200, Batch: 3/29, Batch_Loss_Train: 34.203
2024-06-20 23:14:08,768 - INFO: Epoch: 14/200, Batch: 4/29, Batch_Loss_Train: 29.611
2024-06-20 23:14:09,177 - INFO: Epoch: 14/200, Batch: 5/29, Batch_Loss_Train: 25.362
2024-06-20 23:14:09,494 - INFO: Epoch: 14/200, Batch: 6/29, Batch_Loss_Train: 25.496
2024-06-20 23:14:09,902 - INFO: Epoch: 14/200, Batch: 7/29, Batch_Loss_Train: 39.128
2024-06-20 23:14:10,219 - INFO: Epoch: 14/200, Batch: 8/29, Batch_Loss_Train: 65.478
2024-06-20 23:14:10,620 - INFO: Epoch: 14/200, Batch: 9/29, Batch_Loss_Train: 113.140
2024-06-20 23:14:10,933 - INFO: Epoch: 14/200, Batch: 10/29, Batch_Loss_Train: 215.216
2024-06-20 23:14:11,343 - INFO: Epoch: 14/200, Batch: 11/29, Batch_Loss_Train: 335.211
2024-06-20 23:14:11,663 - INFO: Epoch: 14/200, Batch: 12/29, Batch_Loss_Train: 405.582
2024-06-20 23:14:12,077 - INFO: Epoch: 14/200, Batch: 13/29, Batch_Loss_Train: 381.472
2024-06-20 23:14:12,396 - INFO: Epoch: 14/200, Batch: 14/29, Batch_Loss_Train: 167.520
2024-06-20 23:14:12,796 - INFO: Epoch: 14/200, Batch: 15/29, Batch_Loss_Train: 60.374
2024-06-20 23:14:13,114 - INFO: Epoch: 14/200, Batch: 16/29, Batch_Loss_Train: 54.178
2024-06-20 23:14:13,526 - INFO: Epoch: 14/200, Batch: 17/29, Batch_Loss_Train: 75.876
2024-06-20 23:14:13,843 - INFO: Epoch: 14/200, Batch: 18/29, Batch_Loss_Train: 52.438
2024-06-20 23:14:14,250 - INFO: Epoch: 14/200, Batch: 19/29, Batch_Loss_Train: 27.845
2024-06-20 23:14:14,564 - INFO: Epoch: 14/200, Batch: 20/29, Batch_Loss_Train: 21.986
2024-06-20 23:14:14,991 - INFO: Epoch: 14/200, Batch: 21/29, Batch_Loss_Train: 15.984
2024-06-20 23:14:15,318 - INFO: Epoch: 14/200, Batch: 22/29, Batch_Loss_Train: 18.753
2024-06-20 23:14:15,734 - INFO: Epoch: 14/200, Batch: 23/29, Batch_Loss_Train: 16.875
2024-06-20 23:14:16,060 - INFO: Epoch: 14/200, Batch: 24/29, Batch_Loss_Train: 17.532
2024-06-20 23:14:16,460 - INFO: Epoch: 14/200, Batch: 25/29, Batch_Loss_Train: 21.666
2024-06-20 23:14:16,773 - INFO: Epoch: 14/200, Batch: 26/29, Batch_Loss_Train: 20.904
2024-06-20 23:14:17,172 - INFO: Epoch: 14/200, Batch: 27/29, Batch_Loss_Train: 21.016
2024-06-20 23:14:17,484 - INFO: Epoch: 14/200, Batch: 28/29, Batch_Loss_Train: 17.637
2024-06-20 23:14:17,705 - INFO: Epoch: 14/200, Batch: 29/29, Batch_Loss_Train: 25.472
2024-06-20 23:14:28,594 - INFO: 14/200 final results:
2024-06-20 23:14:28,595 - INFO: Training loss: 82.174.
2024-06-20 23:14:28,595 - INFO: Training MAE: 6.489.
2024-06-20 23:14:28,595 - INFO: Training MSE: 83.295.
2024-06-20 23:14:48,916 - INFO: Epoch: 14/200, Loss_train: 82.17370549563704, Loss_val: 20.96248116986505
2024-06-20 23:14:48,974 - INFO: Saved new best metric model for epoch 14.
2024-06-20 23:14:48,974 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:14:48,974 - INFO: Epoch 15/200...
2024-06-20 23:14:48,974 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:14:48,974 - INFO: Batch size: 32.
2024-06-20 23:14:48,977 - INFO: Dataset:
2024-06-20 23:14:48,977 - INFO: Batch size:
2024-06-20 23:14:48,978 - INFO: Number of workers:
2024-06-20 23:14:50,107 - INFO: Epoch: 15/200, Batch: 1/29, Batch_Loss_Train: 21.317
2024-06-20 23:14:50,416 - INFO: Epoch: 15/200, Batch: 2/29, Batch_Loss_Train: 17.794
2024-06-20 23:14:50,828 - INFO: Epoch: 15/200, Batch: 3/29, Batch_Loss_Train: 20.325
2024-06-20 23:14:51,150 - INFO: Epoch: 15/200, Batch: 4/29, Batch_Loss_Train: 18.891
2024-06-20 23:14:51,571 - INFO: Epoch: 15/200, Batch: 5/29, Batch_Loss_Train: 17.639
2024-06-20 23:14:51,879 - INFO: Epoch: 15/200, Batch: 6/29, Batch_Loss_Train: 13.157
2024-06-20 23:14:52,273 - INFO: Epoch: 15/200, Batch: 7/29, Batch_Loss_Train: 20.492
2024-06-20 23:14:52,594 - INFO: Epoch: 15/200, Batch: 8/29, Batch_Loss_Train: 25.199
2024-06-20 23:14:53,016 - INFO: Epoch: 15/200, Batch: 9/29, Batch_Loss_Train: 27.349
2024-06-20 23:14:53,320 - INFO: Epoch: 15/200, Batch: 10/29, Batch_Loss_Train: 28.336
2024-06-20 23:14:53,717 - INFO: Epoch: 15/200, Batch: 11/29, Batch_Loss_Train: 21.830
2024-06-20 23:14:54,038 - INFO: Epoch: 15/200, Batch: 12/29, Batch_Loss_Train: 22.350
2024-06-20 23:14:54,467 - INFO: Epoch: 15/200, Batch: 13/29, Batch_Loss_Train: 25.092
2024-06-20 23:14:54,776 - INFO: Epoch: 15/200, Batch: 14/29, Batch_Loss_Train: 23.880
2024-06-20 23:14:55,179 - INFO: Epoch: 15/200, Batch: 15/29, Batch_Loss_Train: 16.543
2024-06-20 23:14:55,497 - INFO: Epoch: 15/200, Batch: 16/29, Batch_Loss_Train: 17.939
2024-06-20 23:14:55,922 - INFO: Epoch: 15/200, Batch: 17/29, Batch_Loss_Train: 18.385
2024-06-20 23:14:56,227 - INFO: Epoch: 15/200, Batch: 18/29, Batch_Loss_Train: 31.255
2024-06-20 23:14:56,621 - INFO: Epoch: 15/200, Batch: 19/29, Batch_Loss_Train: 30.276
2024-06-20 23:14:56,937 - INFO: Epoch: 15/200, Batch: 20/29, Batch_Loss_Train: 22.927
2024-06-20 23:14:57,354 - INFO: Epoch: 15/200, Batch: 21/29, Batch_Loss_Train: 19.892
2024-06-20 23:14:57,660 - INFO: Epoch: 15/200, Batch: 22/29, Batch_Loss_Train: 23.734
2024-06-20 23:14:58,068 - INFO: Epoch: 15/200, Batch: 23/29, Batch_Loss_Train: 20.377
2024-06-20 23:14:58,386 - INFO: Epoch: 15/200, Batch: 24/29, Batch_Loss_Train: 20.300
2024-06-20 23:14:58,799 - INFO: Epoch: 15/200, Batch: 25/29, Batch_Loss_Train: 21.711
2024-06-20 23:14:59,101 - INFO: Epoch: 15/200, Batch: 26/29, Batch_Loss_Train: 30.428
2024-06-20 23:14:59,504 - INFO: Epoch: 15/200, Batch: 27/29, Batch_Loss_Train: 54.348
2024-06-20 23:14:59,819 - INFO: Epoch: 15/200, Batch: 28/29, Batch_Loss_Train: 104.007
2024-06-20 23:15:00,042 - INFO: Epoch: 15/200, Batch: 29/29, Batch_Loss_Train: 171.523
2024-06-20 23:15:10,976 - INFO: 15/200 final results:
2024-06-20 23:15:10,976 - INFO: Training loss: 31.286.
2024-06-20 23:15:10,976 - INFO: Training MAE: 4.120.
2024-06-20 23:15:10,976 - INFO: Training MSE: 28.512.
2024-06-20 23:15:31,087 - INFO: Epoch: 15/200, Loss_train: 31.28610058488517, Loss_val: 216.55172992574757
2024-06-20 23:15:31,087 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:15:31,087 - INFO: Epoch 16/200...
2024-06-20 23:15:31,087 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:15:31,087 - INFO: Batch size: 32.
2024-06-20 23:15:31,091 - INFO: Dataset:
2024-06-20 23:15:31,091 - INFO: Batch size:
2024-06-20 23:15:31,091 - INFO: Number of workers:
2024-06-20 23:15:32,195 - INFO: Epoch: 16/200, Batch: 1/29, Batch_Loss_Train: 168.303
2024-06-20 23:15:32,530 - INFO: Epoch: 16/200, Batch: 2/29, Batch_Loss_Train: 142.291
2024-06-20 23:15:32,925 - INFO: Epoch: 16/200, Batch: 3/29, Batch_Loss_Train: 117.567
2024-06-20 23:15:33,245 - INFO: Epoch: 16/200, Batch: 4/29, Batch_Loss_Train: 76.968
2024-06-20 23:15:33,661 - INFO: Epoch: 16/200, Batch: 5/29, Batch_Loss_Train: 39.819
2024-06-20 23:15:33,978 - INFO: Epoch: 16/200, Batch: 6/29, Batch_Loss_Train: 18.423
2024-06-20 23:15:34,369 - INFO: Epoch: 16/200, Batch: 7/29, Batch_Loss_Train: 30.631
2024-06-20 23:15:34,688 - INFO: Epoch: 16/200, Batch: 8/29, Batch_Loss_Train: 28.765
2024-06-20 23:15:35,099 - INFO: Epoch: 16/200, Batch: 9/29, Batch_Loss_Train: 21.325
2024-06-20 23:15:35,412 - INFO: Epoch: 16/200, Batch: 10/29, Batch_Loss_Train: 32.139
2024-06-20 23:15:35,792 - INFO: Epoch: 16/200, Batch: 11/29, Batch_Loss_Train: 31.006
2024-06-20 23:15:36,110 - INFO: Epoch: 16/200, Batch: 12/29, Batch_Loss_Train: 23.916
2024-06-20 23:15:36,544 - INFO: Epoch: 16/200, Batch: 13/29, Batch_Loss_Train: 29.404
2024-06-20 23:15:36,849 - INFO: Epoch: 16/200, Batch: 14/29, Batch_Loss_Train: 21.508
2024-06-20 23:15:37,243 - INFO: Epoch: 16/200, Batch: 15/29, Batch_Loss_Train: 21.684
2024-06-20 23:15:37,557 - INFO: Epoch: 16/200, Batch: 16/29, Batch_Loss_Train: 16.547
2024-06-20 23:15:37,988 - INFO: Epoch: 16/200, Batch: 17/29, Batch_Loss_Train: 25.431
2024-06-20 23:15:38,289 - INFO: Epoch: 16/200, Batch: 18/29, Batch_Loss_Train: 30.083
2024-06-20 23:15:38,676 - INFO: Epoch: 16/200, Batch: 19/29, Batch_Loss_Train: 41.433
2024-06-20 23:15:38,988 - INFO: Epoch: 16/200, Batch: 20/29, Batch_Loss_Train: 50.583
2024-06-20 23:15:39,413 - INFO: Epoch: 16/200, Batch: 21/29, Batch_Loss_Train: 53.916
2024-06-20 23:15:39,716 - INFO: Epoch: 16/200, Batch: 22/29, Batch_Loss_Train: 44.429
2024-06-20 23:15:40,108 - INFO: Epoch: 16/200, Batch: 23/29, Batch_Loss_Train: 50.193
2024-06-20 23:15:40,426 - INFO: Epoch: 16/200, Batch: 24/29, Batch_Loss_Train: 60.891
2024-06-20 23:15:40,844 - INFO: Epoch: 16/200, Batch: 25/29, Batch_Loss_Train: 69.702
2024-06-20 23:15:41,144 - INFO: Epoch: 16/200, Batch: 26/29, Batch_Loss_Train: 81.628
2024-06-20 23:15:41,530 - INFO: Epoch: 16/200, Batch: 27/29, Batch_Loss_Train: 95.513
2024-06-20 23:15:41,843 - INFO: Epoch: 16/200, Batch: 28/29, Batch_Loss_Train: 101.558
2024-06-20 23:15:42,065 - INFO: Epoch: 16/200, Batch: 29/29, Batch_Loss_Train: 97.155
2024-06-20 23:15:52,953 - INFO: 16/200 final results:
2024-06-20 23:15:52,953 - INFO: Training loss: 55.959.
2024-06-20 23:15:52,954 - INFO: Training MAE: 5.906.
2024-06-20 23:15:52,954 - INFO: Training MSE: 55.144.
2024-06-20 23:16:12,983 - INFO: Epoch: 16/200, Loss_train: 55.958985493100926, Loss_val: 109.00802796462486
2024-06-20 23:16:12,983 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:16:12,983 - INFO: Epoch 17/200...
2024-06-20 23:16:12,983 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:16:12,983 - INFO: Batch size: 32.
2024-06-20 23:16:12,986 - INFO: Dataset:
2024-06-20 23:16:12,987 - INFO: Batch size:
2024-06-20 23:16:12,987 - INFO: Number of workers:
2024-06-20 23:16:14,098 - INFO: Epoch: 17/200, Batch: 1/29, Batch_Loss_Train: 108.522
2024-06-20 23:16:14,419 - INFO: Epoch: 17/200, Batch: 2/29, Batch_Loss_Train: 86.621
2024-06-20 23:16:14,821 - INFO: Epoch: 17/200, Batch: 3/29, Batch_Loss_Train: 43.997
2024-06-20 23:16:15,142 - INFO: Epoch: 17/200, Batch: 4/29, Batch_Loss_Train: 17.873
2024-06-20 23:16:15,550 - INFO: Epoch: 17/200, Batch: 5/29, Batch_Loss_Train: 17.631
2024-06-20 23:16:15,871 - INFO: Epoch: 17/200, Batch: 6/29, Batch_Loss_Train: 18.476
2024-06-20 23:16:16,266 - INFO: Epoch: 17/200, Batch: 7/29, Batch_Loss_Train: 19.872
2024-06-20 23:16:16,587 - INFO: Epoch: 17/200, Batch: 8/29, Batch_Loss_Train: 21.070
2024-06-20 23:16:16,987 - INFO: Epoch: 17/200, Batch: 9/29, Batch_Loss_Train: 22.999
2024-06-20 23:16:17,303 - INFO: Epoch: 17/200, Batch: 10/29, Batch_Loss_Train: 23.053
2024-06-20 23:16:17,711 - INFO: Epoch: 17/200, Batch: 11/29, Batch_Loss_Train: 25.639
2024-06-20 23:16:18,033 - INFO: Epoch: 17/200, Batch: 12/29, Batch_Loss_Train: 14.884
2024-06-20 23:16:18,448 - INFO: Epoch: 17/200, Batch: 13/29, Batch_Loss_Train: 15.186
2024-06-20 23:16:18,766 - INFO: Epoch: 17/200, Batch: 14/29, Batch_Loss_Train: 17.223
2024-06-20 23:16:19,175 - INFO: Epoch: 17/200, Batch: 15/29, Batch_Loss_Train: 29.362
2024-06-20 23:16:19,490 - INFO: Epoch: 17/200, Batch: 16/29, Batch_Loss_Train: 29.069
2024-06-20 23:16:19,896 - INFO: Epoch: 17/200, Batch: 17/29, Batch_Loss_Train: 31.605
2024-06-20 23:16:20,212 - INFO: Epoch: 17/200, Batch: 18/29, Batch_Loss_Train: 30.708
2024-06-20 23:16:20,611 - INFO: Epoch: 17/200, Batch: 19/29, Batch_Loss_Train: 35.983
2024-06-20 23:16:20,928 - INFO: Epoch: 17/200, Batch: 20/29, Batch_Loss_Train: 41.417
2024-06-20 23:16:21,333 - INFO: Epoch: 17/200, Batch: 21/29, Batch_Loss_Train: 20.269
2024-06-20 23:16:21,658 - INFO: Epoch: 17/200, Batch: 22/29, Batch_Loss_Train: 15.796
2024-06-20 23:16:22,085 - INFO: Epoch: 17/200, Batch: 23/29, Batch_Loss_Train: 18.402
2024-06-20 23:16:22,413 - INFO: Epoch: 17/200, Batch: 24/29, Batch_Loss_Train: 20.482
2024-06-20 23:16:22,836 - INFO: Epoch: 17/200, Batch: 25/29, Batch_Loss_Train: 17.448
2024-06-20 23:16:23,157 - INFO: Epoch: 17/200, Batch: 26/29, Batch_Loss_Train: 21.914
2024-06-20 23:16:23,560 - INFO: Epoch: 17/200, Batch: 27/29, Batch_Loss_Train: 15.331
2024-06-20 23:16:23,876 - INFO: Epoch: 17/200, Batch: 28/29, Batch_Loss_Train: 30.725
2024-06-20 23:16:24,100 - INFO: Epoch: 17/200, Batch: 29/29, Batch_Loss_Train: 32.365
2024-06-20 23:16:35,022 - INFO: 17/200 final results:
2024-06-20 23:16:35,023 - INFO: Training loss: 29.101.
2024-06-20 23:16:35,023 - INFO: Training MAE: 4.113.
2024-06-20 23:16:35,023 - INFO: Training MSE: 29.036.
2024-06-20 23:16:55,417 - INFO: Epoch: 17/200, Loss_train: 29.10069567581703, Loss_val: 33.189337565981106
2024-06-20 23:16:55,417 - INFO: Best internal validation val_loss: 20.962 at epoch: 14.
2024-06-20 23:16:55,417 - INFO: Epoch 18/200...
2024-06-20 23:16:55,417 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:16:55,417 - INFO: Batch size: 32.
2024-06-20 23:16:55,420 - INFO: Dataset:
2024-06-20 23:16:55,420 - INFO: Batch size:
2024-06-20 23:16:55,421 - INFO: Number of workers:
2024-06-20 23:16:56,542 - INFO: Epoch: 18/200, Batch: 1/29, Batch_Loss_Train: 21.435
2024-06-20 23:16:56,878 - INFO: Epoch: 18/200, Batch: 2/29, Batch_Loss_Train: 18.157
2024-06-20 23:16:57,280 - INFO: Epoch: 18/200, Batch: 3/29, Batch_Loss_Train: 16.317
2024-06-20 23:16:57,601 - INFO: Epoch: 18/200, Batch: 4/29, Batch_Loss_Train: 30.448
2024-06-20 23:16:57,999 - INFO: Epoch: 18/200, Batch: 5/29, Batch_Loss_Train: 51.118
2024-06-20 23:16:58,328 - INFO: Epoch: 18/200, Batch: 6/29, Batch_Loss_Train: 108.370
2024-06-20 23:16:58,711 - INFO: Epoch: 18/200, Batch: 7/29, Batch_Loss_Train: 265.196
2024-06-20 23:16:59,032 - INFO: Epoch: 18/200, Batch: 8/29, Batch_Loss_Train: 506.576
2024-06-20 23:16:59,424 - INFO: Epoch: 18/200, Batch: 9/29, Batch_Loss_Train: 480.233
2024-06-20 23:16:59,758 - INFO: Epoch: 18/200, Batch: 10/29, Batch_Loss_Train: 140.405
2024-06-20 23:17:00,141 - INFO: Epoch: 18/200, Batch: 11/29, Batch_Loss_Train: 92.981
2024-06-20 23:17:00,462 - INFO: Epoch: 18/200, Batch: 12/29, Batch_Loss_Train: 83.574
2024-06-20 23:17:00,877 - INFO: Epoch: 18/200, Batch: 13/29, Batch_Loss_Train: 30.323
2024-06-20 23:17:01,212 - INFO: Epoch: 18/200, Batch: 14/29, Batch_Loss_Train: 25.492
2024-06-20 23:17:01,601 - INFO: Epoch: 18/200, Batch: 15/29, Batch_Loss_Train: 21.217
2024-06-20 23:17:01,919 - INFO: Epoch: 18/200, Batch: 16/29, Batch_Loss_Train: 20.905
2024-06-20 23:17:02,331 - INFO: Epoch: 18/200, Batch: 17/29, Batch_Loss_Train: 17.496
2024-06-20 23:17:02,661 - INFO: Epoch: 18/200, Batch: 18/29, Batch_Loss_Train: 19.345
2024-06-20 23:17:03,055 - INFO: Epoch: 18/200, Batch: 19/29, Batch_Loss_Train: 24.762
2024-06-20 23:17:03,370 - INFO: Epoch: 18/200, Batch: 20/29, Batch_Loss_Train: 16.868
2024-06-20 23:17:03,775 - INFO: Epoch: 18/200, Batch: 21/29, Batch_Loss_Train: 25.646
2024-06-20 23:17:04,104 - INFO: Epoch: 18/200, Batch: 22/29, Batch_Loss_Train: 28.140
2024-06-20 23:17:04,496 - INFO: Epoch: 18/200, Batch: 23/29, Batch_Loss_Train: 25.178
2024-06-20 23:17:04,812 - INFO: Epoch: 18/200, Batch: 24/29, Batch_Loss_Train: 17.282
2024-06-20 23:17:05,211 - INFO: Epoch: 18/200, Batch: 25/29, Batch_Loss_Train: 17.850
2024-06-20 23:17:05,535 - INFO: Epoch: 18/200, Batch: 26/29, Batch_Loss_Train: 18.171
2024-06-20 23:17:05,922 - INFO: Epoch: 18/200, Batch: 27/29, Batch_Loss_Train: 21.507
2024-06-20 23:17:06,234 - INFO: Epoch: 18/200, Batch: 28/29, Batch_Loss_Train: 17.678
2024-06-20 23:17:06,455 - INFO: Epoch: 18/200, Batch: 29/29, Batch_Loss_Train: 17.746
2024-06-20 23:17:17,334 - INFO: 18/200 final results:
2024-06-20 23:17:17,334 - INFO: Training loss: 75.187.
2024-06-20 23:17:17,334 - INFO: Training MAE: 6.038.
2024-06-20 23:17:17,334 - INFO: Training MSE: 76.323.
2024-06-20 23:17:37,466 - INFO: Epoch: 18/200, Loss_train: 75.18679921380405, Loss_val: 20.244176963279987
2024-06-20 23:17:37,523 - INFO: Saved new best metric model for epoch 18.
2024-06-20 23:17:37,523 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:17:37,523 - INFO: Epoch 19/200...
2024-06-20 23:17:37,523 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:17:37,524 - INFO: Batch size: 32.
2024-06-20 23:17:37,527 - INFO: Dataset:
2024-06-20 23:17:37,527 - INFO: Batch size:
2024-06-20 23:17:37,527 - INFO: Number of workers:
2024-06-20 23:17:38,681 - INFO: Epoch: 19/200, Batch: 1/29, Batch_Loss_Train: 27.962
2024-06-20 23:17:38,989 - INFO: Epoch: 19/200, Batch: 2/29, Batch_Loss_Train: 42.540
2024-06-20 23:17:39,391 - INFO: Epoch: 19/200, Batch: 3/29, Batch_Loss_Train: 55.703
2024-06-20 23:17:39,711 - INFO: Epoch: 19/200, Batch: 4/29, Batch_Loss_Train: 85.728
2024-06-20 23:17:40,139 - INFO: Epoch: 19/200, Batch: 5/29, Batch_Loss_Train: 119.125
2024-06-20 23:17:40,447 - INFO: Epoch: 19/200, Batch: 6/29, Batch_Loss_Train: 107.066
2024-06-20 23:17:40,829 - INFO: Epoch: 19/200, Batch: 7/29, Batch_Loss_Train: 69.058
2024-06-20 23:17:41,149 - INFO: Epoch: 19/200, Batch: 8/29, Batch_Loss_Train: 31.474
2024-06-20 23:17:41,591 - INFO: Epoch: 19/200, Batch: 9/29, Batch_Loss_Train: 19.071
2024-06-20 23:17:41,895 - INFO: Epoch: 19/200, Batch: 10/29, Batch_Loss_Train: 25.911
2024-06-20 23:17:42,278 - INFO: Epoch: 19/200, Batch: 11/29, Batch_Loss_Train: 25.188
2024-06-20 23:17:42,600 - INFO: Epoch: 19/200, Batch: 12/29, Batch_Loss_Train: 18.567
2024-06-20 23:17:43,040 - INFO: Epoch: 19/200, Batch: 13/29, Batch_Loss_Train: 22.757
2024-06-20 23:17:43,349 - INFO: Epoch: 19/200, Batch: 14/29, Batch_Loss_Train: 12.637
2024-06-20 23:17:43,739 - INFO: Epoch: 19/200, Batch: 15/29, Batch_Loss_Train: 16.145
2024-06-20 23:17:44,057 - INFO: Epoch: 19/200, Batch: 16/29, Batch_Loss_Train: 19.298
2024-06-20 23:17:44,490 - INFO: Epoch: 19/200, Batch: 17/29, Batch_Loss_Train: 20.607
2024-06-20 23:17:44,794 - INFO: Epoch: 19/200, Batch: 18/29, Batch_Loss_Train: 15.923
2024-06-20 23:17:45,171 - INFO: Epoch: 19/200, Batch: 19/29, Batch_Loss_Train: 16.393
2024-06-20 23:17:45,487 - INFO: Epoch: 19/200, Batch: 20/29, Batch_Loss_Train: 12.379
2024-06-20 23:17:45,911 - INFO: Epoch: 19/200, Batch: 21/29, Batch_Loss_Train: 15.170
2024-06-20 23:17:46,218 - INFO: Epoch: 19/200, Batch: 22/29, Batch_Loss_Train: 18.375
2024-06-20 23:17:46,600 - INFO: Epoch: 19/200, Batch: 23/29, Batch_Loss_Train: 15.227
2024-06-20 23:17:46,920 - INFO: Epoch: 19/200, Batch: 24/29, Batch_Loss_Train: 22.059
2024-06-20 23:17:47,334 - INFO: Epoch: 19/200, Batch: 25/29, Batch_Loss_Train: 22.063
2024-06-20 23:17:47,638 - INFO: Epoch: 19/200, Batch: 26/29, Batch_Loss_Train: 25.233
2024-06-20 23:17:48,019 - INFO: Epoch: 19/200, Batch: 27/29, Batch_Loss_Train: 22.310
2024-06-20 23:17:48,335 - INFO: Epoch: 19/200, Batch: 28/29, Batch_Loss_Train: 23.600
2024-06-20 23:17:48,551 - INFO: Epoch: 19/200, Batch: 29/29, Batch_Loss_Train: 24.279
2024-06-20 23:17:59,637 - INFO: 19/200 final results:
2024-06-20 23:17:59,637 - INFO: Training loss: 32.822.
2024-06-20 23:17:59,637 - INFO: Training MAE: 4.349.
2024-06-20 23:17:59,637 - INFO: Training MSE: 32.991.
2024-06-20 23:18:20,086 - INFO: Epoch: 19/200, Loss_train: 32.822263914963294, Loss_val: 37.03066766673121
2024-06-20 23:18:20,086 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:18:20,086 - INFO: Epoch 20/200...
2024-06-20 23:18:20,086 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:18:20,087 - INFO: Batch size: 32.
2024-06-20 23:18:20,089 - INFO: Dataset:
2024-06-20 23:18:20,090 - INFO: Batch size:
2024-06-20 23:18:20,090 - INFO: Number of workers:
2024-06-20 23:18:21,222 - INFO: Epoch: 20/200, Batch: 1/29, Batch_Loss_Train: 46.971
2024-06-20 23:18:21,531 - INFO: Epoch: 20/200, Batch: 2/29, Batch_Loss_Train: 48.652
2024-06-20 23:18:21,943 - INFO: Epoch: 20/200, Batch: 3/29, Batch_Loss_Train: 38.041
2024-06-20 23:18:22,264 - INFO: Epoch: 20/200, Batch: 4/29, Batch_Loss_Train: 30.604
2024-06-20 23:18:22,693 - INFO: Epoch: 20/200, Batch: 5/29, Batch_Loss_Train: 20.787
2024-06-20 23:18:22,997 - INFO: Epoch: 20/200, Batch: 6/29, Batch_Loss_Train: 19.377
2024-06-20 23:18:23,390 - INFO: Epoch: 20/200, Batch: 7/29, Batch_Loss_Train: 16.440
2024-06-20 23:18:23,707 - INFO: Epoch: 20/200, Batch: 8/29, Batch_Loss_Train: 14.600
2024-06-20 23:18:24,140 - INFO: Epoch: 20/200, Batch: 9/29, Batch_Loss_Train: 16.969
2024-06-20 23:18:24,442 - INFO: Epoch: 20/200, Batch: 10/29, Batch_Loss_Train: 22.870
2024-06-20 23:18:24,841 - INFO: Epoch: 20/200, Batch: 11/29, Batch_Loss_Train: 19.444
2024-06-20 23:18:25,162 - INFO: Epoch: 20/200, Batch: 12/29, Batch_Loss_Train: 20.528
2024-06-20 23:18:25,596 - INFO: Epoch: 20/200, Batch: 13/29, Batch_Loss_Train: 12.516
2024-06-20 23:18:25,901 - INFO: Epoch: 20/200, Batch: 14/29, Batch_Loss_Train: 12.739
2024-06-20 23:18:26,300 - INFO: Epoch: 20/200, Batch: 15/29, Batch_Loss_Train: 17.328
2024-06-20 23:18:26,614 - INFO: Epoch: 20/200, Batch: 16/29, Batch_Loss_Train: 22.071
2024-06-20 23:18:27,044 - INFO: Epoch: 20/200, Batch: 17/29, Batch_Loss_Train: 16.666
2024-06-20 23:18:27,346 - INFO: Epoch: 20/200, Batch: 18/29, Batch_Loss_Train: 22.956
2024-06-20 23:18:27,736 - INFO: Epoch: 20/200, Batch: 19/29, Batch_Loss_Train: 28.802
2024-06-20 23:18:28,048 - INFO: Epoch: 20/200, Batch: 20/29, Batch_Loss_Train: 48.320
2024-06-20 23:18:28,472 - INFO: Epoch: 20/200, Batch: 21/29, Batch_Loss_Train: 43.869
2024-06-20 23:18:28,776 - INFO: Epoch: 20/200, Batch: 22/29, Batch_Loss_Train: 26.774
2024-06-20 23:18:29,167 - INFO: Epoch: 20/200, Batch: 23/29, Batch_Loss_Train: 20.790
2024-06-20 23:18:29,483 - INFO: Epoch: 20/200, Batch: 24/29, Batch_Loss_Train: 16.512
2024-06-20 23:18:29,905 - INFO: Epoch: 20/200, Batch: 25/29, Batch_Loss_Train: 18.115
2024-06-20 23:18:30,206 - INFO: Epoch: 20/200, Batch: 26/29, Batch_Loss_Train: 24.399
2024-06-20 23:18:30,592 - INFO: Epoch: 20/200, Batch: 27/29, Batch_Loss_Train: 21.570
2024-06-20 23:18:30,905 - INFO: Epoch: 20/200, Batch: 28/29, Batch_Loss_Train: 25.708
2024-06-20 23:18:31,125 - INFO: Epoch: 20/200, Batch: 29/29, Batch_Loss_Train: 21.241
2024-06-20 23:18:41,942 - INFO: 20/200 final results:
2024-06-20 23:18:41,942 - INFO: Training loss: 24.678.
2024-06-20 23:18:41,942 - INFO: Training MAE: 3.900.
2024-06-20 23:18:41,942 - INFO: Training MSE: 24.746.
2024-06-20 23:19:02,500 - INFO: Epoch: 20/200, Loss_train: 24.677991275129646, Loss_val: 21.663095013848668
2024-06-20 23:19:02,500 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:19:02,500 - INFO: Epoch 21/200...
2024-06-20 23:19:02,500 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:19:02,500 - INFO: Batch size: 32.
2024-06-20 23:19:02,504 - INFO: Dataset:
2024-06-20 23:19:02,504 - INFO: Batch size:
2024-06-20 23:19:02,504 - INFO: Number of workers:
2024-06-20 23:19:03,688 - INFO: Epoch: 21/200, Batch: 1/29, Batch_Loss_Train: 26.377
2024-06-20 23:19:04,000 - INFO: Epoch: 21/200, Batch: 2/29, Batch_Loss_Train: 33.193
2024-06-20 23:19:04,403 - INFO: Epoch: 21/200, Batch: 3/29, Batch_Loss_Train: 27.915
2024-06-20 23:19:04,726 - INFO: Epoch: 21/200, Batch: 4/29, Batch_Loss_Train: 16.364
2024-06-20 23:19:05,169 - INFO: Epoch: 21/200, Batch: 5/29, Batch_Loss_Train: 11.198
2024-06-20 23:19:05,473 - INFO: Epoch: 21/200, Batch: 6/29, Batch_Loss_Train: 14.608
2024-06-20 23:19:05,865 - INFO: Epoch: 21/200, Batch: 7/29, Batch_Loss_Train: 23.743
2024-06-20 23:19:06,170 - INFO: Epoch: 21/200, Batch: 8/29, Batch_Loss_Train: 35.741
2024-06-20 23:19:06,626 - INFO: Epoch: 21/200, Batch: 9/29, Batch_Loss_Train: 65.766
2024-06-20 23:19:06,928 - INFO: Epoch: 21/200, Batch: 10/29, Batch_Loss_Train: 115.895
2024-06-20 23:19:07,323 - INFO: Epoch: 21/200, Batch: 11/29, Batch_Loss_Train: 174.212
2024-06-20 23:19:07,630 - INFO: Epoch: 21/200, Batch: 12/29, Batch_Loss_Train: 207.094
2024-06-20 23:19:08,082 - INFO: Epoch: 21/200, Batch: 13/29, Batch_Loss_Train: 86.682
2024-06-20 23:19:08,389 - INFO: Epoch: 21/200, Batch: 14/29, Batch_Loss_Train: 22.055
2024-06-20 23:19:08,792 - INFO: Epoch: 21/200, Batch: 15/29, Batch_Loss_Train: 28.185
2024-06-20 23:19:09,096 - INFO: Epoch: 21/200, Batch: 16/29, Batch_Loss_Train: 28.613
2024-06-20 23:19:09,543 - INFO: Epoch: 21/200, Batch: 17/29, Batch_Loss_Train: 19.728
2024-06-20 23:19:09,848 - INFO: Epoch: 21/200, Batch: 18/29, Batch_Loss_Train: 18.063
2024-06-20 23:19:10,242 - INFO: Epoch: 21/200, Batch: 19/29, Batch_Loss_Train: 17.549
2024-06-20 23:19:10,544 - INFO: Epoch: 21/200, Batch: 20/29, Batch_Loss_Train: 20.499
2024-06-20 23:19:10,988 - INFO: Epoch: 21/200, Batch: 21/29, Batch_Loss_Train: 16.115
2024-06-20 23:19:11,294 - INFO: Epoch: 21/200, Batch: 22/29, Batch_Loss_Train: 18.126
2024-06-20 23:19:11,683 - INFO: Epoch: 21/200, Batch: 23/29, Batch_Loss_Train: 19.525
2024-06-20 23:19:11,990 - INFO: Epoch: 21/200, Batch: 24/29, Batch_Loss_Train: 25.345
2024-06-20 23:19:12,429 - INFO: Epoch: 21/200, Batch: 25/29, Batch_Loss_Train: 16.914
2024-06-20 23:19:12,732 - INFO: Epoch: 21/200, Batch: 26/29, Batch_Loss_Train: 19.888
2024-06-20 23:19:13,115 - INFO: Epoch: 21/200, Batch: 27/29, Batch_Loss_Train: 29.994
2024-06-20 23:19:13,418 - INFO: Epoch: 21/200, Batch: 28/29, Batch_Loss_Train: 41.197
2024-06-20 23:19:13,639 - INFO: Epoch: 21/200, Batch: 29/29, Batch_Loss_Train: 34.170
2024-06-20 23:19:24,315 - INFO: 21/200 final results:
2024-06-20 23:19:24,316 - INFO: Training loss: 41.888.
2024-06-20 23:19:24,316 - INFO: Training MAE: 4.717.
2024-06-20 23:19:24,316 - INFO: Training MSE: 42.041.
2024-06-20 23:19:44,497 - INFO: Epoch: 21/200, Loss_train: 41.887994075643604, Loss_val: 49.95502498232085
2024-06-20 23:19:44,497 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:19:44,497 - INFO: Epoch 22/200...
2024-06-20 23:19:44,497 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:19:44,497 - INFO: Batch size: 32.
2024-06-20 23:19:44,501 - INFO: Dataset:
2024-06-20 23:19:44,501 - INFO: Batch size:
2024-06-20 23:19:44,501 - INFO: Number of workers:
2024-06-20 23:19:45,619 - INFO: Epoch: 22/200, Batch: 1/29, Batch_Loss_Train: 44.155
2024-06-20 23:19:45,940 - INFO: Epoch: 22/200, Batch: 2/29, Batch_Loss_Train: 31.496
2024-06-20 23:19:46,344 - INFO: Epoch: 22/200, Batch: 3/29, Batch_Loss_Train: 20.320
2024-06-20 23:19:46,668 - INFO: Epoch: 22/200, Batch: 4/29, Batch_Loss_Train: 22.162
2024-06-20 23:19:47,076 - INFO: Epoch: 22/200, Batch: 5/29, Batch_Loss_Train: 27.918
2024-06-20 23:19:47,396 - INFO: Epoch: 22/200, Batch: 6/29, Batch_Loss_Train: 29.282
2024-06-20 23:19:47,790 - INFO: Epoch: 22/200, Batch: 7/29, Batch_Loss_Train: 17.914
2024-06-20 23:19:48,112 - INFO: Epoch: 22/200, Batch: 8/29, Batch_Loss_Train: 19.135
2024-06-20 23:19:48,509 - INFO: Epoch: 22/200, Batch: 9/29, Batch_Loss_Train: 25.572
2024-06-20 23:19:48,825 - INFO: Epoch: 22/200, Batch: 10/29, Batch_Loss_Train: 30.720
2024-06-20 23:19:49,239 - INFO: Epoch: 22/200, Batch: 11/29, Batch_Loss_Train: 26.067
2024-06-20 23:19:49,560 - INFO: Epoch: 22/200, Batch: 12/29, Batch_Loss_Train: 27.036
2024-06-20 23:19:49,974 - INFO: Epoch: 22/200, Batch: 13/29, Batch_Loss_Train: 23.453
2024-06-20 23:19:50,295 - INFO: Epoch: 22/200, Batch: 14/29, Batch_Loss_Train: 25.393
2024-06-20 23:19:50,709 - INFO: Epoch: 22/200, Batch: 15/29, Batch_Loss_Train: 34.178
2024-06-20 23:19:51,027 - INFO: Epoch: 22/200, Batch: 16/29, Batch_Loss_Train: 39.743
2024-06-20 23:19:51,439 - INFO: Epoch: 22/200, Batch: 17/29, Batch_Loss_Train: 38.317
2024-06-20 23:19:51,757 - INFO: Epoch: 22/200, Batch: 18/29, Batch_Loss_Train: 29.803
2024-06-20 23:19:52,165 - INFO: Epoch: 22/200, Batch: 19/29, Batch_Loss_Train: 18.298
2024-06-20 23:19:52,481 - INFO: Epoch: 22/200, Batch: 20/29, Batch_Loss_Train: 15.671
2024-06-20 23:19:52,886 - INFO: Epoch: 22/200, Batch: 21/29, Batch_Loss_Train: 28.623
2024-06-20 23:19:53,206 - INFO: Epoch: 22/200, Batch: 22/29, Batch_Loss_Train: 24.246
2024-06-20 23:19:53,615 - INFO: Epoch: 22/200, Batch: 23/29, Batch_Loss_Train: 21.975
2024-06-20 23:19:53,935 - INFO: Epoch: 22/200, Batch: 24/29, Batch_Loss_Train: 21.071
2024-06-20 23:19:54,338 - INFO: Epoch: 22/200, Batch: 25/29, Batch_Loss_Train: 18.434
2024-06-20 23:19:54,654 - INFO: Epoch: 22/200, Batch: 26/29, Batch_Loss_Train: 14.741
2024-06-20 23:19:55,058 - INFO: Epoch: 22/200, Batch: 27/29, Batch_Loss_Train: 15.134
2024-06-20 23:19:55,374 - INFO: Epoch: 22/200, Batch: 28/29, Batch_Loss_Train: 14.028
2024-06-20 23:19:55,595 - INFO: Epoch: 22/200, Batch: 29/29, Batch_Loss_Train: 15.569
2024-06-20 23:20:06,569 - INFO: 22/200 final results:
2024-06-20 23:20:06,570 - INFO: Training loss: 24.843.
2024-06-20 23:20:06,570 - INFO: Training MAE: 3.954.
2024-06-20 23:20:06,570 - INFO: Training MSE: 25.027.
2024-06-20 23:20:27,049 - INFO: Epoch: 22/200, Loss_train: 24.84321666585988, Loss_val: 29.776788086726746
2024-06-20 23:20:27,049 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:20:27,049 - INFO: Epoch 23/200...
2024-06-20 23:20:27,049 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:20:27,050 - INFO: Batch size: 32.
2024-06-20 23:20:27,053 - INFO: Dataset:
2024-06-20 23:20:27,053 - INFO: Batch size:
2024-06-20 23:20:27,053 - INFO: Number of workers:
2024-06-20 23:20:28,202 - INFO: Epoch: 23/200, Batch: 1/29, Batch_Loss_Train: 22.373
2024-06-20 23:20:28,512 - INFO: Epoch: 23/200, Batch: 2/29, Batch_Loss_Train: 19.379
2024-06-20 23:20:28,910 - INFO: Epoch: 23/200, Batch: 3/29, Batch_Loss_Train: 18.034
2024-06-20 23:20:29,232 - INFO: Epoch: 23/200, Batch: 4/29, Batch_Loss_Train: 19.688
2024-06-20 23:20:29,656 - INFO: Epoch: 23/200, Batch: 5/29, Batch_Loss_Train: 25.486
2024-06-20 23:20:29,961 - INFO: Epoch: 23/200, Batch: 6/29, Batch_Loss_Train: 23.465
2024-06-20 23:20:30,353 - INFO: Epoch: 23/200, Batch: 7/29, Batch_Loss_Train: 29.117
2024-06-20 23:20:30,671 - INFO: Epoch: 23/200, Batch: 8/29, Batch_Loss_Train: 25.767
2024-06-20 23:20:31,106 - INFO: Epoch: 23/200, Batch: 9/29, Batch_Loss_Train: 21.331
2024-06-20 23:20:31,406 - INFO: Epoch: 23/200, Batch: 10/29, Batch_Loss_Train: 24.872
2024-06-20 23:20:31,802 - INFO: Epoch: 23/200, Batch: 11/29, Batch_Loss_Train: 40.529
2024-06-20 23:20:32,122 - INFO: Epoch: 23/200, Batch: 12/29, Batch_Loss_Train: 89.039
2024-06-20 23:20:32,562 - INFO: Epoch: 23/200, Batch: 13/29, Batch_Loss_Train: 135.424
2024-06-20 23:20:32,871 - INFO: Epoch: 23/200, Batch: 14/29, Batch_Loss_Train: 98.713
2024-06-20 23:20:33,275 - INFO: Epoch: 23/200, Batch: 15/29, Batch_Loss_Train: 36.960
2024-06-20 23:20:33,595 - INFO: Epoch: 23/200, Batch: 16/29, Batch_Loss_Train: 37.079
2024-06-20 23:20:34,030 - INFO: Epoch: 23/200, Batch: 17/29, Batch_Loss_Train: 22.031
2024-06-20 23:20:34,336 - INFO: Epoch: 23/200, Batch: 18/29, Batch_Loss_Train: 15.571
2024-06-20 23:20:34,731 - INFO: Epoch: 23/200, Batch: 19/29, Batch_Loss_Train: 18.158
2024-06-20 23:20:35,047 - INFO: Epoch: 23/200, Batch: 20/29, Batch_Loss_Train: 29.813
2024-06-20 23:20:35,478 - INFO: Epoch: 23/200, Batch: 21/29, Batch_Loss_Train: 30.625
2024-06-20 23:20:35,785 - INFO: Epoch: 23/200, Batch: 22/29, Batch_Loss_Train: 50.372
2024-06-20 23:20:36,170 - INFO: Epoch: 23/200, Batch: 23/29, Batch_Loss_Train: 60.958
2024-06-20 23:20:36,490 - INFO: Epoch: 23/200, Batch: 24/29, Batch_Loss_Train: 73.372
2024-06-20 23:20:36,907 - INFO: Epoch: 23/200, Batch: 25/29, Batch_Loss_Train: 55.125
2024-06-20 23:20:37,210 - INFO: Epoch: 23/200, Batch: 26/29, Batch_Loss_Train: 31.294
2024-06-20 23:20:37,586 - INFO: Epoch: 23/200, Batch: 27/29, Batch_Loss_Train: 25.276
2024-06-20 23:20:37,901 - INFO: Epoch: 23/200, Batch: 28/29, Batch_Loss_Train: 35.198
2024-06-20 23:20:38,115 - INFO: Epoch: 23/200, Batch: 29/29, Batch_Loss_Train: 21.158
2024-06-20 23:20:49,031 - INFO: 23/200 final results:
2024-06-20 23:20:49,031 - INFO: Training loss: 39.180.
2024-06-20 23:20:49,031 - INFO: Training MAE: 4.852.
2024-06-20 23:20:49,031 - INFO: Training MSE: 39.536.
2024-06-20 23:21:09,515 - INFO: Epoch: 23/200, Loss_train: 39.1795948291647, Loss_val: 21.044230526891248
2024-06-20 23:21:09,515 - INFO: Best internal validation val_loss: 20.244 at epoch: 18.
2024-06-20 23:21:09,515 - INFO: Epoch 24/200...
2024-06-20 23:21:09,515 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:21:09,515 - INFO: Batch size: 32.
2024-06-20 23:21:09,518 - INFO: Dataset:
2024-06-20 23:21:09,518 - INFO: Batch size:
2024-06-20 23:21:09,518 - INFO: Number of workers:
2024-06-20 23:21:10,683 - INFO: Epoch: 24/200, Batch: 1/29, Batch_Loss_Train: 20.914
2024-06-20 23:21:11,005 - INFO: Epoch: 24/200, Batch: 2/29, Batch_Loss_Train: 16.381
2024-06-20 23:21:11,405 - INFO: Epoch: 24/200, Batch: 3/29, Batch_Loss_Train: 26.716
2024-06-20 23:21:11,724 - INFO: Epoch: 24/200, Batch: 4/29, Batch_Loss_Train: 22.335
2024-06-20 23:21:12,146 - INFO: Epoch: 24/200, Batch: 5/29, Batch_Loss_Train: 19.195
2024-06-20 23:21:12,464 - INFO: Epoch: 24/200, Batch: 6/29, Batch_Loss_Train: 26.155
2024-06-20 23:21:12,859 - INFO: Epoch: 24/200, Batch: 7/29, Batch_Loss_Train: 30.521
2024-06-20 23:21:13,178 - INFO: Epoch: 24/200, Batch: 8/29, Batch_Loss_Train: 33.452
2024-06-20 23:21:13,599 - INFO: Epoch: 24/200, Batch: 9/29, Batch_Loss_Train: 32.030
2024-06-20 23:21:13,913 - INFO: Epoch: 24/200, Batch: 10/29, Batch_Loss_Train: 24.773
2024-06-20 23:21:14,307 - INFO: Epoch: 24/200, Batch: 11/29, Batch_Loss_Train: 24.090
2024-06-20 23:21:14,627 - INFO: Epoch: 24/200, Batch: 12/29, Batch_Loss_Train: 31.177
2024-06-20 23:21:15,053 - INFO: Epoch: 24/200, Batch: 13/29, Batch_Loss_Train: 42.318
2024-06-20 23:21:15,373 - INFO: Epoch: 24/200, Batch: 14/29, Batch_Loss_Train: 41.445
2024-06-20 23:21:15,776 - INFO: Epoch: 24/200, Batch: 15/29, Batch_Loss_Train: 24.278
2024-06-20 23:21:16,092 - INFO: Epoch: 24/200, Batch: 16/29, Batch_Loss_Train: 17.933
2024-06-20 23:21:16,512 - INFO: Epoch: 24/200, Batch: 17/29, Batch_Loss_Train: 20.540
2024-06-20 23:21:16,829 - INFO: Epoch: 24/200, Batch: 18/29, Batch_Loss_Train: 13.308
2024-06-20 23:21:17,221 - INFO: Epoch: 24/200, Batch: 19/29, Batch_Loss_Train: 13.739
2024-06-20 23:21:17,534 - INFO: Epoch: 24/200, Batch: 20/29, Batch_Loss_Train: 17.174
2024-06-20 23:21:17,945 - INFO: Epoch: 24/200, Batch: 21/29, Batch_Loss_Train: 17.553
2024-06-20 23:21:18,263 - INFO: Epoch: 24/200, Batch: 22/29, Batch_Loss_Train: 16.362
2024-06-20 23:21:18,657 - INFO: Epoch: 24/200, Batch: 23/29, Batch_Loss_Train: 23.266
2024-06-20 23:21:18,976 - INFO: Epoch: 24/200, Batch: 24/29, Batch_Loss_Train: 25.364
2024-06-20 23:21:19,389 - INFO: Epoch: 24/200, Batch: 25/29, Batch_Loss_Train: 23.418
2024-06-20 23:21:19,704 - INFO: Epoch: 24/200, Batch: 26/29, Batch_Loss_Train: 19.996
2024-06-20 23:21:20,094 - INFO: Epoch: 24/200, Batch: 27/29, Batch_Loss_Train: 19.735
2024-06-20 23:21:20,408 - INFO: Epoch: 24/200, Batch: 28/29, Batch_Loss_Train: 20.431
2024-06-20 23:21:20,626 - INFO: Epoch: 24/200, Batch: 29/29, Batch_Loss_Train: 11.089
2024-06-20 23:21:31,303 - INFO: 24/200 final results:
2024-06-20 23:21:31,303 - INFO: Training loss: 23.300.
2024-06-20 23:21:31,303 - INFO: Training MAE: 3.828.
2024-06-20 23:21:31,303 - INFO: Training MSE: 23.541.
2024-06-20 23:21:51,577 - INFO: Epoch: 24/200, Loss_train: 23.299630099329455, Loss_val: 16.40978901961754
2024-06-20 23:21:51,633 - INFO: Saved new best metric model for epoch 24.
2024-06-20 23:21:51,634 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:21:51,634 - INFO: Epoch 25/200...
2024-06-20 23:21:51,634 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:21:51,634 - INFO: Batch size: 32.
2024-06-20 23:21:51,637 - INFO: Dataset:
2024-06-20 23:21:51,637 - INFO: Batch size:
2024-06-20 23:21:51,637 - INFO: Number of workers:
2024-06-20 23:21:52,760 - INFO: Epoch: 25/200, Batch: 1/29, Batch_Loss_Train: 15.071
2024-06-20 23:21:53,072 - INFO: Epoch: 25/200, Batch: 2/29, Batch_Loss_Train: 13.900
2024-06-20 23:21:53,484 - INFO: Epoch: 25/200, Batch: 3/29, Batch_Loss_Train: 13.795
2024-06-20 23:21:53,792 - INFO: Epoch: 25/200, Batch: 4/29, Batch_Loss_Train: 22.782
2024-06-20 23:21:54,212 - INFO: Epoch: 25/200, Batch: 5/29, Batch_Loss_Train: 45.408
2024-06-20 23:21:54,520 - INFO: Epoch: 25/200, Batch: 6/29, Batch_Loss_Train: 74.123
2024-06-20 23:21:54,926 - INFO: Epoch: 25/200, Batch: 7/29, Batch_Loss_Train: 78.874
2024-06-20 23:21:55,235 - INFO: Epoch: 25/200, Batch: 8/29, Batch_Loss_Train: 51.984
2024-06-20 23:21:55,653 - INFO: Epoch: 25/200, Batch: 9/29, Batch_Loss_Train: 47.123
2024-06-20 23:21:55,956 - INFO: Epoch: 25/200, Batch: 10/29, Batch_Loss_Train: 51.117
2024-06-20 23:21:56,366 - INFO: Epoch: 25/200, Batch: 11/29, Batch_Loss_Train: 24.833
2024-06-20 23:21:56,675 - INFO: Epoch: 25/200, Batch: 12/29, Batch_Loss_Train: 21.333
2024-06-20 23:21:57,104 - INFO: Epoch: 25/200, Batch: 13/29, Batch_Loss_Train: 25.199
2024-06-20 23:21:57,413 - INFO: Epoch: 25/200, Batch: 14/29, Batch_Loss_Train: 27.933
2024-06-20 23:21:57,838 - INFO: Epoch: 25/200, Batch: 15/29, Batch_Loss_Train: 35.735
2024-06-20 23:21:58,143 - INFO: Epoch: 25/200, Batch: 16/29, Batch_Loss_Train: 54.313
2024-06-20 23:21:58,555 - INFO: Epoch: 25/200, Batch: 17/29, Batch_Loss_Train: 82.367
2024-06-20 23:21:58,860 - INFO: Epoch: 25/200, Batch: 18/29, Batch_Loss_Train: 81.053
2024-06-20 23:21:59,278 - INFO: Epoch: 25/200, Batch: 19/29, Batch_Loss_Train: 50.653
2024-06-20 23:21:59,582 - INFO: Epoch: 25/200, Batch: 20/29, Batch_Loss_Train: 19.429
2024-06-20 23:21:59,988 - INFO: Epoch: 25/200, Batch: 21/29, Batch_Loss_Train: 14.300
2024-06-20 23:22:00,296 - INFO: Epoch: 25/200, Batch: 22/29, Batch_Loss_Train: 13.011
2024-06-20 23:22:00,729 - INFO: Epoch: 25/200, Batch: 23/29, Batch_Loss_Train: 21.276
2024-06-20 23:22:01,036 - INFO: Epoch: 25/200, Batch: 24/29, Batch_Loss_Train: 18.529
2024-06-20 23:22:01,440 - INFO: Epoch: 25/200, Batch: 25/29, Batch_Loss_Train: 17.800
2024-06-20 23:22:01,743 - INFO: Epoch: 25/200, Batch: 26/29, Batch_Loss_Train: 17.339
2024-06-20 23:22:02,165 - INFO: Epoch: 25/200, Batch: 27/29, Batch_Loss_Train: 16.340
2024-06-20 23:22:02,469 - INFO: Epoch: 25/200, Batch: 28/29, Batch_Loss_Train: 15.896
2024-06-20 23:22:02,685 - INFO: Epoch: 25/200, Batch: 29/29, Batch_Loss_Train: 21.799
2024-06-20 23:22:13,650 - INFO: 25/200 final results:
2024-06-20 23:22:13,650 - INFO: Training loss: 34.252.
2024-06-20 23:22:13,650 - INFO: Training MAE: 4.678.
2024-06-20 23:22:13,650 - INFO: Training MSE: 34.499.
2024-06-20 23:22:34,069 - INFO: Epoch: 25/200, Loss_train: 34.25227625616665, Loss_val: 27.314928153465534
2024-06-20 23:22:34,069 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:22:34,069 - INFO: Epoch 26/200...
2024-06-20 23:22:34,069 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:22:34,069 - INFO: Batch size: 32.
2024-06-20 23:22:34,072 - INFO: Dataset:
2024-06-20 23:22:34,072 - INFO: Batch size:
2024-06-20 23:22:34,072 - INFO: Number of workers:
2024-06-20 23:22:35,196 - INFO: Epoch: 26/200, Batch: 1/29, Batch_Loss_Train: 24.884
2024-06-20 23:22:35,508 - INFO: Epoch: 26/200, Batch: 2/29, Batch_Loss_Train: 22.150
2024-06-20 23:22:35,940 - INFO: Epoch: 26/200, Batch: 3/29, Batch_Loss_Train: 24.378
2024-06-20 23:22:36,251 - INFO: Epoch: 26/200, Batch: 4/29, Batch_Loss_Train: 29.086
2024-06-20 23:22:36,671 - INFO: Epoch: 26/200, Batch: 5/29, Batch_Loss_Train: 32.610
2024-06-20 23:22:36,979 - INFO: Epoch: 26/200, Batch: 6/29, Batch_Loss_Train: 24.724
2024-06-20 23:22:37,385 - INFO: Epoch: 26/200, Batch: 7/29, Batch_Loss_Train: 21.209
2024-06-20 23:22:37,693 - INFO: Epoch: 26/200, Batch: 8/29, Batch_Loss_Train: 20.110
2024-06-20 23:22:38,109 - INFO: Epoch: 26/200, Batch: 9/29, Batch_Loss_Train: 24.634
2024-06-20 23:22:38,413 - INFO: Epoch: 26/200, Batch: 10/29, Batch_Loss_Train: 23.364
2024-06-20 23:22:38,831 - INFO: Epoch: 26/200, Batch: 11/29, Batch_Loss_Train: 22.027
2024-06-20 23:22:39,140 - INFO: Epoch: 26/200, Batch: 12/29, Batch_Loss_Train: 24.318
2024-06-20 23:22:39,567 - INFO: Epoch: 26/200, Batch: 13/29, Batch_Loss_Train: 24.087
2024-06-20 23:22:39,875 - INFO: Epoch: 26/200, Batch: 14/29, Batch_Loss_Train: 29.656
2024-06-20 23:22:40,301 - INFO: Epoch: 26/200, Batch: 15/29, Batch_Loss_Train: 25.018
2024-06-20 23:22:40,606 - INFO: Epoch: 26/200, Batch: 16/29, Batch_Loss_Train: 28.761
2024-06-20 23:22:41,017 - INFO: Epoch: 26/200, Batch: 17/29, Batch_Loss_Train: 41.506
2024-06-20 23:22:41,323 - INFO: Epoch: 26/200, Batch: 18/29, Batch_Loss_Train: 28.720
2024-06-20 23:22:41,745 - INFO: Epoch: 26/200, Batch: 19/29, Batch_Loss_Train: 15.130
2024-06-20 23:22:42,048 - INFO: Epoch: 26/200, Batch: 20/29, Batch_Loss_Train: 15.370
2024-06-20 23:22:42,457 - INFO: Epoch: 26/200, Batch: 21/29, Batch_Loss_Train: 17.522
2024-06-20 23:22:42,765 - INFO: Epoch: 26/200, Batch: 22/29, Batch_Loss_Train: 22.776
2024-06-20 23:22:43,198 - INFO: Epoch: 26/200, Batch: 23/29, Batch_Loss_Train: 22.305
2024-06-20 23:22:43,506 - INFO: Epoch: 26/200, Batch: 24/29, Batch_Loss_Train: 27.409
2024-06-20 23:22:43,912 - INFO: Epoch: 26/200, Batch: 25/29, Batch_Loss_Train: 37.790
2024-06-20 23:22:44,216 - INFO: Epoch: 26/200, Batch: 26/29, Batch_Loss_Train: 57.680
2024-06-20 23:22:44,628 - INFO: Epoch: 26/200, Batch: 27/29, Batch_Loss_Train: 67.570
2024-06-20 23:22:44,928 - INFO: Epoch: 26/200, Batch: 28/29, Batch_Loss_Train: 44.536
2024-06-20 23:22:45,142 - INFO: Epoch: 26/200, Batch: 29/29, Batch_Loss_Train: 40.002
2024-06-20 23:22:56,159 - INFO: 26/200 final results:
2024-06-20 23:22:56,159 - INFO: Training loss: 28.942.
2024-06-20 23:22:56,159 - INFO: Training MAE: 4.213.
2024-06-20 23:22:56,159 - INFO: Training MSE: 28.724.
2024-06-20 23:23:16,155 - INFO: Epoch: 26/200, Loss_train: 28.942419019238702, Loss_val: 40.95774354605839
2024-06-20 23:23:16,155 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:23:16,155 - INFO: Epoch 27/200...
2024-06-20 23:23:16,155 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:23:16,155 - INFO: Batch size: 32.
2024-06-20 23:23:16,158 - INFO: Dataset:
2024-06-20 23:23:16,158 - INFO: Batch size:
2024-06-20 23:23:16,158 - INFO: Number of workers:
2024-06-20 23:23:17,295 - INFO: Epoch: 27/200, Batch: 1/29, Batch_Loss_Train: 41.263
2024-06-20 23:23:17,608 - INFO: Epoch: 27/200, Batch: 2/29, Batch_Loss_Train: 25.714
2024-06-20 23:23:18,042 - INFO: Epoch: 27/200, Batch: 3/29, Batch_Loss_Train: 18.066
2024-06-20 23:23:18,373 - INFO: Epoch: 27/200, Batch: 4/29, Batch_Loss_Train: 22.845
2024-06-20 23:23:18,770 - INFO: Epoch: 27/200, Batch: 5/29, Batch_Loss_Train: 30.607
2024-06-20 23:23:19,101 - INFO: Epoch: 27/200, Batch: 6/29, Batch_Loss_Train: 30.452
2024-06-20 23:23:19,492 - INFO: Epoch: 27/200, Batch: 7/29, Batch_Loss_Train: 20.105
2024-06-20 23:23:19,811 - INFO: Epoch: 27/200, Batch: 8/29, Batch_Loss_Train: 20.069
2024-06-20 23:23:20,215 - INFO: Epoch: 27/200, Batch: 9/29, Batch_Loss_Train: 25.456
2024-06-20 23:23:20,550 - INFO: Epoch: 27/200, Batch: 10/29, Batch_Loss_Train: 21.942
2024-06-20 23:23:20,946 - INFO: Epoch: 27/200, Batch: 11/29, Batch_Loss_Train: 15.856
2024-06-20 23:23:21,266 - INFO: Epoch: 27/200, Batch: 12/29, Batch_Loss_Train: 19.102
2024-06-20 23:23:21,668 - INFO: Epoch: 27/200, Batch: 13/29, Batch_Loss_Train: 26.134
2024-06-20 23:23:22,002 - INFO: Epoch: 27/200, Batch: 14/29, Batch_Loss_Train: 30.909
2024-06-20 23:23:22,399 - INFO: Epoch: 27/200, Batch: 15/29, Batch_Loss_Train: 31.043
2024-06-20 23:23:22,717 - INFO: Epoch: 27/200, Batch: 16/29, Batch_Loss_Train: 24.676
2024-06-20 23:23:23,128 - INFO: Epoch: 27/200, Batch: 17/29, Batch_Loss_Train: 25.266
2024-06-20 23:23:23,459 - INFO: Epoch: 27/200, Batch: 18/29, Batch_Loss_Train: 17.227
2024-06-20 23:23:23,850 - INFO: Epoch: 27/200, Batch: 19/29, Batch_Loss_Train: 18.744
2024-06-20 23:23:24,166 - INFO: Epoch: 27/200, Batch: 20/29, Batch_Loss_Train: 18.798
2024-06-20 23:23:24,561 - INFO: Epoch: 27/200, Batch: 21/29, Batch_Loss_Train: 22.143
2024-06-20 23:23:24,890 - INFO: Epoch: 27/200, Batch: 22/29, Batch_Loss_Train: 19.502
2024-06-20 23:23:25,269 - INFO: Epoch: 27/200, Batch: 23/29, Batch_Loss_Train: 25.273
2024-06-20 23:23:25,586 - INFO: Epoch: 27/200, Batch: 24/29, Batch_Loss_Train: 36.121
2024-06-20 23:23:25,977 - INFO: Epoch: 27/200, Batch: 25/29, Batch_Loss_Train: 34.454
2024-06-20 23:23:26,304 - INFO: Epoch: 27/200, Batch: 26/29, Batch_Loss_Train: 38.529
2024-06-20 23:23:26,682 - INFO: Epoch: 27/200, Batch: 27/29, Batch_Loss_Train: 31.344
2024-06-20 23:23:26,998 - INFO: Epoch: 27/200, Batch: 28/29, Batch_Loss_Train: 26.659
2024-06-20 23:23:27,210 - INFO: Epoch: 27/200, Batch: 29/29, Batch_Loss_Train: 29.274
2024-06-20 23:23:38,141 - INFO: 27/200 final results:
2024-06-20 23:23:38,141 - INFO: Training loss: 25.778.
2024-06-20 23:23:38,141 - INFO: Training MAE: 4.018.
2024-06-20 23:23:38,141 - INFO: Training MSE: 25.709.
2024-06-20 23:23:58,675 - INFO: Epoch: 27/200, Loss_train: 25.778320937321105, Loss_val: 43.82487343097555
2024-06-20 23:23:58,675 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:23:58,675 - INFO: Epoch 28/200...
2024-06-20 23:23:58,675 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:23:58,675 - INFO: Batch size: 32.
2024-06-20 23:23:58,678 - INFO: Dataset:
2024-06-20 23:23:58,678 - INFO: Batch size:
2024-06-20 23:23:58,678 - INFO: Number of workers:
2024-06-20 23:23:59,798 - INFO: Epoch: 28/200, Batch: 1/29, Batch_Loss_Train: 42.845
2024-06-20 23:24:00,120 - INFO: Epoch: 28/200, Batch: 2/29, Batch_Loss_Train: 37.340
2024-06-20 23:24:00,533 - INFO: Epoch: 28/200, Batch: 3/29, Batch_Loss_Train: 19.358
2024-06-20 23:24:00,854 - INFO: Epoch: 28/200, Batch: 4/29, Batch_Loss_Train: 27.386
2024-06-20 23:24:01,268 - INFO: Epoch: 28/200, Batch: 5/29, Batch_Loss_Train: 25.291
2024-06-20 23:24:01,571 - INFO: Epoch: 28/200, Batch: 6/29, Batch_Loss_Train: 28.256
2024-06-20 23:24:01,975 - INFO: Epoch: 28/200, Batch: 7/29, Batch_Loss_Train: 41.708
2024-06-20 23:24:02,293 - INFO: Epoch: 28/200, Batch: 8/29, Batch_Loss_Train: 40.385
2024-06-20 23:24:02,713 - INFO: Epoch: 28/200, Batch: 9/29, Batch_Loss_Train: 22.612
2024-06-20 23:24:03,016 - INFO: Epoch: 28/200, Batch: 10/29, Batch_Loss_Train: 25.229
2024-06-20 23:24:03,412 - INFO: Epoch: 28/200, Batch: 11/29, Batch_Loss_Train: 13.194
2024-06-20 23:24:03,731 - INFO: Epoch: 28/200, Batch: 12/29, Batch_Loss_Train: 17.074
2024-06-20 23:24:04,160 - INFO: Epoch: 28/200, Batch: 13/29, Batch_Loss_Train: 21.870
2024-06-20 23:24:04,469 - INFO: Epoch: 28/200, Batch: 14/29, Batch_Loss_Train: 35.980
2024-06-20 23:24:04,881 - INFO: Epoch: 28/200, Batch: 15/29, Batch_Loss_Train: 27.578
2024-06-20 23:24:05,197 - INFO: Epoch: 28/200, Batch: 16/29, Batch_Loss_Train: 20.168
2024-06-20 23:24:05,622 - INFO: Epoch: 28/200, Batch: 17/29, Batch_Loss_Train: 25.291
2024-06-20 23:24:05,927 - INFO: Epoch: 28/200, Batch: 18/29, Batch_Loss_Train: 17.097
2024-06-20 23:24:06,321 - INFO: Epoch: 28/200, Batch: 19/29, Batch_Loss_Train: 17.599
2024-06-20 23:24:06,635 - INFO: Epoch: 28/200, Batch: 20/29, Batch_Loss_Train: 18.398
2024-06-20 23:24:07,049 - INFO: Epoch: 28/200, Batch: 21/29, Batch_Loss_Train: 28.455
2024-06-20 23:24:07,353 - INFO: Epoch: 28/200, Batch: 22/29, Batch_Loss_Train: 39.760
2024-06-20 23:24:07,762 - INFO: Epoch: 28/200, Batch: 23/29, Batch_Loss_Train: 26.600
2024-06-20 23:24:08,081 - INFO: Epoch: 28/200, Batch: 24/29, Batch_Loss_Train: 34.979
2024-06-20 23:24:08,492 - INFO: Epoch: 28/200, Batch: 25/29, Batch_Loss_Train: 52.680
2024-06-20 23:24:08,793 - INFO: Epoch: 28/200, Batch: 26/29, Batch_Loss_Train: 66.935
2024-06-20 23:24:09,196 - INFO: Epoch: 28/200, Batch: 27/29, Batch_Loss_Train: 59.748
2024-06-20 23:24:09,512 - INFO: Epoch: 28/200, Batch: 28/29, Batch_Loss_Train: 17.106
2024-06-20 23:24:09,735 - INFO: Epoch: 28/200, Batch: 29/29, Batch_Loss_Train: 18.772
2024-06-20 23:24:20,740 - INFO: 28/200 final results:
2024-06-20 23:24:20,740 - INFO: Training loss: 29.989.
2024-06-20 23:24:20,740 - INFO: Training MAE: 4.267.
2024-06-20 23:24:20,740 - INFO: Training MSE: 30.211.
2024-06-20 23:24:40,922 - INFO: Epoch: 28/200, Loss_train: 29.98938537466115, Loss_val: 22.80927089164997
2024-06-20 23:24:40,922 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:24:40,922 - INFO: Epoch 29/200...
2024-06-20 23:24:40,922 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:24:40,922 - INFO: Batch size: 32.
2024-06-20 23:24:40,925 - INFO: Dataset:
2024-06-20 23:24:40,925 - INFO: Batch size:
2024-06-20 23:24:40,925 - INFO: Number of workers:
2024-06-20 23:24:42,038 - INFO: Epoch: 29/200, Batch: 1/29, Batch_Loss_Train: 27.932
2024-06-20 23:24:42,350 - INFO: Epoch: 29/200, Batch: 2/29, Batch_Loss_Train: 38.225
2024-06-20 23:24:42,755 - INFO: Epoch: 29/200, Batch: 3/29, Batch_Loss_Train: 15.895
2024-06-20 23:24:43,079 - INFO: Epoch: 29/200, Batch: 4/29, Batch_Loss_Train: 25.008
2024-06-20 23:24:43,503 - INFO: Epoch: 29/200, Batch: 5/29, Batch_Loss_Train: 23.934
2024-06-20 23:24:43,810 - INFO: Epoch: 29/200, Batch: 6/29, Batch_Loss_Train: 16.873
2024-06-20 23:24:44,206 - INFO: Epoch: 29/200, Batch: 7/29, Batch_Loss_Train: 21.549
2024-06-20 23:24:44,527 - INFO: Epoch: 29/200, Batch: 8/29, Batch_Loss_Train: 29.312
2024-06-20 23:24:44,951 - INFO: Epoch: 29/200, Batch: 9/29, Batch_Loss_Train: 25.239
2024-06-20 23:24:45,255 - INFO: Epoch: 29/200, Batch: 10/29, Batch_Loss_Train: 27.387
2024-06-20 23:24:45,651 - INFO: Epoch: 29/200, Batch: 11/29, Batch_Loss_Train: 29.276
2024-06-20 23:24:45,973 - INFO: Epoch: 29/200, Batch: 12/29, Batch_Loss_Train: 17.276
2024-06-20 23:24:46,398 - INFO: Epoch: 29/200, Batch: 13/29, Batch_Loss_Train: 21.048
2024-06-20 23:24:46,706 - INFO: Epoch: 29/200, Batch: 14/29, Batch_Loss_Train: 32.371
2024-06-20 23:24:47,107 - INFO: Epoch: 29/200, Batch: 15/29, Batch_Loss_Train: 24.440
2024-06-20 23:24:47,425 - INFO: Epoch: 29/200, Batch: 16/29, Batch_Loss_Train: 18.090
2024-06-20 23:24:47,852 - INFO: Epoch: 29/200, Batch: 17/29, Batch_Loss_Train: 18.371
2024-06-20 23:24:48,157 - INFO: Epoch: 29/200, Batch: 18/29, Batch_Loss_Train: 19.325
2024-06-20 23:24:48,549 - INFO: Epoch: 29/200, Batch: 19/29, Batch_Loss_Train: 19.582
2024-06-20 23:24:48,863 - INFO: Epoch: 29/200, Batch: 20/29, Batch_Loss_Train: 22.326
2024-06-20 23:24:49,288 - INFO: Epoch: 29/200, Batch: 21/29, Batch_Loss_Train: 23.392
2024-06-20 23:24:49,591 - INFO: Epoch: 29/200, Batch: 22/29, Batch_Loss_Train: 23.121
2024-06-20 23:24:49,974 - INFO: Epoch: 29/200, Batch: 23/29, Batch_Loss_Train: 11.779
2024-06-20 23:24:50,291 - INFO: Epoch: 29/200, Batch: 24/29, Batch_Loss_Train: 16.354
2024-06-20 23:24:50,706 - INFO: Epoch: 29/200, Batch: 25/29, Batch_Loss_Train: 31.025
2024-06-20 23:24:51,006 - INFO: Epoch: 29/200, Batch: 26/29, Batch_Loss_Train: 43.341
2024-06-20 23:24:51,377 - INFO: Epoch: 29/200, Batch: 27/29, Batch_Loss_Train: 32.644
2024-06-20 23:24:51,689 - INFO: Epoch: 29/200, Batch: 28/29, Batch_Loss_Train: 25.483
2024-06-20 23:24:51,899 - INFO: Epoch: 29/200, Batch: 29/29, Batch_Loss_Train: 37.614
2024-06-20 23:25:02,836 - INFO: 29/200 final results:
2024-06-20 23:25:02,836 - INFO: Training loss: 24.766.
2024-06-20 23:25:02,836 - INFO: Training MAE: 3.931.
2024-06-20 23:25:02,836 - INFO: Training MSE: 24.512.
2024-06-20 23:25:22,942 - INFO: Epoch: 29/200, Loss_train: 24.76589140398749, Loss_val: 54.630624836888806
2024-06-20 23:25:22,942 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:25:22,942 - INFO: Epoch 30/200...
2024-06-20 23:25:22,942 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:25:22,942 - INFO: Batch size: 32.
2024-06-20 23:25:22,945 - INFO: Dataset:
2024-06-20 23:25:22,945 - INFO: Batch size:
2024-06-20 23:25:22,945 - INFO: Number of workers:
2024-06-20 23:25:24,057 - INFO: Epoch: 30/200, Batch: 1/29, Batch_Loss_Train: 52.424
2024-06-20 23:25:24,382 - INFO: Epoch: 30/200, Batch: 2/29, Batch_Loss_Train: 27.879
2024-06-20 23:25:24,787 - INFO: Epoch: 30/200, Batch: 3/29, Batch_Loss_Train: 29.401
2024-06-20 23:25:25,110 - INFO: Epoch: 30/200, Batch: 4/29, Batch_Loss_Train: 29.730
2024-06-20 23:25:25,533 - INFO: Epoch: 30/200, Batch: 5/29, Batch_Loss_Train: 25.486
2024-06-20 23:25:25,853 - INFO: Epoch: 30/200, Batch: 6/29, Batch_Loss_Train: 21.298
2024-06-20 23:25:26,248 - INFO: Epoch: 30/200, Batch: 7/29, Batch_Loss_Train: 20.835
2024-06-20 23:25:26,567 - INFO: Epoch: 30/200, Batch: 8/29, Batch_Loss_Train: 14.327
2024-06-20 23:25:26,982 - INFO: Epoch: 30/200, Batch: 9/29, Batch_Loss_Train: 14.765
2024-06-20 23:25:27,296 - INFO: Epoch: 30/200, Batch: 10/29, Batch_Loss_Train: 20.655
2024-06-20 23:25:27,691 - INFO: Epoch: 30/200, Batch: 11/29, Batch_Loss_Train: 25.906
2024-06-20 23:25:28,011 - INFO: Epoch: 30/200, Batch: 12/29, Batch_Loss_Train: 42.809
2024-06-20 23:25:28,435 - INFO: Epoch: 30/200, Batch: 13/29, Batch_Loss_Train: 70.271
2024-06-20 23:25:28,754 - INFO: Epoch: 30/200, Batch: 14/29, Batch_Loss_Train: 57.760
2024-06-20 23:25:29,154 - INFO: Epoch: 30/200, Batch: 15/29, Batch_Loss_Train: 51.578
2024-06-20 23:25:29,469 - INFO: Epoch: 30/200, Batch: 16/29, Batch_Loss_Train: 65.690
2024-06-20 23:25:29,887 - INFO: Epoch: 30/200, Batch: 17/29, Batch_Loss_Train: 37.783
2024-06-20 23:25:30,202 - INFO: Epoch: 30/200, Batch: 18/29, Batch_Loss_Train: 38.702
2024-06-20 23:25:30,592 - INFO: Epoch: 30/200, Batch: 19/29, Batch_Loss_Train: 38.423
2024-06-20 23:25:30,905 - INFO: Epoch: 30/200, Batch: 20/29, Batch_Loss_Train: 19.633
2024-06-20 23:25:31,321 - INFO: Epoch: 30/200, Batch: 21/29, Batch_Loss_Train: 15.914
2024-06-20 23:25:31,638 - INFO: Epoch: 30/200, Batch: 22/29, Batch_Loss_Train: 19.349
2024-06-20 23:25:32,015 - INFO: Epoch: 30/200, Batch: 23/29, Batch_Loss_Train: 19.994
2024-06-20 23:25:32,331 - INFO: Epoch: 30/200, Batch: 24/29, Batch_Loss_Train: 16.714
2024-06-20 23:25:32,734 - INFO: Epoch: 30/200, Batch: 25/29, Batch_Loss_Train: 17.061
2024-06-20 23:25:33,048 - INFO: Epoch: 30/200, Batch: 26/29, Batch_Loss_Train: 19.570
2024-06-20 23:25:33,427 - INFO: Epoch: 30/200, Batch: 27/29, Batch_Loss_Train: 19.010
2024-06-20 23:25:33,739 - INFO: Epoch: 30/200, Batch: 28/29, Batch_Loss_Train: 15.743
2024-06-20 23:25:33,954 - INFO: Epoch: 30/200, Batch: 29/29, Batch_Loss_Train: 13.172
2024-06-20 23:25:44,804 - INFO: 30/200 final results:
2024-06-20 23:25:44,804 - INFO: Training loss: 29.720.
2024-06-20 23:25:44,804 - INFO: Training MAE: 4.325.
2024-06-20 23:25:44,804 - INFO: Training MSE: 30.047.
2024-06-20 23:26:04,985 - INFO: Epoch: 30/200, Loss_train: 29.72009020838244, Loss_val: 16.71727104844718
2024-06-20 23:26:04,985 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:26:04,985 - INFO: Epoch 31/200...
2024-06-20 23:26:04,985 - INFO: Learning rate: 0.0017631196320841366.
2024-06-20 23:26:04,985 - INFO: Batch size: 32.
2024-06-20 23:26:04,989 - INFO: Dataset:
2024-06-20 23:26:04,989 - INFO: Batch size:
2024-06-20 23:26:04,989 - INFO: Number of workers:
2024-06-20 23:26:06,119 - INFO: Epoch: 31/200, Batch: 1/29, Batch_Loss_Train: 19.302
2024-06-20 23:26:06,442 - INFO: Epoch: 31/200, Batch: 2/29, Batch_Loss_Train: 17.336
2024-06-20 23:26:06,843 - INFO: Epoch: 31/200, Batch: 3/29, Batch_Loss_Train: 18.670
2024-06-20 23:26:07,164 - INFO: Epoch: 31/200, Batch: 4/29, Batch_Loss_Train: 11.954
2024-06-20 23:26:07,580 - INFO: Epoch: 31/200, Batch: 5/29, Batch_Loss_Train: 16.805
2024-06-20 23:26:07,897 - INFO: Epoch: 31/200, Batch: 6/29, Batch_Loss_Train: 12.220
2024-06-20 23:26:08,288 - INFO: Epoch: 31/200, Batch: 7/29, Batch_Loss_Train: 12.904
2024-06-20 23:26:08,607 - INFO: Epoch: 31/200, Batch: 8/29, Batch_Loss_Train: 14.305
2024-06-20 23:26:09,022 - INFO: Epoch: 31/200, Batch: 9/29, Batch_Loss_Train: 13.508
2024-06-20 23:26:09,334 - INFO: Epoch: 31/200, Batch: 10/29, Batch_Loss_Train: 16.954
2024-06-20 23:26:09,725 - INFO: Epoch: 31/200, Batch: 11/29, Batch_Loss_Train: 18.817
2024-06-20 23:26:10,043 - INFO: Epoch: 31/200, Batch: 12/29, Batch_Loss_Train: 47.325
2024-06-20 23:26:10,458 - INFO: Epoch: 31/200, Batch: 13/29, Batch_Loss_Train: 85.996
2024-06-20 23:26:10,776 - INFO: Epoch: 31/200, Batch: 14/29, Batch_Loss_Train: 49.453
2024-06-20 23:26:11,182 - INFO: Epoch: 31/200, Batch: 15/29, Batch_Loss_Train: 46.925
2024-06-20 23:26:11,501 - INFO: Epoch: 31/200, Batch: 16/29, Batch_Loss_Train: 49.990
2024-06-20 23:26:11,923 - INFO: Epoch: 31/200, Batch: 17/29, Batch_Loss_Train: 71.212
2024-06-20 23:26:12,241 - INFO: Epoch: 31/200, Batch: 18/29, Batch_Loss_Train: 106.017
2024-06-20 23:26:12,636 - INFO: Epoch: 31/200, Batch: 19/29, Batch_Loss_Train: 64.576
2024-06-20 23:26:12,951 - INFO: Epoch: 31/200, Batch: 20/29, Batch_Loss_Train: 22.469
2024-06-20 23:26:13,368 - INFO: Epoch: 31/200, Batch: 21/29, Batch_Loss_Train: 23.976
2024-06-20 23:26:13,688 - INFO: Epoch: 31/200, Batch: 22/29, Batch_Loss_Train: 28.265
2024-06-20 23:26:14,070 - INFO: Epoch: 31/200, Batch: 23/29, Batch_Loss_Train: 22.862
2024-06-20 23:26:14,388 - INFO: Epoch: 31/200, Batch: 24/29, Batch_Loss_Train: 23.717
2024-06-20 23:26:14,788 - INFO: Epoch: 31/200, Batch: 25/29, Batch_Loss_Train: 18.029
2024-06-20 23:26:15,101 - INFO: Epoch: 31/200, Batch: 26/29, Batch_Loss_Train: 18.052
2024-06-20 23:26:15,473 - INFO: Epoch: 31/200, Batch: 27/29, Batch_Loss_Train: 19.973
2024-06-20 23:26:15,785 - INFO: Epoch: 31/200, Batch: 28/29, Batch_Loss_Train: 18.922
2024-06-20 23:26:16,001 - INFO: Epoch: 31/200, Batch: 29/29, Batch_Loss_Train: 25.278
2024-06-20 23:26:26,921 - INFO: 31/200 final results:
2024-06-20 23:26:26,921 - INFO: Training loss: 31.580.
2024-06-20 23:26:26,921 - INFO: Training MAE: 4.260.
2024-06-20 23:26:26,921 - INFO: Training MSE: 31.704.
2024-06-20 23:26:47,483 - INFO: Epoch: 31/200, Loss_train: 31.579706652411097, Loss_val: 19.333952574894347
2024-06-20 23:26:47,483 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:26:47,483 - INFO: Epoch 32/200...
2024-06-20 23:26:47,483 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:26:47,483 - INFO: Batch size: 32.
2024-06-20 23:26:47,486 - INFO: Dataset:
2024-06-20 23:26:47,486 - INFO: Batch size:
2024-06-20 23:26:47,487 - INFO: Number of workers:
2024-06-20 23:26:48,619 - INFO: Epoch: 32/200, Batch: 1/29, Batch_Loss_Train: 17.834
2024-06-20 23:26:48,929 - INFO: Epoch: 32/200, Batch: 2/29, Batch_Loss_Train: 18.070
2024-06-20 23:26:49,333 - INFO: Epoch: 32/200, Batch: 3/29, Batch_Loss_Train: 17.266
2024-06-20 23:26:49,654 - INFO: Epoch: 32/200, Batch: 4/29, Batch_Loss_Train: 15.104
2024-06-20 23:26:50,072 - INFO: Epoch: 32/200, Batch: 5/29, Batch_Loss_Train: 18.179
2024-06-20 23:26:50,377 - INFO: Epoch: 32/200, Batch: 6/29, Batch_Loss_Train: 16.107
2024-06-20 23:26:50,783 - INFO: Epoch: 32/200, Batch: 7/29, Batch_Loss_Train: 16.268
2024-06-20 23:26:51,101 - INFO: Epoch: 32/200, Batch: 8/29, Batch_Loss_Train: 13.778
2024-06-20 23:26:51,516 - INFO: Epoch: 32/200, Batch: 9/29, Batch_Loss_Train: 19.085
2024-06-20 23:26:51,817 - INFO: Epoch: 32/200, Batch: 10/29, Batch_Loss_Train: 15.989
2024-06-20 23:26:52,226 - INFO: Epoch: 32/200, Batch: 11/29, Batch_Loss_Train: 16.845
2024-06-20 23:26:52,544 - INFO: Epoch: 32/200, Batch: 12/29, Batch_Loss_Train: 14.901
2024-06-20 23:26:52,967 - INFO: Epoch: 32/200, Batch: 13/29, Batch_Loss_Train: 15.261
2024-06-20 23:26:53,276 - INFO: Epoch: 32/200, Batch: 14/29, Batch_Loss_Train: 13.076
2024-06-20 23:26:53,679 - INFO: Epoch: 32/200, Batch: 15/29, Batch_Loss_Train: 18.118
2024-06-20 23:26:53,998 - INFO: Epoch: 32/200, Batch: 16/29, Batch_Loss_Train: 12.102
2024-06-20 23:26:54,419 - INFO: Epoch: 32/200, Batch: 17/29, Batch_Loss_Train: 16.817
2024-06-20 23:26:54,725 - INFO: Epoch: 32/200, Batch: 18/29, Batch_Loss_Train: 13.169
2024-06-20 23:26:55,123 - INFO: Epoch: 32/200, Batch: 19/29, Batch_Loss_Train: 14.088
2024-06-20 23:26:55,439 - INFO: Epoch: 32/200, Batch: 20/29, Batch_Loss_Train: 12.826
2024-06-20 23:26:55,854 - INFO: Epoch: 32/200, Batch: 21/29, Batch_Loss_Train: 13.919
2024-06-20 23:26:56,158 - INFO: Epoch: 32/200, Batch: 22/29, Batch_Loss_Train: 11.939
2024-06-20 23:26:56,554 - INFO: Epoch: 32/200, Batch: 23/29, Batch_Loss_Train: 13.557
2024-06-20 23:26:56,870 - INFO: Epoch: 32/200, Batch: 24/29, Batch_Loss_Train: 13.274
2024-06-20 23:26:57,274 - INFO: Epoch: 32/200, Batch: 25/29, Batch_Loss_Train: 13.656
2024-06-20 23:26:57,575 - INFO: Epoch: 32/200, Batch: 26/29, Batch_Loss_Train: 19.989
2024-06-20 23:26:57,965 - INFO: Epoch: 32/200, Batch: 27/29, Batch_Loss_Train: 15.782
2024-06-20 23:26:58,278 - INFO: Epoch: 32/200, Batch: 28/29, Batch_Loss_Train: 12.474
2024-06-20 23:26:58,488 - INFO: Epoch: 32/200, Batch: 29/29, Batch_Loss_Train: 14.776
2024-06-20 23:27:09,377 - INFO: 32/200 final results:
2024-06-20 23:27:09,377 - INFO: Training loss: 15.319.
2024-06-20 23:27:09,377 - INFO: Training MAE: 3.103.
2024-06-20 23:27:09,377 - INFO: Training MSE: 15.330.
2024-06-20 23:27:29,542 - INFO: Epoch: 32/200, Loss_train: 15.318879028846478, Loss_val: 17.039303089010303
2024-06-20 23:27:29,542 - INFO: Best internal validation val_loss: 16.410 at epoch: 24.
2024-06-20 23:27:29,542 - INFO: Epoch 33/200...
2024-06-20 23:27:29,542 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:27:29,542 - INFO: Batch size: 32.
2024-06-20 23:27:29,546 - INFO: Dataset:
2024-06-20 23:27:29,546 - INFO: Batch size:
2024-06-20 23:27:29,546 - INFO: Number of workers:
2024-06-20 23:27:30,668 - INFO: Epoch: 33/200, Batch: 1/29, Batch_Loss_Train: 16.893
2024-06-20 23:27:30,989 - INFO: Epoch: 33/200, Batch: 2/29, Batch_Loss_Train: 13.135
2024-06-20 23:27:31,403 - INFO: Epoch: 33/200, Batch: 3/29, Batch_Loss_Train: 13.962
2024-06-20 23:27:31,723 - INFO: Epoch: 33/200, Batch: 4/29, Batch_Loss_Train: 15.013
2024-06-20 23:27:32,137 - INFO: Epoch: 33/200, Batch: 5/29, Batch_Loss_Train: 13.781
2024-06-20 23:27:32,441 - INFO: Epoch: 33/200, Batch: 6/29, Batch_Loss_Train: 10.372
2024-06-20 23:27:32,851 - INFO: Epoch: 33/200, Batch: 7/29, Batch_Loss_Train: 14.590
2024-06-20 23:27:33,171 - INFO: Epoch: 33/200, Batch: 8/29, Batch_Loss_Train: 11.801
2024-06-20 23:27:33,593 - INFO: Epoch: 33/200, Batch: 9/29, Batch_Loss_Train: 13.389
2024-06-20 23:27:33,897 - INFO: Epoch: 33/200, Batch: 10/29, Batch_Loss_Train: 14.065
2024-06-20 23:27:34,293 - INFO: Epoch: 33/200, Batch: 11/29, Batch_Loss_Train: 11.509
2024-06-20 23:27:34,614 - INFO: Epoch: 33/200, Batch: 12/29, Batch_Loss_Train: 12.868
2024-06-20 23:27:35,041 - INFO: Epoch: 33/200, Batch: 13/29, Batch_Loss_Train: 15.158
2024-06-20 23:27:35,349 - INFO: Epoch: 33/200, Batch: 14/29, Batch_Loss_Train: 17.074
2024-06-20 23:27:35,763 - INFO: Epoch: 33/200, Batch: 15/29, Batch_Loss_Train: 13.275
2024-06-20 23:27:36,081 - INFO: Epoch: 33/200, Batch: 16/29, Batch_Loss_Train: 12.835
2024-06-20 23:27:36,499 - INFO: Epoch: 33/200, Batch: 17/29, Batch_Loss_Train: 19.462
2024-06-20 23:27:36,802 - INFO: Epoch: 33/200, Batch: 18/29, Batch_Loss_Train: 13.481
2024-06-20 23:27:37,193 - INFO: Epoch: 33/200, Batch: 19/29, Batch_Loss_Train: 11.628
2024-06-20 23:27:37,509 - INFO: Epoch: 33/200, Batch: 20/29, Batch_Loss_Train: 9.532
2024-06-20 23:27:37,928 - INFO: Epoch: 33/200, Batch: 21/29, Batch_Loss_Train: 16.868
2024-06-20 23:27:38,235 - INFO: Epoch: 33/200, Batch: 22/29, Batch_Loss_Train: 19.892
2024-06-20 23:27:38,640 - INFO: Epoch: 33/200, Batch: 23/29, Batch_Loss_Train: 23.709
2024-06-20 23:27:38,960 - INFO: Epoch: 33/200, Batch: 24/29, Batch_Loss_Train: 16.840
2024-06-20 23:27:39,363 - INFO: Epoch: 33/200, Batch: 25/29, Batch_Loss_Train: 10.967
2024-06-20 23:27:39,666 - INFO: Epoch: 33/200, Batch: 26/29, Batch_Loss_Train: 11.375
2024-06-20 23:27:40,052 - INFO: Epoch: 33/200, Batch: 27/29, Batch_Loss_Train: 11.740
2024-06-20 23:27:40,368 - INFO: Epoch: 33/200, Batch: 28/29, Batch_Loss_Train: 13.188
2024-06-20 23:27:40,589 - INFO: Epoch: 33/200, Batch: 29/29, Batch_Loss_Train: 11.603
2024-06-20 23:27:51,499 - INFO: 33/200 final results:
2024-06-20 23:27:51,499 - INFO: Training loss: 14.138.
2024-06-20 23:27:51,499 - INFO: Training MAE: 2.950.
2024-06-20 23:27:51,499 - INFO: Training MSE: 14.188.
2024-06-20 23:28:11,847 - INFO: Epoch: 33/200, Loss_train: 14.137971023033405, Loss_val: 13.350200685961493
2024-06-20 23:28:11,905 - INFO: Saved new best metric model for epoch 33.
2024-06-20 23:28:11,905 - INFO: Best internal validation val_loss: 13.350 at epoch: 33.
2024-06-20 23:28:11,905 - INFO: Epoch 34/200...
2024-06-20 23:28:11,905 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:28:11,905 - INFO: Batch size: 32.
2024-06-20 23:28:11,909 - INFO: Dataset:
2024-06-20 23:28:11,909 - INFO: Batch size:
2024-06-20 23:28:11,909 - INFO: Number of workers:
2024-06-20 23:28:13,044 - INFO: Epoch: 34/200, Batch: 1/29, Batch_Loss_Train: 17.177
2024-06-20 23:28:13,357 - INFO: Epoch: 34/200, Batch: 2/29, Batch_Loss_Train: 27.789
2024-06-20 23:28:13,773 - INFO: Epoch: 34/200, Batch: 3/29, Batch_Loss_Train: 21.303
2024-06-20 23:28:14,083 - INFO: Epoch: 34/200, Batch: 4/29, Batch_Loss_Train: 11.391
2024-06-20 23:28:14,505 - INFO: Epoch: 34/200, Batch: 5/29, Batch_Loss_Train: 15.314
2024-06-20 23:28:14,812 - INFO: Epoch: 34/200, Batch: 6/29, Batch_Loss_Train: 16.075
2024-06-20 23:28:15,222 - INFO: Epoch: 34/200, Batch: 7/29, Batch_Loss_Train: 9.614
2024-06-20 23:28:15,531 - INFO: Epoch: 34/200, Batch: 8/29, Batch_Loss_Train: 14.864
2024-06-20 23:28:15,957 - INFO: Epoch: 34/200, Batch: 9/29, Batch_Loss_Train: 15.637
2024-06-20 23:28:16,260 - INFO: Epoch: 34/200, Batch: 10/29, Batch_Loss_Train: 11.955
2024-06-20 23:28:16,673 - INFO: Epoch: 34/200, Batch: 11/29, Batch_Loss_Train: 12.560
2024-06-20 23:28:16,981 - INFO: Epoch: 34/200, Batch: 12/29, Batch_Loss_Train: 19.750
2024-06-20 23:28:17,408 - INFO: Epoch: 34/200, Batch: 13/29, Batch_Loss_Train: 20.724
2024-06-20 23:28:17,716 - INFO: Epoch: 34/200, Batch: 14/29, Batch_Loss_Train: 16.121
2024-06-20 23:28:18,132 - INFO: Epoch: 34/200, Batch: 15/29, Batch_Loss_Train: 19.406
2024-06-20 23:28:18,437 - INFO: Epoch: 34/200, Batch: 16/29, Batch_Loss_Train: 20.433
2024-06-20 23:28:18,857 - INFO: Epoch: 34/200, Batch: 17/29, Batch_Loss_Train: 14.604
2024-06-20 23:28:19,162 - INFO: Epoch: 34/200, Batch: 18/29, Batch_Loss_Train: 19.031
2024-06-20 23:28:19,578 - INFO: Epoch: 34/200, Batch: 19/29, Batch_Loss_Train: 18.113
2024-06-20 23:28:19,881 - INFO: Epoch: 34/200, Batch: 20/29, Batch_Loss_Train: 18.595
2024-06-20 23:28:20,299 - INFO: Epoch: 34/200, Batch: 21/29, Batch_Loss_Train: 15.581
2024-06-20 23:28:20,606 - INFO: Epoch: 34/200, Batch: 22/29, Batch_Loss_Train: 13.847
2024-06-20 23:28:21,026 - INFO: Epoch: 34/200, Batch: 23/29, Batch_Loss_Train: 10.161
2024-06-20 23:28:21,334 - INFO: Epoch: 34/200, Batch: 24/29, Batch_Loss_Train: 11.063
2024-06-20 23:28:21,748 - INFO: Epoch: 34/200, Batch: 25/29, Batch_Loss_Train: 15.314
2024-06-20 23:28:22,052 - INFO: Epoch: 34/200, Batch: 26/29, Batch_Loss_Train: 20.216
2024-06-20 23:28:22,455 - INFO: Epoch: 34/200, Batch: 27/29, Batch_Loss_Train: 15.214
2024-06-20 23:28:22,759 - INFO: Epoch: 34/200, Batch: 28/29, Batch_Loss_Train: 10.812
2024-06-20 23:28:22,977 - INFO: Epoch: 34/200, Batch: 29/29, Batch_Loss_Train: 8.585
2024-06-20 23:28:33,909 - INFO: 34/200 final results:
2024-06-20 23:28:33,909 - INFO: Training loss: 15.905.
2024-06-20 23:28:33,909 - INFO: Training MAE: 3.143.
2024-06-20 23:28:33,909 - INFO: Training MSE: 16.050.
2024-06-20 23:28:54,166 - INFO: Epoch: 34/200, Loss_train: 15.905151136990252, Loss_val: 11.42141763095198
2024-06-20 23:28:54,224 - INFO: Saved new best metric model for epoch 34.
2024-06-20 23:28:54,224 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:28:54,224 - INFO: Epoch 35/200...
2024-06-20 23:28:54,224 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:28:54,224 - INFO: Batch size: 32.
2024-06-20 23:28:54,227 - INFO: Dataset:
2024-06-20 23:28:54,227 - INFO: Batch size:
2024-06-20 23:28:54,228 - INFO: Number of workers:
2024-06-20 23:28:55,386 - INFO: Epoch: 35/200, Batch: 1/29, Batch_Loss_Train: 11.068
2024-06-20 23:28:55,695 - INFO: Epoch: 35/200, Batch: 2/29, Batch_Loss_Train: 11.089
2024-06-20 23:28:56,112 - INFO: Epoch: 35/200, Batch: 3/29, Batch_Loss_Train: 15.753
2024-06-20 23:28:56,724 - INFO: Epoch: 35/200, Batch: 4/29, Batch_Loss_Train: 15.993
2024-06-20 23:28:57,152 - INFO: Epoch: 35/200, Batch: 5/29, Batch_Loss_Train: 16.319
2024-06-20 23:28:57,457 - INFO: Epoch: 35/200, Batch: 6/29, Batch_Loss_Train: 19.446
2024-06-20 23:28:57,851 - INFO: Epoch: 35/200, Batch: 7/29, Batch_Loss_Train: 25.674
2024-06-20 23:28:58,168 - INFO: Epoch: 35/200, Batch: 8/29, Batch_Loss_Train: 27.378
2024-06-20 23:28:58,599 - INFO: Epoch: 35/200, Batch: 9/29, Batch_Loss_Train: 22.587
2024-06-20 23:28:58,900 - INFO: Epoch: 35/200, Batch: 10/29, Batch_Loss_Train: 14.521
2024-06-20 23:28:59,298 - INFO: Epoch: 35/200, Batch: 11/29, Batch_Loss_Train: 20.201
2024-06-20 23:28:59,614 - INFO: Epoch: 35/200, Batch: 12/29, Batch_Loss_Train: 20.820
2024-06-20 23:29:00,060 - INFO: Epoch: 35/200, Batch: 13/29, Batch_Loss_Train: 16.900
2024-06-20 23:29:00,367 - INFO: Epoch: 35/200, Batch: 14/29, Batch_Loss_Train: 19.115
2024-06-20 23:29:00,769 - INFO: Epoch: 35/200, Batch: 15/29, Batch_Loss_Train: 13.214
2024-06-20 23:29:01,072 - INFO: Epoch: 35/200, Batch: 16/29, Batch_Loss_Train: 12.157
2024-06-20 23:29:01,516 - INFO: Epoch: 35/200, Batch: 17/29, Batch_Loss_Train: 10.888
2024-06-20 23:29:01,819 - INFO: Epoch: 35/200, Batch: 18/29, Batch_Loss_Train: 15.713
2024-06-20 23:29:02,210 - INFO: Epoch: 35/200, Batch: 19/29, Batch_Loss_Train: 23.100
2024-06-20 23:29:02,510 - INFO: Epoch: 35/200, Batch: 20/29, Batch_Loss_Train: 26.532
2024-06-20 23:29:02,948 - INFO: Epoch: 35/200, Batch: 21/29, Batch_Loss_Train: 19.255
2024-06-20 23:29:03,251 - INFO: Epoch: 35/200, Batch: 22/29, Batch_Loss_Train: 10.492
2024-06-20 23:29:03,642 - INFO: Epoch: 35/200, Batch: 23/29, Batch_Loss_Train: 18.996
2024-06-20 23:29:03,946 - INFO: Epoch: 35/200, Batch: 24/29, Batch_Loss_Train: 24.582
2024-06-20 23:29:04,381 - INFO: Epoch: 35/200, Batch: 25/29, Batch_Loss_Train: 14.228
2024-06-20 23:29:04,682 - INFO: Epoch: 35/200, Batch: 26/29, Batch_Loss_Train: 16.135
2024-06-20 23:29:05,068 - INFO: Epoch: 35/200, Batch: 27/29, Batch_Loss_Train: 34.908
2024-06-20 23:29:05,369 - INFO: Epoch: 35/200, Batch: 28/29, Batch_Loss_Train: 53.395
2024-06-20 23:29:05,589 - INFO: Epoch: 35/200, Batch: 29/29, Batch_Loss_Train: 21.200
2024-06-20 23:29:16,195 - INFO: 35/200 final results:
2024-06-20 23:29:16,195 - INFO: Training loss: 19.712.
2024-06-20 23:29:16,195 - INFO: Training MAE: 3.445.
2024-06-20 23:29:16,195 - INFO: Training MSE: 19.683.
2024-06-20 23:29:36,244 - INFO: Epoch: 35/200, Loss_train: 19.712336046942347, Loss_val: 13.231776385471738
2024-06-20 23:29:36,244 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:29:36,244 - INFO: Epoch 36/200...
2024-06-20 23:29:36,244 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:29:36,244 - INFO: Batch size: 32.
2024-06-20 23:29:36,247 - INFO: Dataset:
2024-06-20 23:29:36,247 - INFO: Batch size:
2024-06-20 23:29:36,247 - INFO: Number of workers:
2024-06-20 23:29:37,409 - INFO: Epoch: 36/200, Batch: 1/29, Batch_Loss_Train: 11.899
2024-06-20 23:29:37,717 - INFO: Epoch: 36/200, Batch: 2/29, Batch_Loss_Train: 11.911
2024-06-20 23:29:38,117 - INFO: Epoch: 36/200, Batch: 3/29, Batch_Loss_Train: 10.638
2024-06-20 23:29:38,437 - INFO: Epoch: 36/200, Batch: 4/29, Batch_Loss_Train: 10.113
2024-06-20 23:29:38,884 - INFO: Epoch: 36/200, Batch: 5/29, Batch_Loss_Train: 14.660
2024-06-20 23:29:39,191 - INFO: Epoch: 36/200, Batch: 6/29, Batch_Loss_Train: 13.414
2024-06-20 23:29:39,587 - INFO: Epoch: 36/200, Batch: 7/29, Batch_Loss_Train: 11.361
2024-06-20 23:29:39,895 - INFO: Epoch: 36/200, Batch: 8/29, Batch_Loss_Train: 9.632
2024-06-20 23:29:40,349 - INFO: Epoch: 36/200, Batch: 9/29, Batch_Loss_Train: 8.882
2024-06-20 23:29:40,652 - INFO: Epoch: 36/200, Batch: 10/29, Batch_Loss_Train: 10.104
2024-06-20 23:29:41,052 - INFO: Epoch: 36/200, Batch: 11/29, Batch_Loss_Train: 15.983
2024-06-20 23:29:41,361 - INFO: Epoch: 36/200, Batch: 12/29, Batch_Loss_Train: 17.436
2024-06-20 23:29:41,808 - INFO: Epoch: 36/200, Batch: 13/29, Batch_Loss_Train: 25.699
2024-06-20 23:29:42,114 - INFO: Epoch: 36/200, Batch: 14/29, Batch_Loss_Train: 49.358
2024-06-20 23:29:42,513 - INFO: Epoch: 36/200, Batch: 15/29, Batch_Loss_Train: 62.331
2024-06-20 23:29:42,815 - INFO: Epoch: 36/200, Batch: 16/29, Batch_Loss_Train: 31.118
2024-06-20 23:29:43,264 - INFO: Epoch: 36/200, Batch: 17/29, Batch_Loss_Train: 13.490
2024-06-20 23:29:43,569 - INFO: Epoch: 36/200, Batch: 18/29, Batch_Loss_Train: 25.548
2024-06-20 23:29:43,963 - INFO: Epoch: 36/200, Batch: 19/29, Batch_Loss_Train: 14.273
2024-06-20 23:29:44,265 - INFO: Epoch: 36/200, Batch: 20/29, Batch_Loss_Train: 19.294
2024-06-20 23:29:44,710 - INFO: Epoch: 36/200, Batch: 21/29, Batch_Loss_Train: 17.196
2024-06-20 23:29:45,016 - INFO: Epoch: 36/200, Batch: 22/29, Batch_Loss_Train: 19.148
2024-06-20 23:29:45,411 - INFO: Epoch: 36/200, Batch: 23/29, Batch_Loss_Train: 19.402
2024-06-20 23:29:45,718 - INFO: Epoch: 36/200, Batch: 24/29, Batch_Loss_Train: 16.433
2024-06-20 23:29:46,156 - INFO: Epoch: 36/200, Batch: 25/29, Batch_Loss_Train: 10.392
2024-06-20 23:29:46,458 - INFO: Epoch: 36/200, Batch: 26/29, Batch_Loss_Train: 13.957
2024-06-20 23:29:46,847 - INFO: Epoch: 36/200, Batch: 27/29, Batch_Loss_Train: 10.978
2024-06-20 23:29:47,149 - INFO: Epoch: 36/200, Batch: 28/29, Batch_Loss_Train: 12.668
2024-06-20 23:29:47,371 - INFO: Epoch: 36/200, Batch: 29/29, Batch_Loss_Train: 16.250
2024-06-20 23:29:58,310 - INFO: 36/200 final results:
2024-06-20 23:29:58,310 - INFO: Training loss: 18.054.
2024-06-20 23:29:58,310 - INFO: Training MAE: 3.329.
2024-06-20 23:29:58,310 - INFO: Training MSE: 18.090.
2024-06-20 23:30:18,658 - INFO: Epoch: 36/200, Loss_train: 18.05408714557516, Loss_val: 30.23344901512409
2024-06-20 23:30:18,658 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:30:18,658 - INFO: Epoch 37/200...
2024-06-20 23:30:18,658 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:30:18,658 - INFO: Batch size: 32.
2024-06-20 23:30:18,661 - INFO: Dataset:
2024-06-20 23:30:18,662 - INFO: Batch size:
2024-06-20 23:30:18,662 - INFO: Number of workers:
2024-06-20 23:30:19,757 - INFO: Epoch: 37/200, Batch: 1/29, Batch_Loss_Train: 26.346
2024-06-20 23:30:20,078 - INFO: Epoch: 37/200, Batch: 2/29, Batch_Loss_Train: 36.694
2024-06-20 23:30:20,479 - INFO: Epoch: 37/200, Batch: 3/29, Batch_Loss_Train: 30.494
2024-06-20 23:30:20,797 - INFO: Epoch: 37/200, Batch: 4/29, Batch_Loss_Train: 14.500
2024-06-20 23:30:21,213 - INFO: Epoch: 37/200, Batch: 5/29, Batch_Loss_Train: 17.868
2024-06-20 23:30:21,518 - INFO: Epoch: 37/200, Batch: 6/29, Batch_Loss_Train: 19.288
2024-06-20 23:30:21,923 - INFO: Epoch: 37/200, Batch: 7/29, Batch_Loss_Train: 11.710
2024-06-20 23:30:22,242 - INFO: Epoch: 37/200, Batch: 8/29, Batch_Loss_Train: 14.892
2024-06-20 23:30:22,665 - INFO: Epoch: 37/200, Batch: 9/29, Batch_Loss_Train: 11.842
2024-06-20 23:30:22,967 - INFO: Epoch: 37/200, Batch: 10/29, Batch_Loss_Train: 11.413
2024-06-20 23:30:23,367 - INFO: Epoch: 37/200, Batch: 11/29, Batch_Loss_Train: 11.059
2024-06-20 23:30:23,686 - INFO: Epoch: 37/200, Batch: 12/29, Batch_Loss_Train: 15.257
2024-06-20 23:30:24,099 - INFO: Epoch: 37/200, Batch: 13/29, Batch_Loss_Train: 12.485
2024-06-20 23:30:24,407 - INFO: Epoch: 37/200, Batch: 14/29, Batch_Loss_Train: 12.395
2024-06-20 23:30:24,817 - INFO: Epoch: 37/200, Batch: 15/29, Batch_Loss_Train: 16.579
2024-06-20 23:30:25,133 - INFO: Epoch: 37/200, Batch: 16/29, Batch_Loss_Train: 16.672
2024-06-20 23:30:25,543 - INFO: Epoch: 37/200, Batch: 17/29, Batch_Loss_Train: 15.603
2024-06-20 23:30:25,848 - INFO: Epoch: 37/200, Batch: 18/29, Batch_Loss_Train: 12.644
2024-06-20 23:30:26,249 - INFO: Epoch: 37/200, Batch: 19/29, Batch_Loss_Train: 24.643
2024-06-20 23:30:26,564 - INFO: Epoch: 37/200, Batch: 20/29, Batch_Loss_Train: 38.113
2024-06-20 23:30:26,972 - INFO: Epoch: 37/200, Batch: 21/29, Batch_Loss_Train: 27.271
2024-06-20 23:30:27,276 - INFO: Epoch: 37/200, Batch: 22/29, Batch_Loss_Train: 24.505
2024-06-20 23:30:27,672 - INFO: Epoch: 37/200, Batch: 23/29, Batch_Loss_Train: 38.049
2024-06-20 23:30:27,989 - INFO: Epoch: 37/200, Batch: 24/29, Batch_Loss_Train: 28.015
2024-06-20 23:30:28,392 - INFO: Epoch: 37/200, Batch: 25/29, Batch_Loss_Train: 13.670
2024-06-20 23:30:28,695 - INFO: Epoch: 37/200, Batch: 26/29, Batch_Loss_Train: 11.691
2024-06-20 23:30:29,091 - INFO: Epoch: 37/200, Batch: 27/29, Batch_Loss_Train: 18.531
2024-06-20 23:30:29,407 - INFO: Epoch: 37/200, Batch: 28/29, Batch_Loss_Train: 19.782
2024-06-20 23:30:29,621 - INFO: Epoch: 37/200, Batch: 29/29, Batch_Loss_Train: 14.958
2024-06-20 23:30:40,617 - INFO: 37/200 final results:
2024-06-20 23:30:40,618 - INFO: Training loss: 19.551.
2024-06-20 23:30:40,618 - INFO: Training MAE: 3.467.
2024-06-20 23:30:40,618 - INFO: Training MSE: 19.641.
2024-06-20 23:31:00,868 - INFO: Epoch: 37/200, Loss_train: 19.550638593476393, Loss_val: 13.282214296275171
2024-06-20 23:31:00,868 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:31:00,868 - INFO: Epoch 38/200...
2024-06-20 23:31:00,868 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:31:00,868 - INFO: Batch size: 32.
2024-06-20 23:31:00,871 - INFO: Dataset:
2024-06-20 23:31:00,872 - INFO: Batch size:
2024-06-20 23:31:00,872 - INFO: Number of workers:
2024-06-20 23:31:02,004 - INFO: Epoch: 38/200, Batch: 1/29, Batch_Loss_Train: 9.771
2024-06-20 23:31:02,317 - INFO: Epoch: 38/200, Batch: 2/29, Batch_Loss_Train: 9.872
2024-06-20 23:31:02,724 - INFO: Epoch: 38/200, Batch: 3/29, Batch_Loss_Train: 9.771
2024-06-20 23:31:03,045 - INFO: Epoch: 38/200, Batch: 4/29, Batch_Loss_Train: 11.893
2024-06-20 23:31:03,461 - INFO: Epoch: 38/200, Batch: 5/29, Batch_Loss_Train: 14.384
2024-06-20 23:31:03,765 - INFO: Epoch: 38/200, Batch: 6/29, Batch_Loss_Train: 11.938
2024-06-20 23:31:04,169 - INFO: Epoch: 38/200, Batch: 7/29, Batch_Loss_Train: 11.915
2024-06-20 23:31:04,487 - INFO: Epoch: 38/200, Batch: 8/29, Batch_Loss_Train: 11.723
2024-06-20 23:31:04,906 - INFO: Epoch: 38/200, Batch: 9/29, Batch_Loss_Train: 15.053
2024-06-20 23:31:05,210 - INFO: Epoch: 38/200, Batch: 10/29, Batch_Loss_Train: 14.340
2024-06-20 23:31:05,616 - INFO: Epoch: 38/200, Batch: 11/29, Batch_Loss_Train: 10.506
2024-06-20 23:31:05,937 - INFO: Epoch: 38/200, Batch: 12/29, Batch_Loss_Train: 11.802
2024-06-20 23:31:06,364 - INFO: Epoch: 38/200, Batch: 13/29, Batch_Loss_Train: 13.902
2024-06-20 23:31:06,672 - INFO: Epoch: 38/200, Batch: 14/29, Batch_Loss_Train: 13.505
2024-06-20 23:31:07,085 - INFO: Epoch: 38/200, Batch: 15/29, Batch_Loss_Train: 12.993
2024-06-20 23:31:07,404 - INFO: Epoch: 38/200, Batch: 16/29, Batch_Loss_Train: 12.783
2024-06-20 23:31:07,826 - INFO: Epoch: 38/200, Batch: 17/29, Batch_Loss_Train: 13.129
2024-06-20 23:31:08,131 - INFO: Epoch: 38/200, Batch: 18/29, Batch_Loss_Train: 13.147
2024-06-20 23:31:08,536 - INFO: Epoch: 38/200, Batch: 19/29, Batch_Loss_Train: 14.854
2024-06-20 23:31:08,851 - INFO: Epoch: 38/200, Batch: 20/29, Batch_Loss_Train: 22.420
2024-06-20 23:31:09,268 - INFO: Epoch: 38/200, Batch: 21/29, Batch_Loss_Train: 18.080
2024-06-20 23:31:09,576 - INFO: Epoch: 38/200, Batch: 22/29, Batch_Loss_Train: 23.785
2024-06-20 23:31:09,981 - INFO: Epoch: 38/200, Batch: 23/29, Batch_Loss_Train: 35.887
2024-06-20 23:31:10,300 - INFO: Epoch: 38/200, Batch: 24/29, Batch_Loss_Train: 41.457
2024-06-20 23:31:10,715 - INFO: Epoch: 38/200, Batch: 25/29, Batch_Loss_Train: 41.529
2024-06-20 23:31:11,019 - INFO: Epoch: 38/200, Batch: 26/29, Batch_Loss_Train: 57.131
2024-06-20 23:31:11,424 - INFO: Epoch: 38/200, Batch: 27/29, Batch_Loss_Train: 48.610
2024-06-20 23:31:11,740 - INFO: Epoch: 38/200, Batch: 28/29, Batch_Loss_Train: 25.038
2024-06-20 23:31:11,964 - INFO: Epoch: 38/200, Batch: 29/29, Batch_Loss_Train: 19.379
2024-06-20 23:31:23,043 - INFO: 38/200 final results:
2024-06-20 23:31:23,043 - INFO: Training loss: 19.676.
2024-06-20 23:31:23,043 - INFO: Training MAE: 3.426.
2024-06-20 23:31:23,043 - INFO: Training MSE: 19.682.
2024-06-20 23:31:43,267 - INFO: Epoch: 38/200, Loss_train: 19.675778520518335, Loss_val: 14.400071374301252
2024-06-20 23:31:43,267 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:31:43,267 - INFO: Epoch 39/200...
2024-06-20 23:31:43,267 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:31:43,267 - INFO: Batch size: 32.
2024-06-20 23:31:43,270 - INFO: Dataset:
2024-06-20 23:31:43,271 - INFO: Batch size:
2024-06-20 23:31:43,271 - INFO: Number of workers:
2024-06-20 23:31:44,392 - INFO: Epoch: 39/200, Batch: 1/29, Batch_Loss_Train: 12.661
2024-06-20 23:31:44,715 - INFO: Epoch: 39/200, Batch: 2/29, Batch_Loss_Train: 15.247
2024-06-20 23:31:45,116 - INFO: Epoch: 39/200, Batch: 3/29, Batch_Loss_Train: 18.673
2024-06-20 23:31:45,437 - INFO: Epoch: 39/200, Batch: 4/29, Batch_Loss_Train: 11.153
2024-06-20 23:31:45,857 - INFO: Epoch: 39/200, Batch: 5/29, Batch_Loss_Train: 10.375
2024-06-20 23:31:46,176 - INFO: Epoch: 39/200, Batch: 6/29, Batch_Loss_Train: 12.198
2024-06-20 23:31:46,571 - INFO: Epoch: 39/200, Batch: 7/29, Batch_Loss_Train: 13.507
2024-06-20 23:31:46,891 - INFO: Epoch: 39/200, Batch: 8/29, Batch_Loss_Train: 14.012
2024-06-20 23:31:47,310 - INFO: Epoch: 39/200, Batch: 9/29, Batch_Loss_Train: 10.013
2024-06-20 23:31:47,624 - INFO: Epoch: 39/200, Batch: 10/29, Batch_Loss_Train: 11.724
2024-06-20 23:31:48,019 - INFO: Epoch: 39/200, Batch: 11/29, Batch_Loss_Train: 14.630
2024-06-20 23:31:48,339 - INFO: Epoch: 39/200, Batch: 12/29, Batch_Loss_Train: 14.129
2024-06-20 23:31:48,767 - INFO: Epoch: 39/200, Batch: 13/29, Batch_Loss_Train: 13.236
2024-06-20 23:31:49,086 - INFO: Epoch: 39/200, Batch: 14/29, Batch_Loss_Train: 14.015
2024-06-20 23:31:49,487 - INFO: Epoch: 39/200, Batch: 15/29, Batch_Loss_Train: 18.189
2024-06-20 23:31:49,803 - INFO: Epoch: 39/200, Batch: 16/29, Batch_Loss_Train: 22.975
2024-06-20 23:31:50,227 - INFO: Epoch: 39/200, Batch: 17/29, Batch_Loss_Train: 25.295
2024-06-20 23:31:50,543 - INFO: Epoch: 39/200, Batch: 18/29, Batch_Loss_Train: 15.491
2024-06-20 23:31:50,937 - INFO: Epoch: 39/200, Batch: 19/29, Batch_Loss_Train: 8.802
2024-06-20 23:31:51,251 - INFO: Epoch: 39/200, Batch: 20/29, Batch_Loss_Train: 8.823
2024-06-20 23:31:51,668 - INFO: Epoch: 39/200, Batch: 21/29, Batch_Loss_Train: 9.000
2024-06-20 23:31:51,985 - INFO: Epoch: 39/200, Batch: 22/29, Batch_Loss_Train: 9.625
2024-06-20 23:31:52,378 - INFO: Epoch: 39/200, Batch: 23/29, Batch_Loss_Train: 8.946
2024-06-20 23:31:52,695 - INFO: Epoch: 39/200, Batch: 24/29, Batch_Loss_Train: 8.448
2024-06-20 23:31:53,105 - INFO: Epoch: 39/200, Batch: 25/29, Batch_Loss_Train: 13.931
2024-06-20 23:31:53,419 - INFO: Epoch: 39/200, Batch: 26/29, Batch_Loss_Train: 12.067
2024-06-20 23:31:53,808 - INFO: Epoch: 39/200, Batch: 27/29, Batch_Loss_Train: 9.902
2024-06-20 23:31:54,121 - INFO: Epoch: 39/200, Batch: 28/29, Batch_Loss_Train: 11.093
2024-06-20 23:31:54,344 - INFO: Epoch: 39/200, Batch: 29/29, Batch_Loss_Train: 7.677
2024-06-20 23:32:05,211 - INFO: 39/200 final results:
2024-06-20 23:32:05,211 - INFO: Training loss: 12.960.
2024-06-20 23:32:05,211 - INFO: Training MAE: 2.838.
2024-06-20 23:32:05,211 - INFO: Training MSE: 13.064.
2024-06-20 23:32:25,533 - INFO: Epoch: 39/200, Loss_train: 12.959802068513016, Loss_val: 20.851204378851527
2024-06-20 23:32:25,533 - INFO: Best internal validation val_loss: 11.421 at epoch: 34.
2024-06-20 23:32:25,533 - INFO: Epoch 40/200...
2024-06-20 23:32:25,533 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:32:25,533 - INFO: Batch size: 32.
2024-06-20 23:32:25,536 - INFO: Dataset:
2024-06-20 23:32:25,536 - INFO: Batch size:
2024-06-20 23:32:25,536 - INFO: Number of workers:
2024-06-20 23:32:26,663 - INFO: Epoch: 40/200, Batch: 1/29, Batch_Loss_Train: 19.806
2024-06-20 23:32:26,972 - INFO: Epoch: 40/200, Batch: 2/29, Batch_Loss_Train: 25.366
2024-06-20 23:32:27,387 - INFO: Epoch: 40/200, Batch: 3/29, Batch_Loss_Train: 40.137
2024-06-20 23:32:27,708 - INFO: Epoch: 40/200, Batch: 4/29, Batch_Loss_Train: 44.476
2024-06-20 23:32:28,125 - INFO: Epoch: 40/200, Batch: 5/29, Batch_Loss_Train: 27.014
2024-06-20 23:32:28,429 - INFO: Epoch: 40/200, Batch: 6/29, Batch_Loss_Train: 15.012
2024-06-20 23:32:28,836 - INFO: Epoch: 40/200, Batch: 7/29, Batch_Loss_Train: 31.430
2024-06-20 23:32:29,153 - INFO: Epoch: 40/200, Batch: 8/29, Batch_Loss_Train: 24.584
2024-06-20 23:32:29,571 - INFO: Epoch: 40/200, Batch: 9/29, Batch_Loss_Train: 28.770
2024-06-20 23:32:29,871 - INFO: Epoch: 40/200, Batch: 10/29, Batch_Loss_Train: 27.858
2024-06-20 23:32:30,282 - INFO: Epoch: 40/200, Batch: 11/29, Batch_Loss_Train: 12.613
2024-06-20 23:32:30,600 - INFO: Epoch: 40/200, Batch: 12/29, Batch_Loss_Train: 10.328
2024-06-20 23:32:31,023 - INFO: Epoch: 40/200, Batch: 13/29, Batch_Loss_Train: 13.093
2024-06-20 23:32:31,328 - INFO: Epoch: 40/200, Batch: 14/29, Batch_Loss_Train: 9.568
2024-06-20 23:32:31,741 - INFO: Epoch: 40/200, Batch: 15/29, Batch_Loss_Train: 10.938
2024-06-20 23:32:32,056 - INFO: Epoch: 40/200, Batch: 16/29, Batch_Loss_Train: 17.538
2024-06-20 23:32:32,476 - INFO: Epoch: 40/200, Batch: 17/29, Batch_Loss_Train: 8.886
2024-06-20 23:32:32,778 - INFO: Epoch: 40/200, Batch: 18/29, Batch_Loss_Train: 9.214
2024-06-20 23:32:33,182 - INFO: Epoch: 40/200, Batch: 19/29, Batch_Loss_Train: 16.059
2024-06-20 23:32:33,495 - INFO: Epoch: 40/200, Batch: 20/29, Batch_Loss_Train: 14.140
2024-06-20 23:32:33,909 - INFO: Epoch: 40/200, Batch: 21/29, Batch_Loss_Train: 11.468
2024-06-20 23:32:34,213 - INFO: Epoch: 40/200, Batch: 22/29, Batch_Loss_Train: 14.016
2024-06-20 23:32:34,619 - INFO: Epoch: 40/200, Batch: 23/29, Batch_Loss_Train: 11.725
2024-06-20 23:32:34,936 - INFO: Epoch: 40/200, Batch: 24/29, Batch_Loss_Train: 13.737
2024-06-20 23:32:35,347 - INFO: Epoch: 40/200, Batch: 25/29, Batch_Loss_Train: 11.237
2024-06-20 23:32:35,648 - INFO: Epoch: 40/200, Batch: 26/29, Batch_Loss_Train: 13.487
2024-06-20 23:32:36,048 - INFO: Epoch: 40/200, Batch: 27/29, Batch_Loss_Train: 13.516
2024-06-20 23:32:36,362 - INFO: Epoch: 40/200, Batch: 28/29, Batch_Loss_Train: 13.054
2024-06-20 23:32:36,583 - INFO: Epoch: 40/200, Batch: 29/29, Batch_Loss_Train: 7.909
2024-06-20 23:32:47,461 - INFO: 40/200 final results:
2024-06-20 23:32:47,461 - INFO: Training loss: 17.827.
2024-06-20 23:32:47,461 - INFO: Training MAE: 3.256.
2024-06-20 23:32:47,461 - INFO: Training MSE: 18.023.
2024-06-20 23:33:07,410 - INFO: Epoch: 40/200, Loss_train: 17.826813484060352, Loss_val: 9.685338661588471
2024-06-20 23:33:07,468 - INFO: Saved new best metric model for epoch 40.
2024-06-20 23:33:07,468 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:33:07,468 - INFO: Epoch 41/200...
2024-06-20 23:33:07,468 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:33:07,469 - INFO: Batch size: 32.
2024-06-20 23:33:07,472 - INFO: Dataset:
2024-06-20 23:33:07,472 - INFO: Batch size:
2024-06-20 23:33:07,472 - INFO: Number of workers:
2024-06-20 23:33:08,629 - INFO: Epoch: 41/200, Batch: 1/29, Batch_Loss_Train: 9.272
2024-06-20 23:33:08,942 - INFO: Epoch: 41/200, Batch: 2/29, Batch_Loss_Train: 18.348
2024-06-20 23:33:09,336 - INFO: Epoch: 41/200, Batch: 3/29, Batch_Loss_Train: 23.630
2024-06-20 23:33:09,660 - INFO: Epoch: 41/200, Batch: 4/29, Batch_Loss_Train: 25.312
2024-06-20 23:33:10,091 - INFO: Epoch: 41/200, Batch: 5/29, Batch_Loss_Train: 19.667
2024-06-20 23:33:10,398 - INFO: Epoch: 41/200, Batch: 6/29, Batch_Loss_Train: 10.231
2024-06-20 23:33:10,779 - INFO: Epoch: 41/200, Batch: 7/29, Batch_Loss_Train: 12.206
2024-06-20 23:33:11,099 - INFO: Epoch: 41/200, Batch: 8/29, Batch_Loss_Train: 13.375
2024-06-20 23:33:11,534 - INFO: Epoch: 41/200, Batch: 9/29, Batch_Loss_Train: 13.914
2024-06-20 23:33:11,838 - INFO: Epoch: 41/200, Batch: 10/29, Batch_Loss_Train: 12.865
2024-06-20 23:33:12,231 - INFO: Epoch: 41/200, Batch: 11/29, Batch_Loss_Train: 11.890
2024-06-20 23:33:12,552 - INFO: Epoch: 41/200, Batch: 12/29, Batch_Loss_Train: 9.121
2024-06-20 23:33:12,996 - INFO: Epoch: 41/200, Batch: 13/29, Batch_Loss_Train: 11.281
2024-06-20 23:33:13,305 - INFO: Epoch: 41/200, Batch: 14/29, Batch_Loss_Train: 11.638
2024-06-20 23:33:13,695 - INFO: Epoch: 41/200, Batch: 15/29, Batch_Loss_Train: 8.863
2024-06-20 23:33:14,013 - INFO: Epoch: 41/200, Batch: 16/29, Batch_Loss_Train: 10.237
2024-06-20 23:33:14,448 - INFO: Epoch: 41/200, Batch: 17/29, Batch_Loss_Train: 14.005
2024-06-20 23:33:14,754 - INFO: Epoch: 41/200, Batch: 18/29, Batch_Loss_Train: 17.252
2024-06-20 23:33:15,143 - INFO: Epoch: 41/200, Batch: 19/29, Batch_Loss_Train: 22.572
2024-06-20 23:33:15,460 - INFO: Epoch: 41/200, Batch: 20/29, Batch_Loss_Train: 18.556
2024-06-20 23:33:15,892 - INFO: Epoch: 41/200, Batch: 21/29, Batch_Loss_Train: 17.280
2024-06-20 23:33:16,199 - INFO: Epoch: 41/200, Batch: 22/29, Batch_Loss_Train: 22.279
2024-06-20 23:33:16,583 - INFO: Epoch: 41/200, Batch: 23/29, Batch_Loss_Train: 12.179
2024-06-20 23:33:16,903 - INFO: Epoch: 41/200, Batch: 24/29, Batch_Loss_Train: 11.972
2024-06-20 23:33:17,320 - INFO: Epoch: 41/200, Batch: 25/29, Batch_Loss_Train: 28.475
2024-06-20 23:33:17,623 - INFO: Epoch: 41/200, Batch: 26/29, Batch_Loss_Train: 64.945
2024-06-20 23:33:17,998 - INFO: Epoch: 41/200, Batch: 27/29, Batch_Loss_Train: 81.425
2024-06-20 23:33:18,314 - INFO: Epoch: 41/200, Batch: 28/29, Batch_Loss_Train: 22.628
2024-06-20 23:33:18,526 - INFO: Epoch: 41/200, Batch: 29/29, Batch_Loss_Train: 20.776
2024-06-20 23:33:29,473 - INFO: 41/200 final results:
2024-06-20 23:33:29,473 - INFO: Training loss: 19.869.
2024-06-20 23:33:29,473 - INFO: Training MAE: 3.452.
2024-06-20 23:33:29,473 - INFO: Training MSE: 19.851.
2024-06-20 23:33:49,953 - INFO: Epoch: 41/200, Loss_train: 19.86883965853987, Loss_val: 13.9575100931628
2024-06-20 23:33:49,953 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:33:49,953 - INFO: Epoch 42/200...
2024-06-20 23:33:49,953 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:33:49,953 - INFO: Batch size: 32.
2024-06-20 23:33:49,957 - INFO: Dataset:
2024-06-20 23:33:49,957 - INFO: Batch size:
2024-06-20 23:33:49,957 - INFO: Number of workers:
2024-06-20 23:33:51,107 - INFO: Epoch: 42/200, Batch: 1/29, Batch_Loss_Train: 13.470
2024-06-20 23:33:51,417 - INFO: Epoch: 42/200, Batch: 2/29, Batch_Loss_Train: 20.255
2024-06-20 23:33:51,802 - INFO: Epoch: 42/200, Batch: 3/29, Batch_Loss_Train: 13.420
2024-06-20 23:33:52,122 - INFO: Epoch: 42/200, Batch: 4/29, Batch_Loss_Train: 12.838
2024-06-20 23:33:52,551 - INFO: Epoch: 42/200, Batch: 5/29, Batch_Loss_Train: 24.433
2024-06-20 23:33:52,855 - INFO: Epoch: 42/200, Batch: 6/29, Batch_Loss_Train: 33.322
2024-06-20 23:33:53,237 - INFO: Epoch: 42/200, Batch: 7/29, Batch_Loss_Train: 31.719
2024-06-20 23:33:53,555 - INFO: Epoch: 42/200, Batch: 8/29, Batch_Loss_Train: 39.066
2024-06-20 23:33:53,979 - INFO: Epoch: 42/200, Batch: 9/29, Batch_Loss_Train: 25.768
2024-06-20 23:33:54,280 - INFO: Epoch: 42/200, Batch: 10/29, Batch_Loss_Train: 21.846
2024-06-20 23:33:54,657 - INFO: Epoch: 42/200, Batch: 11/29, Batch_Loss_Train: 11.210
2024-06-20 23:33:54,975 - INFO: Epoch: 42/200, Batch: 12/29, Batch_Loss_Train: 12.404
2024-06-20 23:33:55,410 - INFO: Epoch: 42/200, Batch: 13/29, Batch_Loss_Train: 15.209
2024-06-20 23:33:55,716 - INFO: Epoch: 42/200, Batch: 14/29, Batch_Loss_Train: 11.038
2024-06-20 23:33:56,100 - INFO: Epoch: 42/200, Batch: 15/29, Batch_Loss_Train: 14.008
2024-06-20 23:33:56,415 - INFO: Epoch: 42/200, Batch: 16/29, Batch_Loss_Train: 16.325
2024-06-20 23:33:56,844 - INFO: Epoch: 42/200, Batch: 17/29, Batch_Loss_Train: 7.401
2024-06-20 23:33:57,147 - INFO: Epoch: 42/200, Batch: 18/29, Batch_Loss_Train: 9.362
2024-06-20 23:33:57,522 - INFO: Epoch: 42/200, Batch: 19/29, Batch_Loss_Train: 11.771
2024-06-20 23:33:57,833 - INFO: Epoch: 42/200, Batch: 20/29, Batch_Loss_Train: 10.777
2024-06-20 23:33:58,259 - INFO: Epoch: 42/200, Batch: 21/29, Batch_Loss_Train: 12.919
2024-06-20 23:33:58,563 - INFO: Epoch: 42/200, Batch: 22/29, Batch_Loss_Train: 14.759
2024-06-20 23:33:58,946 - INFO: Epoch: 42/200, Batch: 23/29, Batch_Loss_Train: 16.304
2024-06-20 23:33:59,262 - INFO: Epoch: 42/200, Batch: 24/29, Batch_Loss_Train: 10.186
2024-06-20 23:33:59,672 - INFO: Epoch: 42/200, Batch: 25/29, Batch_Loss_Train: 9.129
2024-06-20 23:33:59,972 - INFO: Epoch: 42/200, Batch: 26/29, Batch_Loss_Train: 8.440
2024-06-20 23:34:00,342 - INFO: Epoch: 42/200, Batch: 27/29, Batch_Loss_Train: 6.772
2024-06-20 23:34:00,654 - INFO: Epoch: 42/200, Batch: 28/29, Batch_Loss_Train: 6.736
2024-06-20 23:34:00,866 - INFO: Epoch: 42/200, Batch: 29/29, Batch_Loss_Train: 6.882
2024-06-20 23:34:11,766 - INFO: 42/200 final results:
2024-06-20 23:34:11,766 - INFO: Training loss: 15.440.
2024-06-20 23:34:11,766 - INFO: Training MAE: 3.071.
2024-06-20 23:34:11,766 - INFO: Training MSE: 15.610.
2024-06-20 23:34:32,086 - INFO: Epoch: 42/200, Loss_train: 15.440301237435177, Loss_val: 11.163223003518992
2024-06-20 23:34:32,086 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:34:32,086 - INFO: Epoch 43/200...
2024-06-20 23:34:32,086 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:34:32,086 - INFO: Batch size: 32.
2024-06-20 23:34:32,089 - INFO: Dataset:
2024-06-20 23:34:32,089 - INFO: Batch size:
2024-06-20 23:34:32,089 - INFO: Number of workers:
2024-06-20 23:34:33,214 - INFO: Epoch: 43/200, Batch: 1/29, Batch_Loss_Train: 10.056
2024-06-20 23:34:33,540 - INFO: Epoch: 43/200, Batch: 2/29, Batch_Loss_Train: 14.272
2024-06-20 23:34:33,956 - INFO: Epoch: 43/200, Batch: 3/29, Batch_Loss_Train: 13.812
2024-06-20 23:34:34,280 - INFO: Epoch: 43/200, Batch: 4/29, Batch_Loss_Train: 16.078
2024-06-20 23:34:34,700 - INFO: Epoch: 43/200, Batch: 5/29, Batch_Loss_Train: 13.310
2024-06-20 23:34:35,007 - INFO: Epoch: 43/200, Batch: 6/29, Batch_Loss_Train: 14.101
2024-06-20 23:34:35,416 - INFO: Epoch: 43/200, Batch: 7/29, Batch_Loss_Train: 16.372
2024-06-20 23:34:35,738 - INFO: Epoch: 43/200, Batch: 8/29, Batch_Loss_Train: 19.748
2024-06-20 23:34:36,157 - INFO: Epoch: 43/200, Batch: 9/29, Batch_Loss_Train: 19.026
2024-06-20 23:34:36,461 - INFO: Epoch: 43/200, Batch: 10/29, Batch_Loss_Train: 26.680
2024-06-20 23:34:36,873 - INFO: Epoch: 43/200, Batch: 11/29, Batch_Loss_Train: 24.063
2024-06-20 23:34:37,194 - INFO: Epoch: 43/200, Batch: 12/29, Batch_Loss_Train: 7.374
2024-06-20 23:34:37,621 - INFO: Epoch: 43/200, Batch: 13/29, Batch_Loss_Train: 11.869
2024-06-20 23:34:37,929 - INFO: Epoch: 43/200, Batch: 14/29, Batch_Loss_Train: 13.609
2024-06-20 23:34:38,345 - INFO: Epoch: 43/200, Batch: 15/29, Batch_Loss_Train: 10.594
2024-06-20 23:34:38,663 - INFO: Epoch: 43/200, Batch: 16/29, Batch_Loss_Train: 10.621
2024-06-20 23:34:39,084 - INFO: Epoch: 43/200, Batch: 17/29, Batch_Loss_Train: 10.598
2024-06-20 23:34:39,390 - INFO: Epoch: 43/200, Batch: 18/29, Batch_Loss_Train: 9.360
2024-06-20 23:34:39,795 - INFO: Epoch: 43/200, Batch: 19/29, Batch_Loss_Train: 11.707
2024-06-20 23:34:40,110 - INFO: Epoch: 43/200, Batch: 20/29, Batch_Loss_Train: 10.684
2024-06-20 23:34:40,528 - INFO: Epoch: 43/200, Batch: 21/29, Batch_Loss_Train: 8.509
2024-06-20 23:34:40,835 - INFO: Epoch: 43/200, Batch: 22/29, Batch_Loss_Train: 7.194
2024-06-20 23:34:41,244 - INFO: Epoch: 43/200, Batch: 23/29, Batch_Loss_Train: 6.852
2024-06-20 23:34:41,563 - INFO: Epoch: 43/200, Batch: 24/29, Batch_Loss_Train: 7.546
2024-06-20 23:34:41,978 - INFO: Epoch: 43/200, Batch: 25/29, Batch_Loss_Train: 8.562
2024-06-20 23:34:42,281 - INFO: Epoch: 43/200, Batch: 26/29, Batch_Loss_Train: 15.886
2024-06-20 23:34:42,685 - INFO: Epoch: 43/200, Batch: 27/29, Batch_Loss_Train: 34.938
2024-06-20 23:34:43,001 - INFO: Epoch: 43/200, Batch: 28/29, Batch_Loss_Train: 61.400
2024-06-20 23:34:43,224 - INFO: Epoch: 43/200, Batch: 29/29, Batch_Loss_Train: 32.119
2024-06-20 23:34:54,201 - INFO: 43/200 final results:
2024-06-20 23:34:54,201 - INFO: Training loss: 16.101.
2024-06-20 23:34:54,201 - INFO: Training MAE: 3.034.
2024-06-20 23:34:54,201 - INFO: Training MSE: 15.785.
2024-06-20 23:35:14,690 - INFO: Epoch: 43/200, Loss_train: 16.101466935256433, Loss_val: 12.546308073504218
2024-06-20 23:35:14,690 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:35:14,690 - INFO: Epoch 44/200...
2024-06-20 23:35:14,690 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:35:14,690 - INFO: Batch size: 32.
2024-06-20 23:35:14,693 - INFO: Dataset:
2024-06-20 23:35:14,693 - INFO: Batch size:
2024-06-20 23:35:14,693 - INFO: Number of workers:
2024-06-20 23:35:15,846 - INFO: Epoch: 44/200, Batch: 1/29, Batch_Loss_Train: 14.342
2024-06-20 23:35:16,158 - INFO: Epoch: 44/200, Batch: 2/29, Batch_Loss_Train: 19.139
2024-06-20 23:35:16,560 - INFO: Epoch: 44/200, Batch: 3/29, Batch_Loss_Train: 19.595
2024-06-20 23:35:16,885 - INFO: Epoch: 44/200, Batch: 4/29, Batch_Loss_Train: 39.499
2024-06-20 23:35:17,320 - INFO: Epoch: 44/200, Batch: 5/29, Batch_Loss_Train: 59.712
2024-06-20 23:35:17,627 - INFO: Epoch: 44/200, Batch: 6/29, Batch_Loss_Train: 117.971
2024-06-20 23:35:18,016 - INFO: Epoch: 44/200, Batch: 7/29, Batch_Loss_Train: 80.747
2024-06-20 23:35:18,337 - INFO: Epoch: 44/200, Batch: 8/29, Batch_Loss_Train: 97.230
2024-06-20 23:35:18,764 - INFO: Epoch: 44/200, Batch: 9/29, Batch_Loss_Train: 109.314
2024-06-20 23:35:19,062 - INFO: Epoch: 44/200, Batch: 10/29, Batch_Loss_Train: 107.690
2024-06-20 23:35:19,454 - INFO: Epoch: 44/200, Batch: 11/29, Batch_Loss_Train: 243.727
2024-06-20 23:35:19,772 - INFO: Epoch: 44/200, Batch: 12/29, Batch_Loss_Train: 215.655
2024-06-20 23:35:20,209 - INFO: Epoch: 44/200, Batch: 13/29, Batch_Loss_Train: 128.997
2024-06-20 23:35:20,513 - INFO: Epoch: 44/200, Batch: 14/29, Batch_Loss_Train: 33.779
2024-06-20 23:35:20,906 - INFO: Epoch: 44/200, Batch: 15/29, Batch_Loss_Train: 35.323
2024-06-20 23:35:21,221 - INFO: Epoch: 44/200, Batch: 16/29, Batch_Loss_Train: 24.501
2024-06-20 23:35:21,653 - INFO: Epoch: 44/200, Batch: 17/29, Batch_Loss_Train: 30.406
2024-06-20 23:35:21,954 - INFO: Epoch: 44/200, Batch: 18/29, Batch_Loss_Train: 29.990
2024-06-20 23:35:22,340 - INFO: Epoch: 44/200, Batch: 19/29, Batch_Loss_Train: 22.180
2024-06-20 23:35:22,652 - INFO: Epoch: 44/200, Batch: 20/29, Batch_Loss_Train: 26.783
2024-06-20 23:35:23,077 - INFO: Epoch: 44/200, Batch: 21/29, Batch_Loss_Train: 19.524
2024-06-20 23:35:23,380 - INFO: Epoch: 44/200, Batch: 22/29, Batch_Loss_Train: 20.479
2024-06-20 23:35:23,771 - INFO: Epoch: 44/200, Batch: 23/29, Batch_Loss_Train: 14.778
2024-06-20 23:35:24,087 - INFO: Epoch: 44/200, Batch: 24/29, Batch_Loss_Train: 16.907
2024-06-20 23:35:24,509 - INFO: Epoch: 44/200, Batch: 25/29, Batch_Loss_Train: 13.248
2024-06-20 23:35:24,808 - INFO: Epoch: 44/200, Batch: 26/29, Batch_Loss_Train: 13.037
2024-06-20 23:35:25,195 - INFO: Epoch: 44/200, Batch: 27/29, Batch_Loss_Train: 13.876
2024-06-20 23:35:25,507 - INFO: Epoch: 44/200, Batch: 28/29, Batch_Loss_Train: 14.936
2024-06-20 23:35:25,725 - INFO: Epoch: 44/200, Batch: 29/29, Batch_Loss_Train: 12.695
2024-06-20 23:35:36,586 - INFO: 44/200 final results:
2024-06-20 23:35:36,586 - INFO: Training loss: 55.037.
2024-06-20 23:35:36,586 - INFO: Training MAE: 5.496.
2024-06-20 23:35:36,586 - INFO: Training MSE: 55.874.
2024-06-20 23:35:56,993 - INFO: Epoch: 44/200, Loss_train: 55.036530626231226, Loss_val: 18.276927652030157
2024-06-20 23:35:56,993 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:35:56,993 - INFO: Epoch 45/200...
2024-06-20 23:35:56,993 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:35:56,993 - INFO: Batch size: 32.
2024-06-20 23:35:56,997 - INFO: Dataset:
2024-06-20 23:35:56,997 - INFO: Batch size:
2024-06-20 23:35:56,997 - INFO: Number of workers:
2024-06-20 23:35:58,106 - INFO: Epoch: 45/200, Batch: 1/29, Batch_Loss_Train: 17.902
2024-06-20 23:35:58,429 - INFO: Epoch: 45/200, Batch: 2/29, Batch_Loss_Train: 20.778
2024-06-20 23:35:58,840 - INFO: Epoch: 45/200, Batch: 3/29, Batch_Loss_Train: 16.396
2024-06-20 23:35:59,162 - INFO: Epoch: 45/200, Batch: 4/29, Batch_Loss_Train: 12.697
2024-06-20 23:35:59,561 - INFO: Epoch: 45/200, Batch: 5/29, Batch_Loss_Train: 13.263
2024-06-20 23:35:59,878 - INFO: Epoch: 45/200, Batch: 6/29, Batch_Loss_Train: 11.088
2024-06-20 23:36:00,280 - INFO: Epoch: 45/200, Batch: 7/29, Batch_Loss_Train: 12.573
2024-06-20 23:36:00,600 - INFO: Epoch: 45/200, Batch: 8/29, Batch_Loss_Train: 8.845
2024-06-20 23:36:00,987 - INFO: Epoch: 45/200, Batch: 9/29, Batch_Loss_Train: 11.914
2024-06-20 23:36:01,304 - INFO: Epoch: 45/200, Batch: 10/29, Batch_Loss_Train: 11.100
2024-06-20 23:36:01,711 - INFO: Epoch: 45/200, Batch: 11/29, Batch_Loss_Train: 9.094
2024-06-20 23:36:02,031 - INFO: Epoch: 45/200, Batch: 12/29, Batch_Loss_Train: 12.060
2024-06-20 23:36:02,450 - INFO: Epoch: 45/200, Batch: 13/29, Batch_Loss_Train: 12.908
2024-06-20 23:36:02,770 - INFO: Epoch: 45/200, Batch: 14/29, Batch_Loss_Train: 11.349
2024-06-20 23:36:03,178 - INFO: Epoch: 45/200, Batch: 15/29, Batch_Loss_Train: 14.880
2024-06-20 23:36:03,495 - INFO: Epoch: 45/200, Batch: 16/29, Batch_Loss_Train: 12.034
2024-06-20 23:36:03,907 - INFO: Epoch: 45/200, Batch: 17/29, Batch_Loss_Train: 16.177
2024-06-20 23:36:04,224 - INFO: Epoch: 45/200, Batch: 18/29, Batch_Loss_Train: 16.963
2024-06-20 23:36:04,626 - INFO: Epoch: 45/200, Batch: 19/29, Batch_Loss_Train: 14.143
2024-06-20 23:36:04,940 - INFO: Epoch: 45/200, Batch: 20/29, Batch_Loss_Train: 13.489
2024-06-20 23:36:05,349 - INFO: Epoch: 45/200, Batch: 21/29, Batch_Loss_Train: 9.416
2024-06-20 23:36:05,667 - INFO: Epoch: 45/200, Batch: 22/29, Batch_Loss_Train: 6.739
2024-06-20 23:36:06,073 - INFO: Epoch: 45/200, Batch: 23/29, Batch_Loss_Train: 7.206
2024-06-20 23:36:06,392 - INFO: Epoch: 45/200, Batch: 24/29, Batch_Loss_Train: 6.655
2024-06-20 23:36:06,793 - INFO: Epoch: 45/200, Batch: 25/29, Batch_Loss_Train: 8.499
2024-06-20 23:36:07,108 - INFO: Epoch: 45/200, Batch: 26/29, Batch_Loss_Train: 10.226
2024-06-20 23:36:07,502 - INFO: Epoch: 45/200, Batch: 27/29, Batch_Loss_Train: 15.843
2024-06-20 23:36:07,817 - INFO: Epoch: 45/200, Batch: 28/29, Batch_Loss_Train: 15.470
2024-06-20 23:36:08,037 - INFO: Epoch: 45/200, Batch: 29/29, Batch_Loss_Train: 11.612
2024-06-20 23:36:18,690 - INFO: 45/200 final results:
2024-06-20 23:36:18,690 - INFO: Training loss: 12.459.
2024-06-20 23:36:18,690 - INFO: Training MAE: 2.782.
2024-06-20 23:36:18,690 - INFO: Training MSE: 12.476.
2024-06-20 23:36:38,982 - INFO: Epoch: 45/200, Loss_train: 12.459171196510052, Loss_val: 17.49535810536352
2024-06-20 23:36:38,982 - INFO: Best internal validation val_loss: 9.685 at epoch: 40.
2024-06-20 23:36:38,982 - INFO: Epoch 46/200...
2024-06-20 23:36:38,982 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:36:38,982 - INFO: Batch size: 32.
2024-06-20 23:36:38,985 - INFO: Dataset:
2024-06-20 23:36:38,985 - INFO: Batch size:
2024-06-20 23:36:38,985 - INFO: Number of workers:
2024-06-20 23:36:40,124 - INFO: Epoch: 46/200, Batch: 1/29, Batch_Loss_Train: 16.893
2024-06-20 23:36:40,437 - INFO: Epoch: 46/200, Batch: 2/29, Batch_Loss_Train: 20.443
2024-06-20 23:36:40,841 - INFO: Epoch: 46/200, Batch: 3/29, Batch_Loss_Train: 6.152
2024-06-20 23:36:41,166 - INFO: Epoch: 46/200, Batch: 4/29, Batch_Loss_Train: 15.121
2024-06-20 23:36:41,598 - INFO: Epoch: 46/200, Batch: 5/29, Batch_Loss_Train: 18.410
2024-06-20 23:36:41,906 - INFO: Epoch: 46/200, Batch: 6/29, Batch_Loss_Train: 25.445
2024-06-20 23:36:42,298 - INFO: Epoch: 46/200, Batch: 7/29, Batch_Loss_Train: 38.905
2024-06-20 23:36:42,619 - INFO: Epoch: 46/200, Batch: 8/29, Batch_Loss_Train: 38.924
2024-06-20 23:36:43,050 - INFO: Epoch: 46/200, Batch: 9/29, Batch_Loss_Train: 22.041
2024-06-20 23:36:43,353 - INFO: Epoch: 46/200, Batch: 10/29, Batch_Loss_Train: 10.803
2024-06-20 23:36:43,744 - INFO: Epoch: 46/200, Batch: 11/29, Batch_Loss_Train: 14.907
2024-06-20 23:36:44,066 - INFO: Epoch: 46/200, Batch: 12/29, Batch_Loss_Train: 13.626
2024-06-20 23:36:44,502 - INFO: Epoch: 46/200, Batch: 13/29, Batch_Loss_Train: 11.913
2024-06-20 23:36:44,811 - INFO: Epoch: 46/200, Batch: 14/29, Batch_Loss_Train: 13.554
2024-06-20 23:36:45,212 - INFO: Epoch: 46/200, Batch: 15/29, Batch_Loss_Train: 9.584
2024-06-20 23:36:45,530 - INFO: Epoch: 46/200, Batch: 16/29, Batch_Loss_Train: 5.959
2024-06-20 23:36:45,960 - INFO: Epoch: 46/200, Batch: 17/29, Batch_Loss_Train: 9.309
2024-06-20 23:36:46,263 - INFO: Epoch: 46/200, Batch: 18/29, Batch_Loss_Train: 9.373
2024-06-20 23:36:46,654 - INFO: Epoch: 46/200, Batch: 19/29, Batch_Loss_Train: 14.098
2024-06-20 23:36:46,967 - INFO: Epoch: 46/200, Batch: 20/29, Batch_Loss_Train: 18.743
2024-06-20 23:36:47,389 - INFO: Epoch: 46/200, Batch: 21/29, Batch_Loss_Train: 15.669
2024-06-20 23:36:47,694 - INFO: Epoch: 46/200, Batch: 22/29, Batch_Loss_Train: 7.169
2024-06-20 23:36:48,074 - INFO: Epoch: 46/200, Batch: 23/29, Batch_Loss_Train: 8.598
2024-06-20 23:36:48,391 - INFO: Epoch: 46/200, Batch: 24/29, Batch_Loss_Train: 8.298
2024-06-20 23:36:48,810 - INFO: Epoch: 46/200, Batch: 25/29, Batch_Loss_Train: 7.202
2024-06-20 23:36:49,112 - INFO: Epoch: 46/200, Batch: 26/29, Batch_Loss_Train: 8.205
2024-06-20 23:36:49,488 - INFO: Epoch: 46/200, Batch: 27/29, Batch_Loss_Train: 8.948
2024-06-20 23:36:49,803 - INFO: Epoch: 46/200, Batch: 28/29, Batch_Loss_Train: 8.884
2024-06-20 23:36:50,022 - INFO: Epoch: 46/200, Batch: 29/29, Batch_Loss_Train: 4.218
2024-06-20 23:37:00,921 - INFO: 46/200 final results:
2024-06-20 23:37:00,921 - INFO: Training loss: 14.186.
2024-06-20 23:37:00,921 - INFO: Training MAE: 2.837.
2024-06-20 23:37:00,921 - INFO: Training MSE: 14.383.
2024-06-20 23:37:21,414 - INFO: Epoch: 46/200, Loss_train: 14.185986880598398, Loss_val: 8.871050925090396
2024-06-20 23:37:21,471 - INFO: Saved new best metric model for epoch 46.
2024-06-20 23:37:21,471 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:37:21,471 - INFO: Epoch 47/200...
2024-06-20 23:37:21,472 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:37:21,472 - INFO: Batch size: 32.
2024-06-20 23:37:21,475 - INFO: Dataset:
2024-06-20 23:37:21,475 - INFO: Batch size:
2024-06-20 23:37:21,475 - INFO: Number of workers:
2024-06-20 23:37:22,616 - INFO: Epoch: 47/200, Batch: 1/29, Batch_Loss_Train: 7.343
2024-06-20 23:37:22,927 - INFO: Epoch: 47/200, Batch: 2/29, Batch_Loss_Train: 10.923
2024-06-20 23:37:23,345 - INFO: Epoch: 47/200, Batch: 3/29, Batch_Loss_Train: 12.413
2024-06-20 23:37:23,667 - INFO: Epoch: 47/200, Batch: 4/29, Batch_Loss_Train: 9.687
2024-06-20 23:37:24,086 - INFO: Epoch: 47/200, Batch: 5/29, Batch_Loss_Train: 14.613
2024-06-20 23:37:24,393 - INFO: Epoch: 47/200, Batch: 6/29, Batch_Loss_Train: 28.439
2024-06-20 23:37:24,793 - INFO: Epoch: 47/200, Batch: 7/29, Batch_Loss_Train: 28.999
2024-06-20 23:37:25,115 - INFO: Epoch: 47/200, Batch: 8/29, Batch_Loss_Train: 15.074
2024-06-20 23:37:25,523 - INFO: Epoch: 47/200, Batch: 9/29, Batch_Loss_Train: 11.311
2024-06-20 23:37:25,826 - INFO: Epoch: 47/200, Batch: 10/29, Batch_Loss_Train: 9.499
2024-06-20 23:37:26,222 - INFO: Epoch: 47/200, Batch: 11/29, Batch_Loss_Train: 8.456
2024-06-20 23:37:26,544 - INFO: Epoch: 47/200, Batch: 12/29, Batch_Loss_Train: 9.611
2024-06-20 23:37:26,970 - INFO: Epoch: 47/200, Batch: 13/29, Batch_Loss_Train: 10.148
2024-06-20 23:37:27,279 - INFO: Epoch: 47/200, Batch: 14/29, Batch_Loss_Train: 7.922
2024-06-20 23:37:27,690 - INFO: Epoch: 47/200, Batch: 15/29, Batch_Loss_Train: 8.985
2024-06-20 23:37:28,008 - INFO: Epoch: 47/200, Batch: 16/29, Batch_Loss_Train: 10.926
2024-06-20 23:37:28,425 - INFO: Epoch: 47/200, Batch: 17/29, Batch_Loss_Train: 13.982
2024-06-20 23:37:28,731 - INFO: Epoch: 47/200, Batch: 18/29, Batch_Loss_Train: 14.224
2024-06-20 23:37:29,126 - INFO: Epoch: 47/200, Batch: 19/29, Batch_Loss_Train: 17.085
2024-06-20 23:37:29,442 - INFO: Epoch: 47/200, Batch: 20/29, Batch_Loss_Train: 17.038
2024-06-20 23:37:29,853 - INFO: Epoch: 47/200, Batch: 21/29, Batch_Loss_Train: 7.201
2024-06-20 23:37:30,160 - INFO: Epoch: 47/200, Batch: 22/29, Batch_Loss_Train: 7.931
2024-06-20 23:37:30,562 - INFO: Epoch: 47/200, Batch: 23/29, Batch_Loss_Train: 9.891
2024-06-20 23:37:30,881 - INFO: Epoch: 47/200, Batch: 24/29, Batch_Loss_Train: 7.703
2024-06-20 23:37:31,290 - INFO: Epoch: 47/200, Batch: 25/29, Batch_Loss_Train: 6.572
2024-06-20 23:37:31,590 - INFO: Epoch: 47/200, Batch: 26/29, Batch_Loss_Train: 9.810
2024-06-20 23:37:31,974 - INFO: Epoch: 47/200, Batch: 27/29, Batch_Loss_Train: 9.908
2024-06-20 23:37:32,287 - INFO: Epoch: 47/200, Batch: 28/29, Batch_Loss_Train: 17.974
2024-06-20 23:37:32,498 - INFO: Epoch: 47/200, Batch: 29/29, Batch_Loss_Train: 19.831
2024-06-20 23:37:43,370 - INFO: 47/200 final results:
2024-06-20 23:37:43,370 - INFO: Training loss: 12.534.
2024-06-20 23:37:43,370 - INFO: Training MAE: 2.728.
2024-06-20 23:37:43,370 - INFO: Training MSE: 12.390.
2024-06-20 23:38:03,855 - INFO: Epoch: 47/200, Loss_train: 12.53444607504483, Loss_val: 24.71023829229947
2024-06-20 23:38:03,856 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:38:03,856 - INFO: Epoch 48/200...
2024-06-20 23:38:03,856 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:38:03,856 - INFO: Batch size: 32.
2024-06-20 23:38:03,859 - INFO: Dataset:
2024-06-20 23:38:03,859 - INFO: Batch size:
2024-06-20 23:38:03,859 - INFO: Number of workers:
2024-06-20 23:38:04,990 - INFO: Epoch: 48/200, Batch: 1/29, Batch_Loss_Train: 22.045
2024-06-20 23:38:05,303 - INFO: Epoch: 48/200, Batch: 2/29, Batch_Loss_Train: 15.339
2024-06-20 23:38:05,726 - INFO: Epoch: 48/200, Batch: 3/29, Batch_Loss_Train: 12.963
2024-06-20 23:38:06,038 - INFO: Epoch: 48/200, Batch: 4/29, Batch_Loss_Train: 17.571
2024-06-20 23:38:06,459 - INFO: Epoch: 48/200, Batch: 5/29, Batch_Loss_Train: 14.765
2024-06-20 23:38:06,767 - INFO: Epoch: 48/200, Batch: 6/29, Batch_Loss_Train: 18.015
2024-06-20 23:38:07,182 - INFO: Epoch: 48/200, Batch: 7/29, Batch_Loss_Train: 13.374
2024-06-20 23:38:07,491 - INFO: Epoch: 48/200, Batch: 8/29, Batch_Loss_Train: 8.579
2024-06-20 23:38:07,909 - INFO: Epoch: 48/200, Batch: 9/29, Batch_Loss_Train: 9.108
2024-06-20 23:38:08,213 - INFO: Epoch: 48/200, Batch: 10/29, Batch_Loss_Train: 8.185
2024-06-20 23:38:08,650 - INFO: Epoch: 48/200, Batch: 11/29, Batch_Loss_Train: 10.164
2024-06-20 23:38:08,960 - INFO: Epoch: 48/200, Batch: 12/29, Batch_Loss_Train: 8.422
2024-06-20 23:38:09,389 - INFO: Epoch: 48/200, Batch: 13/29, Batch_Loss_Train: 7.520
2024-06-20 23:38:09,698 - INFO: Epoch: 48/200, Batch: 14/29, Batch_Loss_Train: 9.872
2024-06-20 23:38:10,132 - INFO: Epoch: 48/200, Batch: 15/29, Batch_Loss_Train: 13.495
2024-06-20 23:38:10,444 - INFO: Epoch: 48/200, Batch: 16/29, Batch_Loss_Train: 13.637
2024-06-20 23:38:10,848 - INFO: Epoch: 48/200, Batch: 17/29, Batch_Loss_Train: 8.167
2024-06-20 23:38:11,152 - INFO: Epoch: 48/200, Batch: 18/29, Batch_Loss_Train: 8.409
2024-06-20 23:38:11,576 - INFO: Epoch: 48/200, Batch: 19/29, Batch_Loss_Train: 8.619
2024-06-20 23:38:11,878 - INFO: Epoch: 48/200, Batch: 20/29, Batch_Loss_Train: 7.396
2024-06-20 23:38:12,286 - INFO: Epoch: 48/200, Batch: 21/29, Batch_Loss_Train: 5.431
2024-06-20 23:38:12,593 - INFO: Epoch: 48/200, Batch: 22/29, Batch_Loss_Train: 6.857
2024-06-20 23:38:13,026 - INFO: Epoch: 48/200, Batch: 23/29, Batch_Loss_Train: 8.689
2024-06-20 23:38:13,333 - INFO: Epoch: 48/200, Batch: 24/29, Batch_Loss_Train: 11.570
2024-06-20 23:38:13,736 - INFO: Epoch: 48/200, Batch: 25/29, Batch_Loss_Train: 11.998
2024-06-20 23:38:14,038 - INFO: Epoch: 48/200, Batch: 26/29, Batch_Loss_Train: 11.033
2024-06-20 23:38:14,459 - INFO: Epoch: 48/200, Batch: 27/29, Batch_Loss_Train: 14.114
2024-06-20 23:38:14,762 - INFO: Epoch: 48/200, Batch: 28/29, Batch_Loss_Train: 18.575
2024-06-20 23:38:14,971 - INFO: Epoch: 48/200, Batch: 29/29, Batch_Loss_Train: 21.241
2024-06-20 23:38:25,940 - INFO: 48/200 final results:
2024-06-20 23:38:25,940 - INFO: Training loss: 11.902.
2024-06-20 23:38:25,940 - INFO: Training MAE: 2.663.
2024-06-20 23:38:25,941 - INFO: Training MSE: 11.717.
2024-06-20 23:38:46,169 - INFO: Epoch: 48/200, Loss_train: 11.901887038658405, Loss_val: 24.771332115962586
2024-06-20 23:38:46,169 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:38:46,169 - INFO: Epoch 49/200...
2024-06-20 23:38:46,169 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:38:46,169 - INFO: Batch size: 32.
2024-06-20 23:38:46,172 - INFO: Dataset:
2024-06-20 23:38:46,172 - INFO: Batch size:
2024-06-20 23:38:46,173 - INFO: Number of workers:
2024-06-20 23:38:47,310 - INFO: Epoch: 49/200, Batch: 1/29, Batch_Loss_Train: 25.408
2024-06-20 23:38:47,623 - INFO: Epoch: 49/200, Batch: 2/29, Batch_Loss_Train: 23.600
2024-06-20 23:38:48,039 - INFO: Epoch: 49/200, Batch: 3/29, Batch_Loss_Train: 24.430
2024-06-20 23:38:48,364 - INFO: Epoch: 49/200, Batch: 4/29, Batch_Loss_Train: 26.506
2024-06-20 23:38:48,783 - INFO: Epoch: 49/200, Batch: 5/29, Batch_Loss_Train: 20.747
2024-06-20 23:38:49,089 - INFO: Epoch: 49/200, Batch: 6/29, Batch_Loss_Train: 11.505
2024-06-20 23:38:49,491 - INFO: Epoch: 49/200, Batch: 7/29, Batch_Loss_Train: 10.677
2024-06-20 23:38:49,812 - INFO: Epoch: 49/200, Batch: 8/29, Batch_Loss_Train: 12.553
2024-06-20 23:38:50,229 - INFO: Epoch: 49/200, Batch: 9/29, Batch_Loss_Train: 7.638
2024-06-20 23:38:50,532 - INFO: Epoch: 49/200, Batch: 10/29, Batch_Loss_Train: 7.996
2024-06-20 23:38:50,942 - INFO: Epoch: 49/200, Batch: 11/29, Batch_Loss_Train: 9.447
2024-06-20 23:38:51,262 - INFO: Epoch: 49/200, Batch: 12/29, Batch_Loss_Train: 6.653
2024-06-20 23:38:51,689 - INFO: Epoch: 49/200, Batch: 13/29, Batch_Loss_Train: 6.439
2024-06-20 23:38:51,997 - INFO: Epoch: 49/200, Batch: 14/29, Batch_Loss_Train: 10.069
2024-06-20 23:38:52,410 - INFO: Epoch: 49/200, Batch: 15/29, Batch_Loss_Train: 9.430
2024-06-20 23:38:52,727 - INFO: Epoch: 49/200, Batch: 16/29, Batch_Loss_Train: 11.243
2024-06-20 23:38:53,148 - INFO: Epoch: 49/200, Batch: 17/29, Batch_Loss_Train: 14.867
2024-06-20 23:38:53,452 - INFO: Epoch: 49/200, Batch: 18/29, Batch_Loss_Train: 20.827
2024-06-20 23:38:53,855 - INFO: Epoch: 49/200, Batch: 19/29, Batch_Loss_Train: 32.389
2024-06-20 23:38:54,169 - INFO: Epoch: 49/200, Batch: 20/29, Batch_Loss_Train: 26.995
2024-06-20 23:38:54,586 - INFO: Epoch: 49/200, Batch: 21/29, Batch_Loss_Train: 16.461
2024-06-20 23:38:54,893 - INFO: Epoch: 49/200, Batch: 22/29, Batch_Loss_Train: 14.123
2024-06-20 23:38:55,302 - INFO: Epoch: 49/200, Batch: 23/29, Batch_Loss_Train: 17.515
2024-06-20 23:38:55,622 - INFO: Epoch: 49/200, Batch: 24/29, Batch_Loss_Train: 11.489
2024-06-20 23:38:56,037 - INFO: Epoch: 49/200, Batch: 25/29, Batch_Loss_Train: 9.610
2024-06-20 23:38:56,340 - INFO: Epoch: 49/200, Batch: 26/29, Batch_Loss_Train: 10.725
2024-06-20 23:38:56,742 - INFO: Epoch: 49/200, Batch: 27/29, Batch_Loss_Train: 11.094
2024-06-20 23:38:57,058 - INFO: Epoch: 49/200, Batch: 28/29, Batch_Loss_Train: 7.227
2024-06-20 23:38:57,281 - INFO: Epoch: 49/200, Batch: 29/29, Batch_Loss_Train: 11.304
2024-06-20 23:39:08,254 - INFO: 49/200 final results:
2024-06-20 23:39:08,254 - INFO: Training loss: 14.792.
2024-06-20 23:39:08,254 - INFO: Training MAE: 2.995.
2024-06-20 23:39:08,254 - INFO: Training MSE: 14.861.
2024-06-20 23:39:28,484 - INFO: Epoch: 49/200, Loss_train: 14.792020140023068, Loss_val: 29.1626128163831
2024-06-20 23:39:28,484 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:39:28,484 - INFO: Epoch 50/200...
2024-06-20 23:39:28,484 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:39:28,484 - INFO: Batch size: 32.
2024-06-20 23:39:28,488 - INFO: Dataset:
2024-06-20 23:39:28,488 - INFO: Batch size:
2024-06-20 23:39:28,488 - INFO: Number of workers:
2024-06-20 23:39:29,616 - INFO: Epoch: 50/200, Batch: 1/29, Batch_Loss_Train: 18.736
2024-06-20 23:39:29,939 - INFO: Epoch: 50/200, Batch: 2/29, Batch_Loss_Train: 18.666
2024-06-20 23:39:30,336 - INFO: Epoch: 50/200, Batch: 3/29, Batch_Loss_Train: 15.359
2024-06-20 23:39:30,657 - INFO: Epoch: 50/200, Batch: 4/29, Batch_Loss_Train: 10.131
2024-06-20 23:39:31,085 - INFO: Epoch: 50/200, Batch: 5/29, Batch_Loss_Train: 8.311
2024-06-20 23:39:31,389 - INFO: Epoch: 50/200, Batch: 6/29, Batch_Loss_Train: 8.972
2024-06-20 23:39:31,766 - INFO: Epoch: 50/200, Batch: 7/29, Batch_Loss_Train: 7.607
2024-06-20 23:39:32,083 - INFO: Epoch: 50/200, Batch: 8/29, Batch_Loss_Train: 5.324
2024-06-20 23:39:32,515 - INFO: Epoch: 50/200, Batch: 9/29, Batch_Loss_Train: 8.614
2024-06-20 23:39:32,818 - INFO: Epoch: 50/200, Batch: 10/29, Batch_Loss_Train: 10.186
2024-06-20 23:39:33,202 - INFO: Epoch: 50/200, Batch: 11/29, Batch_Loss_Train: 10.111
2024-06-20 23:39:33,524 - INFO: Epoch: 50/200, Batch: 12/29, Batch_Loss_Train: 17.579
2024-06-20 23:39:33,962 - INFO: Epoch: 50/200, Batch: 13/29, Batch_Loss_Train: 26.903
2024-06-20 23:39:34,271 - INFO: Epoch: 50/200, Batch: 14/29, Batch_Loss_Train: 15.647
2024-06-20 23:39:34,665 - INFO: Epoch: 50/200, Batch: 15/29, Batch_Loss_Train: 8.032
2024-06-20 23:39:34,983 - INFO: Epoch: 50/200, Batch: 16/29, Batch_Loss_Train: 18.012
2024-06-20 23:39:35,412 - INFO: Epoch: 50/200, Batch: 17/29, Batch_Loss_Train: 29.325
2024-06-20 23:39:35,717 - INFO: Epoch: 50/200, Batch: 18/29, Batch_Loss_Train: 31.351
2024-06-20 23:39:36,097 - INFO: Epoch: 50/200, Batch: 19/29, Batch_Loss_Train: 14.757
2024-06-20 23:39:36,412 - INFO: Epoch: 50/200, Batch: 20/29, Batch_Loss_Train: 9.623
2024-06-20 23:39:36,839 - INFO: Epoch: 50/200, Batch: 21/29, Batch_Loss_Train: 10.622
2024-06-20 23:39:37,146 - INFO: Epoch: 50/200, Batch: 22/29, Batch_Loss_Train: 6.492
2024-06-20 23:39:37,528 - INFO: Epoch: 50/200, Batch: 23/29, Batch_Loss_Train: 10.239
2024-06-20 23:39:37,847 - INFO: Epoch: 50/200, Batch: 24/29, Batch_Loss_Train: 12.417
2024-06-20 23:39:38,262 - INFO: Epoch: 50/200, Batch: 25/29, Batch_Loss_Train: 15.102
2024-06-20 23:39:38,565 - INFO: Epoch: 50/200, Batch: 26/29, Batch_Loss_Train: 14.398
2024-06-20 23:39:38,941 - INFO: Epoch: 50/200, Batch: 27/29, Batch_Loss_Train: 12.634
2024-06-20 23:39:39,256 - INFO: Epoch: 50/200, Batch: 28/29, Batch_Loss_Train: 6.603
2024-06-20 23:39:39,468 - INFO: Epoch: 50/200, Batch: 29/29, Batch_Loss_Train: 7.212
2024-06-20 23:39:50,575 - INFO: 50/200 final results:
2024-06-20 23:39:50,575 - INFO: Training loss: 13.413.
2024-06-20 23:39:50,575 - INFO: Training MAE: 2.835.
2024-06-20 23:39:50,575 - INFO: Training MSE: 13.535.
2024-06-20 23:40:10,931 - INFO: Epoch: 50/200, Loss_train: 13.412553063754377, Loss_val: 11.72341325365264
2024-06-20 23:40:10,931 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:40:10,931 - INFO: Epoch 51/200...
2024-06-20 23:40:10,931 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:40:10,931 - INFO: Batch size: 32.
2024-06-20 23:40:10,934 - INFO: Dataset:
2024-06-20 23:40:10,934 - INFO: Batch size:
2024-06-20 23:40:10,934 - INFO: Number of workers:
2024-06-20 23:40:12,070 - INFO: Epoch: 51/200, Batch: 1/29, Batch_Loss_Train: 11.286
2024-06-20 23:40:12,383 - INFO: Epoch: 51/200, Batch: 2/29, Batch_Loss_Train: 7.198
2024-06-20 23:40:12,783 - INFO: Epoch: 51/200, Batch: 3/29, Batch_Loss_Train: 7.148
2024-06-20 23:40:13,108 - INFO: Epoch: 51/200, Batch: 4/29, Batch_Loss_Train: 11.229
2024-06-20 23:40:13,556 - INFO: Epoch: 51/200, Batch: 5/29, Batch_Loss_Train: 25.847
2024-06-20 23:40:13,861 - INFO: Epoch: 51/200, Batch: 6/29, Batch_Loss_Train: 26.802
2024-06-20 23:40:14,249 - INFO: Epoch: 51/200, Batch: 7/29, Batch_Loss_Train: 23.632
2024-06-20 23:40:14,556 - INFO: Epoch: 51/200, Batch: 8/29, Batch_Loss_Train: 23.397
2024-06-20 23:40:15,008 - INFO: Epoch: 51/200, Batch: 9/29, Batch_Loss_Train: 13.465
2024-06-20 23:40:15,311 - INFO: Epoch: 51/200, Batch: 10/29, Batch_Loss_Train: 10.172
2024-06-20 23:40:15,696 - INFO: Epoch: 51/200, Batch: 11/29, Batch_Loss_Train: 8.604
2024-06-20 23:40:16,004 - INFO: Epoch: 51/200, Batch: 12/29, Batch_Loss_Train: 7.337
2024-06-20 23:40:16,443 - INFO: Epoch: 51/200, Batch: 13/29, Batch_Loss_Train: 5.961
2024-06-20 23:40:16,752 - INFO: Epoch: 51/200, Batch: 14/29, Batch_Loss_Train: 5.771
2024-06-20 23:40:17,145 - INFO: Epoch: 51/200, Batch: 15/29, Batch_Loss_Train: 6.073
2024-06-20 23:40:17,450 - INFO: Epoch: 51/200, Batch: 16/29, Batch_Loss_Train: 8.352
2024-06-20 23:40:17,904 - INFO: Epoch: 51/200, Batch: 17/29, Batch_Loss_Train: 7.385
2024-06-20 23:40:18,210 - INFO: Epoch: 51/200, Batch: 18/29, Batch_Loss_Train: 7.633
2024-06-20 23:40:18,603 - INFO: Epoch: 51/200, Batch: 19/29, Batch_Loss_Train: 6.500
2024-06-20 23:40:18,906 - INFO: Epoch: 51/200, Batch: 20/29, Batch_Loss_Train: 7.005
2024-06-20 23:40:19,351 - INFO: Epoch: 51/200, Batch: 21/29, Batch_Loss_Train: 8.827
2024-06-20 23:40:19,657 - INFO: Epoch: 51/200, Batch: 22/29, Batch_Loss_Train: 13.995
2024-06-20 23:40:20,047 - INFO: Epoch: 51/200, Batch: 23/29, Batch_Loss_Train: 17.716
2024-06-20 23:40:20,353 - INFO: Epoch: 51/200, Batch: 24/29, Batch_Loss_Train: 21.161
2024-06-20 23:40:20,789 - INFO: Epoch: 51/200, Batch: 25/29, Batch_Loss_Train: 20.612
2024-06-20 23:40:21,090 - INFO: Epoch: 51/200, Batch: 26/29, Batch_Loss_Train: 15.272
2024-06-20 23:40:21,474 - INFO: Epoch: 51/200, Batch: 27/29, Batch_Loss_Train: 6.308
2024-06-20 23:40:21,776 - INFO: Epoch: 51/200, Batch: 28/29, Batch_Loss_Train: 5.904
2024-06-20 23:40:21,992 - INFO: Epoch: 51/200, Batch: 29/29, Batch_Loss_Train: 7.856
2024-06-20 23:40:32,876 - INFO: 51/200 final results:
2024-06-20 23:40:32,876 - INFO: Training loss: 12.015.
2024-06-20 23:40:32,876 - INFO: Training MAE: 2.632.
2024-06-20 23:40:32,876 - INFO: Training MSE: 12.098.
2024-06-20 23:40:53,204 - INFO: Epoch: 51/200, Loss_train: 12.015449112859265, Loss_val: 10.95815060056489
2024-06-20 23:40:53,204 - INFO: Best internal validation val_loss: 8.871 at epoch: 46.
2024-06-20 23:40:53,204 - INFO: Epoch 52/200...
2024-06-20 23:40:53,204 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:40:53,204 - INFO: Batch size: 32.
2024-06-20 23:40:53,207 - INFO: Dataset:
2024-06-20 23:40:53,207 - INFO: Batch size:
2024-06-20 23:40:53,207 - INFO: Number of workers:
2024-06-20 23:40:54,323 - INFO: Epoch: 52/200, Batch: 1/29, Batch_Loss_Train: 10.859
2024-06-20 23:40:54,649 - INFO: Epoch: 52/200, Batch: 2/29, Batch_Loss_Train: 6.244
2024-06-20 23:40:55,065 - INFO: Epoch: 52/200, Batch: 3/29, Batch_Loss_Train: 10.514
2024-06-20 23:40:55,388 - INFO: Epoch: 52/200, Batch: 4/29, Batch_Loss_Train: 18.907
2024-06-20 23:40:55,793 - INFO: Epoch: 52/200, Batch: 5/29, Batch_Loss_Train: 23.361
2024-06-20 23:40:56,111 - INFO: Epoch: 52/200, Batch: 6/29, Batch_Loss_Train: 25.260
2024-06-20 23:40:56,517 - INFO: Epoch: 52/200, Batch: 7/29, Batch_Loss_Train: 40.621
2024-06-20 23:40:56,838 - INFO: Epoch: 52/200, Batch: 8/29, Batch_Loss_Train: 45.387
2024-06-20 23:40:57,236 - INFO: Epoch: 52/200, Batch: 9/29, Batch_Loss_Train: 20.065
2024-06-20 23:40:57,553 - INFO: Epoch: 52/200, Batch: 10/29, Batch_Loss_Train: 12.503
2024-06-20 23:40:57,956 - INFO: Epoch: 52/200, Batch: 11/29, Batch_Loss_Train: 11.457
2024-06-20 23:40:58,277 - INFO: Epoch: 52/200, Batch: 12/29, Batch_Loss_Train: 7.917
2024-06-20 23:40:58,690 - INFO: Epoch: 52/200, Batch: 13/29, Batch_Loss_Train: 12.940
2024-06-20 23:40:59,012 - INFO: Epoch: 52/200, Batch: 14/29, Batch_Loss_Train: 9.028
2024-06-20 23:40:59,421 - INFO: Epoch: 52/200, Batch: 15/29, Batch_Loss_Train: 10.002
2024-06-20 23:40:59,739 - INFO: Epoch: 52/200, Batch: 16/29, Batch_Loss_Train: 8.401
2024-06-20 23:41:00,146 - INFO: Epoch: 52/200, Batch: 17/29, Batch_Loss_Train: 7.621
2024-06-20 23:41:00,463 - INFO: Epoch: 52/200, Batch: 18/29, Batch_Loss_Train: 5.725
2024-06-20 23:41:00,866 - INFO: Epoch: 52/200, Batch: 19/29, Batch_Loss_Train: 8.093
2024-06-20 23:41:01,181 - INFO: Epoch: 52/200, Batch: 20/29, Batch_Loss_Train: 7.679
2024-06-20 23:41:01,584 - INFO: Epoch: 52/200, Batch: 21/29, Batch_Loss_Train: 8.320
2024-06-20 23:41:01,904 - INFO: Epoch: 52/200, Batch: 22/29, Batch_Loss_Train: 6.795
2024-06-20 23:41:02,300 - INFO: Epoch: 52/200, Batch: 23/29, Batch_Loss_Train: 6.053
2024-06-20 23:41:02,619 - INFO: Epoch: 52/200, Batch: 24/29, Batch_Loss_Train: 5.945
2024-06-20 23:41:03,009 - INFO: Epoch: 52/200, Batch: 25/29, Batch_Loss_Train: 6.206
2024-06-20 23:41:03,325 - INFO: Epoch: 52/200, Batch: 26/29, Batch_Loss_Train: 6.655
2024-06-20 23:41:03,714 - INFO: Epoch: 52/200, Batch: 27/29, Batch_Loss_Train: 7.507
2024-06-20 23:41:04,029 - INFO: Epoch: 52/200, Batch: 28/29, Batch_Loss_Train: 6.419
2024-06-20 23:41:04,242 - INFO: Epoch: 52/200, Batch: 29/29, Batch_Loss_Train: 5.439
2024-06-20 23:41:14,714 - INFO: 52/200 final results:
2024-06-20 23:41:14,714 - INFO: Training loss: 12.480.
2024-06-20 23:41:14,715 - INFO: Training MAE: 2.623.
2024-06-20 23:41:14,715 - INFO: Training MSE: 12.619.
2024-06-20 23:41:35,106 - INFO: Epoch: 52/200, Loss_train: 12.480043542796167, Loss_val: 8.29516052377635
2024-06-20 23:41:35,161 - INFO: Saved new best metric model for epoch 52.
2024-06-20 23:41:35,161 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:41:35,161 - INFO: Epoch 53/200...
2024-06-20 23:41:35,161 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:41:35,161 - INFO: Batch size: 32.
2024-06-20 23:41:35,164 - INFO: Dataset:
2024-06-20 23:41:35,165 - INFO: Batch size:
2024-06-20 23:41:35,165 - INFO: Number of workers:
2024-06-20 23:41:36,280 - INFO: Epoch: 53/200, Batch: 1/29, Batch_Loss_Train: 7.467
2024-06-20 23:41:36,604 - INFO: Epoch: 53/200, Batch: 2/29, Batch_Loss_Train: 6.768
2024-06-20 23:41:37,009 - INFO: Epoch: 53/200, Batch: 3/29, Batch_Loss_Train: 7.562
2024-06-20 23:41:37,332 - INFO: Epoch: 53/200, Batch: 4/29, Batch_Loss_Train: 7.378
2024-06-20 23:41:37,726 - INFO: Epoch: 53/200, Batch: 5/29, Batch_Loss_Train: 8.880
2024-06-20 23:41:38,043 - INFO: Epoch: 53/200, Batch: 6/29, Batch_Loss_Train: 13.018
2024-06-20 23:41:38,445 - INFO: Epoch: 53/200, Batch: 7/29, Batch_Loss_Train: 9.475
2024-06-20 23:41:38,763 - INFO: Epoch: 53/200, Batch: 8/29, Batch_Loss_Train: 10.970
2024-06-20 23:41:39,148 - INFO: Epoch: 53/200, Batch: 9/29, Batch_Loss_Train: 20.461
2024-06-20 23:41:39,461 - INFO: Epoch: 53/200, Batch: 10/29, Batch_Loss_Train: 43.769
2024-06-20 23:41:39,867 - INFO: Epoch: 53/200, Batch: 11/29, Batch_Loss_Train: 47.393
2024-06-20 23:41:40,188 - INFO: Epoch: 53/200, Batch: 12/29, Batch_Loss_Train: 22.832
2024-06-20 23:41:40,606 - INFO: Epoch: 53/200, Batch: 13/29, Batch_Loss_Train: 18.196
2024-06-20 23:41:40,928 - INFO: Epoch: 53/200, Batch: 14/29, Batch_Loss_Train: 41.999
2024-06-20 23:41:41,343 - INFO: Epoch: 53/200, Batch: 15/29, Batch_Loss_Train: 49.829
2024-06-20 23:41:41,662 - INFO: Epoch: 53/200, Batch: 16/29, Batch_Loss_Train: 32.890
2024-06-20 23:41:42,068 - INFO: Epoch: 53/200, Batch: 17/29, Batch_Loss_Train: 22.081
2024-06-20 23:41:42,386 - INFO: Epoch: 53/200, Batch: 18/29, Batch_Loss_Train: 19.241
2024-06-20 23:41:42,791 - INFO: Epoch: 53/200, Batch: 19/29, Batch_Loss_Train: 12.282
2024-06-20 23:41:43,108 - INFO: Epoch: 53/200, Batch: 20/29, Batch_Loss_Train: 11.508
2024-06-20 23:41:43,507 - INFO: Epoch: 53/200, Batch: 21/29, Batch_Loss_Train: 8.515
2024-06-20 23:41:43,828 - INFO: Epoch: 53/200, Batch: 22/29, Batch_Loss_Train: 9.460
2024-06-20 23:41:44,226 - INFO: Epoch: 53/200, Batch: 23/29, Batch_Loss_Train: 11.583
2024-06-20 23:41:44,546 - INFO: Epoch: 53/200, Batch: 24/29, Batch_Loss_Train: 6.062
2024-06-20 23:41:44,944 - INFO: Epoch: 53/200, Batch: 25/29, Batch_Loss_Train: 8.522
2024-06-20 23:41:45,260 - INFO: Epoch: 53/200, Batch: 26/29, Batch_Loss_Train: 6.850
2024-06-20 23:41:45,655 - INFO: Epoch: 53/200, Batch: 27/29, Batch_Loss_Train: 9.017
2024-06-20 23:41:45,971 - INFO: Epoch: 53/200, Batch: 28/29, Batch_Loss_Train: 10.241
2024-06-20 23:41:46,192 - INFO: Epoch: 53/200, Batch: 29/29, Batch_Loss_Train: 7.162
2024-06-20 23:41:57,149 - INFO: 53/200 final results:
2024-06-20 23:41:57,149 - INFO: Training loss: 16.945.
2024-06-20 23:41:57,149 - INFO: Training MAE: 3.086.
2024-06-20 23:41:57,149 - INFO: Training MSE: 17.139.
2024-06-20 23:42:17,280 - INFO: Epoch: 53/200, Loss_train: 16.94515790610478, Loss_val: 8.587418901509253
2024-06-20 23:42:17,281 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:42:17,281 - INFO: Epoch 54/200...
2024-06-20 23:42:17,281 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:42:17,281 - INFO: Batch size: 32.
2024-06-20 23:42:17,284 - INFO: Dataset:
2024-06-20 23:42:17,284 - INFO: Batch size:
2024-06-20 23:42:17,284 - INFO: Number of workers:
2024-06-20 23:42:18,401 - INFO: Epoch: 54/200, Batch: 1/29, Batch_Loss_Train: 8.919
2024-06-20 23:42:18,725 - INFO: Epoch: 54/200, Batch: 2/29, Batch_Loss_Train: 10.391
2024-06-20 23:42:19,147 - INFO: Epoch: 54/200, Batch: 3/29, Batch_Loss_Train: 6.388
2024-06-20 23:42:19,456 - INFO: Epoch: 54/200, Batch: 4/29, Batch_Loss_Train: 6.253
2024-06-20 23:42:19,859 - INFO: Epoch: 54/200, Batch: 5/29, Batch_Loss_Train: 9.557
2024-06-20 23:42:20,181 - INFO: Epoch: 54/200, Batch: 6/29, Batch_Loss_Train: 11.600
2024-06-20 23:42:20,602 - INFO: Epoch: 54/200, Batch: 7/29, Batch_Loss_Train: 12.274
2024-06-20 23:42:20,912 - INFO: Epoch: 54/200, Batch: 8/29, Batch_Loss_Train: 9.623
2024-06-20 23:42:21,316 - INFO: Epoch: 54/200, Batch: 9/29, Batch_Loss_Train: 8.432
2024-06-20 23:42:21,633 - INFO: Epoch: 54/200, Batch: 10/29, Batch_Loss_Train: 12.696
2024-06-20 23:42:22,059 - INFO: Epoch: 54/200, Batch: 11/29, Batch_Loss_Train: 9.000
2024-06-20 23:42:22,370 - INFO: Epoch: 54/200, Batch: 12/29, Batch_Loss_Train: 7.202
2024-06-20 23:42:22,785 - INFO: Epoch: 54/200, Batch: 13/29, Batch_Loss_Train: 7.864
2024-06-20 23:42:23,108 - INFO: Epoch: 54/200, Batch: 14/29, Batch_Loss_Train: 7.222
2024-06-20 23:42:23,547 - INFO: Epoch: 54/200, Batch: 15/29, Batch_Loss_Train: 8.310
2024-06-20 23:42:23,853 - INFO: Epoch: 54/200, Batch: 16/29, Batch_Loss_Train: 6.080
2024-06-20 23:42:24,251 - INFO: Epoch: 54/200, Batch: 17/29, Batch_Loss_Train: 6.077
2024-06-20 23:42:24,570 - INFO: Epoch: 54/200, Batch: 18/29, Batch_Loss_Train: 6.442
2024-06-20 23:42:25,000 - INFO: Epoch: 54/200, Batch: 19/29, Batch_Loss_Train: 6.724
2024-06-20 23:42:25,303 - INFO: Epoch: 54/200, Batch: 20/29, Batch_Loss_Train: 5.701
2024-06-20 23:42:25,698 - INFO: Epoch: 54/200, Batch: 21/29, Batch_Loss_Train: 6.921
2024-06-20 23:42:26,019 - INFO: Epoch: 54/200, Batch: 22/29, Batch_Loss_Train: 9.578
2024-06-20 23:42:26,438 - INFO: Epoch: 54/200, Batch: 23/29, Batch_Loss_Train: 9.546
2024-06-20 23:42:26,745 - INFO: Epoch: 54/200, Batch: 24/29, Batch_Loss_Train: 11.432
2024-06-20 23:42:27,125 - INFO: Epoch: 54/200, Batch: 25/29, Batch_Loss_Train: 15.755
2024-06-20 23:42:27,440 - INFO: Epoch: 54/200, Batch: 26/29, Batch_Loss_Train: 14.705
2024-06-20 23:42:27,845 - INFO: Epoch: 54/200, Batch: 27/29, Batch_Loss_Train: 10.078
2024-06-20 23:42:28,147 - INFO: Epoch: 54/200, Batch: 28/29, Batch_Loss_Train: 11.747
2024-06-20 23:42:28,352 - INFO: Epoch: 54/200, Batch: 29/29, Batch_Loss_Train: 21.429
2024-06-20 23:42:39,278 - INFO: 54/200 final results:
2024-06-20 23:42:39,278 - INFO: Training loss: 9.584.
2024-06-20 23:42:39,278 - INFO: Training MAE: 2.374.
2024-06-20 23:42:39,278 - INFO: Training MSE: 9.350.
2024-06-20 23:42:59,829 - INFO: Epoch: 54/200, Loss_train: 9.584306108540503, Loss_val: 25.11360010607489
2024-06-20 23:42:59,829 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:42:59,829 - INFO: Epoch 55/200...
2024-06-20 23:42:59,829 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:42:59,829 - INFO: Batch size: 32.
2024-06-20 23:42:59,832 - INFO: Dataset:
2024-06-20 23:42:59,833 - INFO: Batch size:
2024-06-20 23:42:59,833 - INFO: Number of workers:
2024-06-20 23:43:00,960 - INFO: Epoch: 55/200, Batch: 1/29, Batch_Loss_Train: 26.289
2024-06-20 23:43:01,270 - INFO: Epoch: 55/200, Batch: 2/29, Batch_Loss_Train: 24.019
2024-06-20 23:43:01,671 - INFO: Epoch: 55/200, Batch: 3/29, Batch_Loss_Train: 33.688
2024-06-20 23:43:01,992 - INFO: Epoch: 55/200, Batch: 4/29, Batch_Loss_Train: 43.233
2024-06-20 23:43:02,421 - INFO: Epoch: 55/200, Batch: 5/29, Batch_Loss_Train: 28.113
2024-06-20 23:43:02,725 - INFO: Epoch: 55/200, Batch: 6/29, Batch_Loss_Train: 19.139
2024-06-20 23:43:03,116 - INFO: Epoch: 55/200, Batch: 7/29, Batch_Loss_Train: 23.476
2024-06-20 23:43:03,433 - INFO: Epoch: 55/200, Batch: 8/29, Batch_Loss_Train: 10.362
2024-06-20 23:43:03,870 - INFO: Epoch: 55/200, Batch: 9/29, Batch_Loss_Train: 11.814
2024-06-20 23:43:04,170 - INFO: Epoch: 55/200, Batch: 10/29, Batch_Loss_Train: 12.012
2024-06-20 23:43:04,558 - INFO: Epoch: 55/200, Batch: 11/29, Batch_Loss_Train: 12.808
2024-06-20 23:43:04,877 - INFO: Epoch: 55/200, Batch: 12/29, Batch_Loss_Train: 12.928
2024-06-20 23:43:05,313 - INFO: Epoch: 55/200, Batch: 13/29, Batch_Loss_Train: 6.640
2024-06-20 23:43:05,619 - INFO: Epoch: 55/200, Batch: 14/29, Batch_Loss_Train: 8.791
2024-06-20 23:43:06,016 - INFO: Epoch: 55/200, Batch: 15/29, Batch_Loss_Train: 12.845
2024-06-20 23:43:06,330 - INFO: Epoch: 55/200, Batch: 16/29, Batch_Loss_Train: 9.889
2024-06-20 23:43:06,759 - INFO: Epoch: 55/200, Batch: 17/29, Batch_Loss_Train: 13.237
2024-06-20 23:43:07,061 - INFO: Epoch: 55/200, Batch: 18/29, Batch_Loss_Train: 12.867
2024-06-20 23:43:07,448 - INFO: Epoch: 55/200, Batch: 19/29, Batch_Loss_Train: 10.769
2024-06-20 23:43:07,764 - INFO: Epoch: 55/200, Batch: 20/29, Batch_Loss_Train: 7.503
2024-06-20 23:43:08,195 - INFO: Epoch: 55/200, Batch: 21/29, Batch_Loss_Train: 3.221
2024-06-20 23:43:08,503 - INFO: Epoch: 55/200, Batch: 22/29, Batch_Loss_Train: 5.244
2024-06-20 23:43:08,892 - INFO: Epoch: 55/200, Batch: 23/29, Batch_Loss_Train: 6.201
2024-06-20 23:43:09,213 - INFO: Epoch: 55/200, Batch: 24/29, Batch_Loss_Train: 7.150
2024-06-20 23:43:09,628 - INFO: Epoch: 55/200, Batch: 25/29, Batch_Loss_Train: 8.391
2024-06-20 23:43:09,931 - INFO: Epoch: 55/200, Batch: 26/29, Batch_Loss_Train: 9.914
2024-06-20 23:43:10,307 - INFO: Epoch: 55/200, Batch: 27/29, Batch_Loss_Train: 10.694
2024-06-20 23:43:10,623 - INFO: Epoch: 55/200, Batch: 28/29, Batch_Loss_Train: 11.342
2024-06-20 23:43:10,836 - INFO: Epoch: 55/200, Batch: 29/29, Batch_Loss_Train: 19.701
2024-06-20 23:43:21,769 - INFO: 55/200 final results:
2024-06-20 23:43:21,769 - INFO: Training loss: 14.561.
2024-06-20 23:43:21,769 - INFO: Training MAE: 2.859.
2024-06-20 23:43:21,769 - INFO: Training MSE: 14.460.
2024-06-20 23:43:41,692 - INFO: Epoch: 55/200, Loss_train: 14.56138693053147, Loss_val: 44.228185390603954
2024-06-20 23:43:41,692 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:43:41,692 - INFO: Epoch 56/200...
2024-06-20 23:43:41,692 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:43:41,693 - INFO: Batch size: 32.
2024-06-20 23:43:41,696 - INFO: Dataset:
2024-06-20 23:43:41,696 - INFO: Batch size:
2024-06-20 23:43:41,696 - INFO: Number of workers:
2024-06-20 23:43:42,816 - INFO: Epoch: 56/200, Batch: 1/29, Batch_Loss_Train: 36.906
2024-06-20 23:43:43,127 - INFO: Epoch: 56/200, Batch: 2/29, Batch_Loss_Train: 26.513
2024-06-20 23:43:43,528 - INFO: Epoch: 56/200, Batch: 3/29, Batch_Loss_Train: 18.114
2024-06-20 23:43:43,850 - INFO: Epoch: 56/200, Batch: 4/29, Batch_Loss_Train: 21.256
2024-06-20 23:43:44,286 - INFO: Epoch: 56/200, Batch: 5/29, Batch_Loss_Train: 14.992
2024-06-20 23:43:44,594 - INFO: Epoch: 56/200, Batch: 6/29, Batch_Loss_Train: 11.085
2024-06-20 23:43:44,987 - INFO: Epoch: 56/200, Batch: 7/29, Batch_Loss_Train: 7.676
2024-06-20 23:43:45,308 - INFO: Epoch: 56/200, Batch: 8/29, Batch_Loss_Train: 7.065
2024-06-20 23:43:45,745 - INFO: Epoch: 56/200, Batch: 9/29, Batch_Loss_Train: 6.557
2024-06-20 23:43:46,048 - INFO: Epoch: 56/200, Batch: 10/29, Batch_Loss_Train: 6.417
2024-06-20 23:43:46,451 - INFO: Epoch: 56/200, Batch: 11/29, Batch_Loss_Train: 6.814
2024-06-20 23:43:46,775 - INFO: Epoch: 56/200, Batch: 12/29, Batch_Loss_Train: 6.175
2024-06-20 23:43:47,204 - INFO: Epoch: 56/200, Batch: 13/29, Batch_Loss_Train: 10.292
2024-06-20 23:43:47,513 - INFO: Epoch: 56/200, Batch: 14/29, Batch_Loss_Train: 8.030
2024-06-20 23:43:47,916 - INFO: Epoch: 56/200, Batch: 15/29, Batch_Loss_Train: 9.464
2024-06-20 23:43:48,234 - INFO: Epoch: 56/200, Batch: 16/29, Batch_Loss_Train: 6.779
2024-06-20 23:43:48,671 - INFO: Epoch: 56/200, Batch: 17/29, Batch_Loss_Train: 7.072
2024-06-20 23:43:48,976 - INFO: Epoch: 56/200, Batch: 18/29, Batch_Loss_Train: 7.334
2024-06-20 23:43:49,372 - INFO: Epoch: 56/200, Batch: 19/29, Batch_Loss_Train: 8.877
2024-06-20 23:43:49,688 - INFO: Epoch: 56/200, Batch: 20/29, Batch_Loss_Train: 9.794
2024-06-20 23:43:50,122 - INFO: Epoch: 56/200, Batch: 21/29, Batch_Loss_Train: 9.555
2024-06-20 23:43:50,430 - INFO: Epoch: 56/200, Batch: 22/29, Batch_Loss_Train: 7.703
2024-06-20 23:43:50,829 - INFO: Epoch: 56/200, Batch: 23/29, Batch_Loss_Train: 7.429
2024-06-20 23:43:51,149 - INFO: Epoch: 56/200, Batch: 24/29, Batch_Loss_Train: 3.578
2024-06-20 23:43:51,578 - INFO: Epoch: 56/200, Batch: 25/29, Batch_Loss_Train: 5.933
2024-06-20 23:43:51,881 - INFO: Epoch: 56/200, Batch: 26/29, Batch_Loss_Train: 6.605
2024-06-20 23:43:52,270 - INFO: Epoch: 56/200, Batch: 27/29, Batch_Loss_Train: 6.756
2024-06-20 23:43:52,586 - INFO: Epoch: 56/200, Batch: 28/29, Batch_Loss_Train: 7.787
2024-06-20 23:43:52,809 - INFO: Epoch: 56/200, Batch: 29/29, Batch_Loss_Train: 12.246
2024-06-20 23:44:03,936 - INFO: 56/200 final results:
2024-06-20 23:44:03,936 - INFO: Training loss: 10.511.
2024-06-20 23:44:03,936 - INFO: Training MAE: 2.450.
2024-06-20 23:44:03,936 - INFO: Training MSE: 10.476.
2024-06-20 23:44:24,326 - INFO: Epoch: 56/200, Loss_train: 10.510510033574597, Loss_val: 20.745111564110065
2024-06-20 23:44:24,327 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:44:24,327 - INFO: Epoch 57/200...
2024-06-20 23:44:24,327 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:44:24,327 - INFO: Batch size: 32.
2024-06-20 23:44:24,330 - INFO: Dataset:
2024-06-20 23:44:24,330 - INFO: Batch size:
2024-06-20 23:44:24,330 - INFO: Number of workers:
2024-06-20 23:44:25,451 - INFO: Epoch: 57/200, Batch: 1/29, Batch_Loss_Train: 21.079
2024-06-20 23:44:25,762 - INFO: Epoch: 57/200, Batch: 2/29, Batch_Loss_Train: 24.297
2024-06-20 23:44:26,164 - INFO: Epoch: 57/200, Batch: 3/29, Batch_Loss_Train: 13.257
2024-06-20 23:44:26,487 - INFO: Epoch: 57/200, Batch: 4/29, Batch_Loss_Train: 7.747
2024-06-20 23:44:26,931 - INFO: Epoch: 57/200, Batch: 5/29, Batch_Loss_Train: 7.269
2024-06-20 23:44:27,236 - INFO: Epoch: 57/200, Batch: 6/29, Batch_Loss_Train: 8.793
2024-06-20 23:44:27,630 - INFO: Epoch: 57/200, Batch: 7/29, Batch_Loss_Train: 6.964
2024-06-20 23:44:27,937 - INFO: Epoch: 57/200, Batch: 8/29, Batch_Loss_Train: 5.995
2024-06-20 23:44:28,378 - INFO: Epoch: 57/200, Batch: 9/29, Batch_Loss_Train: 6.873
2024-06-20 23:44:28,680 - INFO: Epoch: 57/200, Batch: 10/29, Batch_Loss_Train: 9.058
2024-06-20 23:44:29,078 - INFO: Epoch: 57/200, Batch: 11/29, Batch_Loss_Train: 8.095
2024-06-20 23:44:29,386 - INFO: Epoch: 57/200, Batch: 12/29, Batch_Loss_Train: 19.210
2024-06-20 23:44:29,839 - INFO: Epoch: 57/200, Batch: 13/29, Batch_Loss_Train: 26.033
2024-06-20 23:44:30,146 - INFO: Epoch: 57/200, Batch: 14/29, Batch_Loss_Train: 27.749
2024-06-20 23:44:30,547 - INFO: Epoch: 57/200, Batch: 15/29, Batch_Loss_Train: 13.764
2024-06-20 23:44:30,850 - INFO: Epoch: 57/200, Batch: 16/29, Batch_Loss_Train: 9.843
2024-06-20 23:44:31,292 - INFO: Epoch: 57/200, Batch: 17/29, Batch_Loss_Train: 18.414
2024-06-20 23:44:31,596 - INFO: Epoch: 57/200, Batch: 18/29, Batch_Loss_Train: 21.220
2024-06-20 23:44:31,986 - INFO: Epoch: 57/200, Batch: 19/29, Batch_Loss_Train: 15.058
2024-06-20 23:44:32,287 - INFO: Epoch: 57/200, Batch: 20/29, Batch_Loss_Train: 7.587
2024-06-20 23:44:32,728 - INFO: Epoch: 57/200, Batch: 21/29, Batch_Loss_Train: 6.052
2024-06-20 23:44:33,034 - INFO: Epoch: 57/200, Batch: 22/29, Batch_Loss_Train: 7.548
2024-06-20 23:44:33,429 - INFO: Epoch: 57/200, Batch: 23/29, Batch_Loss_Train: 10.868
2024-06-20 23:44:33,734 - INFO: Epoch: 57/200, Batch: 24/29, Batch_Loss_Train: 16.108
2024-06-20 23:44:34,167 - INFO: Epoch: 57/200, Batch: 25/29, Batch_Loss_Train: 8.423
2024-06-20 23:44:34,468 - INFO: Epoch: 57/200, Batch: 26/29, Batch_Loss_Train: 7.411
2024-06-20 23:44:34,856 - INFO: Epoch: 57/200, Batch: 27/29, Batch_Loss_Train: 7.627
2024-06-20 23:44:35,156 - INFO: Epoch: 57/200, Batch: 28/29, Batch_Loss_Train: 10.883
2024-06-20 23:44:35,376 - INFO: Epoch: 57/200, Batch: 29/29, Batch_Loss_Train: 12.942
2024-06-20 23:44:46,237 - INFO: 57/200 final results:
2024-06-20 23:44:46,237 - INFO: Training loss: 12.626.
2024-06-20 23:44:46,237 - INFO: Training MAE: 2.743.
2024-06-20 23:44:46,237 - INFO: Training MSE: 12.620.
2024-06-20 23:45:06,622 - INFO: Epoch: 57/200, Loss_train: 12.62645252819719, Loss_val: 16.18989191384151
2024-06-20 23:45:06,623 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:45:06,623 - INFO: Epoch 58/200...
2024-06-20 23:45:06,623 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:45:06,623 - INFO: Batch size: 32.
2024-06-20 23:45:06,626 - INFO: Dataset:
2024-06-20 23:45:06,626 - INFO: Batch size:
2024-06-20 23:45:06,626 - INFO: Number of workers:
2024-06-20 23:45:07,747 - INFO: Epoch: 58/200, Batch: 1/29, Batch_Loss_Train: 13.424
2024-06-20 23:45:08,058 - INFO: Epoch: 58/200, Batch: 2/29, Batch_Loss_Train: 8.505
2024-06-20 23:45:08,459 - INFO: Epoch: 58/200, Batch: 3/29, Batch_Loss_Train: 5.771
2024-06-20 23:45:08,782 - INFO: Epoch: 58/200, Batch: 4/29, Batch_Loss_Train: 8.909
2024-06-20 23:45:09,222 - INFO: Epoch: 58/200, Batch: 5/29, Batch_Loss_Train: 10.421
2024-06-20 23:45:09,527 - INFO: Epoch: 58/200, Batch: 6/29, Batch_Loss_Train: 7.981
2024-06-20 23:45:09,917 - INFO: Epoch: 58/200, Batch: 7/29, Batch_Loss_Train: 7.819
2024-06-20 23:45:10,224 - INFO: Epoch: 58/200, Batch: 8/29, Batch_Loss_Train: 9.482
2024-06-20 23:45:10,676 - INFO: Epoch: 58/200, Batch: 9/29, Batch_Loss_Train: 15.089
2024-06-20 23:45:10,977 - INFO: Epoch: 58/200, Batch: 10/29, Batch_Loss_Train: 20.021
2024-06-20 23:45:11,369 - INFO: Epoch: 58/200, Batch: 11/29, Batch_Loss_Train: 19.140
2024-06-20 23:45:11,677 - INFO: Epoch: 58/200, Batch: 12/29, Batch_Loss_Train: 13.512
2024-06-20 23:45:12,127 - INFO: Epoch: 58/200, Batch: 13/29, Batch_Loss_Train: 8.050
2024-06-20 23:45:12,434 - INFO: Epoch: 58/200, Batch: 14/29, Batch_Loss_Train: 9.971
2024-06-20 23:45:12,831 - INFO: Epoch: 58/200, Batch: 15/29, Batch_Loss_Train: 9.030
2024-06-20 23:45:13,135 - INFO: Epoch: 58/200, Batch: 16/29, Batch_Loss_Train: 9.078
2024-06-20 23:45:13,576 - INFO: Epoch: 58/200, Batch: 17/29, Batch_Loss_Train: 4.532
2024-06-20 23:45:13,881 - INFO: Epoch: 58/200, Batch: 18/29, Batch_Loss_Train: 4.745
2024-06-20 23:45:14,271 - INFO: Epoch: 58/200, Batch: 19/29, Batch_Loss_Train: 6.973
2024-06-20 23:45:14,574 - INFO: Epoch: 58/200, Batch: 20/29, Batch_Loss_Train: 7.698
2024-06-20 23:45:15,014 - INFO: Epoch: 58/200, Batch: 21/29, Batch_Loss_Train: 7.064
2024-06-20 23:45:15,319 - INFO: Epoch: 58/200, Batch: 22/29, Batch_Loss_Train: 6.092
2024-06-20 23:45:15,708 - INFO: Epoch: 58/200, Batch: 23/29, Batch_Loss_Train: 8.906
2024-06-20 23:45:16,014 - INFO: Epoch: 58/200, Batch: 24/29, Batch_Loss_Train: 10.508
2024-06-20 23:45:16,443 - INFO: Epoch: 58/200, Batch: 25/29, Batch_Loss_Train: 7.356
2024-06-20 23:45:16,745 - INFO: Epoch: 58/200, Batch: 26/29, Batch_Loss_Train: 3.564
2024-06-20 23:45:17,117 - INFO: Epoch: 58/200, Batch: 27/29, Batch_Loss_Train: 3.291
2024-06-20 23:45:17,419 - INFO: Epoch: 58/200, Batch: 28/29, Batch_Loss_Train: 6.235
2024-06-20 23:45:17,630 - INFO: Epoch: 58/200, Batch: 29/29, Batch_Loss_Train: 11.115
2024-06-20 23:45:28,508 - INFO: 58/200 final results:
2024-06-20 23:45:28,509 - INFO: Training loss: 9.113.
2024-06-20 23:45:28,509 - INFO: Training MAE: 2.285.
2024-06-20 23:45:28,509 - INFO: Training MSE: 9.074.
2024-06-20 23:45:48,329 - INFO: Epoch: 58/200, Loss_train: 9.113219088521497, Loss_val: 11.091456742122256
2024-06-20 23:45:48,329 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:45:48,329 - INFO: Epoch 59/200...
2024-06-20 23:45:48,329 - INFO: Learning rate: 0.0008815598160420683.
2024-06-20 23:45:48,329 - INFO: Batch size: 32.
2024-06-20 23:45:48,332 - INFO: Dataset:
2024-06-20 23:45:48,332 - INFO: Batch size:
2024-06-20 23:45:48,332 - INFO: Number of workers:
2024-06-20 23:45:49,452 - INFO: Epoch: 59/200, Batch: 1/29, Batch_Loss_Train: 9.208
2024-06-20 23:45:49,777 - INFO: Epoch: 59/200, Batch: 2/29, Batch_Loss_Train: 6.606
2024-06-20 23:45:50,190 - INFO: Epoch: 59/200, Batch: 3/29, Batch_Loss_Train: 7.239
2024-06-20 23:45:50,517 - INFO: Epoch: 59/200, Batch: 4/29, Batch_Loss_Train: 7.702
2024-06-20 23:45:50,925 - INFO: Epoch: 59/200, Batch: 5/29, Batch_Loss_Train: 10.356
2024-06-20 23:45:51,247 - INFO: Epoch: 59/200, Batch: 6/29, Batch_Loss_Train: 19.639
2024-06-20 23:45:51,652 - INFO: Epoch: 59/200, Batch: 7/29, Batch_Loss_Train: 31.401
2024-06-20 23:45:51,973 - INFO: Epoch: 59/200, Batch: 8/29, Batch_Loss_Train: 31.164
2024-06-20 23:45:52,378 - INFO: Epoch: 59/200, Batch: 9/29, Batch_Loss_Train: 72.206
2024-06-20 23:45:52,695 - INFO: Epoch: 59/200, Batch: 10/29, Batch_Loss_Train: 88.140
2024-06-20 23:45:53,092 - INFO: Epoch: 59/200, Batch: 11/29, Batch_Loss_Train: 1203.665
2024-06-20 23:45:53,415 - INFO: Epoch: 59/200, Batch: 12/29, Batch_Loss_Train: 411.993
2024-06-20 23:45:53,829 - INFO: Epoch: 59/200, Batch: 13/29, Batch_Loss_Train: 1797.436
2024-06-20 23:45:54,151 - INFO: Epoch: 59/200, Batch: 14/29, Batch_Loss_Train: 865.872
2024-06-20 23:45:54,558 - INFO: Epoch: 59/200, Batch: 15/29, Batch_Loss_Train: 193.960
2024-06-20 23:45:54,877 - INFO: Epoch: 59/200, Batch: 16/29, Batch_Loss_Train: 88.193
2024-06-20 23:45:55,280 - INFO: Epoch: 59/200, Batch: 17/29, Batch_Loss_Train: 59.836
2024-06-20 23:45:55,598 - INFO: Epoch: 59/200, Batch: 18/29, Batch_Loss_Train: 43.473
2024-06-20 23:45:56,000 - INFO: Epoch: 59/200, Batch: 19/29, Batch_Loss_Train: 36.788
2024-06-20 23:45:56,316 - INFO: Epoch: 59/200, Batch: 20/29, Batch_Loss_Train: 28.796
2024-06-20 23:45:56,714 - INFO: Epoch: 59/200, Batch: 21/29, Batch_Loss_Train: 28.984
2024-06-20 23:45:57,031 - INFO: Epoch: 59/200, Batch: 22/29, Batch_Loss_Train: 32.067
2024-06-20 23:45:57,430 - INFO: Epoch: 59/200, Batch: 23/29, Batch_Loss_Train: 22.849
2024-06-20 23:45:57,747 - INFO: Epoch: 59/200, Batch: 24/29, Batch_Loss_Train: 22.502
2024-06-20 23:45:58,139 - INFO: Epoch: 59/200, Batch: 25/29, Batch_Loss_Train: 29.099
2024-06-20 23:45:58,451 - INFO: Epoch: 59/200, Batch: 26/29, Batch_Loss_Train: 23.946
2024-06-20 23:45:58,842 - INFO: Epoch: 59/200, Batch: 27/29, Batch_Loss_Train: 22.061
2024-06-20 23:45:59,155 - INFO: Epoch: 59/200, Batch: 28/29, Batch_Loss_Train: 18.436
2024-06-20 23:45:59,373 - INFO: Epoch: 59/200, Batch: 29/29, Batch_Loss_Train: 17.719
2024-06-20 23:46:10,245 - INFO: 59/200 final results:
2024-06-20 23:46:10,245 - INFO: Training loss: 180.391.
2024-06-20 23:46:10,245 - INFO: Training MAE: 8.012.
2024-06-20 23:46:10,245 - INFO: Training MSE: 183.609.
2024-06-20 23:46:30,728 - INFO: Epoch: 59/200, Loss_train: 180.39087471468696, Loss_val: 37.431474093733165
2024-06-20 23:46:30,728 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:46:30,728 - INFO: Epoch 60/200...
2024-06-20 23:46:30,728 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:46:30,728 - INFO: Batch size: 32.
2024-06-20 23:46:30,732 - INFO: Dataset:
2024-06-20 23:46:30,732 - INFO: Batch size:
2024-06-20 23:46:30,732 - INFO: Number of workers:
2024-06-20 23:46:31,873 - INFO: Epoch: 60/200, Batch: 1/29, Batch_Loss_Train: 35.483
2024-06-20 23:46:32,185 - INFO: Epoch: 60/200, Batch: 2/29, Batch_Loss_Train: 25.327
2024-06-20 23:46:32,602 - INFO: Epoch: 60/200, Batch: 3/29, Batch_Loss_Train: 22.366
2024-06-20 23:46:32,926 - INFO: Epoch: 60/200, Batch: 4/29, Batch_Loss_Train: 20.446
2024-06-20 23:46:33,338 - INFO: Epoch: 60/200, Batch: 5/29, Batch_Loss_Train: 19.723
2024-06-20 23:46:33,654 - INFO: Epoch: 60/200, Batch: 6/29, Batch_Loss_Train: 22.599
2024-06-20 23:46:34,065 - INFO: Epoch: 60/200, Batch: 7/29, Batch_Loss_Train: 17.539
2024-06-20 23:46:34,386 - INFO: Epoch: 60/200, Batch: 8/29, Batch_Loss_Train: 23.622
2024-06-20 23:46:34,785 - INFO: Epoch: 60/200, Batch: 9/29, Batch_Loss_Train: 20.348
2024-06-20 23:46:35,102 - INFO: Epoch: 60/200, Batch: 10/29, Batch_Loss_Train: 17.429
2024-06-20 23:46:35,517 - INFO: Epoch: 60/200, Batch: 11/29, Batch_Loss_Train: 12.638
2024-06-20 23:46:35,839 - INFO: Epoch: 60/200, Batch: 12/29, Batch_Loss_Train: 17.316
2024-06-20 23:46:36,253 - INFO: Epoch: 60/200, Batch: 13/29, Batch_Loss_Train: 18.131
2024-06-20 23:46:36,575 - INFO: Epoch: 60/200, Batch: 14/29, Batch_Loss_Train: 19.981
2024-06-20 23:46:36,993 - INFO: Epoch: 60/200, Batch: 15/29, Batch_Loss_Train: 21.316
2024-06-20 23:46:37,311 - INFO: Epoch: 60/200, Batch: 16/29, Batch_Loss_Train: 16.338
2024-06-20 23:46:37,715 - INFO: Epoch: 60/200, Batch: 17/29, Batch_Loss_Train: 16.911
2024-06-20 23:46:38,034 - INFO: Epoch: 60/200, Batch: 18/29, Batch_Loss_Train: 18.739
2024-06-20 23:46:38,442 - INFO: Epoch: 60/200, Batch: 19/29, Batch_Loss_Train: 19.817
2024-06-20 23:46:38,760 - INFO: Epoch: 60/200, Batch: 20/29, Batch_Loss_Train: 15.627
2024-06-20 23:46:39,167 - INFO: Epoch: 60/200, Batch: 21/29, Batch_Loss_Train: 19.315
2024-06-20 23:46:39,488 - INFO: Epoch: 60/200, Batch: 22/29, Batch_Loss_Train: 17.010
2024-06-20 23:46:39,893 - INFO: Epoch: 60/200, Batch: 23/29, Batch_Loss_Train: 27.650
2024-06-20 23:46:40,214 - INFO: Epoch: 60/200, Batch: 24/29, Batch_Loss_Train: 17.138
2024-06-20 23:46:40,612 - INFO: Epoch: 60/200, Batch: 25/29, Batch_Loss_Train: 16.329
2024-06-20 23:46:40,928 - INFO: Epoch: 60/200, Batch: 26/29, Batch_Loss_Train: 17.100
2024-06-20 23:46:41,323 - INFO: Epoch: 60/200, Batch: 27/29, Batch_Loss_Train: 10.840
2024-06-20 23:46:41,639 - INFO: Epoch: 60/200, Batch: 28/29, Batch_Loss_Train: 12.502
2024-06-20 23:46:41,855 - INFO: Epoch: 60/200, Batch: 29/29, Batch_Loss_Train: 18.175
2024-06-20 23:46:52,845 - INFO: 60/200 final results:
2024-06-20 23:46:52,846 - INFO: Training loss: 19.233.
2024-06-20 23:46:52,846 - INFO: Training MAE: 3.468.
2024-06-20 23:46:52,846 - INFO: Training MSE: 19.254.
2024-06-20 23:47:13,460 - INFO: Epoch: 60/200, Loss_train: 19.23291308304359, Loss_val: 18.14210184689226
2024-06-20 23:47:13,460 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:47:13,460 - INFO: Epoch 61/200...
2024-06-20 23:47:13,460 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:47:13,460 - INFO: Batch size: 32.
2024-06-20 23:47:13,463 - INFO: Dataset:
2024-06-20 23:47:13,463 - INFO: Batch size:
2024-06-20 23:47:13,463 - INFO: Number of workers:
2024-06-20 23:47:14,601 - INFO: Epoch: 61/200, Batch: 1/29, Batch_Loss_Train: 18.956
2024-06-20 23:47:14,911 - INFO: Epoch: 61/200, Batch: 2/29, Batch_Loss_Train: 18.340
2024-06-20 23:47:15,311 - INFO: Epoch: 61/200, Batch: 3/29, Batch_Loss_Train: 16.853
2024-06-20 23:47:15,633 - INFO: Epoch: 61/200, Batch: 4/29, Batch_Loss_Train: 12.339
2024-06-20 23:47:16,053 - INFO: Epoch: 61/200, Batch: 5/29, Batch_Loss_Train: 15.003
2024-06-20 23:47:16,369 - INFO: Epoch: 61/200, Batch: 6/29, Batch_Loss_Train: 16.085
2024-06-20 23:47:16,759 - INFO: Epoch: 61/200, Batch: 7/29, Batch_Loss_Train: 12.187
2024-06-20 23:47:17,077 - INFO: Epoch: 61/200, Batch: 8/29, Batch_Loss_Train: 10.850
2024-06-20 23:47:17,486 - INFO: Epoch: 61/200, Batch: 9/29, Batch_Loss_Train: 15.905
2024-06-20 23:47:17,798 - INFO: Epoch: 61/200, Batch: 10/29, Batch_Loss_Train: 16.388
2024-06-20 23:47:18,188 - INFO: Epoch: 61/200, Batch: 11/29, Batch_Loss_Train: 10.852
2024-06-20 23:47:18,506 - INFO: Epoch: 61/200, Batch: 12/29, Batch_Loss_Train: 12.447
2024-06-20 23:47:18,931 - INFO: Epoch: 61/200, Batch: 13/29, Batch_Loss_Train: 16.039
2024-06-20 23:47:19,249 - INFO: Epoch: 61/200, Batch: 14/29, Batch_Loss_Train: 13.407
2024-06-20 23:47:19,645 - INFO: Epoch: 61/200, Batch: 15/29, Batch_Loss_Train: 12.859
2024-06-20 23:47:19,959 - INFO: Epoch: 61/200, Batch: 16/29, Batch_Loss_Train: 12.546
2024-06-20 23:47:20,367 - INFO: Epoch: 61/200, Batch: 17/29, Batch_Loss_Train: 13.577
2024-06-20 23:47:20,681 - INFO: Epoch: 61/200, Batch: 18/29, Batch_Loss_Train: 11.642
2024-06-20 23:47:21,067 - INFO: Epoch: 61/200, Batch: 19/29, Batch_Loss_Train: 11.789
2024-06-20 23:47:21,377 - INFO: Epoch: 61/200, Batch: 20/29, Batch_Loss_Train: 11.134
2024-06-20 23:47:21,785 - INFO: Epoch: 61/200, Batch: 21/29, Batch_Loss_Train: 15.826
2024-06-20 23:47:22,102 - INFO: Epoch: 61/200, Batch: 22/29, Batch_Loss_Train: 13.220
2024-06-20 23:47:22,491 - INFO: Epoch: 61/200, Batch: 23/29, Batch_Loss_Train: 10.452
2024-06-20 23:47:22,807 - INFO: Epoch: 61/200, Batch: 24/29, Batch_Loss_Train: 10.316
2024-06-20 23:47:23,217 - INFO: Epoch: 61/200, Batch: 25/29, Batch_Loss_Train: 12.363
2024-06-20 23:47:23,529 - INFO: Epoch: 61/200, Batch: 26/29, Batch_Loss_Train: 10.991
2024-06-20 23:47:23,915 - INFO: Epoch: 61/200, Batch: 27/29, Batch_Loss_Train: 14.552
2024-06-20 23:47:24,227 - INFO: Epoch: 61/200, Batch: 28/29, Batch_Loss_Train: 13.460
2024-06-20 23:47:24,443 - INFO: Epoch: 61/200, Batch: 29/29, Batch_Loss_Train: 8.481
2024-06-20 23:47:35,404 - INFO: 61/200 final results:
2024-06-20 23:47:35,405 - INFO: Training loss: 13.409.
2024-06-20 23:47:35,405 - INFO: Training MAE: 2.896.
2024-06-20 23:47:35,405 - INFO: Training MSE: 13.506.
2024-06-20 23:47:55,341 - INFO: Epoch: 61/200, Loss_train: 13.408825874328613, Loss_val: 12.173755810178559
2024-06-20 23:47:55,341 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:47:55,341 - INFO: Epoch 62/200...
2024-06-20 23:47:55,341 - INFO: Learning rate: 0.00044077990802103415.
2024-06-20 23:47:55,341 - INFO: Batch size: 32.
2024-06-20 23:47:55,344 - INFO: Dataset:
2024-06-20 23:47:55,344 - INFO: Batch size:
2024-06-20 23:47:55,344 - INFO: Number of workers:
2024-06-20 23:47:56,457 - INFO: Epoch: 62/200, Batch: 1/29, Batch_Loss_Train: 11.396
2024-06-20 23:47:56,783 - INFO: Epoch: 62/200, Batch: 2/29, Batch_Loss_Train: 15.690
2024-06-20 23:47:57,204 - INFO: Epoch: 62/200, Batch: 3/29, Batch_Loss_Train: 10.155
2024-06-20 23:47:57,529 - INFO: Epoch: 62/200, Batch: 4/29, Batch_Loss_Train: 11.876
2024-06-20 23:47:57,942 - INFO: Epoch: 62/200, Batch: 5/29, Batch_Loss_Train: 12.008
2024-06-20 23:47:58,261 - INFO: Epoch: 62/200, Batch: 6/29, Batch_Loss_Train: 9.112
2024-06-20 23:47:58,670 - INFO: Epoch: 62/200, Batch: 7/29, Batch_Loss_Train: 9.670
2024-06-20 23:47:58,991 - INFO: Epoch: 62/200, Batch: 8/29, Batch_Loss_Train: 9.736
2024-06-20 23:47:59,396 - INFO: Epoch: 62/200, Batch: 9/29, Batch_Loss_Train: 10.821
2024-06-20 23:47:59,712 - INFO: Epoch: 62/200, Batch: 10/29, Batch_Loss_Train: 12.925
2024-06-20 23:48:00,120 - INFO: Epoch: 62/200, Batch: 11/29, Batch_Loss_Train: 10.143
2024-06-20 23:48:00,443 - INFO: Epoch: 62/200, Batch: 12/29, Batch_Loss_Train: 10.590
2024-06-20 23:48:00,861 - INFO: Epoch: 62/200, Batch: 13/29, Batch_Loss_Train: 7.351
2024-06-20 23:48:01,183 - INFO: Epoch: 62/200, Batch: 14/29, Batch_Loss_Train: 10.103
2024-06-20 23:48:01,598 - INFO: Epoch: 62/200, Batch: 15/29, Batch_Loss_Train: 8.012
2024-06-20 23:48:01,916 - INFO: Epoch: 62/200, Batch: 16/29, Batch_Loss_Train: 10.232
2024-06-20 23:48:02,331 - INFO: Epoch: 62/200, Batch: 17/29, Batch_Loss_Train: 8.042
2024-06-20 23:48:02,649 - INFO: Epoch: 62/200, Batch: 18/29, Batch_Loss_Train: 10.668
2024-06-20 23:48:03,054 - INFO: Epoch: 62/200, Batch: 19/29, Batch_Loss_Train: 10.565
2024-06-20 23:48:03,370 - INFO: Epoch: 62/200, Batch: 20/29, Batch_Loss_Train: 13.550
2024-06-20 23:48:03,779 - INFO: Epoch: 62/200, Batch: 21/29, Batch_Loss_Train: 10.933
2024-06-20 23:48:04,100 - INFO: Epoch: 62/200, Batch: 22/29, Batch_Loss_Train: 8.989
2024-06-20 23:48:04,505 - INFO: Epoch: 62/200, Batch: 23/29, Batch_Loss_Train: 7.964
2024-06-20 23:48:04,826 - INFO: Epoch: 62/200, Batch: 24/29, Batch_Loss_Train: 7.453
2024-06-20 23:48:05,226 - INFO: Epoch: 62/200, Batch: 25/29, Batch_Loss_Train: 9.615
2024-06-20 23:48:05,542 - INFO: Epoch: 62/200, Batch: 26/29, Batch_Loss_Train: 8.326
2024-06-20 23:48:05,938 - INFO: Epoch: 62/200, Batch: 27/29, Batch_Loss_Train: 9.174
2024-06-20 23:48:06,254 - INFO: Epoch: 62/200, Batch: 28/29, Batch_Loss_Train: 7.408
2024-06-20 23:48:06,472 - INFO: Epoch: 62/200, Batch: 29/29, Batch_Loss_Train: 8.730
2024-06-20 23:48:17,473 - INFO: 62/200 final results:
2024-06-20 23:48:17,473 - INFO: Training loss: 10.043.
2024-06-20 23:48:17,473 - INFO: Training MAE: 2.461.
2024-06-20 23:48:17,473 - INFO: Training MSE: 10.069.
2024-06-20 23:48:37,995 - INFO: Epoch: 62/200, Loss_train: 10.042655320003115, Loss_val: 11.290961725958462
2024-06-20 23:48:37,995 - INFO: Best internal validation val_loss: 8.295 at epoch: 52.
2024-06-20 23:48:37,995 - INFO: Warning: No internal validation improvement during the last {'general': 10, 'lr': 3, 'opt': 1} consecutive epochs. (STOP TRAINING)
