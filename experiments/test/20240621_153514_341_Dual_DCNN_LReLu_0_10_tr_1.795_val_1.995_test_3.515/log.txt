2024-06-21 15:25:26,450 - INFO: Device: cuda.
2024-06-21 15:25:26,450 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 15:25:26,450 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 15:25:26,450 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 15:25:26,450 - INFO: Seed: 4
2024-06-21 15:25:26,450 - INFO: 42 patients have been found in the data directory.
2024-06-21 15:25:26,489 - INFO: Train set contains 32 patients.
2024-06-21 15:25:26,489 - INFO: Val set contains 5 patients.
2024-06-21 15:25:26,489 - INFO: Test set contains 5 patients.
2024-06-21 15:25:26,489 - INFO: Fold: 0
2024-06-21 15:25:26,489 - INFO: Performing 2-fold Cross Validation.
2024-06-21 15:25:26,490 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 15:25:26,490 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 15:25:26,490 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 15:25:26,621 - INFO: To_device: False.
2024-06-21 15:25:26,622 - INFO: Transformers have been made successfully.
2024-06-21 15:25:26,622 - INFO: Dataset type: cache.
2024-06-21 15:25:26,622 - INFO: Dataloader type: standard.
2024-06-21 15:27:17,320 - INFO: Train dataloader arguments.
2024-06-21 15:27:17,320 - INFO: 	Batch_size: 32.
2024-06-21 15:27:17,320 - INFO: 	Shuffle: True.
2024-06-21 15:27:17,320 - INFO: 	Sampler: None.
2024-06-21 15:27:17,320 - INFO: 	Num_workers: 4.
2024-06-21 15:27:17,320 - INFO: 	Drop_last: False.
2024-06-21 15:27:17,371 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 15:27:18,226 - INFO: Weight init name: kaiming_uniform.
2024-06-21 15:27:20,647 - INFO: Number of training iterations per epoch: 29.
2024-06-21 15:27:20,648 - INFO: Epoch 1/10...
2024-06-21 15:27:20,648 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:27:20,648 - INFO: Batch size: 32.
2024-06-21 15:27:20,648 - INFO: Dataset:
2024-06-21 15:27:20,648 - INFO: Batch size:
2024-06-21 15:27:20,648 - INFO: Number of workers:
2024-06-21 15:27:23,632 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 7.229
2024-06-21 15:27:23,939 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.806
2024-06-21 15:27:24,335 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 6.945
2024-06-21 15:27:24,643 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 6.812
2024-06-21 15:27:25,046 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 8.449
2024-06-21 15:27:25,362 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 9.550
2024-06-21 15:27:25,763 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 8.349
2024-06-21 15:27:26,080 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 8.250
2024-06-21 15:27:26,475 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 7.214
2024-06-21 15:27:26,784 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 8.058
2024-06-21 15:27:27,179 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 8.243
2024-06-21 15:27:27,497 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 6.761
2024-06-21 15:27:27,905 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 7.436
2024-06-21 15:27:28,223 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 6.567
2024-06-21 15:27:28,630 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 7.483
2024-06-21 15:27:28,945 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 6.872
2024-06-21 15:27:29,353 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.447
2024-06-21 15:27:29,668 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.072
2024-06-21 15:27:30,069 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 6.995
2024-06-21 15:27:30,379 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 6.700
2024-06-21 15:27:30,777 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 6.843
2024-06-21 15:27:31,093 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 6.861
2024-06-21 15:27:31,488 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 6.372
2024-06-21 15:27:31,804 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 6.393
2024-06-21 15:27:32,195 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 5.508
2024-06-21 15:27:32,507 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 6.639
2024-06-21 15:27:32,897 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 5.952
2024-06-21 15:27:33,208 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 6.516
2024-06-21 15:27:34,488 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 5.062
2024-06-21 15:27:45,531 - INFO: 1/10 final results:
2024-06-21 15:27:45,531 - INFO: Training loss: 7.048.
2024-06-21 15:27:45,531 - INFO: Training MAE: 7.087.
2024-06-21 15:27:45,531 - INFO: Training MSE: 71.907.
2024-06-21 15:28:06,096 - INFO: Epoch: 1/10, Loss_train: 7.047655533099997, Loss_val: 6.851869352932634
2024-06-21 15:28:06,096 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 15:28:06,096 - INFO: Epoch 2/10...
2024-06-21 15:28:06,096 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:28:06,096 - INFO: Batch size: 32.
2024-06-21 15:28:06,099 - INFO: Dataset:
2024-06-21 15:28:06,100 - INFO: Batch size:
2024-06-21 15:28:06,100 - INFO: Number of workers:
2024-06-21 15:28:07,251 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 5.119
2024-06-21 15:28:07,559 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 5.511
2024-06-21 15:28:07,967 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 5.132
2024-06-21 15:28:08,287 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 6.242
2024-06-21 15:28:08,690 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 4.371
2024-06-21 15:28:08,992 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 5.297
2024-06-21 15:28:09,391 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 5.012
2024-06-21 15:28:09,707 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 5.118
2024-06-21 15:28:10,120 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 5.990
2024-06-21 15:28:10,417 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 4.467
2024-06-21 15:28:10,813 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 5.014
2024-06-21 15:28:11,135 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 6.208
2024-06-21 15:28:11,560 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 4.995
2024-06-21 15:28:11,868 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 4.892
2024-06-21 15:28:12,271 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 5.677
2024-06-21 15:28:12,591 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 5.251
2024-06-21 15:28:13,004 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 4.691
2024-06-21 15:28:13,306 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 5.403
2024-06-21 15:28:13,704 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 5.481
2024-06-21 15:28:14,014 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 4.468
2024-06-21 15:28:14,427 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 4.918
2024-06-21 15:28:14,734 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 5.313
2024-06-21 15:28:15,127 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 5.227
2024-06-21 15:28:15,447 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 5.243
2024-06-21 15:28:15,852 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 4.328
2024-06-21 15:28:16,155 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 4.121
2024-06-21 15:28:16,550 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 4.549
2024-06-21 15:28:16,864 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 4.861
2024-06-21 15:28:17,076 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 5.799
2024-06-21 15:28:28,243 - INFO: 2/10 final results:
2024-06-21 15:28:28,244 - INFO: Training loss: 5.128.
2024-06-21 15:28:28,244 - INFO: Training MAE: 5.114.
2024-06-21 15:28:28,244 - INFO: Training MSE: 40.514.
2024-06-21 15:28:48,316 - INFO: Epoch: 2/10, Loss_train: 5.127507390647099, Loss_val: 4.839470731801
2024-06-21 15:28:48,362 - INFO: Saved new best metric model for epoch 2.
2024-06-21 15:28:48,363 - INFO: Best internal validation val_loss: 4.839 at epoch: 2.
2024-06-21 15:28:48,363 - INFO: Epoch 3/10...
2024-06-21 15:28:48,363 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:28:48,363 - INFO: Batch size: 32.
2024-06-21 15:28:48,366 - INFO: Dataset:
2024-06-21 15:28:48,366 - INFO: Batch size:
2024-06-21 15:28:48,366 - INFO: Number of workers:
2024-06-21 15:28:49,516 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 4.840
2024-06-21 15:28:49,825 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 4.721
2024-06-21 15:28:50,238 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 4.782
2024-06-21 15:28:50,560 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 4.563
2024-06-21 15:28:50,972 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 4.686
2024-06-21 15:28:51,277 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 3.913
2024-06-21 15:28:51,674 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 4.051
2024-06-21 15:28:51,995 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 4.193
2024-06-21 15:28:52,710 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 4.237
2024-06-21 15:28:53,012 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 4.293
2024-06-21 15:28:53,400 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 3.881
2024-06-21 15:28:53,720 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 4.040
2024-06-21 15:28:54,144 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 4.406
2024-06-21 15:28:54,453 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 5.337
2024-06-21 15:28:54,849 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 3.929
2024-06-21 15:28:55,162 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 4.254
2024-06-21 15:28:55,578 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 4.319
2024-06-21 15:28:55,881 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 4.844
2024-06-21 15:28:56,284 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 4.254
2024-06-21 15:28:56,596 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 4.416
2024-06-21 15:28:57,004 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 4.356
2024-06-21 15:28:57,314 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 3.969
2024-06-21 15:28:57,716 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 4.152
2024-06-21 15:28:58,037 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 4.158
2024-06-21 15:28:58,446 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 4.052
2024-06-21 15:28:58,750 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 4.583
2024-06-21 15:28:59,147 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 3.912
2024-06-21 15:28:59,464 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 3.693
2024-06-21 15:28:59,686 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 5.113
2024-06-21 15:29:10,563 - INFO: 3/10 final results:
2024-06-21 15:29:10,564 - INFO: Training loss: 4.343.
2024-06-21 15:29:10,564 - INFO: Training MAE: 4.328.
2024-06-21 15:29:10,564 - INFO: Training MSE: 30.820.
2024-06-21 15:29:31,102 - INFO: Epoch: 3/10, Loss_train: 4.342985778019346, Loss_val: 4.020687818527222
2024-06-21 15:29:31,160 - INFO: Saved new best metric model for epoch 3.
2024-06-21 15:29:31,160 - INFO: Best internal validation val_loss: 4.021 at epoch: 3.
2024-06-21 15:29:31,160 - INFO: Epoch 4/10...
2024-06-21 15:29:31,160 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:29:31,160 - INFO: Batch size: 32.
2024-06-21 15:29:31,163 - INFO: Dataset:
2024-06-21 15:29:31,163 - INFO: Batch size:
2024-06-21 15:29:31,163 - INFO: Number of workers:
2024-06-21 15:29:32,319 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 3.491
2024-06-21 15:29:32,629 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 3.491
2024-06-21 15:29:33,027 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 3.257
2024-06-21 15:29:33,348 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 4.229
2024-06-21 15:29:33,773 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 4.010
2024-06-21 15:29:34,077 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 2.921
2024-06-21 15:29:34,461 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 4.249
2024-06-21 15:29:34,782 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 3.859
2024-06-21 15:29:35,219 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 3.300
2024-06-21 15:29:35,520 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 3.518
2024-06-21 15:29:35,907 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 3.228
2024-06-21 15:29:36,230 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 3.424
2024-06-21 15:29:36,685 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 4.188
2024-06-21 15:29:36,991 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 4.367
2024-06-21 15:29:37,392 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 4.006
2024-06-21 15:29:37,699 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 3.719
2024-06-21 15:29:38,148 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 4.267
2024-06-21 15:29:38,454 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 3.793
2024-06-21 15:29:38,848 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 3.472
2024-06-21 15:29:39,151 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 3.674
2024-06-21 15:29:39,586 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 3.913
2024-06-21 15:29:39,893 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 4.277
2024-06-21 15:29:40,278 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 3.690
2024-06-21 15:29:40,585 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 4.431
2024-06-21 15:29:41,015 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 3.794
2024-06-21 15:29:41,317 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 4.154
2024-06-21 15:29:41,704 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 4.470
2024-06-21 15:29:42,006 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 4.091
2024-06-21 15:29:42,223 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 3.913
2024-06-21 15:29:53,390 - INFO: 4/10 final results:
2024-06-21 15:29:53,390 - INFO: Training loss: 3.834.
2024-06-21 15:29:53,390 - INFO: Training MAE: 3.833.
2024-06-21 15:29:53,390 - INFO: Training MSE: 25.643.
2024-06-21 15:30:13,824 - INFO: Epoch: 4/10, Loss_train: 3.8343281170417525, Loss_val: 4.090719765630261
2024-06-21 15:30:13,824 - INFO: Best internal validation val_loss: 4.021 at epoch: 3.
2024-06-21 15:30:13,824 - INFO: Epoch 5/10...
2024-06-21 15:30:13,824 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:30:13,824 - INFO: Batch size: 32.
2024-06-21 15:30:13,827 - INFO: Dataset:
2024-06-21 15:30:13,827 - INFO: Batch size:
2024-06-21 15:30:13,827 - INFO: Number of workers:
2024-06-21 15:30:15,003 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 3.954
2024-06-21 15:30:15,316 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 3.961
2024-06-21 15:30:15,709 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 2.851
2024-06-21 15:30:16,034 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 3.814
2024-06-21 15:30:16,450 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 3.497
2024-06-21 15:30:16,782 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 3.532
2024-06-21 15:30:17,166 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 3.159
2024-06-21 15:30:17,473 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 4.321
2024-06-21 15:30:17,889 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 2.945
2024-06-21 15:30:18,215 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 3.778
2024-06-21 15:30:18,591 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 3.844
2024-06-21 15:30:18,902 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 3.555
2024-06-21 15:30:19,329 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 3.408
2024-06-21 15:30:19,663 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 2.856
2024-06-21 15:30:20,050 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 3.238
2024-06-21 15:30:20,355 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 3.375
2024-06-21 15:30:20,774 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 2.758
2024-06-21 15:30:21,105 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 2.980
2024-06-21 15:30:21,482 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 3.173
2024-06-21 15:30:21,784 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 3.116
2024-06-21 15:30:22,193 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 3.698
2024-06-21 15:30:22,522 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 4.077
2024-06-21 15:30:22,898 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 3.391
2024-06-21 15:30:23,201 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 3.613
2024-06-21 15:30:23,605 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 2.840
2024-06-21 15:30:23,928 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 3.247
2024-06-21 15:30:24,304 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 2.975
2024-06-21 15:30:24,604 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 2.890
2024-06-21 15:30:24,813 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 3.403
2024-06-21 15:30:35,957 - INFO: 5/10 final results:
2024-06-21 15:30:35,957 - INFO: Training loss: 3.388.
2024-06-21 15:30:35,957 - INFO: Training MAE: 3.388.
2024-06-21 15:30:35,957 - INFO: Training MSE: 19.746.
2024-06-21 15:30:56,448 - INFO: Epoch: 5/10, Loss_train: 3.38784593549268, Loss_val: 3.0535319015897553
2024-06-21 15:30:56,505 - INFO: Saved new best metric model for epoch 5.
2024-06-21 15:30:56,506 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:30:56,506 - INFO: Epoch 6/10...
2024-06-21 15:30:56,506 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:30:56,506 - INFO: Batch size: 32.
2024-06-21 15:30:56,509 - INFO: Dataset:
2024-06-21 15:30:56,509 - INFO: Batch size:
2024-06-21 15:30:56,509 - INFO: Number of workers:
2024-06-21 15:30:57,650 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 2.906
2024-06-21 15:30:57,999 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 2.817
2024-06-21 15:30:58,404 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 2.691
2024-06-21 15:30:58,728 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 2.784
2024-06-21 15:30:59,135 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 2.922
2024-06-21 15:30:59,481 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 2.484
2024-06-21 15:30:59,876 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 3.049
2024-06-21 15:31:00,184 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 3.086
2024-06-21 15:31:00,578 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 2.467
2024-06-21 15:31:00,926 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 4.275
2024-06-21 15:31:01,302 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 2.427
2024-06-21 15:31:01,613 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 3.115
2024-06-21 15:31:02,026 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 3.205
2024-06-21 15:31:02,374 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 3.658
2024-06-21 15:31:02,777 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 3.282
2024-06-21 15:31:03,082 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 3.243
2024-06-21 15:31:03,489 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 2.718
2024-06-21 15:31:03,832 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 2.473
2024-06-21 15:31:04,211 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 2.631
2024-06-21 15:31:04,513 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 2.800
2024-06-21 15:31:04,912 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 3.404
2024-06-21 15:31:05,257 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 2.291
2024-06-21 15:31:05,650 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 3.252
2024-06-21 15:31:05,957 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 3.957
2024-06-21 15:31:06,359 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 2.385
2024-06-21 15:31:06,698 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 2.570
2024-06-21 15:31:07,089 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 2.359
2024-06-21 15:31:07,391 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 2.624
2024-06-21 15:31:07,615 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 3.141
2024-06-21 15:31:18,701 - INFO: 6/10 final results:
2024-06-21 15:31:18,701 - INFO: Training loss: 2.932.
2024-06-21 15:31:18,701 - INFO: Training MAE: 2.927.
2024-06-21 15:31:18,701 - INFO: Training MSE: 14.923.
2024-06-21 15:31:38,934 - INFO: Epoch: 6/10, Loss_train: 2.931596953293373, Loss_val: 4.9731613520918225
2024-06-21 15:31:38,934 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:31:38,934 - INFO: Epoch 7/10...
2024-06-21 15:31:38,934 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:31:38,934 - INFO: Batch size: 32.
2024-06-21 15:31:38,937 - INFO: Dataset:
2024-06-21 15:31:38,937 - INFO: Batch size:
2024-06-21 15:31:38,937 - INFO: Number of workers:
2024-06-21 15:31:40,082 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 5.409
2024-06-21 15:31:40,392 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 3.119
2024-06-21 15:31:40,812 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 2.452
2024-06-21 15:31:41,120 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 2.651
2024-06-21 15:31:41,535 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 3.319
2024-06-21 15:31:41,839 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 2.424
2024-06-21 15:31:42,239 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 2.787
2024-06-21 15:31:42,542 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 2.324
2024-06-21 15:31:42,951 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 2.731
2024-06-21 15:31:43,249 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 2.091
2024-06-21 15:31:43,656 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 2.289
2024-06-21 15:31:43,963 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 2.732
2024-06-21 15:31:44,385 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 3.488
2024-06-21 15:31:44,691 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 2.744
2024-06-21 15:31:45,117 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 2.526
2024-06-21 15:31:45,420 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 2.931
2024-06-21 15:31:45,823 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 2.448
2024-06-21 15:31:46,125 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 2.437
2024-06-21 15:31:46,535 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 2.296
2024-06-21 15:31:46,833 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 2.310
2024-06-21 15:31:47,233 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 1.822
2024-06-21 15:31:47,537 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 2.968
2024-06-21 15:31:47,960 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 2.694
2024-06-21 15:31:48,266 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 2.195
2024-06-21 15:31:48,654 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 2.415
2024-06-21 15:31:48,955 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 2.102
2024-06-21 15:31:49,357 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 2.765
2024-06-21 15:31:49,658 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 2.902
2024-06-21 15:31:49,866 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 2.513
2024-06-21 15:32:01,141 - INFO: 7/10 final results:
2024-06-21 15:32:01,141 - INFO: Training loss: 2.686.
2024-06-21 15:32:01,141 - INFO: Training MAE: 2.689.
2024-06-21 15:32:01,141 - INFO: Training MSE: 13.348.
2024-06-21 15:32:21,797 - INFO: Epoch: 7/10, Loss_train: 2.685634292405227, Loss_val: 3.2043060598702264
2024-06-21 15:32:21,797 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:32:21,797 - INFO: Epoch 8/10...
2024-06-21 15:32:21,797 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:32:21,797 - INFO: Batch size: 32.
2024-06-21 15:32:21,800 - INFO: Dataset:
2024-06-21 15:32:21,800 - INFO: Batch size:
2024-06-21 15:32:21,800 - INFO: Number of workers:
2024-06-21 15:32:22,961 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 3.029
2024-06-21 15:32:23,274 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 3.508
2024-06-21 15:32:23,691 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 2.667
2024-06-21 15:32:24,015 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 3.325
2024-06-21 15:32:24,438 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 2.703
2024-06-21 15:32:24,745 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 2.334
2024-06-21 15:32:25,150 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 2.546
2024-06-21 15:32:25,471 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 3.780
2024-06-21 15:32:25,886 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 2.700
2024-06-21 15:32:26,187 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 2.555
2024-06-21 15:32:26,589 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 2.248
2024-06-21 15:32:26,911 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 3.220
2024-06-21 15:32:27,338 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 2.381
2024-06-21 15:32:27,647 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 2.063
2024-06-21 15:32:28,056 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 2.399
2024-06-21 15:32:28,374 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 2.840
2024-06-21 15:32:28,800 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 2.147
2024-06-21 15:32:29,105 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 2.387
2024-06-21 15:32:29,508 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 2.608
2024-06-21 15:32:29,821 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 2.350
2024-06-21 15:32:30,239 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 1.753
2024-06-21 15:32:30,546 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 1.822
2024-06-21 15:32:30,950 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 2.744
2024-06-21 15:32:31,271 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 2.686
2024-06-21 15:32:31,684 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 3.013
2024-06-21 15:32:31,987 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 2.193
2024-06-21 15:32:32,390 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 2.395
2024-06-21 15:32:32,705 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 1.942
2024-06-21 15:32:32,928 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 2.429
2024-06-21 15:32:44,087 - INFO: 8/10 final results:
2024-06-21 15:32:44,087 - INFO: Training loss: 2.578.
2024-06-21 15:32:44,087 - INFO: Training MAE: 2.581.
2024-06-21 15:32:44,087 - INFO: Training MSE: 12.021.
2024-06-21 15:33:04,350 - INFO: Epoch: 8/10, Loss_train: 2.5781373319954706, Loss_val: 3.772336425452397
2024-06-21 15:33:04,350 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:33:04,350 - INFO: Epoch 9/10...
2024-06-21 15:33:04,350 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:33:04,350 - INFO: Batch size: 32.
2024-06-21 15:33:04,353 - INFO: Dataset:
2024-06-21 15:33:04,354 - INFO: Batch size:
2024-06-21 15:33:04,354 - INFO: Number of workers:
2024-06-21 15:33:05,496 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 3.416
2024-06-21 15:33:05,822 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 2.396
2024-06-21 15:33:06,212 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 2.147
2024-06-21 15:33:06,534 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 2.094
2024-06-21 15:33:06,949 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 1.805
2024-06-21 15:33:07,268 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 2.131
2024-06-21 15:33:07,651 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 2.455
2024-06-21 15:33:07,970 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 2.230
2024-06-21 15:33:08,372 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 3.033
2024-06-21 15:33:08,686 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 2.183
2024-06-21 15:33:09,071 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 3.031
2024-06-21 15:33:09,393 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 1.932
2024-06-21 15:33:09,815 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 2.006
2024-06-21 15:33:10,137 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 2.141
2024-06-21 15:33:10,534 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 2.453
2024-06-21 15:33:10,851 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 2.325
2024-06-21 15:33:11,270 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 2.811
2024-06-21 15:33:11,585 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 1.685
2024-06-21 15:33:11,970 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 2.183
2024-06-21 15:33:12,280 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 2.549
2024-06-21 15:33:12,697 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 2.235
2024-06-21 15:33:13,019 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 2.054
2024-06-21 15:33:13,404 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 3.160
2024-06-21 15:33:13,725 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 1.873
2024-06-21 15:33:14,135 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 2.973
2024-06-21 15:33:14,452 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 3.006
2024-06-21 15:33:14,843 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 1.862
2024-06-21 15:33:15,159 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 2.521
2024-06-21 15:33:15,382 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 2.281
2024-06-21 15:33:26,557 - INFO: 9/10 final results:
2024-06-21 15:33:26,557 - INFO: Training loss: 2.378.
2024-06-21 15:33:26,557 - INFO: Training MAE: 2.380.
2024-06-21 15:33:26,557 - INFO: Training MSE: 10.250.
2024-06-21 15:33:46,960 - INFO: Epoch: 9/10, Loss_train: 2.3783312994858314, Loss_val: 2.082735842671888
2024-06-21 15:33:47,017 - INFO: Saved new best metric model for epoch 9.
2024-06-21 15:33:47,017 - INFO: Best internal validation val_loss: 2.083 at epoch: 9.
2024-06-21 15:33:47,017 - INFO: Epoch 10/10...
2024-06-21 15:33:47,017 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:33:47,017 - INFO: Batch size: 32.
2024-06-21 15:33:47,020 - INFO: Dataset:
2024-06-21 15:33:47,021 - INFO: Batch size:
2024-06-21 15:33:47,021 - INFO: Number of workers:
2024-06-21 15:33:48,154 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 1.792
2024-06-21 15:33:48,494 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 2.430
2024-06-21 15:33:48,898 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 2.247
2024-06-21 15:33:49,221 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 2.187
2024-06-21 15:33:49,643 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 2.329
2024-06-21 15:33:49,962 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 2.394
2024-06-21 15:33:50,355 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 2.076
2024-06-21 15:33:50,674 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 1.813
2024-06-21 15:33:51,093 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 1.734
2024-06-21 15:33:51,407 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 2.864
2024-06-21 15:33:51,799 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 2.887
2024-06-21 15:33:52,120 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 2.368
2024-06-21 15:33:52,544 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 2.876
2024-06-21 15:33:52,866 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 2.181
2024-06-21 15:33:53,268 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 2.771
2024-06-21 15:33:53,586 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 1.786
2024-06-21 15:33:54,006 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 2.567
2024-06-21 15:33:54,323 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 2.620
2024-06-21 15:33:54,717 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 2.299
2024-06-21 15:33:55,030 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 2.421
2024-06-21 15:33:55,444 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 2.156
2024-06-21 15:33:55,762 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 2.513
2024-06-21 15:33:56,145 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 1.887
2024-06-21 15:33:56,462 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 1.913
2024-06-21 15:33:56,864 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 2.995
2024-06-21 15:33:57,177 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 2.446
2024-06-21 15:33:57,550 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 1.864
2024-06-21 15:33:57,862 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 1.858
2024-06-21 15:33:58,072 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 2.008
2024-06-21 15:34:09,153 - INFO: 10/10 final results:
2024-06-21 15:34:09,153 - INFO: Training loss: 2.286.
2024-06-21 15:34:09,153 - INFO: Training MAE: 2.291.
2024-06-21 15:34:09,153 - INFO: Training MSE: 9.325.
2024-06-21 15:34:29,401 - INFO: Epoch: 10/10, Loss_train: 2.2856254413210113, Loss_val: 1.9951291783102627
2024-06-21 15:34:29,458 - INFO: Saved new best metric model for epoch 10.
2024-06-21 15:34:29,458 - INFO: Best internal validation val_loss: 1.995 at epoch: 10.
