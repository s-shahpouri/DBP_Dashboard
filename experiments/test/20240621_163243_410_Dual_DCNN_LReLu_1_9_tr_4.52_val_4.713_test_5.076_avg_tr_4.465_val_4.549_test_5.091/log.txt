2024-06-21 16:13:15,408 - INFO: Device: cuda.
2024-06-21 16:13:15,408 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 16:13:15,408 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 16:13:15,408 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 16:13:15,408 - INFO: Seed: 4
2024-06-21 16:13:15,408 - INFO: 42 patients have been found in the data directory.
2024-06-21 16:13:15,447 - INFO: Train set contains 32 patients.
2024-06-21 16:13:15,447 - INFO: Val set contains 5 patients.
2024-06-21 16:13:15,447 - INFO: Test set contains 5 patients.
2024-06-21 16:13:15,447 - INFO: Fold: 0
2024-06-21 16:13:15,448 - INFO: Performing 2-fold Cross Validation.
2024-06-21 16:13:15,448 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 16:13:15,448 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 16:13:15,448 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 16:13:15,578 - INFO: To_device: False.
2024-06-21 16:13:15,580 - INFO: Transformers have been made successfully.
2024-06-21 16:13:15,580 - INFO: Dataset type: cache.
2024-06-21 16:13:15,580 - INFO: Dataloader type: standard.
2024-06-21 16:15:07,224 - INFO: Train dataloader arguments.
2024-06-21 16:15:07,224 - INFO: 	Batch_size: 32.
2024-06-21 16:15:07,224 - INFO: 	Shuffle: True.
2024-06-21 16:15:07,224 - INFO: 	Sampler: None.
2024-06-21 16:15:07,224 - INFO: 	Num_workers: 4.
2024-06-21 16:15:07,224 - INFO: 	Drop_last: False.
2024-06-21 16:15:07,273 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 16:15:08,140 - INFO: Weight init name: kaiming_uniform.
2024-06-21 16:15:10,572 - INFO: Number of training iterations per epoch: 29.
2024-06-21 16:15:10,572 - INFO: Epoch 1/10...
2024-06-21 16:15:10,572 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:15:10,572 - INFO: Batch size: 32.
2024-06-21 16:15:10,572 - INFO: Dataset:
2024-06-21 16:15:10,572 - INFO: Batch size:
2024-06-21 16:15:10,572 - INFO: Number of workers:
2024-06-21 16:15:13,532 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 7.229
2024-06-21 16:15:13,831 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.810
2024-06-21 16:15:14,205 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 7.015
2024-06-21 16:15:14,506 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 12.883
2024-06-21 16:15:14,915 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 42.780
2024-06-21 16:15:15,213 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 24.597
2024-06-21 16:15:15,588 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 13.667
2024-06-21 16:15:15,901 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 11.263
2024-06-21 16:15:16,303 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 26.127
2024-06-21 16:15:16,599 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 8.977
2024-06-21 16:15:16,972 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 10.009
2024-06-21 16:15:17,287 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 7.528
2024-06-21 16:15:17,717 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 7.150
2024-06-21 16:15:18,019 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 6.755
2024-06-21 16:15:18,408 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 7.574
2024-06-21 16:15:18,723 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 7.173
2024-06-21 16:15:19,146 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.798
2024-06-21 16:15:19,450 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.385
2024-06-21 16:15:19,830 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 7.713
2024-06-21 16:15:20,141 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 6.960
2024-06-21 16:15:20,556 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 7.609
2024-06-21 16:15:20,855 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 7.825
2024-06-21 16:15:21,223 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 7.104
2024-06-21 16:15:21,534 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 7.229
2024-06-21 16:15:21,937 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 6.837
2024-06-21 16:15:22,233 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 7.199
2024-06-21 16:15:22,596 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 7.617
2024-06-21 16:15:22,905 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 7.453
2024-06-21 16:15:24,179 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 6.640
2024-06-21 16:15:35,148 - INFO: 1/10 final results:
2024-06-21 16:15:35,148 - INFO: Training loss: 10.411.
2024-06-21 16:15:35,148 - INFO: Training MAE: 10.485.
2024-06-21 16:15:35,149 - INFO: Training MSE: 221.202.
2024-06-21 16:15:55,314 - INFO: Epoch: 1/10, Loss_train: 10.410537867710508, Loss_val: 7.031782364023143
2024-06-21 16:15:55,314 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 16:15:55,315 - INFO: Epoch 2/10...
2024-06-21 16:15:55,315 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:15:55,315 - INFO: Batch size: 32.
2024-06-21 16:15:55,317 - INFO: Dataset:
2024-06-21 16:15:55,318 - INFO: Batch size:
2024-06-21 16:15:55,318 - INFO: Number of workers:
2024-06-21 16:15:56,457 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 6.542
2024-06-21 16:15:56,765 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 6.902
2024-06-21 16:15:57,162 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 6.019
2024-06-21 16:15:57,482 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 7.551
2024-06-21 16:15:57,909 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 6.484
2024-06-21 16:15:58,211 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 6.533
2024-06-21 16:15:58,599 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 6.539
2024-06-21 16:15:58,914 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 6.645
2024-06-21 16:15:59,340 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 7.726
2024-06-21 16:15:59,636 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 6.145
2024-06-21 16:16:00,017 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 6.473
2024-06-21 16:16:00,334 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 7.161
2024-06-21 16:16:00,771 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 6.254
2024-06-21 16:16:01,075 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 6.297
2024-06-21 16:16:01,475 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 6.597
2024-06-21 16:16:01,791 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 7.475
2024-06-21 16:16:02,520 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 6.390
2024-06-21 16:16:02,824 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 7.330
2024-06-21 16:16:03,213 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 6.602
2024-06-21 16:16:03,523 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 6.857
2024-06-21 16:16:03,948 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 6.756
2024-06-21 16:16:04,249 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 7.020
2024-06-21 16:16:04,637 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 6.421
2024-06-21 16:16:04,953 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 7.264
2024-06-21 16:16:05,374 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 5.801
2024-06-21 16:16:05,673 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 6.369
2024-06-21 16:16:06,061 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 6.496
2024-06-21 16:16:06,375 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 6.606
2024-06-21 16:16:06,597 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 7.138
2024-06-21 16:16:17,045 - INFO: 2/10 final results:
2024-06-21 16:16:17,045 - INFO: Training loss: 6.703.
2024-06-21 16:16:17,045 - INFO: Training MAE: 6.695.
2024-06-21 16:16:17,045 - INFO: Training MSE: 62.974.
2024-06-21 16:16:37,381 - INFO: Epoch: 2/10, Loss_train: 6.703199846991177, Loss_val: 6.208545569715829
2024-06-21 16:16:37,428 - INFO: Saved new best metric model for epoch 2.
2024-06-21 16:16:37,428 - INFO: Best internal validation val_loss: 6.209 at epoch: 2.
2024-06-21 16:16:37,428 - INFO: Epoch 3/10...
2024-06-21 16:16:37,428 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:16:37,428 - INFO: Batch size: 32.
2024-06-21 16:16:37,431 - INFO: Dataset:
2024-06-21 16:16:37,431 - INFO: Batch size:
2024-06-21 16:16:37,431 - INFO: Number of workers:
2024-06-21 16:16:38,542 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 6.510
2024-06-21 16:16:38,885 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 6.127
2024-06-21 16:16:39,277 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 5.717
2024-06-21 16:16:39,596 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 5.567
2024-06-21 16:16:39,995 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 5.524
2024-06-21 16:16:40,320 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 4.672
2024-06-21 16:16:40,706 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 6.116
2024-06-21 16:16:41,008 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 6.529
2024-06-21 16:16:41,420 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 7.501
2024-06-21 16:16:41,752 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 6.069
2024-06-21 16:16:42,131 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 6.494
2024-06-21 16:16:42,434 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 6.308
2024-06-21 16:16:42,837 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 5.240
2024-06-21 16:16:43,164 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 5.927
2024-06-21 16:16:43,557 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 6.049
2024-06-21 16:16:43,858 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 4.941
2024-06-21 16:16:44,268 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 5.258
2024-06-21 16:16:44,596 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 5.649
2024-06-21 16:16:44,982 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 6.194
2024-06-21 16:16:45,282 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 5.933
2024-06-21 16:16:45,695 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 5.195
2024-06-21 16:16:46,025 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 5.558
2024-06-21 16:16:46,408 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 5.741
2024-06-21 16:16:46,713 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 5.822
2024-06-21 16:16:47,116 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 6.899
2024-06-21 16:16:47,443 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 5.645
2024-06-21 16:16:47,828 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 5.720
2024-06-21 16:16:48,132 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 5.619
2024-06-21 16:16:48,351 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 6.538
2024-06-21 16:16:59,422 - INFO: 3/10 final results:
2024-06-21 16:16:59,422 - INFO: Training loss: 5.899.
2024-06-21 16:16:59,422 - INFO: Training MAE: 5.886.
2024-06-21 16:16:59,422 - INFO: Training MSE: 51.467.
2024-06-21 16:17:19,807 - INFO: Epoch: 3/10, Loss_train: 5.898688332787875, Loss_val: 5.481679390216696
2024-06-21 16:17:19,863 - INFO: Saved new best metric model for epoch 3.
2024-06-21 16:17:19,863 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:17:19,863 - INFO: Epoch 4/10...
2024-06-21 16:17:19,863 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:17:19,863 - INFO: Batch size: 32.
2024-06-21 16:17:19,866 - INFO: Dataset:
2024-06-21 16:17:19,867 - INFO: Batch size:
2024-06-21 16:17:19,867 - INFO: Number of workers:
2024-06-21 16:17:20,965 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 4.809
2024-06-21 16:17:21,274 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 5.364
2024-06-21 16:17:21,672 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 5.296
2024-06-21 16:17:21,993 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 5.418
2024-06-21 16:17:22,399 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 5.304
2024-06-21 16:17:22,705 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 4.647
2024-06-21 16:17:23,108 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 6.451
2024-06-21 16:17:23,426 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 5.603
2024-06-21 16:17:23,830 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 5.441
2024-06-21 16:17:24,129 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 5.125
2024-06-21 16:17:24,524 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 4.468
2024-06-21 16:17:24,844 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 7.389
2024-06-21 16:17:25,266 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 5.968
2024-06-21 16:17:25,574 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 6.222
2024-06-21 16:17:25,985 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 5.526
2024-06-21 16:17:26,303 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 5.903
2024-06-21 16:17:26,721 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 5.441
2024-06-21 16:17:27,026 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 6.076
2024-06-21 16:17:27,426 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 4.935
2024-06-21 16:17:27,738 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 5.120
2024-06-21 16:17:28,145 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 5.020
2024-06-21 16:17:28,450 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 5.091
2024-06-21 16:17:28,840 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 5.912
2024-06-21 16:17:29,157 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 6.746
2024-06-21 16:17:29,557 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 5.343
2024-06-21 16:17:29,859 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 5.561
2024-06-21 16:17:30,249 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 4.990
2024-06-21 16:17:30,564 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 6.375
2024-06-21 16:17:30,781 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 5.929
2024-06-21 16:17:41,759 - INFO: 4/10 final results:
2024-06-21 16:17:41,759 - INFO: Training loss: 5.568.
2024-06-21 16:17:41,759 - INFO: Training MAE: 5.561.
2024-06-21 16:17:41,759 - INFO: Training MSE: 47.218.
2024-06-21 16:18:01,894 - INFO: Epoch: 4/10, Loss_train: 5.568026065826416, Loss_val: 6.020817625111547
2024-06-21 16:18:01,894 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:18:01,894 - INFO: Epoch 5/10...
2024-06-21 16:18:01,894 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:18:01,894 - INFO: Batch size: 32.
2024-06-21 16:18:01,898 - INFO: Dataset:
2024-06-21 16:18:01,898 - INFO: Batch size:
2024-06-21 16:18:01,898 - INFO: Number of workers:
2024-06-21 16:18:02,992 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 5.913
2024-06-21 16:18:03,316 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 4.615
2024-06-21 16:18:03,713 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 4.764
2024-06-21 16:18:04,036 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 5.386
2024-06-21 16:18:04,450 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 5.120
2024-06-21 16:18:04,757 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 5.672
2024-06-21 16:18:05,158 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 4.911
2024-06-21 16:18:05,477 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 4.480
2024-06-21 16:18:05,884 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 4.733
2024-06-21 16:18:06,179 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 5.748
2024-06-21 16:18:06,567 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 6.264
2024-06-21 16:18:06,884 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 5.713
2024-06-21 16:18:07,304 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 4.669
2024-06-21 16:18:07,609 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 4.788
2024-06-21 16:18:08,015 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 4.852
2024-06-21 16:18:08,330 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 5.257
2024-06-21 16:18:08,744 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 4.992
2024-06-21 16:18:09,046 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 4.775
2024-06-21 16:18:09,441 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 4.678
2024-06-21 16:18:09,754 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 4.940
2024-06-21 16:18:10,167 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 5.165
2024-06-21 16:18:10,473 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 5.522
2024-06-21 16:18:10,867 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 4.887
2024-06-21 16:18:11,194 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 5.884
2024-06-21 16:18:11,628 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 5.165
2024-06-21 16:18:11,936 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 4.248
2024-06-21 16:18:12,350 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 5.144
2024-06-21 16:18:12,668 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 5.149
2024-06-21 16:18:12,881 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 4.755
2024-06-21 16:18:23,958 - INFO: 5/10 final results:
2024-06-21 16:18:23,958 - INFO: Training loss: 5.110.
2024-06-21 16:18:23,958 - INFO: Training MAE: 5.117.
2024-06-21 16:18:23,958 - INFO: Training MSE: 41.169.
2024-06-21 16:18:44,631 - INFO: Epoch: 5/10, Loss_train: 5.109810319440118, Loss_val: 5.825266805188409
2024-06-21 16:18:44,632 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:18:44,632 - INFO: Epoch 6/10...
2024-06-21 16:18:44,632 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:18:44,632 - INFO: Batch size: 32.
2024-06-21 16:18:44,635 - INFO: Dataset:
2024-06-21 16:18:44,635 - INFO: Batch size:
2024-06-21 16:18:44,635 - INFO: Number of workers:
2024-06-21 16:18:45,725 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 4.856
2024-06-21 16:18:46,051 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 5.745
2024-06-21 16:18:46,465 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 4.404
2024-06-21 16:18:46,790 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 4.755
2024-06-21 16:18:47,208 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 4.267
2024-06-21 16:18:47,515 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 5.473
2024-06-21 16:18:47,919 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 5.950
2024-06-21 16:18:48,239 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 5.295
2024-06-21 16:18:48,647 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 4.282
2024-06-21 16:18:48,946 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 5.138
2024-06-21 16:18:49,342 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 3.950
2024-06-21 16:18:49,663 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 4.114
2024-06-21 16:18:50,089 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 4.724
2024-06-21 16:18:50,398 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 5.413
2024-06-21 16:18:50,810 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 4.465
2024-06-21 16:18:51,128 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 5.221
2024-06-21 16:18:51,548 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 4.606
2024-06-21 16:18:51,854 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 4.503
2024-06-21 16:18:52,259 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 3.648
2024-06-21 16:18:52,572 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 5.391
2024-06-21 16:18:52,982 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 4.347
2024-06-21 16:18:53,289 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 4.514
2024-06-21 16:18:53,691 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 4.686
2024-06-21 16:18:54,011 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 5.574
2024-06-21 16:18:54,412 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 6.042
2024-06-21 16:18:54,715 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 4.558
2024-06-21 16:18:55,109 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 4.442
2024-06-21 16:18:55,425 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 4.501
2024-06-21 16:18:55,646 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 4.241
2024-06-21 16:19:06,474 - INFO: 6/10 final results:
2024-06-21 16:19:06,474 - INFO: Training loss: 4.797.
2024-06-21 16:19:06,474 - INFO: Training MAE: 4.808.
2024-06-21 16:19:06,474 - INFO: Training MSE: 37.841.
2024-06-21 16:19:26,641 - INFO: Epoch: 6/10, Loss_train: 4.7967655412082015, Loss_val: 6.854615951406545
2024-06-21 16:19:26,641 - INFO: Best internal validation val_loss: 5.482 at epoch: 3.
2024-06-21 16:19:26,641 - INFO: Epoch 7/10...
2024-06-21 16:19:26,641 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:19:26,641 - INFO: Batch size: 32.
2024-06-21 16:19:26,644 - INFO: Dataset:
2024-06-21 16:19:26,644 - INFO: Batch size:
2024-06-21 16:19:26,644 - INFO: Number of workers:
2024-06-21 16:19:27,753 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 4.674
2024-06-21 16:19:28,078 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 5.263
2024-06-21 16:19:28,481 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 4.855
2024-06-21 16:19:28,805 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 4.586
2024-06-21 16:19:29,210 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 4.395
2024-06-21 16:19:29,530 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 4.749
2024-06-21 16:19:29,937 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 4.356
2024-06-21 16:19:30,256 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 4.848
2024-06-21 16:19:30,649 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 4.850
2024-06-21 16:19:30,958 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 4.164
2024-06-21 16:19:31,352 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 4.523
2024-06-21 16:19:31,672 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 5.649
2024-06-21 16:19:32,080 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 5.714
2024-06-21 16:19:32,399 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 4.143
2024-06-21 16:19:32,809 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 3.955
2024-06-21 16:19:33,125 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 4.509
2024-06-21 16:19:33,533 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 4.668
2024-06-21 16:19:33,849 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 4.135
2024-06-21 16:19:34,252 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 4.233
2024-06-21 16:19:34,562 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 4.183
2024-06-21 16:19:34,959 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 4.341
2024-06-21 16:19:35,277 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 3.281
2024-06-21 16:19:35,679 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 4.316
2024-06-21 16:19:35,997 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 4.826
2024-06-21 16:19:36,396 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 4.684
2024-06-21 16:19:36,710 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 4.537
2024-06-21 16:19:37,110 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 4.815
2024-06-21 16:19:37,424 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 4.382
2024-06-21 16:19:37,648 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 4.267
2024-06-21 16:19:48,616 - INFO: 7/10 final results:
2024-06-21 16:19:48,616 - INFO: Training loss: 4.548.
2024-06-21 16:19:48,616 - INFO: Training MAE: 4.554.
2024-06-21 16:19:48,616 - INFO: Training MSE: 34.874.
2024-06-21 16:20:09,123 - INFO: Epoch: 7/10, Loss_train: 4.548353310289054, Loss_val: 4.569577841923155
2024-06-21 16:20:09,180 - INFO: Saved new best metric model for epoch 7.
2024-06-21 16:20:09,180 - INFO: Best internal validation val_loss: 4.570 at epoch: 7.
2024-06-21 16:20:09,180 - INFO: Epoch 8/10...
2024-06-21 16:20:09,180 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:20:09,180 - INFO: Batch size: 32.
2024-06-21 16:20:09,183 - INFO: Dataset:
2024-06-21 16:20:09,184 - INFO: Batch size:
2024-06-21 16:20:09,184 - INFO: Number of workers:
2024-06-21 16:20:10,303 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 4.445
2024-06-21 16:20:10,640 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 4.811
2024-06-21 16:20:11,040 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 4.096
2024-06-21 16:20:11,362 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 4.070
2024-06-21 16:20:11,770 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 4.139
2024-06-21 16:20:12,102 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 4.981
2024-06-21 16:20:12,481 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 3.988
2024-06-21 16:20:12,801 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 5.150
2024-06-21 16:20:13,192 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 4.262
2024-06-21 16:20:13,541 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 4.374
2024-06-21 16:20:13,907 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 5.037
2024-06-21 16:20:14,226 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 4.330
2024-06-21 16:20:14,637 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 4.543
2024-06-21 16:20:14,969 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 4.266
2024-06-21 16:20:15,353 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 4.710
2024-06-21 16:20:15,669 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 3.989
2024-06-21 16:20:16,073 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 4.780
2024-06-21 16:20:16,401 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 3.914
2024-06-21 16:20:16,774 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 4.601
2024-06-21 16:20:17,084 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 4.510
2024-06-21 16:20:17,481 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 3.771
2024-06-21 16:20:17,810 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 4.207
2024-06-21 16:20:18,198 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 4.434
2024-06-21 16:20:18,516 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 4.697
2024-06-21 16:20:18,914 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 4.838
2024-06-21 16:20:19,242 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 4.711
2024-06-21 16:20:19,630 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 4.244
2024-06-21 16:20:19,945 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 4.735
2024-06-21 16:20:20,167 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 3.614
2024-06-21 16:20:31,229 - INFO: 8/10 final results:
2024-06-21 16:20:31,229 - INFO: Training loss: 4.422.
2024-06-21 16:20:31,229 - INFO: Training MAE: 4.438.
2024-06-21 16:20:31,229 - INFO: Training MSE: 33.222.
2024-06-21 16:20:51,383 - INFO: Epoch: 8/10, Loss_train: 4.422263712718569, Loss_val: 4.821652741267763
2024-06-21 16:20:51,384 - INFO: Best internal validation val_loss: 4.570 at epoch: 7.
2024-06-21 16:20:51,384 - INFO: Epoch 9/10...
2024-06-21 16:20:51,384 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:20:51,384 - INFO: Batch size: 32.
2024-06-21 16:20:51,387 - INFO: Dataset:
2024-06-21 16:20:51,387 - INFO: Batch size:
2024-06-21 16:20:51,387 - INFO: Number of workers:
2024-06-21 16:20:52,496 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 4.608
2024-06-21 16:20:52,834 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 4.047
2024-06-21 16:20:53,236 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 4.470
2024-06-21 16:20:53,562 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 4.366
2024-06-21 16:20:53,985 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 4.966
2024-06-21 16:20:54,305 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 3.985
2024-06-21 16:20:54,700 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 4.463
2024-06-21 16:20:55,020 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 3.641
2024-06-21 16:20:55,435 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 4.204
2024-06-21 16:20:55,747 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 4.307
2024-06-21 16:20:56,131 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 4.512
2024-06-21 16:20:56,453 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 4.038
2024-06-21 16:20:56,878 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 4.662
2024-06-21 16:20:57,201 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 4.212
2024-06-21 16:20:57,601 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 4.076
2024-06-21 16:20:57,920 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 4.226
2024-06-21 16:20:58,344 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 4.654
2024-06-21 16:20:58,663 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 4.295
2024-06-21 16:20:59,056 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 4.047
2024-06-21 16:20:59,369 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 4.338
2024-06-21 16:20:59,781 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 3.868
2024-06-21 16:21:00,101 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 3.925
2024-06-21 16:21:00,483 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 4.062
2024-06-21 16:21:00,803 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 4.417
2024-06-21 16:21:01,209 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 3.943
2024-06-21 16:21:01,525 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 4.626
2024-06-21 16:21:01,902 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 3.821
2024-06-21 16:21:02,218 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 4.458
2024-06-21 16:21:02,440 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 4.832
2024-06-21 16:21:13,462 - INFO: 9/10 final results:
2024-06-21 16:21:13,463 - INFO: Training loss: 4.278.
2024-06-21 16:21:13,463 - INFO: Training MAE: 4.267.
2024-06-21 16:21:13,463 - INFO: Training MSE: 31.759.
2024-06-21 16:21:33,813 - INFO: Epoch: 9/10, Loss_train: 4.278311828087116, Loss_val: 4.386174645917169
2024-06-21 16:21:33,870 - INFO: Saved new best metric model for epoch 9.
2024-06-21 16:21:33,870 - INFO: Best internal validation val_loss: 4.386 at epoch: 9.
2024-06-21 16:21:33,870 - INFO: Epoch 10/10...
2024-06-21 16:21:33,870 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:21:33,870 - INFO: Batch size: 32.
2024-06-21 16:21:33,873 - INFO: Dataset:
2024-06-21 16:21:33,873 - INFO: Batch size:
2024-06-21 16:21:33,873 - INFO: Number of workers:
2024-06-21 16:21:34,980 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 4.883
2024-06-21 16:21:35,289 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 5.123
2024-06-21 16:21:35,687 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 4.306
2024-06-21 16:21:36,006 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 4.397
2024-06-21 16:21:36,420 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 3.445
2024-06-21 16:21:36,723 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 4.692
2024-06-21 16:21:37,115 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 4.132
2024-06-21 16:21:37,430 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 4.430
2024-06-21 16:21:37,837 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 3.893
2024-06-21 16:21:38,132 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 4.136
2024-06-21 16:21:38,516 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 4.799
2024-06-21 16:21:38,837 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 3.626
2024-06-21 16:21:39,263 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 5.897
2024-06-21 16:21:39,573 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 4.677
2024-06-21 16:21:39,980 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 5.032
2024-06-21 16:21:40,298 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 4.654
2024-06-21 16:21:40,713 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 4.793
2024-06-21 16:21:41,016 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 3.857
2024-06-21 16:21:41,415 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 3.755
2024-06-21 16:21:41,728 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 4.367
2024-06-21 16:21:42,143 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 4.390
2024-06-21 16:21:42,451 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 3.883
2024-06-21 16:21:42,852 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 3.769
2024-06-21 16:21:43,171 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 4.752
2024-06-21 16:21:43,584 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 3.972
2024-06-21 16:21:43,887 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 3.921
2024-06-21 16:21:44,284 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 3.972
2024-06-21 16:21:44,599 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 3.946
2024-06-21 16:21:44,821 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 3.954
2024-06-21 16:21:56,009 - INFO: 10/10 final results:
2024-06-21 16:21:56,009 - INFO: Training loss: 4.326.
2024-06-21 16:21:56,009 - INFO: Training MAE: 4.333.
2024-06-21 16:21:56,009 - INFO: Training MSE: 32.426.
2024-06-21 16:22:16,035 - INFO: Epoch: 10/10, Loss_train: 4.32594424280627, Loss_val: 4.851717685831004
2024-06-21 16:22:16,035 - INFO: Best internal validation val_loss: 4.386 at epoch: 9.
2024-06-21 16:23:00,047 - INFO: Experiment has been saved in 20240621_162300_341_Dual_DCNN_LReLu_0_9_tr_4.411_val_4.386_test_5.107 folder.
2024-06-21 16:23:00,047 - INFO: Fold: 1
2024-06-21 16:23:00,048 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 16:23:00,048 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 16:23:00,048 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 16:23:00,179 - INFO: To_device: False.
2024-06-21 16:23:00,180 - INFO: Transformers have been made successfully.
2024-06-21 16:23:00,180 - INFO: Dataset type: cache.
2024-06-21 16:23:00,180 - INFO: Dataloader type: standard.
2024-06-21 16:24:51,212 - INFO: Train dataloader arguments.
2024-06-21 16:24:51,213 - INFO: 	Batch_size: 32.
2024-06-21 16:24:51,213 - INFO: 	Shuffle: True.
2024-06-21 16:24:51,213 - INFO: 	Sampler: None.
2024-06-21 16:24:51,213 - INFO: 	Num_workers: 4.
2024-06-21 16:24:51,213 - INFO: 	Drop_last: False.
2024-06-21 16:24:51,393 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 16:24:51,400 - INFO: Weight init name: kaiming_uniform.
2024-06-21 16:24:52,131 - INFO: Number of training iterations per epoch: 29.
2024-06-21 16:24:52,131 - INFO: Epoch 1/10...
2024-06-21 16:24:52,131 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:24:52,131 - INFO: Batch size: 32.
2024-06-21 16:24:52,132 - INFO: Dataset:
2024-06-21 16:24:52,132 - INFO: Batch size:
2024-06-21 16:24:52,132 - INFO: Number of workers:
2024-06-21 16:24:53,527 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 6.775
2024-06-21 16:24:53,870 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.832
2024-06-21 16:24:54,260 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 7.792
2024-06-21 16:24:54,565 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 8.903
2024-06-21 16:24:54,956 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 23.409
2024-06-21 16:24:55,289 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 43.268
2024-06-21 16:24:55,651 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 8.900
2024-06-21 16:24:55,953 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 19.853
2024-06-21 16:24:56,336 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 25.765
2024-06-21 16:24:56,674 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 8.882
2024-06-21 16:24:57,050 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 13.031
2024-06-21 16:24:57,354 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 11.384
2024-06-21 16:24:57,828 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 8.997
2024-06-21 16:24:58,186 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 7.336
2024-06-21 16:24:58,573 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 7.919
2024-06-21 16:24:58,873 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 7.177
2024-06-21 16:24:59,287 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.414
2024-06-21 16:24:59,632 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.664
2024-06-21 16:25:00,021 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 6.608
2024-06-21 16:25:00,320 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 7.054
2024-06-21 16:25:00,722 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 6.507
2024-06-21 16:25:01,067 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 7.339
2024-06-21 16:25:01,447 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 7.462
2024-06-21 16:25:01,749 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 6.860
2024-06-21 16:25:02,137 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 5.995
2024-06-21 16:25:02,475 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 6.987
2024-06-21 16:25:02,843 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 6.630
2024-06-21 16:25:03,143 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 6.420
2024-06-21 16:25:03,351 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 6.674
2024-06-21 16:25:14,487 - INFO: 1/10 final results:
2024-06-21 16:25:14,488 - INFO: Training loss: 10.512.
2024-06-21 16:25:14,488 - INFO: Training MAE: 10.588.
2024-06-21 16:25:14,488 - INFO: Training MSE: 232.094.
2024-06-21 16:25:34,626 - INFO: Epoch: 1/10, Loss_train: 10.511620685972016, Loss_val: 6.323357993158801
2024-06-21 16:25:34,626 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 16:25:34,626 - INFO: Epoch 2/10...
2024-06-21 16:25:34,626 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:25:34,626 - INFO: Batch size: 32.
2024-06-21 16:25:34,658 - INFO: Dataset:
2024-06-21 16:25:34,659 - INFO: Batch size:
2024-06-21 16:25:34,659 - INFO: Number of workers:
2024-06-21 16:25:35,877 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 6.013
2024-06-21 16:25:36,189 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 7.097
2024-06-21 16:25:36,593 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 5.869
2024-06-21 16:25:36,918 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 6.385
2024-06-21 16:25:37,351 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 7.730
2024-06-21 16:25:37,658 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 6.501
2024-06-21 16:25:38,051 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 6.822
2024-06-21 16:25:38,370 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 5.531
2024-06-21 16:25:38,798 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 5.469
2024-06-21 16:25:39,096 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 5.758
2024-06-21 16:25:39,480 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 5.397
2024-06-21 16:25:39,803 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 5.721
2024-06-21 16:25:40,244 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 5.700
2024-06-21 16:25:40,553 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 5.871
2024-06-21 16:25:40,955 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 6.470
2024-06-21 16:25:41,275 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 5.859
2024-06-21 16:25:41,713 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 5.812
2024-06-21 16:25:42,020 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 6.468
2024-06-21 16:25:42,413 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 6.004
2024-06-21 16:25:42,726 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 6.435
2024-06-21 16:25:43,149 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 7.002
2024-06-21 16:25:43,455 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 5.330
2024-06-21 16:25:43,836 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 5.726
2024-06-21 16:25:44,155 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 6.100
2024-06-21 16:25:44,578 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 5.278
2024-06-21 16:25:44,878 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 6.177
2024-06-21 16:25:45,258 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 5.790
2024-06-21 16:25:45,572 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 5.929
2024-06-21 16:25:45,788 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 5.557
2024-06-21 16:25:56,867 - INFO: 2/10 final results:
2024-06-21 16:25:56,867 - INFO: Training loss: 6.062.
2024-06-21 16:25:56,867 - INFO: Training MAE: 6.072.
2024-06-21 16:25:56,867 - INFO: Training MSE: 55.401.
2024-06-21 16:26:17,392 - INFO: Epoch: 2/10, Loss_train: 6.062143227149701, Loss_val: 6.347483026570287
2024-06-21 16:26:17,449 - INFO: Saved new best metric model for epoch 2.
2024-06-21 16:26:17,449 - INFO: Best internal validation val_loss: 6.347 at epoch: 2.
2024-06-21 16:26:17,449 - INFO: Epoch 3/10...
2024-06-21 16:26:17,449 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:26:17,449 - INFO: Batch size: 32.
2024-06-21 16:26:17,453 - INFO: Dataset:
2024-06-21 16:26:17,453 - INFO: Batch size:
2024-06-21 16:26:17,453 - INFO: Number of workers:
2024-06-21 16:26:18,654 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 5.307
2024-06-21 16:26:18,979 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 5.789
2024-06-21 16:26:19,380 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 5.074
2024-06-21 16:26:19,704 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 5.328
2024-06-21 16:26:20,124 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 5.491
2024-06-21 16:26:20,428 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 5.397
2024-06-21 16:26:20,817 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 5.022
2024-06-21 16:26:21,137 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 6.686
2024-06-21 16:26:21,557 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 6.101
2024-06-21 16:26:21,855 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 6.163
2024-06-21 16:26:22,234 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 5.790
2024-06-21 16:26:22,556 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 5.892
2024-06-21 16:26:22,988 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 5.781
2024-06-21 16:26:23,298 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 7.110
2024-06-21 16:26:23,700 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 5.618
2024-06-21 16:26:24,019 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 5.466
2024-06-21 16:26:24,457 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 5.316
2024-06-21 16:26:24,762 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 5.577
2024-06-21 16:26:25,152 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 5.870
2024-06-21 16:26:25,465 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 5.926
2024-06-21 16:26:25,890 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 5.583
2024-06-21 16:26:26,196 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 5.250
2024-06-21 16:26:26,590 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 5.292
2024-06-21 16:26:26,910 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 5.507
2024-06-21 16:26:27,337 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 5.336
2024-06-21 16:26:27,639 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 5.176
2024-06-21 16:26:28,029 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 5.356
2024-06-21 16:26:28,346 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 6.803
2024-06-21 16:26:28,571 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 4.894
2024-06-21 16:26:39,654 - INFO: 3/10 final results:
2024-06-21 16:26:39,654 - INFO: Training loss: 5.652.
2024-06-21 16:26:39,654 - INFO: Training MAE: 5.667.
2024-06-21 16:26:39,654 - INFO: Training MSE: 48.300.
2024-06-21 16:27:00,301 - INFO: Epoch: 3/10, Loss_train: 5.651721247311296, Loss_val: 5.574916231221166
2024-06-21 16:27:00,356 - INFO: Saved new best metric model for epoch 3.
2024-06-21 16:27:00,356 - INFO: Best internal validation val_loss: 5.575 at epoch: 3.
2024-06-21 16:27:00,357 - INFO: Epoch 4/10...
2024-06-21 16:27:00,357 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:27:00,357 - INFO: Batch size: 32.
2024-06-21 16:27:00,360 - INFO: Dataset:
2024-06-21 16:27:00,360 - INFO: Batch size:
2024-06-21 16:27:00,361 - INFO: Number of workers:
2024-06-21 16:27:01,551 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 4.544
2024-06-21 16:27:01,875 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 5.004
2024-06-21 16:27:02,288 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 5.331
2024-06-21 16:27:02,611 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 4.798
2024-06-21 16:27:03,017 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 5.558
2024-06-21 16:27:03,324 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 4.570
2024-06-21 16:27:03,732 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 4.761
2024-06-21 16:27:04,050 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 5.868
2024-06-21 16:27:04,467 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 5.309
2024-06-21 16:27:04,767 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 5.551
2024-06-21 16:27:05,164 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 4.931
2024-06-21 16:27:05,487 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 5.030
2024-06-21 16:27:05,902 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 5.899
2024-06-21 16:27:06,211 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 6.111
2024-06-21 16:27:06,628 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 6.160
2024-06-21 16:27:06,946 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 5.152
2024-06-21 16:27:07,355 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 4.792
2024-06-21 16:27:07,661 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 4.940
2024-06-21 16:27:08,050 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 4.624
2024-06-21 16:27:08,363 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 5.049
2024-06-21 16:27:08,758 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 5.500
2024-06-21 16:27:09,062 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 5.272
2024-06-21 16:27:09,456 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 5.880
2024-06-21 16:27:09,772 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 5.152
2024-06-21 16:27:10,174 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 5.447
2024-06-21 16:27:10,473 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 4.827
2024-06-21 16:27:10,854 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 5.551
2024-06-21 16:27:11,166 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 5.594
2024-06-21 16:27:11,375 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 4.360
2024-06-21 16:27:22,635 - INFO: 4/10 final results:
2024-06-21 16:27:22,635 - INFO: Training loss: 5.226.
2024-06-21 16:27:22,636 - INFO: Training MAE: 5.244.
2024-06-21 16:27:22,636 - INFO: Training MSE: 42.857.
2024-06-21 16:27:42,872 - INFO: Epoch: 4/10, Loss_train: 5.226376434852337, Loss_val: 5.360776884802457
2024-06-21 16:27:42,928 - INFO: Saved new best metric model for epoch 4.
2024-06-21 16:27:42,928 - INFO: Best internal validation val_loss: 5.361 at epoch: 4.
2024-06-21 16:27:42,928 - INFO: Epoch 5/10...
2024-06-21 16:27:42,928 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:27:42,929 - INFO: Batch size: 32.
2024-06-21 16:27:42,932 - INFO: Dataset:
2024-06-21 16:27:42,932 - INFO: Batch size:
2024-06-21 16:27:42,932 - INFO: Number of workers:
2024-06-21 16:27:44,121 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 4.610
2024-06-21 16:27:44,459 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 5.531
2024-06-21 16:27:44,850 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 5.866
2024-06-21 16:27:45,174 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 5.219
2024-06-21 16:27:45,578 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 4.956
2024-06-21 16:27:45,897 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 5.797
2024-06-21 16:27:46,284 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 5.857
2024-06-21 16:27:46,605 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 5.915
2024-06-21 16:27:47,025 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 5.317
2024-06-21 16:27:47,337 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 4.766
2024-06-21 16:27:47,716 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 4.813
2024-06-21 16:27:48,039 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 4.148
2024-06-21 16:27:48,466 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 4.695
2024-06-21 16:27:48,787 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 4.789
2024-06-21 16:27:49,175 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 5.333
2024-06-21 16:27:49,494 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 4.869
2024-06-21 16:27:49,896 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 5.337
2024-06-21 16:27:50,214 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 5.548
2024-06-21 16:27:50,602 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 5.236
2024-06-21 16:27:50,914 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 5.354
2024-06-21 16:27:51,327 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 4.908
2024-06-21 16:27:51,646 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 4.602
2024-06-21 16:27:52,038 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 5.451
2024-06-21 16:27:52,358 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 5.477
2024-06-21 16:27:52,772 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 5.193
2024-06-21 16:27:53,087 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 6.029
2024-06-21 16:27:53,470 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 5.255
2024-06-21 16:27:53,784 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 4.294
2024-06-21 16:27:54,006 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 5.196
2024-06-21 16:28:05,107 - INFO: 5/10 final results:
2024-06-21 16:28:05,107 - INFO: Training loss: 5.185.
2024-06-21 16:28:05,107 - INFO: Training MAE: 5.185.
2024-06-21 16:28:05,107 - INFO: Training MSE: 41.560.
2024-06-21 16:28:25,296 - INFO: Epoch: 5/10, Loss_train: 5.184808731079102, Loss_val: 5.410968089925832
2024-06-21 16:28:25,296 - INFO: Best internal validation val_loss: 5.361 at epoch: 4.
2024-06-21 16:28:25,296 - INFO: Epoch 6/10...
2024-06-21 16:28:25,296 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:28:25,296 - INFO: Batch size: 32.
2024-06-21 16:28:25,300 - INFO: Dataset:
2024-06-21 16:28:25,300 - INFO: Batch size:
2024-06-21 16:28:25,300 - INFO: Number of workers:
2024-06-21 16:28:26,498 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 5.308
2024-06-21 16:28:26,822 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 8.198
2024-06-21 16:28:27,223 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 4.969
2024-06-21 16:28:27,546 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 5.976
2024-06-21 16:28:27,964 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 4.583
2024-06-21 16:28:28,283 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 4.652
2024-06-21 16:28:28,671 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 4.682
2024-06-21 16:28:28,989 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 5.100
2024-06-21 16:28:29,408 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 5.047
2024-06-21 16:28:29,719 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 5.083
2024-06-21 16:28:30,101 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 4.735
2024-06-21 16:28:30,422 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 4.965
2024-06-21 16:28:30,842 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 5.168
2024-06-21 16:28:31,163 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 5.136
2024-06-21 16:28:31,560 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 4.564
2024-06-21 16:28:31,877 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 4.553
2024-06-21 16:28:32,294 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 4.339
2024-06-21 16:28:32,612 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 4.996
2024-06-21 16:28:33,004 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 4.443
2024-06-21 16:28:33,317 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 5.163
2024-06-21 16:28:33,730 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 5.594
2024-06-21 16:28:34,050 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 4.837
2024-06-21 16:28:34,439 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 4.763
2024-06-21 16:28:34,758 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 4.740
2024-06-21 16:28:35,172 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 4.758
2024-06-21 16:28:35,487 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 4.357
2024-06-21 16:28:35,879 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 4.577
2024-06-21 16:28:36,196 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 4.146
2024-06-21 16:28:36,411 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 5.756
2024-06-21 16:28:47,495 - INFO: 6/10 final results:
2024-06-21 16:28:47,495 - INFO: Training loss: 5.007.
2024-06-21 16:28:47,495 - INFO: Training MAE: 4.992.
2024-06-21 16:28:47,495 - INFO: Training MSE: 40.188.
2024-06-21 16:29:07,952 - INFO: Epoch: 6/10, Loss_train: 5.00652237596183, Loss_val: 5.125125728804489
2024-06-21 16:29:08,008 - INFO: Saved new best metric model for epoch 6.
2024-06-21 16:29:08,008 - INFO: Best internal validation val_loss: 5.125 at epoch: 6.
2024-06-21 16:29:08,008 - INFO: Epoch 7/10...
2024-06-21 16:29:08,008 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:29:08,008 - INFO: Batch size: 32.
2024-06-21 16:29:08,012 - INFO: Dataset:
2024-06-21 16:29:08,012 - INFO: Batch size:
2024-06-21 16:29:08,012 - INFO: Number of workers:
2024-06-21 16:29:09,232 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 4.683
2024-06-21 16:29:09,543 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 5.556
2024-06-21 16:29:09,944 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 4.408
2024-06-21 16:29:10,266 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 3.933
2024-06-21 16:29:10,698 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 4.471
2024-06-21 16:29:11,004 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 4.849
2024-06-21 16:29:11,397 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 6.539
2024-06-21 16:29:11,716 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 5.659
2024-06-21 16:29:12,147 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 5.066
2024-06-21 16:29:12,445 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 5.044
2024-06-21 16:29:12,829 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 3.908
2024-06-21 16:29:13,151 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 4.624
2024-06-21 16:29:13,587 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 4.262
2024-06-21 16:29:13,896 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 4.390
2024-06-21 16:29:14,297 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 4.387
2024-06-21 16:29:14,616 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 4.935
2024-06-21 16:29:15,054 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 4.422
2024-06-21 16:29:15,360 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 5.172
2024-06-21 16:29:15,750 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 5.646
2024-06-21 16:29:16,062 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 4.710
2024-06-21 16:29:16,486 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 3.888
2024-06-21 16:29:16,790 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 5.279
2024-06-21 16:29:17,178 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 4.206
2024-06-21 16:29:17,494 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 5.388
2024-06-21 16:29:17,916 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 4.635
2024-06-21 16:29:18,218 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 4.635
2024-06-21 16:29:18,598 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 4.814
2024-06-21 16:29:18,911 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 4.758
2024-06-21 16:29:19,132 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 4.600
2024-06-21 16:29:30,111 - INFO: 7/10 final results:
2024-06-21 16:29:30,111 - INFO: Training loss: 4.789.
2024-06-21 16:29:30,111 - INFO: Training MAE: 4.792.
2024-06-21 16:29:30,111 - INFO: Training MSE: 37.433.
2024-06-21 16:29:50,476 - INFO: Epoch: 7/10, Loss_train: 4.7885144332359575, Loss_val: 5.403349021385456
2024-06-21 16:29:50,476 - INFO: Best internal validation val_loss: 5.125 at epoch: 6.
2024-06-21 16:29:50,476 - INFO: Epoch 8/10...
2024-06-21 16:29:50,476 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:29:50,476 - INFO: Batch size: 32.
2024-06-21 16:29:50,479 - INFO: Dataset:
2024-06-21 16:29:50,479 - INFO: Batch size:
2024-06-21 16:29:50,479 - INFO: Number of workers:
2024-06-21 16:29:51,680 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 4.442
2024-06-21 16:29:51,989 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 4.378
2024-06-21 16:29:52,376 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 4.882
2024-06-21 16:29:52,696 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 4.808
2024-06-21 16:29:53,131 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 4.761
2024-06-21 16:29:53,438 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 4.867
2024-06-21 16:29:53,830 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 4.829
2024-06-21 16:29:54,138 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 4.056
2024-06-21 16:29:54,574 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 4.096
2024-06-21 16:29:54,873 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 5.583
2024-06-21 16:29:55,256 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 5.187
2024-06-21 16:29:55,566 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 4.338
2024-06-21 16:29:55,999 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 4.746
2024-06-21 16:29:56,309 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 6.926
2024-06-21 16:29:56,701 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 5.229
2024-06-21 16:29:57,005 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 4.824
2024-06-21 16:29:57,446 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 4.301
2024-06-21 16:29:57,750 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 5.031
2024-06-21 16:29:58,139 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 4.741
2024-06-21 16:29:58,439 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 4.553
2024-06-21 16:29:58,868 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 4.435
2024-06-21 16:29:59,176 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 4.456
2024-06-21 16:29:59,558 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 4.225
2024-06-21 16:29:59,865 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 4.740
2024-06-21 16:30:00,296 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 4.229
2024-06-21 16:30:00,599 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 4.547
2024-06-21 16:30:00,975 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 4.373
2024-06-21 16:30:01,278 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 4.318
2024-06-21 16:30:01,490 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 3.281
2024-06-21 16:30:12,631 - INFO: 8/10 final results:
2024-06-21 16:30:12,631 - INFO: Training loss: 4.661.
2024-06-21 16:30:12,631 - INFO: Training MAE: 4.689.
2024-06-21 16:30:12,631 - INFO: Training MSE: 35.767.
2024-06-21 16:30:33,351 - INFO: Epoch: 8/10, Loss_train: 4.661396750088396, Loss_val: 4.802359580993652
2024-06-21 16:30:33,409 - INFO: Saved new best metric model for epoch 8.
2024-06-21 16:30:33,409 - INFO: Best internal validation val_loss: 4.802 at epoch: 8.
2024-06-21 16:30:33,409 - INFO: Epoch 9/10...
2024-06-21 16:30:33,409 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:30:33,409 - INFO: Batch size: 32.
2024-06-21 16:30:33,413 - INFO: Dataset:
2024-06-21 16:30:33,413 - INFO: Batch size:
2024-06-21 16:30:33,413 - INFO: Number of workers:
2024-06-21 16:30:34,615 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 4.355
2024-06-21 16:30:34,962 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 4.917
2024-06-21 16:30:35,352 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 4.344
2024-06-21 16:30:35,676 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 5.051
2024-06-21 16:30:36,083 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 3.715
2024-06-21 16:30:36,426 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 3.860
2024-06-21 16:30:36,805 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 4.020
2024-06-21 16:30:37,111 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 4.466
2024-06-21 16:30:37,505 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 4.031
2024-06-21 16:30:37,855 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 4.760
2024-06-21 16:30:38,223 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 5.049
2024-06-21 16:30:38,533 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 4.304
2024-06-21 16:30:38,946 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 3.903
2024-06-21 16:30:39,294 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 3.980
2024-06-21 16:30:39,682 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 4.224
2024-06-21 16:30:39,988 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 3.572
2024-06-21 16:30:40,396 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 4.287
2024-06-21 16:30:40,741 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 4.160
2024-06-21 16:30:41,117 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 3.855
2024-06-21 16:30:41,418 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 4.116
2024-06-21 16:30:41,817 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 4.090
2024-06-21 16:30:42,161 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 3.354
2024-06-21 16:30:42,540 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 4.009
2024-06-21 16:30:42,847 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 4.296
2024-06-21 16:30:43,238 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 4.644
2024-06-21 16:30:43,578 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 4.165
2024-06-21 16:30:43,951 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 3.587
2024-06-21 16:30:44,253 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 3.994
2024-06-21 16:30:44,463 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 3.553
2024-06-21 16:30:55,606 - INFO: 9/10 final results:
2024-06-21 16:30:55,606 - INFO: Training loss: 4.161.
2024-06-21 16:30:55,606 - INFO: Training MAE: 4.173.
2024-06-21 16:30:55,606 - INFO: Training MSE: 28.777.
2024-06-21 16:31:16,107 - INFO: Epoch: 9/10, Loss_train: 4.160736758133461, Loss_val: 4.7125867892955915
2024-06-21 16:31:16,164 - INFO: Saved new best metric model for epoch 9.
2024-06-21 16:31:16,165 - INFO: Best internal validation val_loss: 4.713 at epoch: 9.
2024-06-21 16:31:16,165 - INFO: Epoch 10/10...
2024-06-21 16:31:16,165 - INFO: Learning rate: 0.00043653266681613063.
2024-06-21 16:31:16,165 - INFO: Batch size: 32.
2024-06-21 16:31:16,168 - INFO: Dataset:
2024-06-21 16:31:16,168 - INFO: Batch size:
2024-06-21 16:31:16,168 - INFO: Number of workers:
2024-06-21 16:31:17,366 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 4.321
2024-06-21 16:31:17,675 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 3.603
2024-06-21 16:31:18,075 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 4.622
2024-06-21 16:31:18,399 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 4.313
2024-06-21 16:31:18,828 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 3.752
2024-06-21 16:31:19,135 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 3.895
2024-06-21 16:31:19,523 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 3.610
2024-06-21 16:31:19,843 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 4.533
2024-06-21 16:31:20,271 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 4.170
2024-06-21 16:31:20,570 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 3.978
2024-06-21 16:31:20,947 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 3.901
2024-06-21 16:31:21,271 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 3.442
2024-06-21 16:31:21,708 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 3.623
2024-06-21 16:31:22,017 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 3.594
2024-06-21 16:31:22,414 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 3.397
2024-06-21 16:31:22,731 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 3.439
2024-06-21 16:31:23,162 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 4.854
2024-06-21 16:31:23,467 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 4.213
2024-06-21 16:31:23,856 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 5.405
2024-06-21 16:31:24,169 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 3.429
2024-06-21 16:31:24,592 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 3.819
2024-06-21 16:31:24,900 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 4.004
2024-06-21 16:31:25,292 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 3.977
2024-06-21 16:31:25,614 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 4.044
2024-06-21 16:31:26,032 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 3.343
2024-06-21 16:31:26,335 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 4.092
2024-06-21 16:31:26,717 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 3.829
2024-06-21 16:31:27,033 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 4.031
2024-06-21 16:31:27,250 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 3.315
2024-06-21 16:31:38,500 - INFO: 10/10 final results:
2024-06-21 16:31:38,500 - INFO: Training loss: 3.950.
2024-06-21 16:31:38,500 - INFO: Training MAE: 3.963.
2024-06-21 16:31:38,500 - INFO: Training MSE: 26.420.
2024-06-21 16:31:58,843 - INFO: Epoch: 10/10, Loss_train: 3.949955134556211, Loss_val: 5.795191468863652
2024-06-21 16:31:58,843 - INFO: Best internal validation val_loss: 4.713 at epoch: 9.
