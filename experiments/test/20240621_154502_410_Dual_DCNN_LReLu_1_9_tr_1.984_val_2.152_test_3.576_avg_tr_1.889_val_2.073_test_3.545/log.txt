2024-06-21 15:25:26,450 - INFO: Device: cuda.
2024-06-21 15:25:26,450 - INFO: Torch version: 2.0.1+cu117.
2024-06-21 15:25:26,450 - INFO: Torch.backends.cudnn.benchmark: True.
2024-06-21 15:25:26,450 - INFO: Model name: Dual_DCNN_LReLu
2024-06-21 15:25:26,450 - INFO: Seed: 4
2024-06-21 15:25:26,450 - INFO: 42 patients have been found in the data directory.
2024-06-21 15:25:26,489 - INFO: Train set contains 32 patients.
2024-06-21 15:25:26,489 - INFO: Val set contains 5 patients.
2024-06-21 15:25:26,489 - INFO: Test set contains 5 patients.
2024-06-21 15:25:26,489 - INFO: Fold: 0
2024-06-21 15:25:26,489 - INFO: Performing 2-fold Cross Validation.
2024-06-21 15:25:26,490 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 15:25:26,490 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 15:25:26,490 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 15:25:26,621 - INFO: To_device: False.
2024-06-21 15:25:26,622 - INFO: Transformers have been made successfully.
2024-06-21 15:25:26,622 - INFO: Dataset type: cache.
2024-06-21 15:25:26,622 - INFO: Dataloader type: standard.
2024-06-21 15:27:17,320 - INFO: Train dataloader arguments.
2024-06-21 15:27:17,320 - INFO: 	Batch_size: 32.
2024-06-21 15:27:17,320 - INFO: 	Shuffle: True.
2024-06-21 15:27:17,320 - INFO: 	Sampler: None.
2024-06-21 15:27:17,320 - INFO: 	Num_workers: 4.
2024-06-21 15:27:17,320 - INFO: 	Drop_last: False.
2024-06-21 15:27:17,371 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 15:27:18,226 - INFO: Weight init name: kaiming_uniform.
2024-06-21 15:27:20,647 - INFO: Number of training iterations per epoch: 29.
2024-06-21 15:27:20,648 - INFO: Epoch 1/10...
2024-06-21 15:27:20,648 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:27:20,648 - INFO: Batch size: 32.
2024-06-21 15:27:20,648 - INFO: Dataset:
2024-06-21 15:27:20,648 - INFO: Batch size:
2024-06-21 15:27:20,648 - INFO: Number of workers:
2024-06-21 15:27:23,632 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 7.229
2024-06-21 15:27:23,939 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.806
2024-06-21 15:27:24,335 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 6.945
2024-06-21 15:27:24,643 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 6.812
2024-06-21 15:27:25,046 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 8.449
2024-06-21 15:27:25,362 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 9.550
2024-06-21 15:27:25,763 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 8.349
2024-06-21 15:27:26,080 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 8.250
2024-06-21 15:27:26,475 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 7.214
2024-06-21 15:27:26,784 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 8.058
2024-06-21 15:27:27,179 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 8.243
2024-06-21 15:27:27,497 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 6.761
2024-06-21 15:27:27,905 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 7.436
2024-06-21 15:27:28,223 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 6.567
2024-06-21 15:27:28,630 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 7.483
2024-06-21 15:27:28,945 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 6.872
2024-06-21 15:27:29,353 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.447
2024-06-21 15:27:29,668 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.072
2024-06-21 15:27:30,069 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 6.995
2024-06-21 15:27:30,379 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 6.700
2024-06-21 15:27:30,777 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 6.843
2024-06-21 15:27:31,093 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 6.861
2024-06-21 15:27:31,488 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 6.372
2024-06-21 15:27:31,804 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 6.393
2024-06-21 15:27:32,195 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 5.508
2024-06-21 15:27:32,507 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 6.639
2024-06-21 15:27:32,897 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 5.952
2024-06-21 15:27:33,208 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 6.516
2024-06-21 15:27:34,488 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 5.062
2024-06-21 15:27:45,531 - INFO: 1/10 final results:
2024-06-21 15:27:45,531 - INFO: Training loss: 7.048.
2024-06-21 15:27:45,531 - INFO: Training MAE: 7.087.
2024-06-21 15:27:45,531 - INFO: Training MSE: 71.907.
2024-06-21 15:28:06,096 - INFO: Epoch: 1/10, Loss_train: 7.047655533099997, Loss_val: 6.851869352932634
2024-06-21 15:28:06,096 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 15:28:06,096 - INFO: Epoch 2/10...
2024-06-21 15:28:06,096 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:28:06,096 - INFO: Batch size: 32.
2024-06-21 15:28:06,099 - INFO: Dataset:
2024-06-21 15:28:06,100 - INFO: Batch size:
2024-06-21 15:28:06,100 - INFO: Number of workers:
2024-06-21 15:28:07,251 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 5.119
2024-06-21 15:28:07,559 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 5.511
2024-06-21 15:28:07,967 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 5.132
2024-06-21 15:28:08,287 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 6.242
2024-06-21 15:28:08,690 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 4.371
2024-06-21 15:28:08,992 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 5.297
2024-06-21 15:28:09,391 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 5.012
2024-06-21 15:28:09,707 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 5.118
2024-06-21 15:28:10,120 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 5.990
2024-06-21 15:28:10,417 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 4.467
2024-06-21 15:28:10,813 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 5.014
2024-06-21 15:28:11,135 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 6.208
2024-06-21 15:28:11,560 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 4.995
2024-06-21 15:28:11,868 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 4.892
2024-06-21 15:28:12,271 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 5.677
2024-06-21 15:28:12,591 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 5.251
2024-06-21 15:28:13,004 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 4.691
2024-06-21 15:28:13,306 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 5.403
2024-06-21 15:28:13,704 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 5.481
2024-06-21 15:28:14,014 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 4.468
2024-06-21 15:28:14,427 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 4.918
2024-06-21 15:28:14,734 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 5.313
2024-06-21 15:28:15,127 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 5.227
2024-06-21 15:28:15,447 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 5.243
2024-06-21 15:28:15,852 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 4.328
2024-06-21 15:28:16,155 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 4.121
2024-06-21 15:28:16,550 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 4.549
2024-06-21 15:28:16,864 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 4.861
2024-06-21 15:28:17,076 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 5.799
2024-06-21 15:28:28,243 - INFO: 2/10 final results:
2024-06-21 15:28:28,244 - INFO: Training loss: 5.128.
2024-06-21 15:28:28,244 - INFO: Training MAE: 5.114.
2024-06-21 15:28:28,244 - INFO: Training MSE: 40.514.
2024-06-21 15:28:48,316 - INFO: Epoch: 2/10, Loss_train: 5.127507390647099, Loss_val: 4.839470731801
2024-06-21 15:28:48,362 - INFO: Saved new best metric model for epoch 2.
2024-06-21 15:28:48,363 - INFO: Best internal validation val_loss: 4.839 at epoch: 2.
2024-06-21 15:28:48,363 - INFO: Epoch 3/10...
2024-06-21 15:28:48,363 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:28:48,363 - INFO: Batch size: 32.
2024-06-21 15:28:48,366 - INFO: Dataset:
2024-06-21 15:28:48,366 - INFO: Batch size:
2024-06-21 15:28:48,366 - INFO: Number of workers:
2024-06-21 15:28:49,516 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 4.840
2024-06-21 15:28:49,825 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 4.721
2024-06-21 15:28:50,238 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 4.782
2024-06-21 15:28:50,560 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 4.563
2024-06-21 15:28:50,972 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 4.686
2024-06-21 15:28:51,277 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 3.913
2024-06-21 15:28:51,674 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 4.051
2024-06-21 15:28:51,995 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 4.193
2024-06-21 15:28:52,710 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 4.237
2024-06-21 15:28:53,012 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 4.293
2024-06-21 15:28:53,400 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 3.881
2024-06-21 15:28:53,720 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 4.040
2024-06-21 15:28:54,144 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 4.406
2024-06-21 15:28:54,453 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 5.337
2024-06-21 15:28:54,849 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 3.929
2024-06-21 15:28:55,162 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 4.254
2024-06-21 15:28:55,578 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 4.319
2024-06-21 15:28:55,881 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 4.844
2024-06-21 15:28:56,284 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 4.254
2024-06-21 15:28:56,596 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 4.416
2024-06-21 15:28:57,004 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 4.356
2024-06-21 15:28:57,314 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 3.969
2024-06-21 15:28:57,716 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 4.152
2024-06-21 15:28:58,037 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 4.158
2024-06-21 15:28:58,446 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 4.052
2024-06-21 15:28:58,750 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 4.583
2024-06-21 15:28:59,147 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 3.912
2024-06-21 15:28:59,464 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 3.693
2024-06-21 15:28:59,686 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 5.113
2024-06-21 15:29:10,563 - INFO: 3/10 final results:
2024-06-21 15:29:10,564 - INFO: Training loss: 4.343.
2024-06-21 15:29:10,564 - INFO: Training MAE: 4.328.
2024-06-21 15:29:10,564 - INFO: Training MSE: 30.820.
2024-06-21 15:29:31,102 - INFO: Epoch: 3/10, Loss_train: 4.342985778019346, Loss_val: 4.020687818527222
2024-06-21 15:29:31,160 - INFO: Saved new best metric model for epoch 3.
2024-06-21 15:29:31,160 - INFO: Best internal validation val_loss: 4.021 at epoch: 3.
2024-06-21 15:29:31,160 - INFO: Epoch 4/10...
2024-06-21 15:29:31,160 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:29:31,160 - INFO: Batch size: 32.
2024-06-21 15:29:31,163 - INFO: Dataset:
2024-06-21 15:29:31,163 - INFO: Batch size:
2024-06-21 15:29:31,163 - INFO: Number of workers:
2024-06-21 15:29:32,319 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 3.491
2024-06-21 15:29:32,629 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 3.491
2024-06-21 15:29:33,027 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 3.257
2024-06-21 15:29:33,348 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 4.229
2024-06-21 15:29:33,773 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 4.010
2024-06-21 15:29:34,077 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 2.921
2024-06-21 15:29:34,461 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 4.249
2024-06-21 15:29:34,782 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 3.859
2024-06-21 15:29:35,219 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 3.300
2024-06-21 15:29:35,520 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 3.518
2024-06-21 15:29:35,907 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 3.228
2024-06-21 15:29:36,230 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 3.424
2024-06-21 15:29:36,685 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 4.188
2024-06-21 15:29:36,991 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 4.367
2024-06-21 15:29:37,392 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 4.006
2024-06-21 15:29:37,699 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 3.719
2024-06-21 15:29:38,148 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 4.267
2024-06-21 15:29:38,454 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 3.793
2024-06-21 15:29:38,848 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 3.472
2024-06-21 15:29:39,151 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 3.674
2024-06-21 15:29:39,586 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 3.913
2024-06-21 15:29:39,893 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 4.277
2024-06-21 15:29:40,278 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 3.690
2024-06-21 15:29:40,585 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 4.431
2024-06-21 15:29:41,015 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 3.794
2024-06-21 15:29:41,317 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 4.154
2024-06-21 15:29:41,704 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 4.470
2024-06-21 15:29:42,006 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 4.091
2024-06-21 15:29:42,223 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 3.913
2024-06-21 15:29:53,390 - INFO: 4/10 final results:
2024-06-21 15:29:53,390 - INFO: Training loss: 3.834.
2024-06-21 15:29:53,390 - INFO: Training MAE: 3.833.
2024-06-21 15:29:53,390 - INFO: Training MSE: 25.643.
2024-06-21 15:30:13,824 - INFO: Epoch: 4/10, Loss_train: 3.8343281170417525, Loss_val: 4.090719765630261
2024-06-21 15:30:13,824 - INFO: Best internal validation val_loss: 4.021 at epoch: 3.
2024-06-21 15:30:13,824 - INFO: Epoch 5/10...
2024-06-21 15:30:13,824 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:30:13,824 - INFO: Batch size: 32.
2024-06-21 15:30:13,827 - INFO: Dataset:
2024-06-21 15:30:13,827 - INFO: Batch size:
2024-06-21 15:30:13,827 - INFO: Number of workers:
2024-06-21 15:30:15,003 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 3.954
2024-06-21 15:30:15,316 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 3.961
2024-06-21 15:30:15,709 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 2.851
2024-06-21 15:30:16,034 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 3.814
2024-06-21 15:30:16,450 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 3.497
2024-06-21 15:30:16,782 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 3.532
2024-06-21 15:30:17,166 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 3.159
2024-06-21 15:30:17,473 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 4.321
2024-06-21 15:30:17,889 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 2.945
2024-06-21 15:30:18,215 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 3.778
2024-06-21 15:30:18,591 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 3.844
2024-06-21 15:30:18,902 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 3.555
2024-06-21 15:30:19,329 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 3.408
2024-06-21 15:30:19,663 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 2.856
2024-06-21 15:30:20,050 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 3.238
2024-06-21 15:30:20,355 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 3.375
2024-06-21 15:30:20,774 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 2.758
2024-06-21 15:30:21,105 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 2.980
2024-06-21 15:30:21,482 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 3.173
2024-06-21 15:30:21,784 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 3.116
2024-06-21 15:30:22,193 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 3.698
2024-06-21 15:30:22,522 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 4.077
2024-06-21 15:30:22,898 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 3.391
2024-06-21 15:30:23,201 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 3.613
2024-06-21 15:30:23,605 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 2.840
2024-06-21 15:30:23,928 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 3.247
2024-06-21 15:30:24,304 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 2.975
2024-06-21 15:30:24,604 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 2.890
2024-06-21 15:30:24,813 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 3.403
2024-06-21 15:30:35,957 - INFO: 5/10 final results:
2024-06-21 15:30:35,957 - INFO: Training loss: 3.388.
2024-06-21 15:30:35,957 - INFO: Training MAE: 3.388.
2024-06-21 15:30:35,957 - INFO: Training MSE: 19.746.
2024-06-21 15:30:56,448 - INFO: Epoch: 5/10, Loss_train: 3.38784593549268, Loss_val: 3.0535319015897553
2024-06-21 15:30:56,505 - INFO: Saved new best metric model for epoch 5.
2024-06-21 15:30:56,506 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:30:56,506 - INFO: Epoch 6/10...
2024-06-21 15:30:56,506 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:30:56,506 - INFO: Batch size: 32.
2024-06-21 15:30:56,509 - INFO: Dataset:
2024-06-21 15:30:56,509 - INFO: Batch size:
2024-06-21 15:30:56,509 - INFO: Number of workers:
2024-06-21 15:30:57,650 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 2.906
2024-06-21 15:30:57,999 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 2.817
2024-06-21 15:30:58,404 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 2.691
2024-06-21 15:30:58,728 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 2.784
2024-06-21 15:30:59,135 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 2.922
2024-06-21 15:30:59,481 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 2.484
2024-06-21 15:30:59,876 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 3.049
2024-06-21 15:31:00,184 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 3.086
2024-06-21 15:31:00,578 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 2.467
2024-06-21 15:31:00,926 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 4.275
2024-06-21 15:31:01,302 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 2.427
2024-06-21 15:31:01,613 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 3.115
2024-06-21 15:31:02,026 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 3.205
2024-06-21 15:31:02,374 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 3.658
2024-06-21 15:31:02,777 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 3.282
2024-06-21 15:31:03,082 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 3.243
2024-06-21 15:31:03,489 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 2.718
2024-06-21 15:31:03,832 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 2.473
2024-06-21 15:31:04,211 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 2.631
2024-06-21 15:31:04,513 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 2.800
2024-06-21 15:31:04,912 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 3.404
2024-06-21 15:31:05,257 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 2.291
2024-06-21 15:31:05,650 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 3.252
2024-06-21 15:31:05,957 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 3.957
2024-06-21 15:31:06,359 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 2.385
2024-06-21 15:31:06,698 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 2.570
2024-06-21 15:31:07,089 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 2.359
2024-06-21 15:31:07,391 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 2.624
2024-06-21 15:31:07,615 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 3.141
2024-06-21 15:31:18,701 - INFO: 6/10 final results:
2024-06-21 15:31:18,701 - INFO: Training loss: 2.932.
2024-06-21 15:31:18,701 - INFO: Training MAE: 2.927.
2024-06-21 15:31:18,701 - INFO: Training MSE: 14.923.
2024-06-21 15:31:38,934 - INFO: Epoch: 6/10, Loss_train: 2.931596953293373, Loss_val: 4.9731613520918225
2024-06-21 15:31:38,934 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:31:38,934 - INFO: Epoch 7/10...
2024-06-21 15:31:38,934 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:31:38,934 - INFO: Batch size: 32.
2024-06-21 15:31:38,937 - INFO: Dataset:
2024-06-21 15:31:38,937 - INFO: Batch size:
2024-06-21 15:31:38,937 - INFO: Number of workers:
2024-06-21 15:31:40,082 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 5.409
2024-06-21 15:31:40,392 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 3.119
2024-06-21 15:31:40,812 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 2.452
2024-06-21 15:31:41,120 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 2.651
2024-06-21 15:31:41,535 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 3.319
2024-06-21 15:31:41,839 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 2.424
2024-06-21 15:31:42,239 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 2.787
2024-06-21 15:31:42,542 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 2.324
2024-06-21 15:31:42,951 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 2.731
2024-06-21 15:31:43,249 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 2.091
2024-06-21 15:31:43,656 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 2.289
2024-06-21 15:31:43,963 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 2.732
2024-06-21 15:31:44,385 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 3.488
2024-06-21 15:31:44,691 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 2.744
2024-06-21 15:31:45,117 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 2.526
2024-06-21 15:31:45,420 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 2.931
2024-06-21 15:31:45,823 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 2.448
2024-06-21 15:31:46,125 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 2.437
2024-06-21 15:31:46,535 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 2.296
2024-06-21 15:31:46,833 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 2.310
2024-06-21 15:31:47,233 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 1.822
2024-06-21 15:31:47,537 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 2.968
2024-06-21 15:31:47,960 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 2.694
2024-06-21 15:31:48,266 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 2.195
2024-06-21 15:31:48,654 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 2.415
2024-06-21 15:31:48,955 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 2.102
2024-06-21 15:31:49,357 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 2.765
2024-06-21 15:31:49,658 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 2.902
2024-06-21 15:31:49,866 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 2.513
2024-06-21 15:32:01,141 - INFO: 7/10 final results:
2024-06-21 15:32:01,141 - INFO: Training loss: 2.686.
2024-06-21 15:32:01,141 - INFO: Training MAE: 2.689.
2024-06-21 15:32:01,141 - INFO: Training MSE: 13.348.
2024-06-21 15:32:21,797 - INFO: Epoch: 7/10, Loss_train: 2.685634292405227, Loss_val: 3.2043060598702264
2024-06-21 15:32:21,797 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:32:21,797 - INFO: Epoch 8/10...
2024-06-21 15:32:21,797 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:32:21,797 - INFO: Batch size: 32.
2024-06-21 15:32:21,800 - INFO: Dataset:
2024-06-21 15:32:21,800 - INFO: Batch size:
2024-06-21 15:32:21,800 - INFO: Number of workers:
2024-06-21 15:32:22,961 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 3.029
2024-06-21 15:32:23,274 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 3.508
2024-06-21 15:32:23,691 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 2.667
2024-06-21 15:32:24,015 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 3.325
2024-06-21 15:32:24,438 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 2.703
2024-06-21 15:32:24,745 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 2.334
2024-06-21 15:32:25,150 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 2.546
2024-06-21 15:32:25,471 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 3.780
2024-06-21 15:32:25,886 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 2.700
2024-06-21 15:32:26,187 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 2.555
2024-06-21 15:32:26,589 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 2.248
2024-06-21 15:32:26,911 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 3.220
2024-06-21 15:32:27,338 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 2.381
2024-06-21 15:32:27,647 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 2.063
2024-06-21 15:32:28,056 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 2.399
2024-06-21 15:32:28,374 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 2.840
2024-06-21 15:32:28,800 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 2.147
2024-06-21 15:32:29,105 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 2.387
2024-06-21 15:32:29,508 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 2.608
2024-06-21 15:32:29,821 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 2.350
2024-06-21 15:32:30,239 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 1.753
2024-06-21 15:32:30,546 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 1.822
2024-06-21 15:32:30,950 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 2.744
2024-06-21 15:32:31,271 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 2.686
2024-06-21 15:32:31,684 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 3.013
2024-06-21 15:32:31,987 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 2.193
2024-06-21 15:32:32,390 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 2.395
2024-06-21 15:32:32,705 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 1.942
2024-06-21 15:32:32,928 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 2.429
2024-06-21 15:32:44,087 - INFO: 8/10 final results:
2024-06-21 15:32:44,087 - INFO: Training loss: 2.578.
2024-06-21 15:32:44,087 - INFO: Training MAE: 2.581.
2024-06-21 15:32:44,087 - INFO: Training MSE: 12.021.
2024-06-21 15:33:04,350 - INFO: Epoch: 8/10, Loss_train: 2.5781373319954706, Loss_val: 3.772336425452397
2024-06-21 15:33:04,350 - INFO: Best internal validation val_loss: 3.054 at epoch: 5.
2024-06-21 15:33:04,350 - INFO: Epoch 9/10...
2024-06-21 15:33:04,350 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:33:04,350 - INFO: Batch size: 32.
2024-06-21 15:33:04,353 - INFO: Dataset:
2024-06-21 15:33:04,354 - INFO: Batch size:
2024-06-21 15:33:04,354 - INFO: Number of workers:
2024-06-21 15:33:05,496 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 3.416
2024-06-21 15:33:05,822 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 2.396
2024-06-21 15:33:06,212 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 2.147
2024-06-21 15:33:06,534 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 2.094
2024-06-21 15:33:06,949 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 1.805
2024-06-21 15:33:07,268 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 2.131
2024-06-21 15:33:07,651 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 2.455
2024-06-21 15:33:07,970 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 2.230
2024-06-21 15:33:08,372 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 3.033
2024-06-21 15:33:08,686 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 2.183
2024-06-21 15:33:09,071 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 3.031
2024-06-21 15:33:09,393 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 1.932
2024-06-21 15:33:09,815 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 2.006
2024-06-21 15:33:10,137 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 2.141
2024-06-21 15:33:10,534 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 2.453
2024-06-21 15:33:10,851 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 2.325
2024-06-21 15:33:11,270 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 2.811
2024-06-21 15:33:11,585 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 1.685
2024-06-21 15:33:11,970 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 2.183
2024-06-21 15:33:12,280 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 2.549
2024-06-21 15:33:12,697 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 2.235
2024-06-21 15:33:13,019 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 2.054
2024-06-21 15:33:13,404 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 3.160
2024-06-21 15:33:13,725 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 1.873
2024-06-21 15:33:14,135 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 2.973
2024-06-21 15:33:14,452 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 3.006
2024-06-21 15:33:14,843 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 1.862
2024-06-21 15:33:15,159 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 2.521
2024-06-21 15:33:15,382 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 2.281
2024-06-21 15:33:26,557 - INFO: 9/10 final results:
2024-06-21 15:33:26,557 - INFO: Training loss: 2.378.
2024-06-21 15:33:26,557 - INFO: Training MAE: 2.380.
2024-06-21 15:33:26,557 - INFO: Training MSE: 10.250.
2024-06-21 15:33:46,960 - INFO: Epoch: 9/10, Loss_train: 2.3783312994858314, Loss_val: 2.082735842671888
2024-06-21 15:33:47,017 - INFO: Saved new best metric model for epoch 9.
2024-06-21 15:33:47,017 - INFO: Best internal validation val_loss: 2.083 at epoch: 9.
2024-06-21 15:33:47,017 - INFO: Epoch 10/10...
2024-06-21 15:33:47,017 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:33:47,017 - INFO: Batch size: 32.
2024-06-21 15:33:47,020 - INFO: Dataset:
2024-06-21 15:33:47,021 - INFO: Batch size:
2024-06-21 15:33:47,021 - INFO: Number of workers:
2024-06-21 15:33:48,154 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 1.792
2024-06-21 15:33:48,494 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 2.430
2024-06-21 15:33:48,898 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 2.247
2024-06-21 15:33:49,221 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 2.187
2024-06-21 15:33:49,643 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 2.329
2024-06-21 15:33:49,962 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 2.394
2024-06-21 15:33:50,355 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 2.076
2024-06-21 15:33:50,674 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 1.813
2024-06-21 15:33:51,093 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 1.734
2024-06-21 15:33:51,407 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 2.864
2024-06-21 15:33:51,799 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 2.887
2024-06-21 15:33:52,120 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 2.368
2024-06-21 15:33:52,544 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 2.876
2024-06-21 15:33:52,866 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 2.181
2024-06-21 15:33:53,268 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 2.771
2024-06-21 15:33:53,586 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 1.786
2024-06-21 15:33:54,006 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 2.567
2024-06-21 15:33:54,323 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 2.620
2024-06-21 15:33:54,717 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 2.299
2024-06-21 15:33:55,030 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 2.421
2024-06-21 15:33:55,444 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 2.156
2024-06-21 15:33:55,762 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 2.513
2024-06-21 15:33:56,145 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 1.887
2024-06-21 15:33:56,462 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 1.913
2024-06-21 15:33:56,864 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 2.995
2024-06-21 15:33:57,177 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 2.446
2024-06-21 15:33:57,550 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 1.864
2024-06-21 15:33:57,862 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 1.858
2024-06-21 15:33:58,072 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 2.008
2024-06-21 15:34:09,153 - INFO: 10/10 final results:
2024-06-21 15:34:09,153 - INFO: Training loss: 2.286.
2024-06-21 15:34:09,153 - INFO: Training MAE: 2.291.
2024-06-21 15:34:09,153 - INFO: Training MSE: 9.325.
2024-06-21 15:34:29,401 - INFO: Epoch: 10/10, Loss_train: 2.2856254413210113, Loss_val: 1.9951291783102627
2024-06-21 15:34:29,458 - INFO: Saved new best metric model for epoch 10.
2024-06-21 15:34:29,458 - INFO: Best internal validation val_loss: 1.995 at epoch: 10.
2024-06-21 15:35:14,514 - INFO: Experiment has been saved in 20240621_153514_341_Dual_DCNN_LReLu_0_10_tr_1.795_val_1.995_test_3.515 folder.
2024-06-21 15:35:14,514 - INFO: Fold: 1
2024-06-21 15:35:14,515 - INFO: Train Set: 910/1970, percent: 0.462
2024-06-21 15:35:14,515 - INFO: Val Set: 910/1970, percent: 0.462
2024-06-21 15:35:14,515 - INFO: Test Set: 150/1970, percent: 0.076
2024-06-21 15:35:14,647 - INFO: To_device: False.
2024-06-21 15:35:14,648 - INFO: Transformers have been made successfully.
2024-06-21 15:35:14,648 - INFO: Dataset type: cache.
2024-06-21 15:35:14,648 - INFO: Dataloader type: standard.
2024-06-21 15:37:04,691 - INFO: Train dataloader arguments.
2024-06-21 15:37:04,691 - INFO: 	Batch_size: 32.
2024-06-21 15:37:04,692 - INFO: 	Shuffle: True.
2024-06-21 15:37:04,692 - INFO: 	Sampler: None.
2024-06-21 15:37:04,692 - INFO: 	Num_workers: 4.
2024-06-21 15:37:04,692 - INFO: 	Drop_last: False.
2024-06-21 15:37:04,885 - INFO: Dual_DCNN_LReLU(
  (conv_blocks_fixed): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (conv_blocks_moving): ModuleList(
    (0): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(1, 8, kernel_size=(1, 6, 6), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 6, 6), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (1): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (2): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(8, 16, kernel_size=(1, 3, 3), stride=(2, 2, 2))
      (norm1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
    (3): conv_block_two(
      (pad1): conv3d_padding_same()
      (conv1): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation1): LeakyReLU(negative_slope=0.1)
      (pad2): conv3d_padding_same()
      (conv2): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1))
      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation2): LeakyReLU(negative_slope=0.1)
      (max_pooling): MaxPool3d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (linear_layers): ModuleList(
    (0): Dropout(p=0.35973448489278154, inplace=False)
    (1): Linear(in_features=262144, out_features=32, bias=True)
  )
  (out_layer): Output(
    in_features=32, out_features=3, bias=True
    (fc): Linear(in_features=32, out_features=3, bias=True)
  )
)
2024-06-21 15:37:04,892 - INFO: Weight init name: kaiming_uniform.
2024-06-21 15:37:05,624 - INFO: Number of training iterations per epoch: 29.
2024-06-21 15:37:05,624 - INFO: Epoch 1/10...
2024-06-21 15:37:05,624 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:37:05,624 - INFO: Batch size: 32.
2024-06-21 15:37:05,625 - INFO: Dataset:
2024-06-21 15:37:05,625 - INFO: Batch size:
2024-06-21 15:37:05,625 - INFO: Number of workers:
2024-06-21 15:37:06,965 - INFO: Epoch: 1/10, Batch: 1/29, Batch_Loss_Train: 6.775
2024-06-21 15:37:07,288 - INFO: Epoch: 1/10, Batch: 2/29, Batch_Loss_Train: 7.828
2024-06-21 15:37:07,690 - INFO: Epoch: 1/10, Batch: 3/29, Batch_Loss_Train: 7.375
2024-06-21 15:37:08,011 - INFO: Epoch: 1/10, Batch: 4/29, Batch_Loss_Train: 8.208
2024-06-21 15:37:08,420 - INFO: Epoch: 1/10, Batch: 5/29, Batch_Loss_Train: 7.664
2024-06-21 15:37:08,737 - INFO: Epoch: 1/10, Batch: 6/29, Batch_Loss_Train: 7.958
2024-06-21 15:37:09,119 - INFO: Epoch: 1/10, Batch: 7/29, Batch_Loss_Train: 8.510
2024-06-21 15:37:09,437 - INFO: Epoch: 1/10, Batch: 8/29, Batch_Loss_Train: 7.309
2024-06-21 15:37:09,827 - INFO: Epoch: 1/10, Batch: 9/29, Batch_Loss_Train: 6.895
2024-06-21 15:37:10,138 - INFO: Epoch: 1/10, Batch: 10/29, Batch_Loss_Train: 6.814
2024-06-21 15:37:10,521 - INFO: Epoch: 1/10, Batch: 11/29, Batch_Loss_Train: 6.743
2024-06-21 15:37:10,841 - INFO: Epoch: 1/10, Batch: 12/29, Batch_Loss_Train: 7.969
2024-06-21 15:37:11,266 - INFO: Epoch: 1/10, Batch: 13/29, Batch_Loss_Train: 7.613
2024-06-21 15:37:11,588 - INFO: Epoch: 1/10, Batch: 14/29, Batch_Loss_Train: 6.992
2024-06-21 15:37:11,989 - INFO: Epoch: 1/10, Batch: 15/29, Batch_Loss_Train: 6.788
2024-06-21 15:37:12,306 - INFO: Epoch: 1/10, Batch: 16/29, Batch_Loss_Train: 7.885
2024-06-21 15:37:12,731 - INFO: Epoch: 1/10, Batch: 17/29, Batch_Loss_Train: 6.337
2024-06-21 15:37:13,048 - INFO: Epoch: 1/10, Batch: 18/29, Batch_Loss_Train: 6.014
2024-06-21 15:37:13,440 - INFO: Epoch: 1/10, Batch: 19/29, Batch_Loss_Train: 6.198
2024-06-21 15:37:13,750 - INFO: Epoch: 1/10, Batch: 20/29, Batch_Loss_Train: 6.588
2024-06-21 15:37:14,153 - INFO: Epoch: 1/10, Batch: 21/29, Batch_Loss_Train: 6.097
2024-06-21 15:37:14,474 - INFO: Epoch: 1/10, Batch: 22/29, Batch_Loss_Train: 7.072
2024-06-21 15:37:14,870 - INFO: Epoch: 1/10, Batch: 23/29, Batch_Loss_Train: 6.887
2024-06-21 15:37:15,197 - INFO: Epoch: 1/10, Batch: 24/29, Batch_Loss_Train: 5.986
2024-06-21 15:37:15,617 - INFO: Epoch: 1/10, Batch: 25/29, Batch_Loss_Train: 6.164
2024-06-21 15:37:15,939 - INFO: Epoch: 1/10, Batch: 26/29, Batch_Loss_Train: 5.644
2024-06-21 15:37:16,321 - INFO: Epoch: 1/10, Batch: 27/29, Batch_Loss_Train: 6.050
2024-06-21 15:37:16,636 - INFO: Epoch: 1/10, Batch: 28/29, Batch_Loss_Train: 8.068
2024-06-21 15:37:16,845 - INFO: Epoch: 1/10, Batch: 29/29, Batch_Loss_Train: 5.756
2024-06-21 15:37:28,036 - INFO: 1/10 final results:
2024-06-21 15:37:28,036 - INFO: Training loss: 6.972.
2024-06-21 15:37:28,036 - INFO: Training MAE: 6.996.
2024-06-21 15:37:28,036 - INFO: Training MSE: 69.879.
2024-06-21 15:37:48,817 - INFO: Epoch: 1/10, Loss_train: 6.9719830217032595, Loss_val: 6.866725905188199
2024-06-21 15:37:48,817 - INFO: Best internal validation val_loss: inf at epoch: 0.
2024-06-21 15:37:48,817 - INFO: Epoch 2/10...
2024-06-21 15:37:48,817 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:37:48,817 - INFO: Batch size: 32.
2024-06-21 15:37:48,820 - INFO: Dataset:
2024-06-21 15:37:48,821 - INFO: Batch size:
2024-06-21 15:37:48,821 - INFO: Number of workers:
2024-06-21 15:37:50,182 - INFO: Epoch: 2/10, Batch: 1/29, Batch_Loss_Train: 4.915
2024-06-21 15:37:50,495 - INFO: Epoch: 2/10, Batch: 2/29, Batch_Loss_Train: 5.780
2024-06-21 15:37:50,922 - INFO: Epoch: 2/10, Batch: 3/29, Batch_Loss_Train: 5.708
2024-06-21 15:37:51,232 - INFO: Epoch: 2/10, Batch: 4/29, Batch_Loss_Train: 6.140
2024-06-21 15:37:51,635 - INFO: Epoch: 2/10, Batch: 5/29, Batch_Loss_Train: 5.377
2024-06-21 15:37:51,936 - INFO: Epoch: 2/10, Batch: 6/29, Batch_Loss_Train: 5.353
2024-06-21 15:37:52,342 - INFO: Epoch: 2/10, Batch: 7/29, Batch_Loss_Train: 5.481
2024-06-21 15:37:52,646 - INFO: Epoch: 2/10, Batch: 8/29, Batch_Loss_Train: 4.841
2024-06-21 15:37:53,050 - INFO: Epoch: 2/10, Batch: 9/29, Batch_Loss_Train: 4.776
2024-06-21 15:37:53,346 - INFO: Epoch: 2/10, Batch: 10/29, Batch_Loss_Train: 5.104
2024-06-21 15:37:53,760 - INFO: Epoch: 2/10, Batch: 11/29, Batch_Loss_Train: 4.351
2024-06-21 15:37:54,070 - INFO: Epoch: 2/10, Batch: 12/29, Batch_Loss_Train: 5.533
2024-06-21 15:37:54,506 - INFO: Epoch: 2/10, Batch: 13/29, Batch_Loss_Train: 6.067
2024-06-21 15:37:54,812 - INFO: Epoch: 2/10, Batch: 14/29, Batch_Loss_Train: 4.941
2024-06-21 15:37:55,241 - INFO: Epoch: 2/10, Batch: 15/29, Batch_Loss_Train: 5.229
2024-06-21 15:37:55,546 - INFO: Epoch: 2/10, Batch: 16/29, Batch_Loss_Train: 4.767
2024-06-21 15:37:55,963 - INFO: Epoch: 2/10, Batch: 17/29, Batch_Loss_Train: 4.751
2024-06-21 15:37:56,269 - INFO: Epoch: 2/10, Batch: 18/29, Batch_Loss_Train: 5.774
2024-06-21 15:37:56,683 - INFO: Epoch: 2/10, Batch: 19/29, Batch_Loss_Train: 5.472
2024-06-21 15:37:56,984 - INFO: Epoch: 2/10, Batch: 20/29, Batch_Loss_Train: 5.196
2024-06-21 15:37:57,391 - INFO: Epoch: 2/10, Batch: 21/29, Batch_Loss_Train: 4.928
2024-06-21 15:37:57,700 - INFO: Epoch: 2/10, Batch: 22/29, Batch_Loss_Train: 4.042
2024-06-21 15:37:58,123 - INFO: Epoch: 2/10, Batch: 23/29, Batch_Loss_Train: 4.589
2024-06-21 15:37:58,432 - INFO: Epoch: 2/10, Batch: 24/29, Batch_Loss_Train: 5.143
2024-06-21 15:37:58,847 - INFO: Epoch: 2/10, Batch: 25/29, Batch_Loss_Train: 4.610
2024-06-21 15:37:59,151 - INFO: Epoch: 2/10, Batch: 26/29, Batch_Loss_Train: 4.694
2024-06-21 15:37:59,549 - INFO: Epoch: 2/10, Batch: 27/29, Batch_Loss_Train: 4.537
2024-06-21 15:37:59,853 - INFO: Epoch: 2/10, Batch: 28/29, Batch_Loss_Train: 4.810
2024-06-21 15:38:00,070 - INFO: Epoch: 2/10, Batch: 29/29, Batch_Loss_Train: 5.745
2024-06-21 15:38:11,312 - INFO: 2/10 final results:
2024-06-21 15:38:11,312 - INFO: Training loss: 5.126.
2024-06-21 15:38:11,312 - INFO: Training MAE: 5.114.
2024-06-21 15:38:11,312 - INFO: Training MSE: 41.581.
2024-06-21 15:38:32,134 - INFO: Epoch: 2/10, Loss_train: 5.1259907031881395, Loss_val: 5.010889530181885
2024-06-21 15:38:32,192 - INFO: Saved new best metric model for epoch 2.
2024-06-21 15:38:32,193 - INFO: Best internal validation val_loss: 5.011 at epoch: 2.
2024-06-21 15:38:32,193 - INFO: Epoch 3/10...
2024-06-21 15:38:32,193 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:38:32,193 - INFO: Batch size: 32.
2024-06-21 15:38:32,196 - INFO: Dataset:
2024-06-21 15:38:32,196 - INFO: Batch size:
2024-06-21 15:38:32,196 - INFO: Number of workers:
2024-06-21 15:38:33,562 - INFO: Epoch: 3/10, Batch: 1/29, Batch_Loss_Train: 3.599
2024-06-21 15:38:33,873 - INFO: Epoch: 3/10, Batch: 2/29, Batch_Loss_Train: 4.822
2024-06-21 15:38:34,275 - INFO: Epoch: 3/10, Batch: 3/29, Batch_Loss_Train: 3.954
2024-06-21 15:38:34,596 - INFO: Epoch: 3/10, Batch: 4/29, Batch_Loss_Train: 4.500
2024-06-21 15:38:35,001 - INFO: Epoch: 3/10, Batch: 5/29, Batch_Loss_Train: 3.866
2024-06-21 15:38:35,339 - INFO: Epoch: 3/10, Batch: 6/29, Batch_Loss_Train: 4.334
2024-06-21 15:38:35,726 - INFO: Epoch: 3/10, Batch: 7/29, Batch_Loss_Train: 4.330
2024-06-21 15:38:36,029 - INFO: Epoch: 3/10, Batch: 8/29, Batch_Loss_Train: 5.170
2024-06-21 15:38:36,419 - INFO: Epoch: 3/10, Batch: 9/29, Batch_Loss_Train: 3.846
2024-06-21 15:38:36,748 - INFO: Epoch: 3/10, Batch: 10/29, Batch_Loss_Train: 4.353
2024-06-21 15:38:37,142 - INFO: Epoch: 3/10, Batch: 11/29, Batch_Loss_Train: 3.636
2024-06-21 15:38:37,451 - INFO: Epoch: 3/10, Batch: 12/29, Batch_Loss_Train: 4.184
2024-06-21 15:38:37,869 - INFO: Epoch: 3/10, Batch: 13/29, Batch_Loss_Train: 4.067
2024-06-21 15:38:38,202 - INFO: Epoch: 3/10, Batch: 14/29, Batch_Loss_Train: 4.471
2024-06-21 15:38:38,615 - INFO: Epoch: 3/10, Batch: 15/29, Batch_Loss_Train: 4.506
2024-06-21 15:38:38,919 - INFO: Epoch: 3/10, Batch: 16/29, Batch_Loss_Train: 4.260
2024-06-21 15:38:39,312 - INFO: Epoch: 3/10, Batch: 17/29, Batch_Loss_Train: 4.583
2024-06-21 15:38:39,638 - INFO: Epoch: 3/10, Batch: 18/29, Batch_Loss_Train: 4.014
2024-06-21 15:38:40,043 - INFO: Epoch: 3/10, Batch: 19/29, Batch_Loss_Train: 3.599
2024-06-21 15:38:40,343 - INFO: Epoch: 3/10, Batch: 20/29, Batch_Loss_Train: 5.176
2024-06-21 15:38:40,732 - INFO: Epoch: 3/10, Batch: 21/29, Batch_Loss_Train: 4.767
2024-06-21 15:38:41,063 - INFO: Epoch: 3/10, Batch: 22/29, Batch_Loss_Train: 3.383
2024-06-21 15:38:41,459 - INFO: Epoch: 3/10, Batch: 23/29, Batch_Loss_Train: 3.618
2024-06-21 15:38:41,765 - INFO: Epoch: 3/10, Batch: 24/29, Batch_Loss_Train: 4.823
2024-06-21 15:38:42,159 - INFO: Epoch: 3/10, Batch: 25/29, Batch_Loss_Train: 4.558
2024-06-21 15:38:42,482 - INFO: Epoch: 3/10, Batch: 26/29, Batch_Loss_Train: 3.838
2024-06-21 15:38:42,861 - INFO: Epoch: 3/10, Batch: 27/29, Batch_Loss_Train: 3.800
2024-06-21 15:38:43,158 - INFO: Epoch: 3/10, Batch: 28/29, Batch_Loss_Train: 4.279
2024-06-21 15:38:43,365 - INFO: Epoch: 3/10, Batch: 29/29, Batch_Loss_Train: 3.664
2024-06-21 15:38:54,534 - INFO: 3/10 final results:
2024-06-21 15:38:54,534 - INFO: Training loss: 4.207.
2024-06-21 15:38:54,534 - INFO: Training MAE: 4.218.
2024-06-21 15:38:54,534 - INFO: Training MSE: 29.768.
2024-06-21 15:39:14,956 - INFO: Epoch: 3/10, Loss_train: 4.206808937006984, Loss_val: 4.073941707611084
2024-06-21 15:39:15,012 - INFO: Saved new best metric model for epoch 3.
2024-06-21 15:39:15,012 - INFO: Best internal validation val_loss: 4.074 at epoch: 3.
2024-06-21 15:39:15,012 - INFO: Epoch 4/10...
2024-06-21 15:39:15,012 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:39:15,012 - INFO: Batch size: 32.
2024-06-21 15:39:15,016 - INFO: Dataset:
2024-06-21 15:39:15,016 - INFO: Batch size:
2024-06-21 15:39:15,017 - INFO: Number of workers:
2024-06-21 15:39:16,347 - INFO: Epoch: 4/10, Batch: 1/29, Batch_Loss_Train: 3.463
2024-06-21 15:39:16,672 - INFO: Epoch: 4/10, Batch: 2/29, Batch_Loss_Train: 3.620
2024-06-21 15:39:17,079 - INFO: Epoch: 4/10, Batch: 3/29, Batch_Loss_Train: 3.876
2024-06-21 15:39:17,401 - INFO: Epoch: 4/10, Batch: 4/29, Batch_Loss_Train: 3.331
2024-06-21 15:39:17,816 - INFO: Epoch: 4/10, Batch: 5/29, Batch_Loss_Train: 3.787
2024-06-21 15:39:18,117 - INFO: Epoch: 4/10, Batch: 6/29, Batch_Loss_Train: 3.792
2024-06-21 15:39:18,498 - INFO: Epoch: 4/10, Batch: 7/29, Batch_Loss_Train: 3.470
2024-06-21 15:39:18,811 - INFO: Epoch: 4/10, Batch: 8/29, Batch_Loss_Train: 3.535
2024-06-21 15:39:19,215 - INFO: Epoch: 4/10, Batch: 9/29, Batch_Loss_Train: 3.161
2024-06-21 15:39:19,510 - INFO: Epoch: 4/10, Batch: 10/29, Batch_Loss_Train: 3.735
2024-06-21 15:39:19,897 - INFO: Epoch: 4/10, Batch: 11/29, Batch_Loss_Train: 3.386
2024-06-21 15:39:20,216 - INFO: Epoch: 4/10, Batch: 12/29, Batch_Loss_Train: 4.242
2024-06-21 15:39:20,639 - INFO: Epoch: 4/10, Batch: 13/29, Batch_Loss_Train: 4.779
2024-06-21 15:39:20,945 - INFO: Epoch: 4/10, Batch: 14/29, Batch_Loss_Train: 3.556
2024-06-21 15:39:21,348 - INFO: Epoch: 4/10, Batch: 15/29, Batch_Loss_Train: 3.369
2024-06-21 15:39:21,667 - INFO: Epoch: 4/10, Batch: 16/29, Batch_Loss_Train: 3.226
2024-06-21 15:39:22,087 - INFO: Epoch: 4/10, Batch: 17/29, Batch_Loss_Train: 3.082
2024-06-21 15:39:22,393 - INFO: Epoch: 4/10, Batch: 18/29, Batch_Loss_Train: 3.160
2024-06-21 15:39:22,812 - INFO: Epoch: 4/10, Batch: 19/29, Batch_Loss_Train: 2.758
2024-06-21 15:39:23,134 - INFO: Epoch: 4/10, Batch: 20/29, Batch_Loss_Train: 3.002
2024-06-21 15:39:23,569 - INFO: Epoch: 4/10, Batch: 21/29, Batch_Loss_Train: 3.068
2024-06-21 15:39:23,874 - INFO: Epoch: 4/10, Batch: 22/29, Batch_Loss_Train: 3.075
2024-06-21 15:39:24,282 - INFO: Epoch: 4/10, Batch: 23/29, Batch_Loss_Train: 5.310
2024-06-21 15:39:24,600 - INFO: Epoch: 4/10, Batch: 24/29, Batch_Loss_Train: 2.993
2024-06-21 15:39:25,013 - INFO: Epoch: 4/10, Batch: 25/29, Batch_Loss_Train: 3.528
2024-06-21 15:39:25,313 - INFO: Epoch: 4/10, Batch: 26/29, Batch_Loss_Train: 3.261
2024-06-21 15:39:25,715 - INFO: Epoch: 4/10, Batch: 27/29, Batch_Loss_Train: 3.502
2024-06-21 15:39:26,030 - INFO: Epoch: 4/10, Batch: 28/29, Batch_Loss_Train: 3.580
2024-06-21 15:39:26,251 - INFO: Epoch: 4/10, Batch: 29/29, Batch_Loss_Train: 2.905
2024-06-21 15:39:37,368 - INFO: 4/10 final results:
2024-06-21 15:39:37,369 - INFO: Training loss: 3.502.
2024-06-21 15:39:37,369 - INFO: Training MAE: 3.514.
2024-06-21 15:39:37,369 - INFO: Training MSE: 21.075.
2024-06-21 15:39:57,763 - INFO: Epoch: 4/10, Loss_train: 3.501837368669181, Loss_val: 3.25950577341277
2024-06-21 15:39:57,820 - INFO: Saved new best metric model for epoch 4.
2024-06-21 15:39:57,820 - INFO: Best internal validation val_loss: 3.260 at epoch: 4.
2024-06-21 15:39:57,820 - INFO: Epoch 5/10...
2024-06-21 15:39:57,820 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:39:57,820 - INFO: Batch size: 32.
2024-06-21 15:39:57,824 - INFO: Dataset:
2024-06-21 15:39:57,824 - INFO: Batch size:
2024-06-21 15:39:57,825 - INFO: Number of workers:
2024-06-21 15:39:59,172 - INFO: Epoch: 5/10, Batch: 1/29, Batch_Loss_Train: 3.193
2024-06-21 15:39:59,482 - INFO: Epoch: 5/10, Batch: 2/29, Batch_Loss_Train: 4.250
2024-06-21 15:39:59,912 - INFO: Epoch: 5/10, Batch: 3/29, Batch_Loss_Train: 2.998
2024-06-21 15:40:00,224 - INFO: Epoch: 5/10, Batch: 4/29, Batch_Loss_Train: 3.503
2024-06-21 15:40:00,654 - INFO: Epoch: 5/10, Batch: 5/29, Batch_Loss_Train: 2.832
2024-06-21 15:40:00,958 - INFO: Epoch: 5/10, Batch: 6/29, Batch_Loss_Train: 4.060
2024-06-21 15:40:01,361 - INFO: Epoch: 5/10, Batch: 7/29, Batch_Loss_Train: 2.682
2024-06-21 15:40:01,665 - INFO: Epoch: 5/10, Batch: 8/29, Batch_Loss_Train: 2.441
2024-06-21 15:40:02,085 - INFO: Epoch: 5/10, Batch: 9/29, Batch_Loss_Train: 3.170
2024-06-21 15:40:02,383 - INFO: Epoch: 5/10, Batch: 10/29, Batch_Loss_Train: 2.530
2024-06-21 15:40:02,777 - INFO: Epoch: 5/10, Batch: 11/29, Batch_Loss_Train: 3.466
2024-06-21 15:40:03,085 - INFO: Epoch: 5/10, Batch: 12/29, Batch_Loss_Train: 3.296
2024-06-21 15:40:03,524 - INFO: Epoch: 5/10, Batch: 13/29, Batch_Loss_Train: 2.807
2024-06-21 15:40:03,833 - INFO: Epoch: 5/10, Batch: 14/29, Batch_Loss_Train: 2.690
2024-06-21 15:40:04,246 - INFO: Epoch: 5/10, Batch: 15/29, Batch_Loss_Train: 3.303
2024-06-21 15:40:04,550 - INFO: Epoch: 5/10, Batch: 16/29, Batch_Loss_Train: 2.639
2024-06-21 15:40:04,978 - INFO: Epoch: 5/10, Batch: 17/29, Batch_Loss_Train: 2.676
2024-06-21 15:40:05,283 - INFO: Epoch: 5/10, Batch: 18/29, Batch_Loss_Train: 4.105
2024-06-21 15:40:05,683 - INFO: Epoch: 5/10, Batch: 19/29, Batch_Loss_Train: 3.451
2024-06-21 15:40:05,981 - INFO: Epoch: 5/10, Batch: 20/29, Batch_Loss_Train: 3.196
2024-06-21 15:40:06,404 - INFO: Epoch: 5/10, Batch: 21/29, Batch_Loss_Train: 3.345
2024-06-21 15:40:06,711 - INFO: Epoch: 5/10, Batch: 22/29, Batch_Loss_Train: 3.128
2024-06-21 15:40:07,109 - INFO: Epoch: 5/10, Batch: 23/29, Batch_Loss_Train: 2.811
2024-06-21 15:40:07,414 - INFO: Epoch: 5/10, Batch: 24/29, Batch_Loss_Train: 2.647
2024-06-21 15:40:07,830 - INFO: Epoch: 5/10, Batch: 25/29, Batch_Loss_Train: 2.390
2024-06-21 15:40:08,132 - INFO: Epoch: 5/10, Batch: 26/29, Batch_Loss_Train: 3.328
2024-06-21 15:40:08,515 - INFO: Epoch: 5/10, Batch: 27/29, Batch_Loss_Train: 2.334
2024-06-21 15:40:08,815 - INFO: Epoch: 5/10, Batch: 28/29, Batch_Loss_Train: 3.347
2024-06-21 15:40:09,023 - INFO: Epoch: 5/10, Batch: 29/29, Batch_Loss_Train: 3.691
2024-06-21 15:40:20,248 - INFO: 5/10 final results:
2024-06-21 15:40:20,248 - INFO: Training loss: 3.114.
2024-06-21 15:40:20,248 - INFO: Training MAE: 3.103.
2024-06-21 15:40:20,248 - INFO: Training MSE: 17.148.
2024-06-21 15:40:41,054 - INFO: Epoch: 5/10, Loss_train: 3.1140405802891173, Loss_val: 3.63911902493444
2024-06-21 15:40:41,054 - INFO: Best internal validation val_loss: 3.260 at epoch: 4.
2024-06-21 15:40:41,054 - INFO: Epoch 6/10...
2024-06-21 15:40:41,054 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:40:41,054 - INFO: Batch size: 32.
2024-06-21 15:40:41,058 - INFO: Dataset:
2024-06-21 15:40:41,058 - INFO: Batch size:
2024-06-21 15:40:41,058 - INFO: Number of workers:
2024-06-21 15:40:42,433 - INFO: Epoch: 6/10, Batch: 1/29, Batch_Loss_Train: 3.271
2024-06-21 15:40:42,742 - INFO: Epoch: 6/10, Batch: 2/29, Batch_Loss_Train: 3.516
2024-06-21 15:40:43,156 - INFO: Epoch: 6/10, Batch: 3/29, Batch_Loss_Train: 2.947
2024-06-21 15:40:43,478 - INFO: Epoch: 6/10, Batch: 4/29, Batch_Loss_Train: 2.262
2024-06-21 15:40:43,893 - INFO: Epoch: 6/10, Batch: 5/29, Batch_Loss_Train: 2.972
2024-06-21 15:40:44,194 - INFO: Epoch: 6/10, Batch: 6/29, Batch_Loss_Train: 2.555
2024-06-21 15:40:44,594 - INFO: Epoch: 6/10, Batch: 7/29, Batch_Loss_Train: 3.094
2024-06-21 15:40:44,909 - INFO: Epoch: 6/10, Batch: 8/29, Batch_Loss_Train: 2.577
2024-06-21 15:40:45,319 - INFO: Epoch: 6/10, Batch: 9/29, Batch_Loss_Train: 3.531
2024-06-21 15:40:45,615 - INFO: Epoch: 6/10, Batch: 10/29, Batch_Loss_Train: 2.313
2024-06-21 15:40:46,001 - INFO: Epoch: 6/10, Batch: 11/29, Batch_Loss_Train: 2.695
2024-06-21 15:40:46,320 - INFO: Epoch: 6/10, Batch: 12/29, Batch_Loss_Train: 2.324
2024-06-21 15:40:46,735 - INFO: Epoch: 6/10, Batch: 13/29, Batch_Loss_Train: 3.091
2024-06-21 15:40:47,041 - INFO: Epoch: 6/10, Batch: 14/29, Batch_Loss_Train: 3.048
2024-06-21 15:40:47,441 - INFO: Epoch: 6/10, Batch: 15/29, Batch_Loss_Train: 2.124
2024-06-21 15:40:47,756 - INFO: Epoch: 6/10, Batch: 16/29, Batch_Loss_Train: 2.325
2024-06-21 15:40:48,170 - INFO: Epoch: 6/10, Batch: 17/29, Batch_Loss_Train: 2.138
2024-06-21 15:40:48,473 - INFO: Epoch: 6/10, Batch: 18/29, Batch_Loss_Train: 2.508
2024-06-21 15:40:48,858 - INFO: Epoch: 6/10, Batch: 19/29, Batch_Loss_Train: 3.026
2024-06-21 15:40:49,168 - INFO: Epoch: 6/10, Batch: 20/29, Batch_Loss_Train: 3.372
2024-06-21 15:40:49,565 - INFO: Epoch: 6/10, Batch: 21/29, Batch_Loss_Train: 2.226
2024-06-21 15:40:49,870 - INFO: Epoch: 6/10, Batch: 22/29, Batch_Loss_Train: 2.219
2024-06-21 15:40:50,273 - INFO: Epoch: 6/10, Batch: 23/29, Batch_Loss_Train: 2.513
2024-06-21 15:40:50,590 - INFO: Epoch: 6/10, Batch: 24/29, Batch_Loss_Train: 2.776
2024-06-21 15:40:50,994 - INFO: Epoch: 6/10, Batch: 25/29, Batch_Loss_Train: 3.330
2024-06-21 15:40:51,293 - INFO: Epoch: 6/10, Batch: 26/29, Batch_Loss_Train: 2.099
2024-06-21 15:40:51,679 - INFO: Epoch: 6/10, Batch: 27/29, Batch_Loss_Train: 3.183
2024-06-21 15:40:51,992 - INFO: Epoch: 6/10, Batch: 28/29, Batch_Loss_Train: 2.529
2024-06-21 15:40:52,203 - INFO: Epoch: 6/10, Batch: 29/29, Batch_Loss_Train: 2.802
2024-06-21 15:41:03,358 - INFO: 6/10 final results:
2024-06-21 15:41:03,358 - INFO: Training loss: 2.737.
2024-06-21 15:41:03,358 - INFO: Training MAE: 2.735.
2024-06-21 15:41:03,358 - INFO: Training MSE: 13.184.
2024-06-21 15:41:23,970 - INFO: Epoch: 6/10, Loss_train: 2.736762630528417, Loss_val: 2.728590184244616
2024-06-21 15:41:24,028 - INFO: Saved new best metric model for epoch 6.
2024-06-21 15:41:24,028 - INFO: Best internal validation val_loss: 2.729 at epoch: 6.
2024-06-21 15:41:24,028 - INFO: Epoch 7/10...
2024-06-21 15:41:24,028 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:41:24,028 - INFO: Batch size: 32.
2024-06-21 15:41:24,032 - INFO: Dataset:
2024-06-21 15:41:24,032 - INFO: Batch size:
2024-06-21 15:41:24,032 - INFO: Number of workers:
2024-06-21 15:41:25,403 - INFO: Epoch: 7/10, Batch: 1/29, Batch_Loss_Train: 2.398
2024-06-21 15:41:25,730 - INFO: Epoch: 7/10, Batch: 2/29, Batch_Loss_Train: 2.022
2024-06-21 15:41:26,132 - INFO: Epoch: 7/10, Batch: 3/29, Batch_Loss_Train: 2.296
2024-06-21 15:41:26,457 - INFO: Epoch: 7/10, Batch: 4/29, Batch_Loss_Train: 3.229
2024-06-21 15:41:26,885 - INFO: Epoch: 7/10, Batch: 5/29, Batch_Loss_Train: 2.649
2024-06-21 15:41:27,188 - INFO: Epoch: 7/10, Batch: 6/29, Batch_Loss_Train: 3.279
2024-06-21 15:41:27,573 - INFO: Epoch: 7/10, Batch: 7/29, Batch_Loss_Train: 2.278
2024-06-21 15:41:27,892 - INFO: Epoch: 7/10, Batch: 8/29, Batch_Loss_Train: 2.653
2024-06-21 15:41:28,313 - INFO: Epoch: 7/10, Batch: 9/29, Batch_Loss_Train: 2.774
2024-06-21 15:41:28,612 - INFO: Epoch: 7/10, Batch: 10/29, Batch_Loss_Train: 2.354
2024-06-21 15:41:28,992 - INFO: Epoch: 7/10, Batch: 11/29, Batch_Loss_Train: 2.361
2024-06-21 15:41:29,313 - INFO: Epoch: 7/10, Batch: 12/29, Batch_Loss_Train: 3.161
2024-06-21 15:41:29,755 - INFO: Epoch: 7/10, Batch: 13/29, Batch_Loss_Train: 3.141
2024-06-21 15:41:30,064 - INFO: Epoch: 7/10, Batch: 14/29, Batch_Loss_Train: 2.205
2024-06-21 15:41:30,466 - INFO: Epoch: 7/10, Batch: 15/29, Batch_Loss_Train: 2.290
2024-06-21 15:41:30,784 - INFO: Epoch: 7/10, Batch: 16/29, Batch_Loss_Train: 2.417
2024-06-21 15:41:31,213 - INFO: Epoch: 7/10, Batch: 17/29, Batch_Loss_Train: 2.061
2024-06-21 15:41:31,518 - INFO: Epoch: 7/10, Batch: 18/29, Batch_Loss_Train: 1.971
2024-06-21 15:41:31,905 - INFO: Epoch: 7/10, Batch: 19/29, Batch_Loss_Train: 2.017
2024-06-21 15:41:32,217 - INFO: Epoch: 7/10, Batch: 20/29, Batch_Loss_Train: 2.942
2024-06-21 15:41:32,637 - INFO: Epoch: 7/10, Batch: 21/29, Batch_Loss_Train: 2.760
2024-06-21 15:41:32,942 - INFO: Epoch: 7/10, Batch: 22/29, Batch_Loss_Train: 3.023
2024-06-21 15:41:33,323 - INFO: Epoch: 7/10, Batch: 23/29, Batch_Loss_Train: 2.124
2024-06-21 15:41:33,641 - INFO: Epoch: 7/10, Batch: 24/29, Batch_Loss_Train: 3.125
2024-06-21 15:41:34,057 - INFO: Epoch: 7/10, Batch: 25/29, Batch_Loss_Train: 2.023
2024-06-21 15:41:34,357 - INFO: Epoch: 7/10, Batch: 26/29, Batch_Loss_Train: 2.987
2024-06-21 15:41:34,728 - INFO: Epoch: 7/10, Batch: 27/29, Batch_Loss_Train: 2.128
2024-06-21 15:41:35,041 - INFO: Epoch: 7/10, Batch: 28/29, Batch_Loss_Train: 2.321
2024-06-21 15:41:35,249 - INFO: Epoch: 7/10, Batch: 29/29, Batch_Loss_Train: 3.031
2024-06-21 15:41:46,481 - INFO: 7/10 final results:
2024-06-21 15:41:46,481 - INFO: Training loss: 2.552.
2024-06-21 15:41:46,481 - INFO: Training MAE: 2.543.
2024-06-21 15:41:46,481 - INFO: Training MSE: 11.692.
2024-06-21 15:42:07,228 - INFO: Epoch: 7/10, Loss_train: 2.5524540605216193, Loss_val: 2.582954932903421
2024-06-21 15:42:07,286 - INFO: Saved new best metric model for epoch 7.
2024-06-21 15:42:07,286 - INFO: Best internal validation val_loss: 2.583 at epoch: 7.
2024-06-21 15:42:07,286 - INFO: Epoch 8/10...
2024-06-21 15:42:07,286 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:42:07,286 - INFO: Batch size: 32.
2024-06-21 15:42:07,290 - INFO: Dataset:
2024-06-21 15:42:07,290 - INFO: Batch size:
2024-06-21 15:42:07,290 - INFO: Number of workers:
2024-06-21 15:42:08,637 - INFO: Epoch: 8/10, Batch: 1/29, Batch_Loss_Train: 2.091
2024-06-21 15:42:08,960 - INFO: Epoch: 8/10, Batch: 2/29, Batch_Loss_Train: 2.038
2024-06-21 15:42:09,360 - INFO: Epoch: 8/10, Batch: 3/29, Batch_Loss_Train: 2.596
2024-06-21 15:42:09,686 - INFO: Epoch: 8/10, Batch: 4/29, Batch_Loss_Train: 2.394
2024-06-21 15:42:10,093 - INFO: Epoch: 8/10, Batch: 5/29, Batch_Loss_Train: 2.006
2024-06-21 15:42:10,411 - INFO: Epoch: 8/10, Batch: 6/29, Batch_Loss_Train: 4.102
2024-06-21 15:42:10,797 - INFO: Epoch: 8/10, Batch: 7/29, Batch_Loss_Train: 2.416
2024-06-21 15:42:11,117 - INFO: Epoch: 8/10, Batch: 8/29, Batch_Loss_Train: 1.810
2024-06-21 15:42:11,511 - INFO: Epoch: 8/10, Batch: 9/29, Batch_Loss_Train: 1.847
2024-06-21 15:42:11,825 - INFO: Epoch: 8/10, Batch: 10/29, Batch_Loss_Train: 2.309
2024-06-21 15:42:12,206 - INFO: Epoch: 8/10, Batch: 11/29, Batch_Loss_Train: 2.731
2024-06-21 15:42:12,530 - INFO: Epoch: 8/10, Batch: 12/29, Batch_Loss_Train: 2.245
2024-06-21 15:42:12,947 - INFO: Epoch: 8/10, Batch: 13/29, Batch_Loss_Train: 2.083
2024-06-21 15:42:13,270 - INFO: Epoch: 8/10, Batch: 14/29, Batch_Loss_Train: 1.823
2024-06-21 15:42:13,685 - INFO: Epoch: 8/10, Batch: 15/29, Batch_Loss_Train: 2.520
2024-06-21 15:42:14,003 - INFO: Epoch: 8/10, Batch: 16/29, Batch_Loss_Train: 2.797
2024-06-21 15:42:14,415 - INFO: Epoch: 8/10, Batch: 17/29, Batch_Loss_Train: 2.238
2024-06-21 15:42:14,734 - INFO: Epoch: 8/10, Batch: 18/29, Batch_Loss_Train: 1.999
2024-06-21 15:42:15,125 - INFO: Epoch: 8/10, Batch: 19/29, Batch_Loss_Train: 1.874
2024-06-21 15:42:15,440 - INFO: Epoch: 8/10, Batch: 20/29, Batch_Loss_Train: 1.997
2024-06-21 15:42:15,842 - INFO: Epoch: 8/10, Batch: 21/29, Batch_Loss_Train: 2.131
2024-06-21 15:42:16,163 - INFO: Epoch: 8/10, Batch: 22/29, Batch_Loss_Train: 1.992
2024-06-21 15:42:16,574 - INFO: Epoch: 8/10, Batch: 23/29, Batch_Loss_Train: 2.958
2024-06-21 15:42:16,895 - INFO: Epoch: 8/10, Batch: 24/29, Batch_Loss_Train: 3.177
2024-06-21 15:42:17,300 - INFO: Epoch: 8/10, Batch: 25/29, Batch_Loss_Train: 1.926
2024-06-21 15:42:17,616 - INFO: Epoch: 8/10, Batch: 26/29, Batch_Loss_Train: 2.642
2024-06-21 15:42:18,021 - INFO: Epoch: 8/10, Batch: 27/29, Batch_Loss_Train: 1.957
2024-06-21 15:42:18,338 - INFO: Epoch: 8/10, Batch: 28/29, Batch_Loss_Train: 1.583
2024-06-21 15:42:18,562 - INFO: Epoch: 8/10, Batch: 29/29, Batch_Loss_Train: 1.812
2024-06-21 15:42:29,748 - INFO: 8/10 final results:
2024-06-21 15:42:29,749 - INFO: Training loss: 2.279.
2024-06-21 15:42:29,749 - INFO: Training MAE: 2.288.
2024-06-21 15:42:29,749 - INFO: Training MSE: 9.733.
2024-06-21 15:42:50,331 - INFO: Epoch: 8/10, Loss_train: 2.2790737645379426, Loss_val: 2.8122073329728225
2024-06-21 15:42:50,331 - INFO: Best internal validation val_loss: 2.583 at epoch: 7.
2024-06-21 15:42:50,331 - INFO: Epoch 9/10...
2024-06-21 15:42:50,331 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:42:50,331 - INFO: Batch size: 32.
2024-06-21 15:42:50,334 - INFO: Dataset:
2024-06-21 15:42:50,335 - INFO: Batch size:
2024-06-21 15:42:50,335 - INFO: Number of workers:
2024-06-21 15:42:51,708 - INFO: Epoch: 9/10, Batch: 1/29, Batch_Loss_Train: 2.313
2024-06-21 15:42:52,034 - INFO: Epoch: 9/10, Batch: 2/29, Batch_Loss_Train: 2.505
2024-06-21 15:42:52,423 - INFO: Epoch: 9/10, Batch: 3/29, Batch_Loss_Train: 2.177
2024-06-21 15:42:52,746 - INFO: Epoch: 9/10, Batch: 4/29, Batch_Loss_Train: 2.332
2024-06-21 15:42:53,176 - INFO: Epoch: 9/10, Batch: 5/29, Batch_Loss_Train: 1.837
2024-06-21 15:42:53,479 - INFO: Epoch: 9/10, Batch: 6/29, Batch_Loss_Train: 2.822
2024-06-21 15:42:53,851 - INFO: Epoch: 9/10, Batch: 7/29, Batch_Loss_Train: 2.063
2024-06-21 15:42:54,170 - INFO: Epoch: 9/10, Batch: 8/29, Batch_Loss_Train: 2.984
2024-06-21 15:42:54,600 - INFO: Epoch: 9/10, Batch: 9/29, Batch_Loss_Train: 1.756
2024-06-21 15:42:54,900 - INFO: Epoch: 9/10, Batch: 10/29, Batch_Loss_Train: 2.480
2024-06-21 15:42:55,287 - INFO: Epoch: 9/10, Batch: 11/29, Batch_Loss_Train: 1.899
2024-06-21 15:42:55,610 - INFO: Epoch: 9/10, Batch: 12/29, Batch_Loss_Train: 2.501
2024-06-21 15:42:56,052 - INFO: Epoch: 9/10, Batch: 13/29, Batch_Loss_Train: 2.367
2024-06-21 15:42:56,361 - INFO: Epoch: 9/10, Batch: 14/29, Batch_Loss_Train: 2.254
2024-06-21 15:42:56,753 - INFO: Epoch: 9/10, Batch: 15/29, Batch_Loss_Train: 3.215
2024-06-21 15:42:57,071 - INFO: Epoch: 9/10, Batch: 16/29, Batch_Loss_Train: 2.303
2024-06-21 15:42:57,510 - INFO: Epoch: 9/10, Batch: 17/29, Batch_Loss_Train: 2.060
2024-06-21 15:42:57,816 - INFO: Epoch: 9/10, Batch: 18/29, Batch_Loss_Train: 1.946
2024-06-21 15:42:58,209 - INFO: Epoch: 9/10, Batch: 19/29, Batch_Loss_Train: 1.603
2024-06-21 15:42:58,522 - INFO: Epoch: 9/10, Batch: 20/29, Batch_Loss_Train: 2.119
2024-06-21 15:42:58,949 - INFO: Epoch: 9/10, Batch: 21/29, Batch_Loss_Train: 1.833
2024-06-21 15:42:59,257 - INFO: Epoch: 9/10, Batch: 22/29, Batch_Loss_Train: 2.242
2024-06-21 15:42:59,655 - INFO: Epoch: 9/10, Batch: 23/29, Batch_Loss_Train: 3.550
2024-06-21 15:42:59,977 - INFO: Epoch: 9/10, Batch: 24/29, Batch_Loss_Train: 2.129
2024-06-21 15:43:00,404 - INFO: Epoch: 9/10, Batch: 25/29, Batch_Loss_Train: 2.050
2024-06-21 15:43:00,707 - INFO: Epoch: 9/10, Batch: 26/29, Batch_Loss_Train: 2.054
2024-06-21 15:43:01,097 - INFO: Epoch: 9/10, Batch: 27/29, Batch_Loss_Train: 1.595
2024-06-21 15:43:01,414 - INFO: Epoch: 9/10, Batch: 28/29, Batch_Loss_Train: 2.347
2024-06-21 15:43:01,637 - INFO: Epoch: 9/10, Batch: 29/29, Batch_Loss_Train: 1.940
2024-06-21 15:43:12,839 - INFO: 9/10 final results:
2024-06-21 15:43:12,839 - INFO: Training loss: 2.251.
2024-06-21 15:43:12,839 - INFO: Training MAE: 2.257.
2024-06-21 15:43:12,839 - INFO: Training MSE: 9.298.
2024-06-21 15:43:33,428 - INFO: Epoch: 9/10, Loss_train: 2.250872250260978, Loss_val: 2.1515594022027376
2024-06-21 15:43:33,484 - INFO: Saved new best metric model for epoch 9.
2024-06-21 15:43:33,484 - INFO: Best internal validation val_loss: 2.152 at epoch: 9.
2024-06-21 15:43:33,484 - INFO: Epoch 10/10...
2024-06-21 15:43:33,485 - INFO: Learning rate: 0.0002196820804763665.
2024-06-21 15:43:33,485 - INFO: Batch size: 32.
2024-06-21 15:43:33,488 - INFO: Dataset:
2024-06-21 15:43:33,488 - INFO: Batch size:
2024-06-21 15:43:33,488 - INFO: Number of workers:
2024-06-21 15:43:34,827 - INFO: Epoch: 10/10, Batch: 1/29, Batch_Loss_Train: 1.865
2024-06-21 15:43:35,138 - INFO: Epoch: 10/10, Batch: 2/29, Batch_Loss_Train: 2.140
2024-06-21 15:43:35,542 - INFO: Epoch: 10/10, Batch: 3/29, Batch_Loss_Train: 1.951
2024-06-21 15:43:35,864 - INFO: Epoch: 10/10, Batch: 4/29, Batch_Loss_Train: 1.806
2024-06-21 15:43:36,287 - INFO: Epoch: 10/10, Batch: 5/29, Batch_Loss_Train: 2.334
2024-06-21 15:43:36,591 - INFO: Epoch: 10/10, Batch: 6/29, Batch_Loss_Train: 2.528
2024-06-21 15:43:36,983 - INFO: Epoch: 10/10, Batch: 7/29, Batch_Loss_Train: 2.221
2024-06-21 15:43:37,302 - INFO: Epoch: 10/10, Batch: 8/29, Batch_Loss_Train: 1.809
2024-06-21 15:43:37,730 - INFO: Epoch: 10/10, Batch: 9/29, Batch_Loss_Train: 2.273
2024-06-21 15:43:38,026 - INFO: Epoch: 10/10, Batch: 10/29, Batch_Loss_Train: 2.082
2024-06-21 15:43:38,404 - INFO: Epoch: 10/10, Batch: 11/29, Batch_Loss_Train: 1.814
2024-06-21 15:43:38,723 - INFO: Epoch: 10/10, Batch: 12/29, Batch_Loss_Train: 1.736
2024-06-21 15:43:39,176 - INFO: Epoch: 10/10, Batch: 13/29, Batch_Loss_Train: 2.163
2024-06-21 15:43:39,492 - INFO: Epoch: 10/10, Batch: 14/29, Batch_Loss_Train: 1.767
2024-06-21 15:43:39,892 - INFO: Epoch: 10/10, Batch: 15/29, Batch_Loss_Train: 1.983
2024-06-21 15:43:40,210 - INFO: Epoch: 10/10, Batch: 16/29, Batch_Loss_Train: 2.124
2024-06-21 15:43:40,631 - INFO: Epoch: 10/10, Batch: 17/29, Batch_Loss_Train: 2.880
2024-06-21 15:43:40,934 - INFO: Epoch: 10/10, Batch: 18/29, Batch_Loss_Train: 1.947
2024-06-21 15:43:41,318 - INFO: Epoch: 10/10, Batch: 19/29, Batch_Loss_Train: 1.796
2024-06-21 15:43:41,629 - INFO: Epoch: 10/10, Batch: 20/29, Batch_Loss_Train: 1.593
2024-06-21 15:43:42,043 - INFO: Epoch: 10/10, Batch: 21/29, Batch_Loss_Train: 2.245
2024-06-21 15:43:42,350 - INFO: Epoch: 10/10, Batch: 22/29, Batch_Loss_Train: 3.222
2024-06-21 15:43:42,741 - INFO: Epoch: 10/10, Batch: 23/29, Batch_Loss_Train: 1.856
2024-06-21 15:43:43,062 - INFO: Epoch: 10/10, Batch: 24/29, Batch_Loss_Train: 1.890
2024-06-21 15:43:43,483 - INFO: Epoch: 10/10, Batch: 25/29, Batch_Loss_Train: 1.709
2024-06-21 15:43:43,783 - INFO: Epoch: 10/10, Batch: 26/29, Batch_Loss_Train: 1.937
2024-06-21 15:43:44,169 - INFO: Epoch: 10/10, Batch: 27/29, Batch_Loss_Train: 2.230
2024-06-21 15:43:44,482 - INFO: Epoch: 10/10, Batch: 28/29, Batch_Loss_Train: 2.099
2024-06-21 15:43:44,702 - INFO: Epoch: 10/10, Batch: 29/29, Batch_Loss_Train: 1.816
2024-06-21 15:43:55,959 - INFO: 10/10 final results:
2024-06-21 15:43:55,959 - INFO: Training loss: 2.063.
2024-06-21 15:43:55,959 - INFO: Training MAE: 2.068.
2024-06-21 15:43:55,959 - INFO: Training MSE: 7.648.
2024-06-21 15:44:16,901 - INFO: Epoch: 10/10, Loss_train: 2.062624894339463, Loss_val: 2.5586065588326288
2024-06-21 15:44:16,901 - INFO: Best internal validation val_loss: 2.152 at epoch: 9.
